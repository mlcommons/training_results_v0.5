Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3

:::MLPv0.5.0 ssd 1541710820.859152555 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710820.881536007 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710820.893475294 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710820.907882929 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710820.914721012 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710820.915311575 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710820.932216883 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710821.016051769 (<string>:1) run_clear_caches
Launching on node sc-sdgx-385
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-394
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-396
++ eval echo srun -N 1 -n 1 -w '$hostn'
+ pids+=($!)
+++ echo srun -N 1 -n 1 -w sc-sdgx-385
+ set +x
Launching on node sc-sdgx-398
+ srun -N 1 -n 1 -w sc-sdgx-385 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=0 --master_addr=172.22.0.186 --master_port=4242' -e SLURM_JOB_ID=155374 -e SLURM_NTASKS_PER_NODE=8 cont_155374 ./run_and_time.sh
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-394
Launching on node sc-sdgx-735
+ pids+=($!)
+ srun -N 1 -n 1 -w sc-sdgx-394 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=1 --master_addr=172.22.0.186 --master_port=4242' -e SLURM_JOB_ID=155374 -e SLURM_NTASKS_PER_NODE=8 cont_155374 ./run_and_time.sh
+ set +x
Launching on node sc-sdgx-736
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-396
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-793
+ srun -N 1 -n 1 -w sc-sdgx-396 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=2 --master_addr=172.22.0.186 --master_port=4242' -e SLURM_JOB_ID=155374 -e SLURM_NTASKS_PER_NODE=8 cont_155374 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-398
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-799
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-735
+ pids+=($!)
+ srun -N 1 -n 1 -w sc-sdgx-398 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=3 --master_addr=172.22.0.186 --master_port=4242' -e SLURM_JOB_ID=155374 -e SLURM_NTASKS_PER_NODE=8 cont_155374 ./run_and_time.sh
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-736
+ srun -N 1 -n 1 -w sc-sdgx-735 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=4 --master_addr=172.22.0.186 --master_port=4242' -e SLURM_JOB_ID=155374 -e SLURM_NTASKS_PER_NODE=8 cont_155374 ./run_and_time.sh
+ srun -N 1 -n 1 -w sc-sdgx-736 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=5 --master_addr=172.22.0.186 --master_port=4242' -e SLURM_JOB_ID=155374 -e SLURM_NTASKS_PER_NODE=8 cont_155374 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-793
+ srun -N 1 -n 1 -w sc-sdgx-793 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=6 --master_addr=172.22.0.186 --master_port=4242' -e SLURM_JOB_ID=155374 -e SLURM_NTASKS_PER_NODE=8 cont_155374 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-799
+ srun -N 1 -n 1 -w sc-sdgx-799 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=7 --master_addr=172.22.0.186 --master_port=4242' -e SLURM_JOB_ID=155374 -e SLURM_NTASKS_PER_NODE=8 cont_155374 ./run_and_time.sh
Run vars: id 155374 gpus 8 mparams  --nnodes=8 --node_rank=0 --master_addr=172.22.0.186 --master_port=4242
Run vars: id 155374 gpus 8 mparams  --nnodes=8 --node_rank=5 --master_addr=172.22.0.186 --master_port=4242
Run vars: id 155374 gpus 8 mparams  --nnodes=8 --node_rank=3 --master_addr=172.22.0.186 --master_port=4242
Run vars: id 155374 gpus 8 mparams  --nnodes=8 --node_rank=7 --master_addr=172.22.0.186 --master_port=4242
Run vars: id 155374 gpus 8 mparams  --nnodes=8 --node_rank=6 --master_addr=172.22.0.186 --master_port=4242
Run vars: id 155374 gpus 8 mparams  --nnodes=8 --node_rank=2 --master_addr=172.22.0.186 --master_port=4242
STARTING TIMING RUN AT 2018-11-08 09:00:21 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=0 --master_addr=172.22.0.186 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
Run vars: id 155374 gpus 8 mparams  --nnodes=8 --node_rank=4 --master_addr=172.22.0.186 --master_port=4242
STARTING TIMING RUN AT 2018-11-08 09:00:21 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=5 --master_addr=172.22.0.186 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
Run vars: id 155374 gpus 8 mparams  --nnodes=8 --node_rank=1 --master_addr=172.22.0.186 --master_port=4242
STARTING TIMING RUN AT 2018-11-08 09:00:21 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=3 --master_addr=172.22.0.186 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:21 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=7 --master_addr=172.22.0.186 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:21 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=6 --master_addr=172.22.0.186 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:21 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=2 --master_addr=172.22.0.186 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:21 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=4 --master_addr=172.22.0.186 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:21 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=1 --master_addr=172.22.0.186 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
0 Using seed = 1708058466
1 Using seed = 1708058467
3 Using seed = 1708058469
5 Using seed = 1708058471
7 Using seed = 1708058473
4 Using seed = 1708058470
14 Using seed = 1708058480
13 Using seed = 1708058479
15 Using seed = 1708058481
12 Using seed = 1708058478
8 Using seed = 1708058474
9 Using seed = 1708058475
10 Using seed = 1708058476
11 Using seed = 1708058477
22 Using seed = 1708058488
21 Using seed = 1708058487
23 Using seed = 1708058489
20 Using seed = 1708058486
18 Using seed = 1708058484
17 Using seed = 1708058483
19 Using seed = 1708058485
16 Using seed = 1708058482
25 Using seed = 1708058491
26 Using seed = 1708058492
27 Using seed = 1708058493
24 Using seed = 1708058490
31 Using seed = 1708058497
30 Using seed = 1708058496
29 Using seed = 1708058495
28 Using seed = 1708058494
38 Using seed = 1708058504
37 Using seed = 1708058503
39 Using seed = 1708058505
36 Using seed = 1708058502
34 Using seed = 1708058500
33 Using seed = 1708058499
35 Using seed = 1708058501
32 Using seed = 1708058498
46 Using seed = 1708058512
45 Using seed = 1708058511
47 Using seed = 1708058513
42 Using seed = 1708058508
41 Using seed = 1708058507
43 Using seed = 1708058509
40 Using seed = 1708058506
44 Using seed = 1708058510
48 Using seed = 1708058514
51 Using seed = 1708058517
50 Using seed = 1708058516
49 Using seed = 1708058515
53 Using seed = 1708058519
55 Using seed = 1708058521
54 Using seed = 1708058520
52 Using seed = 1708058518
57 Using seed = 1708058523
56 Using seed = 1708058522
58 Using seed = 1708058524
59 Using seed = 1708058525
61 Using seed = 1708058527
62 Using seed = 1708058528
63 Using seed = 1708058529
60 Using seed = 1708058526
6 Using seed = 1708058472
2 Using seed = 1708058468

:::MLPv0.5.0 ssd 1541710832.187837362 (train.py:371) run_start

:::MLPv0.5.0 ssd 1541710832.190403938 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1541710832.208630800 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1541710832.222324133 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1541710832.223124981 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1541710832.278279066 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1541710832.287157774 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1541710832.302424431 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1541710832.305078745 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1541710832.311293602 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1541710832.326112747 (train.py:382) input_size: 300
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
index created!
index created!
index created!
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
Done (t=0.51s)
Done (t=0.51s)
creating index...
creating index...
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
Done (t=0.51s)
creating index...
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.52s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
time_check a: 1541710833.261118650
time_check a: 1541710833.276374817
time_check a: 1541710833.311521769
time_check a: 1541710833.315665007
time_check a: 1541710833.316235065
time_check a: 1541710833.316679955
time_check a: 1541710833.321449757
time_check a: 1541710833.322906256
time_check b: 1541710856.830926657
time_check b: 1541710856.873915672
time_check b: 1541710856.881235600
time_check b: 1541710856.882091284
time_check b: 1541710856.957944870
time_check b: 1541710857.032249451
time_check b: 1541710857.137853384
time_check b: 1541710857.512175560

:::MLPv0.5.0 ssd 1541710859.147871256 (train.py:413) input_order

:::MLPv0.5.0 ssd 1541710859.154598236 (train.py:414) input_batch_size: 32

:::MLPv0.5.0 ssd 1541710863.234855652 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710863.235688448 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710863.265019655 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1541710864.222569227 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1541710864.223392248 (train.py:477) opt_learning_rate: 0.16

:::MLPv0.5.0 ssd 1541710864.224117756 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1541710864.224821091 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1541710864.225601673 (train.py:483) opt_learning_rate_warmup_steps: 900

:::MLPv0.5.0 ssd 1541710868.290005684 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710868.290829659 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710868.320220470 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss

:::MLPv0.5.0 ssd 1541710871.172556639 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1541710871.173351526 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1541710871.176412106 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 22.612, Average Loss: 0.023, avg. samples / sec: 42443.32
Iteration:      0, Loss function: 23.128, Average Loss: 0.023, avg. samples / sec: 15476.61
Iteration:      0, Loss function: 23.062, Average Loss: 0.023, avg. samples / sec: 17830.13
Iteration:      0, Loss function: 22.658, Average Loss: 0.023, avg. samples / sec: 17090.72
Iteration:      0, Loss function: 22.633, Average Loss: 0.023, avg. samples / sec: 22979.84
Iteration:      0, Loss function: 23.063, Average Loss: 0.023, avg. samples / sec: 13932.26
Iteration:      0, Loss function: 22.164, Average Loss: 0.022, avg. samples / sec: 8592.54
Iteration:      0, Loss function: 22.161, Average Loss: 0.022, avg. samples / sec: 10317.82

:::MLPv0.5.0 ssd 1541710873.080471754 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.0001777777777777767}

:::MLPv0.5.0 ssd 1541710873.318882227 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0003555555555555534}

:::MLPv0.5.0 ssd 1541710873.426670074 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0005333333333333301}

:::MLPv0.5.0 ssd 1541710873.559102774 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0007111111111111068}

:::MLPv0.5.0 ssd 1541710873.664821386 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0008888888888888835}

:::MLPv0.5.0 ssd 1541710873.768018961 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0010666666666666602}

:::MLPv0.5.0 ssd 1541710873.872883558 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.001244444444444437}

:::MLPv0.5.0 ssd 1541710873.978029728 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0014222222222222136}

:::MLPv0.5.0 ssd 1541710874.078537464 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0015999999999999903}

:::MLPv0.5.0 ssd 1541710874.178835869 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.001777777777777767}

:::MLPv0.5.0 ssd 1541710874.289787292 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0019555555555555437}

:::MLPv0.5.0 ssd 1541710874.403770685 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.0021333333333333204}

:::MLPv0.5.0 ssd 1541710874.508533955 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.002311111111111097}

:::MLPv0.5.0 ssd 1541710874.610107183 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.002488888888888874}

:::MLPv0.5.0 ssd 1541710874.713818312 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.0026666666666666505}

:::MLPv0.5.0 ssd 1541710874.817584276 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.0028444444444444272}

:::MLPv0.5.0 ssd 1541710874.919323206 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.0030222222222222317}

:::MLPv0.5.0 ssd 1541710875.025265932 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.0032000000000000084}

:::MLPv0.5.0 ssd 1541710875.126747847 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.003377777777777785}

:::MLPv0.5.0 ssd 1541710875.226065636 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.003555555555555562}
Iteration:     20, Loss function: 20.490, Average Loss: 0.443, avg. samples / sec: 10155.30
Iteration:     20, Loss function: 19.665, Average Loss: 0.441, avg. samples / sec: 10148.23
Iteration:     20, Loss function: 21.378, Average Loss: 0.445, avg. samples / sec: 10147.75
Iteration:     20, Loss function: 20.749, Average Loss: 0.441, avg. samples / sec: 10153.89
Iteration:     20, Loss function: 20.594, Average Loss: 0.445, avg. samples / sec: 10152.79
Iteration:     20, Loss function: 20.949, Average Loss: 0.441, avg. samples / sec: 10149.53
Iteration:     20, Loss function: 20.654, Average Loss: 0.441, avg. samples / sec: 10168.25
Iteration:     20, Loss function: 20.652, Average Loss: 0.444, avg. samples / sec: 10140.72

:::MLPv0.5.0 ssd 1541710875.328172445 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.0037333333333333385}

:::MLPv0.5.0 ssd 1541710875.426915646 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.003911111111111115}

:::MLPv0.5.0 ssd 1541710875.530842066 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.004088888888888892}

:::MLPv0.5.0 ssd 1541710875.629420519 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.004266666666666669}

:::MLPv0.5.0 ssd 1541710875.731584311 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.004444444444444445}

:::MLPv0.5.0 ssd 1541710875.836975813 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.004622222222222222}

:::MLPv0.5.0 ssd 1541710875.937309027 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.004799999999999999}

:::MLPv0.5.0 ssd 1541710876.036531925 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.004977777777777775}

:::MLPv0.5.0 ssd 1541710876.137332916 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.005155555555555552}

:::MLPv0.5.0 ssd 1541710876.244815350 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.005333333333333329}

:::MLPv0.5.0 ssd 1541710876.349859238 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.0055111111111111055}

:::MLPv0.5.0 ssd 1541710876.450149059 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.005688888888888882}

:::MLPv0.5.0 ssd 1541710876.551028252 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.005866666666666659}

:::MLPv0.5.0 ssd 1541710876.647916317 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.006044444444444436}

:::MLPv0.5.0 ssd 1541710876.751241446 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.006222222222222212}

:::MLPv0.5.0 ssd 1541710876.848314762 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.006399999999999989}

:::MLPv0.5.0 ssd 1541710876.952795982 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.006577777777777766}

:::MLPv0.5.0 ssd 1541710877.051248312 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.0067555555555555424}

:::MLPv0.5.0 ssd 1541710877.150242805 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.006933333333333319}

:::MLPv0.5.0 ssd 1541710877.246571541 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.007111111111111096}
Iteration:     40, Loss function: 16.102, Average Loss: 0.816, avg. samples / sec: 20271.94
Iteration:     40, Loss function: 15.868, Average Loss: 0.818, avg. samples / sec: 20271.08
Iteration:     40, Loss function: 15.847, Average Loss: 0.820, avg. samples / sec: 20274.82
Iteration:     40, Loss function: 16.427, Average Loss: 0.814, avg. samples / sec: 20309.47
Iteration:     40, Loss function: 16.012, Average Loss: 0.817, avg. samples / sec: 20273.44
Iteration:     40, Loss function: 16.024, Average Loss: 0.812, avg. samples / sec: 20283.30
Iteration:     40, Loss function: 16.487, Average Loss: 0.813, avg. samples / sec: 20263.73
Iteration:     40, Loss function: 16.215, Average Loss: 0.813, avg. samples / sec: 20237.92

:::MLPv0.5.0 ssd 1541710877.358095169 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.0072888888888888725}

:::MLPv0.5.0 ssd 1541710877.459402561 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.007466666666666649}

:::MLPv0.5.0 ssd 1541710877.558500767 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.007644444444444454}

:::MLPv0.5.0 ssd 1541710877.657541513 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.00782222222222223}

:::MLPv0.5.0 ssd 1541710877.758239746 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.008000000000000007}

:::MLPv0.5.0 ssd 1541710877.854957104 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.008177777777777784}

:::MLPv0.5.0 ssd 1541710877.957423687 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.00835555555555556}

:::MLPv0.5.0 ssd 1541710878.052692175 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.008533333333333337}

:::MLPv0.5.0 ssd 1541710878.155803919 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.008711111111111114}

:::MLPv0.5.0 ssd 1541710878.254318237 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.00888888888888889}

:::MLPv0.5.0 ssd 1541710878.351430655 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.009066666666666667}

:::MLPv0.5.0 ssd 1541710878.448889017 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.009244444444444444}

:::MLPv0.5.0 ssd 1541710878.546493292 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.00942222222222222}

:::MLPv0.5.0 ssd 1541710878.647305012 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.009599999999999997}

:::MLPv0.5.0 ssd 1541710878.746915817 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.009777777777777774}

:::MLPv0.5.0 ssd 1541710878.846442699 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.00995555555555555}

:::MLPv0.5.0 ssd 1541710878.944311142 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.010133333333333328}

:::MLPv0.5.0 ssd 1541710879.036180019 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1541710879.041635752 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 58, "value": 0.010311111111111104}

:::MLPv0.5.0 ssd 1541710879.159724712 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 59, "value": 0.010488888888888881}

:::MLPv0.5.0 ssd 1541710879.256222486 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 60, "value": 0.010666666666666658}
Iteration:     60, Loss function: 11.257, Average Loss: 1.053, avg. samples / sec: 20384.49
Iteration:     60, Loss function: 11.695, Average Loss: 1.049, avg. samples / sec: 20401.87
Iteration:     60, Loss function: 11.731, Average Loss: 1.056, avg. samples / sec: 20383.47
Iteration:     60, Loss function: 12.046, Average Loss: 1.047, avg. samples / sec: 20425.27
Iteration:     60, Loss function: 11.865, Average Loss: 1.050, avg. samples / sec: 20384.04
Iteration:     60, Loss function: 11.424, Average Loss: 1.052, avg. samples / sec: 20378.87
Iteration:     60, Loss function: 11.974, Average Loss: 1.048, avg. samples / sec: 20378.71
Iteration:     60, Loss function: 11.248, Average Loss: 1.055, avg. samples / sec: 20336.80

:::MLPv0.5.0 ssd 1541710879.352892876 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 61, "value": 0.010844444444444434}

:::MLPv0.5.0 ssd 1541710879.449545622 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 62, "value": 0.011022222222222211}

:::MLPv0.5.0 ssd 1541710879.547876596 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 63, "value": 0.011199999999999988}

:::MLPv0.5.0 ssd 1541710879.644847155 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 64, "value": 0.011377777777777764}

:::MLPv0.5.0 ssd 1541710879.740506172 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 65, "value": 0.011555555555555541}

:::MLPv0.5.0 ssd 1541710879.836874723 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 66, "value": 0.011733333333333318}

:::MLPv0.5.0 ssd 1541710879.935196400 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 67, "value": 0.011911111111111095}

:::MLPv0.5.0 ssd 1541710880.030584812 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 68, "value": 0.012088888888888899}

:::MLPv0.5.0 ssd 1541710880.133177280 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 69, "value": 0.012266666666666676}

:::MLPv0.5.0 ssd 1541710880.229849577 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 70, "value": 0.012444444444444452}

:::MLPv0.5.0 ssd 1541710880.327116966 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 71, "value": 0.012622222222222229}

:::MLPv0.5.0 ssd 1541710880.423710108 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 72, "value": 0.012800000000000006}

:::MLPv0.5.0 ssd 1541710880.523920059 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 73, "value": 0.012977777777777783}

:::MLPv0.5.0 ssd 1541710880.627655268 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 74, "value": 0.01315555555555556}

:::MLPv0.5.0 ssd 1541710880.724721193 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 75, "value": 0.013333333333333336}

:::MLPv0.5.0 ssd 1541710880.823088646 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 76, "value": 0.013511111111111113}

:::MLPv0.5.0 ssd 1541710880.920363188 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 77, "value": 0.01368888888888889}

:::MLPv0.5.0 ssd 1541710881.016444206 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 78, "value": 0.013866666666666666}

:::MLPv0.5.0 ssd 1541710881.115791321 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 79, "value": 0.014044444444444443}

:::MLPv0.5.0 ssd 1541710881.214990616 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 80, "value": 0.01422222222222222}
Iteration:     80, Loss function: 10.796, Average Loss: 1.256, avg. samples / sec: 20960.99
Iteration:     80, Loss function: 10.759, Average Loss: 1.256, avg. samples / sec: 20910.20
Iteration:     80, Loss function: 11.008, Average Loss: 1.249, avg. samples / sec: 20902.86
Iteration:     80, Loss function: 10.715, Average Loss: 1.243, avg. samples / sec: 20914.08
Iteration:     80, Loss function: 10.583, Average Loss: 1.254, avg. samples / sec: 20910.18
Iteration:     80, Loss function: 10.645, Average Loss: 1.252, avg. samples / sec: 20901.45
Iteration:     80, Loss function: 10.326, Average Loss: 1.254, avg. samples / sec: 20884.87
Iteration:     80, Loss function: 10.447, Average Loss: 1.247, avg. samples / sec: 20895.10

:::MLPv0.5.0 ssd 1541710881.313388586 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 81, "value": 0.014399999999999996}

:::MLPv0.5.0 ssd 1541710881.412163019 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 82, "value": 0.014577777777777773}

:::MLPv0.5.0 ssd 1541710881.507296801 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 83, "value": 0.01475555555555555}

:::MLPv0.5.0 ssd 1541710881.607998371 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 84, "value": 0.014933333333333326}

:::MLPv0.5.0 ssd 1541710881.707970858 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 85, "value": 0.015111111111111103}

:::MLPv0.5.0 ssd 1541710881.805980206 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 86, "value": 0.01528888888888888}

:::MLPv0.5.0 ssd 1541710881.903016329 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 87, "value": 0.015466666666666656}

:::MLPv0.5.0 ssd 1541710882.000830412 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 88, "value": 0.015644444444444433}

:::MLPv0.5.0 ssd 1541710882.097169161 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 89, "value": 0.01582222222222221}

:::MLPv0.5.0 ssd 1541710882.194082975 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 90, "value": 0.015999999999999986}

:::MLPv0.5.0 ssd 1541710882.290839434 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 91, "value": 0.016177777777777763}

:::MLPv0.5.0 ssd 1541710882.388579369 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 92, "value": 0.01635555555555554}

:::MLPv0.5.0 ssd 1541710882.485795021 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 93, "value": 0.016533333333333317}

:::MLPv0.5.0 ssd 1541710882.589364529 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 94, "value": 0.01671111111111112}

:::MLPv0.5.0 ssd 1541710882.687072515 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 95, "value": 0.016888888888888898}

:::MLPv0.5.0 ssd 1541710882.784350395 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 96, "value": 0.017066666666666674}

:::MLPv0.5.0 ssd 1541710882.881447315 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 97, "value": 0.01724444444444445}

:::MLPv0.5.0 ssd 1541710882.978585243 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 98, "value": 0.017422222222222228}

:::MLPv0.5.0 ssd 1541710883.083110809 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 99, "value": 0.017600000000000005}

:::MLPv0.5.0 ssd 1541710883.190364838 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 100, "value": 0.01777777777777778}
Iteration:    100, Loss function: 9.208, Average Loss: 1.428, avg. samples / sec: 20732.43
Iteration:    100, Loss function: 9.598, Average Loss: 1.418, avg. samples / sec: 20725.03
Iteration:    100, Loss function: 9.303, Average Loss: 1.414, avg. samples / sec: 20727.56
Iteration:    100, Loss function: 9.689, Average Loss: 1.419, avg. samples / sec: 20734.37
Iteration:    100, Loss function: 9.822, Average Loss: 1.427, avg. samples / sec: 20724.40
Iteration:    100, Loss function: 9.618, Average Loss: 1.424, avg. samples / sec: 20732.96
Iteration:    100, Loss function: 9.447, Average Loss: 1.423, avg. samples / sec: 20724.16
Iteration:    100, Loss function: 9.154, Average Loss: 1.427, avg. samples / sec: 20674.69

:::MLPv0.5.0 ssd 1541710883.286360502 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 101, "value": 0.017955555555555558}

:::MLPv0.5.0 ssd 1541710883.390263557 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 102, "value": 0.018133333333333335}

:::MLPv0.5.0 ssd 1541710883.494356632 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 103, "value": 0.01831111111111111}

:::MLPv0.5.0 ssd 1541710883.602382898 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 104, "value": 0.018488888888888888}

:::MLPv0.5.0 ssd 1541710883.698791027 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 105, "value": 0.018666666666666665}

:::MLPv0.5.0 ssd 1541710883.795687199 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 106, "value": 0.01884444444444444}

:::MLPv0.5.0 ssd 1541710883.896956444 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 107, "value": 0.019022222222222218}

:::MLPv0.5.0 ssd 1541710883.992627859 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 108, "value": 0.019199999999999995}

:::MLPv0.5.0 ssd 1541710884.091519117 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 109, "value": 0.01937777777777777}

:::MLPv0.5.0 ssd 1541710884.188916922 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 110, "value": 0.019555555555555548}

:::MLPv0.5.0 ssd 1541710884.285423756 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 111, "value": 0.019733333333333325}

:::MLPv0.5.0 ssd 1541710884.381734133 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 112, "value": 0.0199111111111111}

:::MLPv0.5.0 ssd 1541710884.479012251 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 113, "value": 0.02008888888888888}

:::MLPv0.5.0 ssd 1541710884.574417353 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 114, "value": 0.020266666666666655}

:::MLPv0.5.0 ssd 1541710884.671210289 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 115, "value": 0.020444444444444432}

:::MLPv0.5.0 ssd 1541710884.764321804 (train.py:553) train_epoch: 2

:::MLPv0.5.0 ssd 1541710884.769358397 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 116, "value": 0.02062222222222221}

:::MLPv0.5.0 ssd 1541710884.866833210 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 117, "value": 0.020799999999999985}

:::MLPv0.5.0 ssd 1541710884.963244438 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 118, "value": 0.020977777777777762}

:::MLPv0.5.0 ssd 1541710885.058517456 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 119, "value": 0.02115555555555554}

:::MLPv0.5.0 ssd 1541710885.154540539 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 120, "value": 0.021333333333333343}
Iteration:    120, Loss function: 8.750, Average Loss: 1.578, avg. samples / sec: 20922.86
Iteration:    120, Loss function: 8.455, Average Loss: 1.575, avg. samples / sec: 20878.10
Iteration:    120, Loss function: 8.834, Average Loss: 1.573, avg. samples / sec: 20871.47
Iteration:    120, Loss function: 8.908, Average Loss: 1.573, avg. samples / sec: 20864.87
Iteration:    120, Loss function: 8.599, Average Loss: 1.578, avg. samples / sec: 20848.31
Iteration:    120, Loss function: 8.785, Average Loss: 1.579, avg. samples / sec: 20868.11
Iteration:    120, Loss function: 8.145, Average Loss: 1.566, avg. samples / sec: 20860.46
Iteration:    120, Loss function: 8.611, Average Loss: 1.577, avg. samples / sec: 20839.58

:::MLPv0.5.0 ssd 1541710885.252475023 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 121, "value": 0.02151111111111112}

:::MLPv0.5.0 ssd 1541710885.349841595 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 122, "value": 0.021688888888888896}

:::MLPv0.5.0 ssd 1541710885.448161602 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 123, "value": 0.021866666666666673}

:::MLPv0.5.0 ssd 1541710885.544614077 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 124, "value": 0.02204444444444445}

:::MLPv0.5.0 ssd 1541710885.639635801 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 125, "value": 0.022222222222222227}

:::MLPv0.5.0 ssd 1541710885.744055510 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 126, "value": 0.022400000000000003}

:::MLPv0.5.0 ssd 1541710885.838064671 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 127, "value": 0.02257777777777778}

:::MLPv0.5.0 ssd 1541710885.939336300 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 128, "value": 0.022755555555555557}

:::MLPv0.5.0 ssd 1541710886.034631968 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 129, "value": 0.022933333333333333}

:::MLPv0.5.0 ssd 1541710886.129797459 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 130, "value": 0.02311111111111111}

:::MLPv0.5.0 ssd 1541710886.225259304 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 131, "value": 0.023288888888888887}

:::MLPv0.5.0 ssd 1541710886.324941397 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 132, "value": 0.023466666666666663}

:::MLPv0.5.0 ssd 1541710886.423653126 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 133, "value": 0.02364444444444444}

:::MLPv0.5.0 ssd 1541710886.518289566 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 134, "value": 0.023822222222222217}

:::MLPv0.5.0 ssd 1541710886.615487576 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 135, "value": 0.023999999999999994}

:::MLPv0.5.0 ssd 1541710886.711401701 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 136, "value": 0.02417777777777777}

:::MLPv0.5.0 ssd 1541710886.808114290 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 137, "value": 0.024355555555555547}

:::MLPv0.5.0 ssd 1541710886.906293869 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 138, "value": 0.024533333333333324}

:::MLPv0.5.0 ssd 1541710887.002891541 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 139, "value": 0.0247111111111111}

:::MLPv0.5.0 ssd 1541710887.099440575 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 140, "value": 0.024888888888888877}
Iteration:    140, Loss function: 8.419, Average Loss: 1.718, avg. samples / sec: 21064.29
Iteration:    140, Loss function: 8.166, Average Loss: 1.715, avg. samples / sec: 21102.84
Iteration:    140, Loss function: 8.452, Average Loss: 1.712, avg. samples / sec: 21066.85
Iteration:    140, Loss function: 8.921, Average Loss: 1.721, avg. samples / sec: 21070.46
Iteration:    140, Loss function: 8.760, Average Loss: 1.714, avg. samples / sec: 21063.42
Iteration:    140, Loss function: 8.761, Average Loss: 1.717, avg. samples / sec: 21063.50
Iteration:    140, Loss function: 8.374, Average Loss: 1.709, avg. samples / sec: 21069.33
Iteration:    140, Loss function: 8.182, Average Loss: 1.719, avg. samples / sec: 21065.73

:::MLPv0.5.0 ssd 1541710887.195484638 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 141, "value": 0.025066666666666654}

:::MLPv0.5.0 ssd 1541710887.295956850 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 142, "value": 0.02524444444444443}

:::MLPv0.5.0 ssd 1541710887.391503334 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 143, "value": 0.025422222222222207}

:::MLPv0.5.0 ssd 1541710887.486902237 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 144, "value": 0.025599999999999984}

:::MLPv0.5.0 ssd 1541710887.586052418 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 145, "value": 0.02577777777777779}

:::MLPv0.5.0 ssd 1541710887.681776285 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 146, "value": 0.025955555555555565}

:::MLPv0.5.0 ssd 1541710887.783276081 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 147, "value": 0.026133333333333342}

:::MLPv0.5.0 ssd 1541710887.879973888 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 148, "value": 0.02631111111111112}

:::MLPv0.5.0 ssd 1541710887.976186275 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 149, "value": 0.026488888888888895}

:::MLPv0.5.0 ssd 1541710888.074182749 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 150, "value": 0.026666666666666672}

:::MLPv0.5.0 ssd 1541710888.172930956 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 151, "value": 0.02684444444444445}

:::MLPv0.5.0 ssd 1541710888.272943258 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 152, "value": 0.027022222222222225}

:::MLPv0.5.0 ssd 1541710888.370079517 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 153, "value": 0.027200000000000002}

:::MLPv0.5.0 ssd 1541710888.465760708 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 154, "value": 0.02737777777777778}

:::MLPv0.5.0 ssd 1541710888.564606667 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 155, "value": 0.027555555555555555}

:::MLPv0.5.0 ssd 1541710888.663590908 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 156, "value": 0.027733333333333332}

:::MLPv0.5.0 ssd 1541710888.759355545 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 157, "value": 0.02791111111111111}

:::MLPv0.5.0 ssd 1541710888.853798866 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 158, "value": 0.028088888888888885}

:::MLPv0.5.0 ssd 1541710888.952383518 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 159, "value": 0.028266666666666662}

:::MLPv0.5.0 ssd 1541710889.048139334 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 160, "value": 0.02844444444444444}
Iteration:    160, Loss function: 8.245, Average Loss: 1.848, avg. samples / sec: 21029.21
Iteration:    160, Loss function: 8.185, Average Loss: 1.844, avg. samples / sec: 21022.86
Iteration:    160, Loss function: 8.479, Average Loss: 1.851, avg. samples / sec: 21019.65
Iteration:    160, Loss function: 8.271, Average Loss: 1.844, avg. samples / sec: 21015.06
Iteration:    160, Loss function: 8.025, Average Loss: 1.850, avg. samples / sec: 21009.05
Iteration:    160, Loss function: 8.332, Average Loss: 1.846, avg. samples / sec: 21011.65
Iteration:    160, Loss function: 7.988, Average Loss: 1.852, avg. samples / sec: 21017.78
Iteration:    160, Loss function: 8.359, Average Loss: 1.841, avg. samples / sec: 20983.90

:::MLPv0.5.0 ssd 1541710889.146050453 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 161, "value": 0.028622222222222216}

:::MLPv0.5.0 ssd 1541710889.242965221 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 162, "value": 0.028799999999999992}

:::MLPv0.5.0 ssd 1541710889.338016033 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 163, "value": 0.02897777777777777}

:::MLPv0.5.0 ssd 1541710889.435947180 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 164, "value": 0.029155555555555546}

:::MLPv0.5.0 ssd 1541710889.534920931 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 165, "value": 0.029333333333333322}

:::MLPv0.5.0 ssd 1541710889.632689476 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 166, "value": 0.0295111111111111}

:::MLPv0.5.0 ssd 1541710889.729173183 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 167, "value": 0.029688888888888876}

:::MLPv0.5.0 ssd 1541710889.823590994 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 168, "value": 0.029866666666666652}

:::MLPv0.5.0 ssd 1541710889.921705008 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 169, "value": 0.03004444444444443}

:::MLPv0.5.0 ssd 1541710890.017779827 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 170, "value": 0.030222222222222206}

:::MLPv0.5.0 ssd 1541710890.142655849 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 171, "value": 0.03040000000000001}

:::MLPv0.5.0 ssd 1541710890.240982056 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 172, "value": 0.030577777777777787}

:::MLPv0.5.0 ssd 1541710890.341374874 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 173, "value": 0.030755555555555564}

:::MLPv0.5.0 ssd 1541710890.432169437 (train.py:553) train_epoch: 3

:::MLPv0.5.0 ssd 1541710890.437984228 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 174, "value": 0.03093333333333334}

:::MLPv0.5.0 ssd 1541710890.533803701 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 175, "value": 0.031111111111111117}

:::MLPv0.5.0 ssd 1541710890.630145788 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 176, "value": 0.031288888888888894}

:::MLPv0.5.0 ssd 1541710890.731047630 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 177, "value": 0.03146666666666667}

:::MLPv0.5.0 ssd 1541710890.825695992 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 178, "value": 0.03164444444444445}

:::MLPv0.5.0 ssd 1541710890.921092272 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 179, "value": 0.031822222222222224}

:::MLPv0.5.0 ssd 1541710891.015823364 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 180, "value": 0.032}
Iteration:    180, Loss function: 8.155, Average Loss: 1.975, avg. samples / sec: 20817.72
Iteration:    180, Loss function: 8.306, Average Loss: 1.976, avg. samples / sec: 20831.46
Iteration:    180, Loss function: 7.917, Average Loss: 1.971, avg. samples / sec: 20823.75
Iteration:    180, Loss function: 7.887, Average Loss: 1.970, avg. samples / sec: 20855.96
Iteration:    180, Loss function: 8.142, Average Loss: 1.981, avg. samples / sec: 20815.60
Iteration:    180, Loss function: 8.430, Average Loss: 1.972, avg. samples / sec: 20816.99
Iteration:    180, Loss function: 8.525, Average Loss: 1.981, avg. samples / sec: 20818.30
Iteration:    180, Loss function: 8.056, Average Loss: 1.979, avg. samples / sec: 20814.01

:::MLPv0.5.0 ssd 1541710891.115186691 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 181, "value": 0.03217777777777778}

:::MLPv0.5.0 ssd 1541710891.211167812 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 182, "value": 0.032355555555555554}

:::MLPv0.5.0 ssd 1541710891.307946920 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 183, "value": 0.03253333333333333}

:::MLPv0.5.0 ssd 1541710891.402463675 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 184, "value": 0.03271111111111111}

:::MLPv0.5.0 ssd 1541710891.498265028 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 185, "value": 0.032888888888888884}

:::MLPv0.5.0 ssd 1541710891.595484972 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 186, "value": 0.03306666666666666}

:::MLPv0.5.0 ssd 1541710891.692604065 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 187, "value": 0.03324444444444444}

:::MLPv0.5.0 ssd 1541710891.797145844 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 188, "value": 0.033422222222222214}

:::MLPv0.5.0 ssd 1541710891.892363071 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 189, "value": 0.03359999999999999}

:::MLPv0.5.0 ssd 1541710891.987156391 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 190, "value": 0.03377777777777777}

:::MLPv0.5.0 ssd 1541710892.083529234 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 191, "value": 0.033955555555555544}

:::MLPv0.5.0 ssd 1541710892.178538799 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 192, "value": 0.03413333333333332}

:::MLPv0.5.0 ssd 1541710892.274546862 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 193, "value": 0.0343111111111111}

:::MLPv0.5.0 ssd 1541710892.373194218 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 194, "value": 0.034488888888888874}

:::MLPv0.5.0 ssd 1541710892.468974113 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 195, "value": 0.03466666666666665}

:::MLPv0.5.0 ssd 1541710892.563665152 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 196, "value": 0.03484444444444443}

:::MLPv0.5.0 ssd 1541710892.658987999 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 197, "value": 0.03502222222222222}

:::MLPv0.5.0 ssd 1541710892.753377438 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 198, "value": 0.035199999999999995}

:::MLPv0.5.0 ssd 1541710892.852276325 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 199, "value": 0.03537777777777777}

:::MLPv0.5.0 ssd 1541710892.949941397 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 200, "value": 0.03555555555555555}
Iteration:    200, Loss function: 7.977, Average Loss: 2.093, avg. samples / sec: 21173.99
Iteration:    200, Loss function: 7.344, Average Loss: 2.089, avg. samples / sec: 21181.59
Iteration:    200, Loss function: 7.746, Average Loss: 2.100, avg. samples / sec: 21182.79
Iteration:    200, Loss function: 7.863, Average Loss: 2.091, avg. samples / sec: 21182.46
Iteration:    200, Loss function: 7.815, Average Loss: 2.092, avg. samples / sec: 21170.70
Iteration:    200, Loss function: 7.750, Average Loss: 2.099, avg. samples / sec: 21173.69
Iteration:    200, Loss function: 8.062, Average Loss: 2.096, avg. samples / sec: 21156.50
Iteration:    200, Loss function: 7.844, Average Loss: 2.099, avg. samples / sec: 21156.94

:::MLPv0.5.0 ssd 1541710893.044942141 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 201, "value": 0.035733333333333325}

:::MLPv0.5.0 ssd 1541710893.139413834 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 202, "value": 0.0359111111111111}

:::MLPv0.5.0 ssd 1541710893.235248089 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 203, "value": 0.03608888888888889}

:::MLPv0.5.0 ssd 1541710893.331433773 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 204, "value": 0.03626666666666667}

:::MLPv0.5.0 ssd 1541710893.428186417 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 205, "value": 0.036444444444444446}

:::MLPv0.5.0 ssd 1541710893.523888111 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 206, "value": 0.03662222222222222}

:::MLPv0.5.0 ssd 1541710893.619062901 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 207, "value": 0.0368}

:::MLPv0.5.0 ssd 1541710893.715716362 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 208, "value": 0.036977777777777776}

:::MLPv0.5.0 ssd 1541710893.811804771 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 209, "value": 0.03715555555555555}

:::MLPv0.5.0 ssd 1541710893.908374071 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 210, "value": 0.03733333333333333}

:::MLPv0.5.0 ssd 1541710894.006623983 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 211, "value": 0.037511111111111106}

:::MLPv0.5.0 ssd 1541710894.104174137 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 212, "value": 0.03768888888888888}

:::MLPv0.5.0 ssd 1541710894.199001551 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 213, "value": 0.03786666666666666}

:::MLPv0.5.0 ssd 1541710894.294948339 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 214, "value": 0.038044444444444436}

:::MLPv0.5.0 ssd 1541710894.389820337 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 215, "value": 0.03822222222222221}

:::MLPv0.5.0 ssd 1541710894.485467196 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 216, "value": 0.038400000000000004}

:::MLPv0.5.0 ssd 1541710894.583935022 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 217, "value": 0.03857777777777778}

:::MLPv0.5.0 ssd 1541710894.679588318 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 218, "value": 0.03875555555555556}

:::MLPv0.5.0 ssd 1541710894.775432348 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 219, "value": 0.038933333333333334}

:::MLPv0.5.0 ssd 1541710894.872168779 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 220, "value": 0.03911111111111111}
Iteration:    220, Loss function: 7.542, Average Loss: 2.210, avg. samples / sec: 21320.31
Iteration:    220, Loss function: 7.412, Average Loss: 2.203, avg. samples / sec: 21307.08
Iteration:    220, Loss function: 7.769, Average Loss: 2.202, avg. samples / sec: 21311.66
Iteration:    220, Loss function: 7.650, Average Loss: 2.205, avg. samples / sec: 21323.13
Iteration:    220, Loss function: 7.606, Average Loss: 2.202, avg. samples / sec: 21305.74
Iteration:    220, Loss function: 7.599, Average Loss: 2.208, avg. samples / sec: 21301.99
Iteration:    220, Loss function: 8.192, Average Loss: 2.201, avg. samples / sec: 21299.27
Iteration:    220, Loss function: 7.637, Average Loss: 2.207, avg. samples / sec: 21333.09

:::MLPv0.5.0 ssd 1541710894.968324184 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 221, "value": 0.03928888888888889}

:::MLPv0.5.0 ssd 1541710895.064940691 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 222, "value": 0.039466666666666664}

:::MLPv0.5.0 ssd 1541710895.160679340 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 223, "value": 0.03964444444444444}

:::MLPv0.5.0 ssd 1541710895.257076740 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 224, "value": 0.03982222222222222}

:::MLPv0.5.0 ssd 1541710895.355680704 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 225, "value": 0.039999999999999994}

:::MLPv0.5.0 ssd 1541710895.456187963 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 226, "value": 0.04017777777777777}

:::MLPv0.5.0 ssd 1541710895.554286003 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 227, "value": 0.04035555555555555}

:::MLPv0.5.0 ssd 1541710895.649833679 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 228, "value": 0.04053333333333334}

:::MLPv0.5.0 ssd 1541710895.745069265 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 229, "value": 0.040711111111111115}

:::MLPv0.5.0 ssd 1541710895.842911482 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 230, "value": 0.04088888888888889}

:::MLPv0.5.0 ssd 1541710895.939636230 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 231, "value": 0.04106666666666667}

:::MLPv0.5.0 ssd 1541710896.033820629 (train.py:553) train_epoch: 4

:::MLPv0.5.0 ssd 1541710896.039323092 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 232, "value": 0.041244444444444445}

:::MLPv0.5.0 ssd 1541710896.134299994 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 233, "value": 0.04142222222222222}

:::MLPv0.5.0 ssd 1541710896.228028774 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 234, "value": 0.0416}

:::MLPv0.5.0 ssd 1541710896.326050043 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 235, "value": 0.041777777777777775}

:::MLPv0.5.0 ssd 1541710896.421129942 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 236, "value": 0.04195555555555555}

:::MLPv0.5.0 ssd 1541710896.517369032 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 237, "value": 0.04213333333333333}

:::MLPv0.5.0 ssd 1541710896.611559629 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 238, "value": 0.042311111111111105}

:::MLPv0.5.0 ssd 1541710896.707728624 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 239, "value": 0.04248888888888888}

:::MLPv0.5.0 ssd 1541710896.804108620 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 240, "value": 0.04266666666666666}
Iteration:    240, Loss function: 7.409, Average Loss: 2.312, avg. samples / sec: 21213.77
Iteration:    240, Loss function: 7.657, Average Loss: 2.313, avg. samples / sec: 21205.01
Iteration:    240, Loss function: 8.053, Average Loss: 2.313, avg. samples / sec: 21200.02
Iteration:    240, Loss function: 7.832, Average Loss: 2.315, avg. samples / sec: 21201.71
Iteration:    240, Loss function: 7.666, Average Loss: 2.320, avg. samples / sec: 21198.48
Iteration:    240, Loss function: 7.262, Average Loss: 2.318, avg. samples / sec: 21200.82
Iteration:    240, Loss function: 6.969, Average Loss: 2.319, avg. samples / sec: 21203.41
Iteration:    240, Loss function: 7.890, Average Loss: 2.313, avg. samples / sec: 21186.25

:::MLPv0.5.0 ssd 1541710896.901468754 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 241, "value": 0.04284444444444445}

:::MLPv0.5.0 ssd 1541710896.997639179 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 242, "value": 0.043022222222222226}

:::MLPv0.5.0 ssd 1541710897.094922304 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 243, "value": 0.0432}

:::MLPv0.5.0 ssd 1541710897.189873219 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 244, "value": 0.04337777777777778}

:::MLPv0.5.0 ssd 1541710897.286551476 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 245, "value": 0.043555555555555556}

:::MLPv0.5.0 ssd 1541710897.382436275 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 246, "value": 0.04373333333333333}

:::MLPv0.5.0 ssd 1541710897.478019953 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 247, "value": 0.04391111111111111}

:::MLPv0.5.0 ssd 1541710897.572254896 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 248, "value": 0.044088888888888886}

:::MLPv0.5.0 ssd 1541710897.669066429 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 249, "value": 0.04426666666666666}

:::MLPv0.5.0 ssd 1541710897.763088226 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 250, "value": 0.04444444444444444}

:::MLPv0.5.0 ssd 1541710897.858597994 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 251, "value": 0.044622222222222216}

:::MLPv0.5.0 ssd 1541710897.955209255 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 252, "value": 0.04479999999999999}

:::MLPv0.5.0 ssd 1541710898.049969435 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 253, "value": 0.04497777777777777}

:::MLPv0.5.0 ssd 1541710898.148987055 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 254, "value": 0.04515555555555556}

:::MLPv0.5.0 ssd 1541710898.245383978 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 255, "value": 0.04533333333333334}

:::MLPv0.5.0 ssd 1541710898.339800358 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 256, "value": 0.04551111111111111}

:::MLPv0.5.0 ssd 1541710898.438353300 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 257, "value": 0.04568888888888889}

:::MLPv0.5.0 ssd 1541710898.536914110 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 258, "value": 0.04586666666666667}

:::MLPv0.5.0 ssd 1541710898.630844355 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 259, "value": 0.04604444444444444}

:::MLPv0.5.0 ssd 1541710898.727108240 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 260, "value": 0.04622222222222222}
Iteration:    260, Loss function: 7.382, Average Loss: 2.414, avg. samples / sec: 21308.40
Iteration:    260, Loss function: 7.541, Average Loss: 2.418, avg. samples / sec: 21316.60
Iteration:    260, Loss function: 7.361, Average Loss: 2.418, avg. samples / sec: 21304.77
Iteration:    260, Loss function: 7.628, Average Loss: 2.411, avg. samples / sec: 21297.07
Iteration:    260, Loss function: 7.585, Average Loss: 2.413, avg. samples / sec: 21314.06
Iteration:    260, Loss function: 7.587, Average Loss: 2.413, avg. samples / sec: 21288.69
Iteration:    260, Loss function: 7.637, Average Loss: 2.415, avg. samples / sec: 21290.47
Iteration:    260, Loss function: 7.417, Average Loss: 2.417, avg. samples / sec: 21293.63

:::MLPv0.5.0 ssd 1541710898.823616505 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 261, "value": 0.0464}

:::MLPv0.5.0 ssd 1541710898.918516636 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 262, "value": 0.046577777777777774}

:::MLPv0.5.0 ssd 1541710899.013913155 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 263, "value": 0.04675555555555555}

:::MLPv0.5.0 ssd 1541710899.108112335 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 264, "value": 0.04693333333333333}

:::MLPv0.5.0 ssd 1541710899.202480316 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 265, "value": 0.047111111111111104}

:::MLPv0.5.0 ssd 1541710899.297353029 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 266, "value": 0.04728888888888888}

:::MLPv0.5.0 ssd 1541710899.393710613 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 267, "value": 0.04746666666666667}

:::MLPv0.5.0 ssd 1541710899.490061522 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 268, "value": 0.04764444444444445}

:::MLPv0.5.0 ssd 1541710899.584866524 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 269, "value": 0.047822222222222224}

:::MLPv0.5.0 ssd 1541710899.678420305 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 270, "value": 0.048}

:::MLPv0.5.0 ssd 1541710899.773008347 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 271, "value": 0.04817777777777778}

:::MLPv0.5.0 ssd 1541710899.867737770 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 272, "value": 0.048355555555555554}

:::MLPv0.5.0 ssd 1541710899.962694407 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 273, "value": 0.04853333333333333}

:::MLPv0.5.0 ssd 1541710900.057370663 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 274, "value": 0.04871111111111111}

:::MLPv0.5.0 ssd 1541710900.151594639 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 275, "value": 0.048888888888888885}

:::MLPv0.5.0 ssd 1541710900.245311260 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 276, "value": 0.04906666666666666}

:::MLPv0.5.0 ssd 1541710900.338005543 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 277, "value": 0.04924444444444444}

:::MLPv0.5.0 ssd 1541710900.432343960 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 278, "value": 0.049422222222222215}

:::MLPv0.5.0 ssd 1541710900.525703192 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 279, "value": 0.04959999999999999}

:::MLPv0.5.0 ssd 1541710900.623332500 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 280, "value": 0.04977777777777778}
Iteration:    280, Loss function: 7.636, Average Loss: 2.517, avg. samples / sec: 21597.74
Iteration:    280, Loss function: 7.397, Average Loss: 2.514, avg. samples / sec: 21596.76
Iteration:    280, Loss function: 7.101, Average Loss: 2.515, avg. samples / sec: 21611.68
Iteration:    280, Loss function: 7.539, Average Loss: 2.511, avg. samples / sec: 21596.96
Iteration:    280, Loss function: 7.409, Average Loss: 2.516, avg. samples / sec: 21593.62
Iteration:    280, Loss function: 7.962, Average Loss: 2.516, avg. samples / sec: 21611.56
Iteration:    280, Loss function: 7.755, Average Loss: 2.514, avg. samples / sec: 21600.06
Iteration:    280, Loss function: 7.315, Average Loss: 2.512, avg. samples / sec: 21599.16

:::MLPv0.5.0 ssd 1541710900.718289614 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 281, "value": 0.04995555555555556}

:::MLPv0.5.0 ssd 1541710900.814920902 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 282, "value": 0.050133333333333335}

:::MLPv0.5.0 ssd 1541710900.911684752 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 283, "value": 0.05031111111111111}

:::MLPv0.5.0 ssd 1541710901.007842779 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 284, "value": 0.05048888888888889}

:::MLPv0.5.0 ssd 1541710901.102185011 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 285, "value": 0.050666666666666665}

:::MLPv0.5.0 ssd 1541710901.196688175 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 286, "value": 0.05084444444444444}

:::MLPv0.5.0 ssd 1541710901.291790962 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 287, "value": 0.05102222222222222}

:::MLPv0.5.0 ssd 1541710901.388211727 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 288, "value": 0.051199999999999996}

:::MLPv0.5.0 ssd 1541710901.480042219 (train.py:553) train_epoch: 5

:::MLPv0.5.0 ssd 1541710901.485245943 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 289, "value": 0.05137777777777777}

:::MLPv0.5.0 ssd 1541710901.581624269 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 290, "value": 0.05155555555555555}

:::MLPv0.5.0 ssd 1541710901.676072598 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 291, "value": 0.051733333333333326}

:::MLPv0.5.0 ssd 1541710901.772930861 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 292, "value": 0.0519111111111111}

:::MLPv0.5.0 ssd 1541710901.870903254 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 293, "value": 0.05208888888888889}

:::MLPv0.5.0 ssd 1541710901.966971397 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 294, "value": 0.05226666666666667}

:::MLPv0.5.0 ssd 1541710902.063224316 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 295, "value": 0.052444444444444446}

:::MLPv0.5.0 ssd 1541710902.157299042 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 296, "value": 0.05262222222222222}

:::MLPv0.5.0 ssd 1541710902.251986980 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 297, "value": 0.0528}

:::MLPv0.5.0 ssd 1541710902.349982262 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 298, "value": 0.052977777777777776}

:::MLPv0.5.0 ssd 1541710902.444062710 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 299, "value": 0.05315555555555555}

:::MLPv0.5.0 ssd 1541710902.538431644 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 300, "value": 0.05333333333333333}
Iteration:    300, Loss function: 7.703, Average Loss: 2.612, avg. samples / sec: 21386.91
Iteration:    300, Loss function: 6.780, Average Loss: 2.611, avg. samples / sec: 21389.47
Iteration:    300, Loss function: 7.128, Average Loss: 2.612, avg. samples / sec: 21381.12
Iteration:    300, Loss function: 7.382, Average Loss: 2.611, avg. samples / sec: 21384.94
Iteration:    300, Loss function: 7.123, Average Loss: 2.609, avg. samples / sec: 21388.42
Iteration:    300, Loss function: 7.026, Average Loss: 2.611, avg. samples / sec: 21382.72
Iteration:    300, Loss function: 6.952, Average Loss: 2.606, avg. samples / sec: 21386.22
Iteration:    300, Loss function: 7.294, Average Loss: 2.605, avg. samples / sec: 21374.67

:::MLPv0.5.0 ssd 1541710902.634387493 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 301, "value": 0.053511111111111107}

:::MLPv0.5.0 ssd 1541710902.728013515 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 302, "value": 0.05368888888888888}

:::MLPv0.5.0 ssd 1541710902.823954582 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 303, "value": 0.05386666666666666}

:::MLPv0.5.0 ssd 1541710902.928445339 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 304, "value": 0.05404444444444444}

:::MLPv0.5.0 ssd 1541710903.025571108 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 305, "value": 0.05422222222222223}

:::MLPv0.5.0 ssd 1541710903.119676590 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 306, "value": 0.054400000000000004}

:::MLPv0.5.0 ssd 1541710903.213762283 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 307, "value": 0.05457777777777778}

:::MLPv0.5.0 ssd 1541710903.307973862 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 308, "value": 0.05475555555555556}

:::MLPv0.5.0 ssd 1541710903.402310848 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 309, "value": 0.054933333333333334}

:::MLPv0.5.0 ssd 1541710903.496285915 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 310, "value": 0.05511111111111111}

:::MLPv0.5.0 ssd 1541710903.590445757 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 311, "value": 0.05528888888888889}

:::MLPv0.5.0 ssd 1541710903.683782816 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 312, "value": 0.055466666666666664}

:::MLPv0.5.0 ssd 1541710903.778599501 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 313, "value": 0.05564444444444444}

:::MLPv0.5.0 ssd 1541710903.872734070 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 314, "value": 0.05582222222222222}

:::MLPv0.5.0 ssd 1541710903.967917919 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 315, "value": 0.055999999999999994}

:::MLPv0.5.0 ssd 1541710904.064235449 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 316, "value": 0.05617777777777777}

:::MLPv0.5.0 ssd 1541710904.158986330 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 317, "value": 0.05635555555555555}

:::MLPv0.5.0 ssd 1541710904.253081560 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 318, "value": 0.05653333333333334}

:::MLPv0.5.0 ssd 1541710904.346913099 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 319, "value": 0.056711111111111115}

:::MLPv0.5.0 ssd 1541710904.440445423 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 320, "value": 0.05688888888888889}
Iteration:    320, Loss function: 6.924, Average Loss: 2.699, avg. samples / sec: 21543.56
Iteration:    320, Loss function: 7.266, Average Loss: 2.691, avg. samples / sec: 21552.27
Iteration:    320, Loss function: 7.300, Average Loss: 2.695, avg. samples / sec: 21546.38
Iteration:    320, Loss function: 6.892, Average Loss: 2.698, avg. samples / sec: 21540.83
Iteration:    320, Loss function: 6.932, Average Loss: 2.689, avg. samples / sec: 21552.11
Iteration:    320, Loss function: 7.144, Average Loss: 2.697, avg. samples / sec: 21532.31
Iteration:    320, Loss function: 6.523, Average Loss: 2.693, avg. samples / sec: 21536.05
Iteration:    320, Loss function: 7.491, Average Loss: 2.697, avg. samples / sec: 21514.86

:::MLPv0.5.0 ssd 1541710904.535160303 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 321, "value": 0.05706666666666667}

:::MLPv0.5.0 ssd 1541710904.629323006 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 322, "value": 0.057244444444444445}

:::MLPv0.5.0 ssd 1541710904.723315716 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 323, "value": 0.05742222222222222}

:::MLPv0.5.0 ssd 1541710904.817397833 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 324, "value": 0.0576}

:::MLPv0.5.0 ssd 1541710904.912280083 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 325, "value": 0.057777777777777775}

:::MLPv0.5.0 ssd 1541710905.007086992 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 326, "value": 0.05795555555555555}

:::MLPv0.5.0 ssd 1541710905.101452351 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 327, "value": 0.05813333333333333}

:::MLPv0.5.0 ssd 1541710905.195144653 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 328, "value": 0.058311111111111105}

:::MLPv0.5.0 ssd 1541710905.289246321 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 329, "value": 0.05848888888888888}

:::MLPv0.5.0 ssd 1541710905.384213209 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 330, "value": 0.05866666666666666}

:::MLPv0.5.0 ssd 1541710905.478845119 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 331, "value": 0.05884444444444445}

:::MLPv0.5.0 ssd 1541710905.574024200 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 332, "value": 0.059022222222222226}

:::MLPv0.5.0 ssd 1541710905.669267893 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 333, "value": 0.0592}

:::MLPv0.5.0 ssd 1541710905.764310598 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 334, "value": 0.05937777777777778}

:::MLPv0.5.0 ssd 1541710905.858280182 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 335, "value": 0.059555555555555556}

:::MLPv0.5.0 ssd 1541710905.952214241 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 336, "value": 0.05973333333333333}

:::MLPv0.5.0 ssd 1541710906.049762726 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 337, "value": 0.05991111111111111}

:::MLPv0.5.0 ssd 1541710906.145049572 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 338, "value": 0.060088888888888886}

:::MLPv0.5.0 ssd 1541710906.241235733 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 339, "value": 0.06026666666666666}

:::MLPv0.5.0 ssd 1541710906.336877108 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 340, "value": 0.06044444444444444}
Iteration:    340, Loss function: 7.157, Average Loss: 2.788, avg. samples / sec: 21627.19
Iteration:    340, Loss function: 7.098, Average Loss: 2.781, avg. samples / sec: 21604.74
Iteration:    340, Loss function: 7.516, Average Loss: 2.791, avg. samples / sec: 21601.38
Iteration:    340, Loss function: 7.877, Average Loss: 2.782, avg. samples / sec: 21596.87
Iteration:    340, Loss function: 6.921, Average Loss: 2.789, avg. samples / sec: 21604.59
Iteration:    340, Loss function: 7.049, Average Loss: 2.792, avg. samples / sec: 21585.39
Iteration:    340, Loss function: 7.675, Average Loss: 2.790, avg. samples / sec: 21584.84
Iteration:    340, Loss function: 7.610, Average Loss: 2.783, avg. samples / sec: 21592.32

:::MLPv0.5.0 ssd 1541710906.433550119 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 341, "value": 0.060622222222222216}

:::MLPv0.5.0 ssd 1541710906.527612925 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 342, "value": 0.06079999999999999}

:::MLPv0.5.0 ssd 1541710906.621365309 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 343, "value": 0.06097777777777777}

:::MLPv0.5.0 ssd 1541710906.717564583 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 344, "value": 0.06115555555555556}

:::MLPv0.5.0 ssd 1541710906.811135292 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 345, "value": 0.06133333333333334}

:::MLPv0.5.0 ssd 1541710906.905837297 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 346, "value": 0.061511111111111114}

:::MLPv0.5.0 ssd 1541710906.996916533 (train.py:553) train_epoch: 6

:::MLPv0.5.0 ssd 1541710907.002318621 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 347, "value": 0.06168888888888889}

:::MLPv0.5.0 ssd 1541710907.097997427 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 348, "value": 0.06186666666666667}

:::MLPv0.5.0 ssd 1541710907.190602779 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 349, "value": 0.062044444444444444}

:::MLPv0.5.0 ssd 1541710907.283486605 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 350, "value": 0.06222222222222222}

:::MLPv0.5.0 ssd 1541710907.377971649 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 351, "value": 0.0624}

:::MLPv0.5.0 ssd 1541710907.474628448 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 352, "value": 0.06257777777777777}

:::MLPv0.5.0 ssd 1541710907.569414854 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 353, "value": 0.06275555555555555}

:::MLPv0.5.0 ssd 1541710907.664938450 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 354, "value": 0.06293333333333333}

:::MLPv0.5.0 ssd 1541710907.759880781 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 355, "value": 0.0631111111111111}

:::MLPv0.5.0 ssd 1541710907.853995323 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 356, "value": 0.0632888888888889}

:::MLPv0.5.0 ssd 1541710907.948988676 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 357, "value": 0.06346666666666667}

:::MLPv0.5.0 ssd 1541710908.051776409 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 358, "value": 0.06364444444444445}

:::MLPv0.5.0 ssd 1541710908.146740437 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 359, "value": 0.06382222222222222}

:::MLPv0.5.0 ssd 1541710908.243623257 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 360, "value": 0.064}
Iteration:    360, Loss function: 6.974, Average Loss: 2.867, avg. samples / sec: 21484.54
Iteration:    360, Loss function: 7.185, Average Loss: 2.878, avg. samples / sec: 21489.73
Iteration:    360, Loss function: 6.503, Average Loss: 2.872, avg. samples / sec: 21482.46
Iteration:    360, Loss function: 7.408, Average Loss: 2.873, avg. samples / sec: 21491.60
Iteration:    360, Loss function: 6.543, Average Loss: 2.875, avg. samples / sec: 21477.30
Iteration:    360, Loss function: 6.933, Average Loss: 2.866, avg. samples / sec: 21497.54
Iteration:    360, Loss function: 6.722, Average Loss: 2.871, avg. samples / sec: 21473.84
Iteration:    360, Loss function: 6.340, Average Loss: 2.863, avg. samples / sec: 21469.59

:::MLPv0.5.0 ssd 1541710908.340905666 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 361, "value": 0.06417777777777778}

:::MLPv0.5.0 ssd 1541710908.437839508 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 362, "value": 0.06435555555555555}

:::MLPv0.5.0 ssd 1541710908.530961514 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 363, "value": 0.06453333333333333}

:::MLPv0.5.0 ssd 1541710908.625156879 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 364, "value": 0.06471111111111111}

:::MLPv0.5.0 ssd 1541710908.721106529 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 365, "value": 0.06488888888888888}

:::MLPv0.5.0 ssd 1541710908.814687252 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 366, "value": 0.06506666666666666}

:::MLPv0.5.0 ssd 1541710908.910493135 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 367, "value": 0.06524444444444444}

:::MLPv0.5.0 ssd 1541710909.004859209 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 368, "value": 0.06542222222222221}

:::MLPv0.5.0 ssd 1541710909.101254940 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 369, "value": 0.0656}

:::MLPv0.5.0 ssd 1541710909.195742607 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 370, "value": 0.06577777777777778}

:::MLPv0.5.0 ssd 1541710909.289437532 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 371, "value": 0.06595555555555556}

:::MLPv0.5.0 ssd 1541710909.384200335 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 372, "value": 0.06613333333333334}

:::MLPv0.5.0 ssd 1541710909.477949858 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 373, "value": 0.06631111111111111}

:::MLPv0.5.0 ssd 1541710909.573084354 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 374, "value": 0.06648888888888889}

:::MLPv0.5.0 ssd 1541710909.668118715 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 375, "value": 0.06666666666666667}

:::MLPv0.5.0 ssd 1541710909.761277437 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 376, "value": 0.06684444444444444}

:::MLPv0.5.0 ssd 1541710909.856121540 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 377, "value": 0.06702222222222222}

:::MLPv0.5.0 ssd 1541710909.953793526 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 378, "value": 0.0672}

:::MLPv0.5.0 ssd 1541710910.048090219 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 379, "value": 0.06737777777777777}

:::MLPv0.5.0 ssd 1541710910.141397953 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 380, "value": 0.06755555555555555}
Iteration:    380, Loss function: 6.754, Average Loss: 2.953, avg. samples / sec: 21588.31
Iteration:    380, Loss function: 6.165, Average Loss: 2.940, avg. samples / sec: 21583.51
Iteration:    380, Loss function: 6.016, Average Loss: 2.940, avg. samples / sec: 21584.88
Iteration:    380, Loss function: 6.678, Average Loss: 2.949, avg. samples / sec: 21583.46
Iteration:    380, Loss function: 6.532, Average Loss: 2.935, avg. samples / sec: 21588.40
Iteration:    380, Loss function: 6.326, Average Loss: 2.947, avg. samples / sec: 21580.92
Iteration:    380, Loss function: 6.661, Average Loss: 2.946, avg. samples / sec: 21577.56
Iteration:    380, Loss function: 6.127, Average Loss: 2.944, avg. samples / sec: 21578.74

:::MLPv0.5.0 ssd 1541710910.236821651 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 381, "value": 0.06773333333333333}

:::MLPv0.5.0 ssd 1541710910.332125187 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 382, "value": 0.06791111111111112}

:::MLPv0.5.0 ssd 1541710910.426511288 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 383, "value": 0.0680888888888889}

:::MLPv0.5.0 ssd 1541710910.520892620 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 384, "value": 0.06826666666666667}

:::MLPv0.5.0 ssd 1541710910.615719080 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 385, "value": 0.06844444444444445}

:::MLPv0.5.0 ssd 1541710910.710864544 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 386, "value": 0.06862222222222222}

:::MLPv0.5.0 ssd 1541710910.805096388 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 387, "value": 0.0688}

:::MLPv0.5.0 ssd 1541710910.899077892 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 388, "value": 0.06897777777777778}

:::MLPv0.5.0 ssd 1541710910.995954275 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 389, "value": 0.06915555555555555}

:::MLPv0.5.0 ssd 1541710911.092048407 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 390, "value": 0.06933333333333333}

:::MLPv0.5.0 ssd 1541710911.185721397 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 391, "value": 0.0695111111111111}

:::MLPv0.5.0 ssd 1541710911.280281305 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 392, "value": 0.06968888888888888}

:::MLPv0.5.0 ssd 1541710911.376436234 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 393, "value": 0.06986666666666666}

:::MLPv0.5.0 ssd 1541710911.474145651 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 394, "value": 0.07004444444444444}

:::MLPv0.5.0 ssd 1541710911.570397377 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 395, "value": 0.07022222222222223}

:::MLPv0.5.0 ssd 1541710911.664134026 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 396, "value": 0.0704}

:::MLPv0.5.0 ssd 1541710911.757961035 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 397, "value": 0.07057777777777778}

:::MLPv0.5.0 ssd 1541710911.852154970 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 398, "value": 0.07075555555555556}

:::MLPv0.5.0 ssd 1541710911.946077585 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 399, "value": 0.07093333333333333}

:::MLPv0.5.0 ssd 1541710912.040535927 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 400, "value": 0.07111111111111111}
Iteration:    400, Loss function: 6.906, Average Loss: 3.026, avg. samples / sec: 21571.16
Iteration:    400, Loss function: 7.103, Average Loss: 3.021, avg. samples / sec: 21580.85
Iteration:    400, Loss function: 6.531, Average Loss: 3.013, avg. samples / sec: 21566.49
Iteration:    400, Loss function: 6.757, Average Loss: 3.022, avg. samples / sec: 21571.30
Iteration:    400, Loss function: 6.374, Average Loss: 3.019, avg. samples / sec: 21574.11
Iteration:    400, Loss function: 7.167, Average Loss: 3.008, avg. samples / sec: 21571.75
Iteration:    400, Loss function: 7.540, Average Loss: 3.015, avg. samples / sec: 21564.98
Iteration:    400, Loss function: 7.182, Average Loss: 3.020, avg. samples / sec: 21573.64

:::MLPv0.5.0 ssd 1541710912.134492397 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 401, "value": 0.07128888888888889}

:::MLPv0.5.0 ssd 1541710912.230449438 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 402, "value": 0.07146666666666666}

:::MLPv0.5.0 ssd 1541710912.325160742 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 403, "value": 0.07164444444444444}

:::MLPv0.5.0 ssd 1541710912.419694185 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 404, "value": 0.07182222222222222}

:::MLPv0.5.0 ssd 1541710912.512119532 (train.py:553) train_epoch: 7

:::MLPv0.5.0 ssd 1541710912.517463446 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 405, "value": 0.072}

:::MLPv0.5.0 ssd 1541710912.611542940 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 406, "value": 0.07217777777777777}

:::MLPv0.5.0 ssd 1541710912.707043886 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 407, "value": 0.07235555555555555}

:::MLPv0.5.0 ssd 1541710912.803857088 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 408, "value": 0.07253333333333334}

:::MLPv0.5.0 ssd 1541710912.900674582 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 409, "value": 0.07271111111111112}

:::MLPv0.5.0 ssd 1541710912.994315863 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 410, "value": 0.07288888888888889}

:::MLPv0.5.0 ssd 1541710913.088906050 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 411, "value": 0.07306666666666667}

:::MLPv0.5.0 ssd 1541710913.183565855 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 412, "value": 0.07324444444444445}

:::MLPv0.5.0 ssd 1541710913.277569294 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 413, "value": 0.07342222222222222}

:::MLPv0.5.0 ssd 1541710913.375930309 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 414, "value": 0.0736}

:::MLPv0.5.0 ssd 1541710913.470988274 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 415, "value": 0.07377777777777778}

:::MLPv0.5.0 ssd 1541710913.564854622 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 416, "value": 0.07395555555555555}

:::MLPv0.5.0 ssd 1541710913.659168005 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 417, "value": 0.07413333333333333}

:::MLPv0.5.0 ssd 1541710913.753975153 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 418, "value": 0.0743111111111111}

:::MLPv0.5.0 ssd 1541710913.850028276 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 419, "value": 0.07448888888888888}

:::MLPv0.5.0 ssd 1541710913.951501131 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 420, "value": 0.07466666666666666}
Iteration:    420, Loss function: 5.771, Average Loss: 3.093, avg. samples / sec: 21440.08
Iteration:    420, Loss function: 6.367, Average Loss: 3.100, avg. samples / sec: 21429.23
Iteration:    420, Loss function: 6.746, Average Loss: 3.094, avg. samples / sec: 21440.89
Iteration:    420, Loss function: 6.403, Average Loss: 3.092, avg. samples / sec: 21436.81
Iteration:    420, Loss function: 6.071, Average Loss: 3.085, avg. samples / sec: 21433.38
Iteration:    420, Loss function: 6.915, Average Loss: 3.095, avg. samples / sec: 21427.88
Iteration:    420, Loss function: 6.393, Average Loss: 3.084, avg. samples / sec: 21434.27
Iteration:    420, Loss function: 6.783, Average Loss: 3.092, avg. samples / sec: 21434.44

:::MLPv0.5.0 ssd 1541710914.046749115 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 421, "value": 0.07484444444444445}

:::MLPv0.5.0 ssd 1541710914.141542673 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 422, "value": 0.07502222222222223}

:::MLPv0.5.0 ssd 1541710914.236196280 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 423, "value": 0.0752}

:::MLPv0.5.0 ssd 1541710914.331609964 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 424, "value": 0.07537777777777778}

:::MLPv0.5.0 ssd 1541710914.426207781 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 425, "value": 0.07555555555555556}

:::MLPv0.5.0 ssd 1541710914.522138834 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 426, "value": 0.07573333333333333}

:::MLPv0.5.0 ssd 1541710914.616375446 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 427, "value": 0.07591111111111111}

:::MLPv0.5.0 ssd 1541710914.711083174 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 428, "value": 0.07608888888888889}

:::MLPv0.5.0 ssd 1541710914.805027008 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 429, "value": 0.07626666666666666}

:::MLPv0.5.0 ssd 1541710914.901320219 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 430, "value": 0.07644444444444444}

:::MLPv0.5.0 ssd 1541710914.995344877 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 431, "value": 0.07662222222222222}

:::MLPv0.5.0 ssd 1541710915.091522217 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 432, "value": 0.0768}

:::MLPv0.5.0 ssd 1541710915.184790611 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 433, "value": 0.07697777777777778}

:::MLPv0.5.0 ssd 1541710915.279819250 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 434, "value": 0.07715555555555556}

:::MLPv0.5.0 ssd 1541710915.375483990 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 435, "value": 0.07733333333333334}

:::MLPv0.5.0 ssd 1541710915.473641634 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 436, "value": 0.07751111111111111}

:::MLPv0.5.0 ssd 1541710915.567936420 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 437, "value": 0.07768888888888889}

:::MLPv0.5.0 ssd 1541710915.663579464 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 438, "value": 0.07786666666666667}

:::MLPv0.5.0 ssd 1541710915.761589766 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 439, "value": 0.07804444444444444}

:::MLPv0.5.0 ssd 1541710915.857652903 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 440, "value": 0.07822222222222222}
Iteration:    440, Loss function: 6.188, Average Loss: 3.162, avg. samples / sec: 21492.32
Iteration:    440, Loss function: 6.461, Average Loss: 3.156, avg. samples / sec: 21490.52
Iteration:    440, Loss function: 5.889, Average Loss: 3.158, avg. samples / sec: 21486.20
Iteration:    440, Loss function: 6.070, Average Loss: 3.147, avg. samples / sec: 21490.62
Iteration:    440, Loss function: 6.507, Average Loss: 3.149, avg. samples / sec: 21487.33
Iteration:    440, Loss function: 5.636, Average Loss: 3.153, avg. samples / sec: 21486.08
Iteration:    440, Loss function: 6.520, Average Loss: 3.155, avg. samples / sec: 21491.42
Iteration:    440, Loss function: 5.709, Average Loss: 3.159, avg. samples / sec: 21484.86

:::MLPv0.5.0 ssd 1541710915.952281952 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 441, "value": 0.0784}

:::MLPv0.5.0 ssd 1541710916.056591511 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 442, "value": 0.07857777777777777}

:::MLPv0.5.0 ssd 1541710916.150434256 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 443, "value": 0.07875555555555555}

:::MLPv0.5.0 ssd 1541710916.245779276 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 444, "value": 0.07893333333333333}

:::MLPv0.5.0 ssd 1541710916.339939117 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 445, "value": 0.0791111111111111}

:::MLPv0.5.0 ssd 1541710916.436364889 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 446, "value": 0.0792888888888889}

:::MLPv0.5.0 ssd 1541710916.530859470 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 447, "value": 0.07946666666666667}

:::MLPv0.5.0 ssd 1541710916.625089169 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 448, "value": 0.07964444444444445}

:::MLPv0.5.0 ssd 1541710916.720227003 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 449, "value": 0.07982222222222222}

:::MLPv0.5.0 ssd 1541710916.814614296 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 450, "value": 0.08}

:::MLPv0.5.0 ssd 1541710916.910660982 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 451, "value": 0.08017777777777778}

:::MLPv0.5.0 ssd 1541710917.006370306 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 452, "value": 0.08035555555555556}

:::MLPv0.5.0 ssd 1541710917.100673199 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 453, "value": 0.08053333333333333}

:::MLPv0.5.0 ssd 1541710917.195168018 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 454, "value": 0.08071111111111111}

:::MLPv0.5.0 ssd 1541710917.291817904 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 455, "value": 0.08088888888888889}

:::MLPv0.5.0 ssd 1541710917.385628223 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 456, "value": 0.08106666666666666}

:::MLPv0.5.0 ssd 1541710917.481655121 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 457, "value": 0.08124444444444444}

:::MLPv0.5.0 ssd 1541710917.575689077 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 458, "value": 0.08142222222222222}

:::MLPv0.5.0 ssd 1541710917.672148228 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 459, "value": 0.0816}

:::MLPv0.5.0 ssd 1541710917.766667604 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 460, "value": 0.08177777777777778}
Iteration:    460, Loss function: 7.455, Average Loss: 3.226, avg. samples / sec: 21462.16
Iteration:    460, Loss function: 7.264, Average Loss: 3.231, avg. samples / sec: 21457.75
Iteration:    460, Loss function: 7.723, Average Loss: 3.224, avg. samples / sec: 21459.95
Iteration:    460, Loss function: 7.443, Average Loss: 3.224, avg. samples / sec: 21455.16
Iteration:    460, Loss function: 6.664, Average Loss: 3.219, avg. samples / sec: 21457.97
Iteration:    460, Loss function: 7.116, Average Loss: 3.221, avg. samples / sec: 21456.72
Iteration:    460, Loss function: 7.635, Average Loss: 3.216, avg. samples / sec: 21454.80
Iteration:    460, Loss function: 7.518, Average Loss: 3.228, avg. samples / sec: 21458.06

:::MLPv0.5.0 ssd 1541710917.861766577 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 461, "value": 0.08195555555555556}

:::MLPv0.5.0 ssd 1541710917.957307577 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 462, "value": 0.08213333333333334}

:::MLPv0.5.0 ssd 1541710918.046844006 (train.py:553) train_epoch: 8

:::MLPv0.5.0 ssd 1541710918.052693844 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 463, "value": 0.08231111111111111}

:::MLPv0.5.0 ssd 1541710918.148788452 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 464, "value": 0.08248888888888889}

:::MLPv0.5.0 ssd 1541710918.243253946 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 465, "value": 0.08266666666666667}

:::MLPv0.5.0 ssd 1541710918.341539621 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 466, "value": 0.08284444444444444}

:::MLPv0.5.0 ssd 1541710918.437570572 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 467, "value": 0.08302222222222222}

:::MLPv0.5.0 ssd 1541710918.531403303 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 468, "value": 0.0832}

:::MLPv0.5.0 ssd 1541710918.626231670 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 469, "value": 0.08337777777777777}

:::MLPv0.5.0 ssd 1541710918.721330643 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 470, "value": 0.08355555555555555}

:::MLPv0.5.0 ssd 1541710918.815608501 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 471, "value": 0.08373333333333333}

:::MLPv0.5.0 ssd 1541710918.911326647 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 472, "value": 0.08391111111111112}

:::MLPv0.5.0 ssd 1541710919.008199692 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 473, "value": 0.0840888888888889}

:::MLPv0.5.0 ssd 1541710919.102388382 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 474, "value": 0.08426666666666667}

:::MLPv0.5.0 ssd 1541710919.197309017 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 475, "value": 0.08444444444444445}

:::MLPv0.5.0 ssd 1541710919.291249752 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 476, "value": 0.08462222222222222}

:::MLPv0.5.0 ssd 1541710919.385310411 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 477, "value": 0.0848}

:::MLPv0.5.0 ssd 1541710919.480607748 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 478, "value": 0.08497777777777778}

:::MLPv0.5.0 ssd 1541710919.576715231 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 479, "value": 0.08515555555555555}

:::MLPv0.5.0 ssd 1541710919.675965071 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 480, "value": 0.08533333333333333}
Iteration:    480, Loss function: 6.229, Average Loss: 3.291, avg. samples / sec: 21457.78
Iteration:    480, Loss function: 6.264, Average Loss: 3.294, avg. samples / sec: 21449.79
Iteration:    480, Loss function: 6.133, Average Loss: 3.285, avg. samples / sec: 21457.06
Iteration:    480, Loss function: 6.515, Average Loss: 3.298, avg. samples / sec: 21446.58
Iteration:    480, Loss function: 6.439, Average Loss: 3.291, avg. samples / sec: 21456.68
Iteration:    480, Loss function: 6.216, Average Loss: 3.296, avg. samples / sec: 21457.83
Iteration:    480, Loss function: 6.298, Average Loss: 3.286, avg. samples / sec: 21456.34
Iteration:    480, Loss function: 6.348, Average Loss: 3.295, avg. samples / sec: 21446.36

:::MLPv0.5.0 ssd 1541710919.770160198 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 481, "value": 0.08551111111111111}

:::MLPv0.5.0 ssd 1541710919.864164829 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 482, "value": 0.08568888888888888}

:::MLPv0.5.0 ssd 1541710919.958284855 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 483, "value": 0.08586666666666666}

:::MLPv0.5.0 ssd 1541710920.052360296 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 484, "value": 0.08604444444444445}

:::MLPv0.5.0 ssd 1541710920.146517038 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 485, "value": 0.08622222222222223}

:::MLPv0.5.0 ssd 1541710920.242223501 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 486, "value": 0.0864}

:::MLPv0.5.0 ssd 1541710920.337135077 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 487, "value": 0.08657777777777778}

:::MLPv0.5.0 ssd 1541710920.431052446 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 488, "value": 0.08675555555555556}

:::MLPv0.5.0 ssd 1541710920.525981903 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 489, "value": 0.08693333333333333}

:::MLPv0.5.0 ssd 1541710920.620793581 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 490, "value": 0.08711111111111111}

:::MLPv0.5.0 ssd 1541710920.714973211 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 491, "value": 0.08728888888888889}

:::MLPv0.5.0 ssd 1541710920.813660145 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 492, "value": 0.08746666666666666}

:::MLPv0.5.0 ssd 1541710920.908670902 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 493, "value": 0.08764444444444444}

:::MLPv0.5.0 ssd 1541710921.003404856 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 494, "value": 0.08782222222222222}

:::MLPv0.5.0 ssd 1541710921.096687555 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 495, "value": 0.088}

:::MLPv0.5.0 ssd 1541710921.190406561 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 496, "value": 0.08817777777777777}

:::MLPv0.5.0 ssd 1541710921.284724236 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 497, "value": 0.08835555555555556}

:::MLPv0.5.0 ssd 1541710921.379172802 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 498, "value": 0.08853333333333334}

:::MLPv0.5.0 ssd 1541710921.473255873 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 499, "value": 0.08871111111111112}

:::MLPv0.5.0 ssd 1541710921.567101240 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 500, "value": 0.08888888888888889}
Iteration:    500, Loss function: 5.841, Average Loss: 3.353, avg. samples / sec: 21670.43
Iteration:    500, Loss function: 6.038, Average Loss: 3.339, avg. samples / sec: 21662.71
Iteration:    500, Loss function: 5.537, Average Loss: 3.349, avg. samples / sec: 21662.71
Iteration:    500, Loss function: 5.896, Average Loss: 3.344, avg. samples / sec: 21658.69
Iteration:    500, Loss function: 5.564, Average Loss: 3.346, avg. samples / sec: 21660.72
Iteration:    500, Loss function: 5.501, Average Loss: 3.348, avg. samples / sec: 21655.53
Iteration:    500, Loss function: 6.083, Average Loss: 3.339, avg. samples / sec: 21658.58
Iteration:    500, Loss function: 6.012, Average Loss: 3.349, avg. samples / sec: 21653.74

:::MLPv0.5.0 ssd 1541710921.663138866 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 501, "value": 0.08906666666666667}

:::MLPv0.5.0 ssd 1541710921.756592274 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 502, "value": 0.08924444444444445}

:::MLPv0.5.0 ssd 1541710921.850224018 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 503, "value": 0.08942222222222222}

:::MLPv0.5.0 ssd 1541710921.946643829 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 504, "value": 0.0896}

:::MLPv0.5.0 ssd 1541710922.046328068 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 505, "value": 0.08977777777777778}

:::MLPv0.5.0 ssd 1541710922.142608166 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 506, "value": 0.08995555555555555}

:::MLPv0.5.0 ssd 1541710922.238178492 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 507, "value": 0.09013333333333333}

:::MLPv0.5.0 ssd 1541710922.333448887 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 508, "value": 0.0903111111111111}

:::MLPv0.5.0 ssd 1541710922.428611279 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 509, "value": 0.09048888888888888}

:::MLPv0.5.0 ssd 1541710922.523637295 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 510, "value": 0.09066666666666667}

:::MLPv0.5.0 ssd 1541710922.619348526 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 511, "value": 0.09084444444444445}

:::MLPv0.5.0 ssd 1541710922.713805676 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 512, "value": 0.09102222222222223}

:::MLPv0.5.0 ssd 1541710922.808145523 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 513, "value": 0.0912}

:::MLPv0.5.0 ssd 1541710922.903492451 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 514, "value": 0.09137777777777778}

:::MLPv0.5.0 ssd 1541710922.998606443 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 515, "value": 0.09155555555555556}

:::MLPv0.5.0 ssd 1541710923.094826698 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 516, "value": 0.09173333333333333}

:::MLPv0.5.0 ssd 1541710923.188282967 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 517, "value": 0.09191111111111111}

:::MLPv0.5.0 ssd 1541710923.283077002 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 518, "value": 0.09208888888888889}

:::MLPv0.5.0 ssd 1541710923.378157616 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 519, "value": 0.09226666666666666}

:::MLPv0.5.0 ssd 1541710923.468601704 (train.py:553) train_epoch: 9

:::MLPv0.5.0 ssd 1541710923.473967791 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 520, "value": 0.09244444444444444}
Iteration:    520, Loss function: 5.929, Average Loss: 3.396, avg. samples / sec: 21485.22
Iteration:    520, Loss function: 5.590, Average Loss: 3.402, avg. samples / sec: 21475.14
Iteration:    520, Loss function: 6.430, Average Loss: 3.401, avg. samples / sec: 21494.29
Iteration:    520, Loss function: 6.592, Average Loss: 3.393, avg. samples / sec: 21483.80
Iteration:    520, Loss function: 5.869, Average Loss: 3.390, avg. samples / sec: 21474.80
Iteration:    520, Loss function: 5.795, Average Loss: 3.393, avg. samples / sec: 21476.48
Iteration:    520, Loss function: 6.152, Average Loss: 3.398, avg. samples / sec: 21479.86
Iteration:    520, Loss function: 6.268, Average Loss: 3.399, avg. samples / sec: 21470.89

:::MLPv0.5.0 ssd 1541710923.568727732 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 521, "value": 0.09262222222222222}

:::MLPv0.5.0 ssd 1541710923.663635969 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 522, "value": 0.0928}

:::MLPv0.5.0 ssd 1541710923.758132219 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 523, "value": 0.09297777777777778}

:::MLPv0.5.0 ssd 1541710923.852672338 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 524, "value": 0.09315555555555556}

:::MLPv0.5.0 ssd 1541710923.946508646 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 525, "value": 0.09333333333333334}

:::MLPv0.5.0 ssd 1541710924.045508385 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 526, "value": 0.09351111111111111}

:::MLPv0.5.0 ssd 1541710924.139433622 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 527, "value": 0.09368888888888889}

:::MLPv0.5.0 ssd 1541710924.235269308 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 528, "value": 0.09386666666666667}

:::MLPv0.5.0 ssd 1541710924.329771280 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 529, "value": 0.09404444444444444}

:::MLPv0.5.0 ssd 1541710924.423851967 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 530, "value": 0.09422222222222222}

:::MLPv0.5.0 ssd 1541710924.517863989 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 531, "value": 0.0944}

:::MLPv0.5.0 ssd 1541710924.612488270 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 532, "value": 0.09457777777777777}

:::MLPv0.5.0 ssd 1541710924.710981846 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 533, "value": 0.09475555555555555}

:::MLPv0.5.0 ssd 1541710924.805709362 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 534, "value": 0.09493333333333333}

:::MLPv0.5.0 ssd 1541710924.901302814 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 535, "value": 0.0951111111111111}

:::MLPv0.5.0 ssd 1541710925.000197411 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 536, "value": 0.0952888888888889}

:::MLPv0.5.0 ssd 1541710925.095659018 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 537, "value": 0.09546666666666667}

:::MLPv0.5.0 ssd 1541710925.190341711 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 538, "value": 0.09564444444444445}

:::MLPv0.5.0 ssd 1541710925.285467625 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 539, "value": 0.09582222222222223}

:::MLPv0.5.0 ssd 1541710925.381048203 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 540, "value": 0.096}
Iteration:    540, Loss function: 5.583, Average Loss: 3.460, avg. samples / sec: 21482.41
Iteration:    540, Loss function: 6.110, Average Loss: 3.448, avg. samples / sec: 21482.02
Iteration:    540, Loss function: 5.784, Average Loss: 3.454, avg. samples / sec: 21488.98
Iteration:    540, Loss function: 5.557, Average Loss: 3.451, avg. samples / sec: 21483.87
Iteration:    540, Loss function: 6.138, Average Loss: 3.452, avg. samples / sec: 21483.20
Iteration:    540, Loss function: 5.943, Average Loss: 3.454, avg. samples / sec: 21473.93
Iteration:    540, Loss function: 6.674, Average Loss: 3.457, avg. samples / sec: 21478.72
Iteration:    540, Loss function: 6.392, Average Loss: 3.455, avg. samples / sec: 21479.58

:::MLPv0.5.0 ssd 1541710925.476629496 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 541, "value": 0.09617777777777778}

:::MLPv0.5.0 ssd 1541710925.571176052 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 542, "value": 0.09635555555555556}

:::MLPv0.5.0 ssd 1541710925.669211388 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 543, "value": 0.09653333333333333}

:::MLPv0.5.0 ssd 1541710925.764798403 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 544, "value": 0.09671111111111111}

:::MLPv0.5.0 ssd 1541710925.861463070 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 545, "value": 0.09688888888888889}

:::MLPv0.5.0 ssd 1541710925.956227303 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 546, "value": 0.09706666666666666}

:::MLPv0.5.0 ssd 1541710926.052426100 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 547, "value": 0.09724444444444444}

:::MLPv0.5.0 ssd 1541710926.146318436 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 548, "value": 0.09742222222222222}

:::MLPv0.5.0 ssd 1541710926.240635157 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 549, "value": 0.09759999999999999}

:::MLPv0.5.0 ssd 1541710926.335136414 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 550, "value": 0.09777777777777777}

:::MLPv0.5.0 ssd 1541710926.429884911 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 551, "value": 0.09795555555555555}

:::MLPv0.5.0 ssd 1541710926.523980856 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 552, "value": 0.09813333333333334}

:::MLPv0.5.0 ssd 1541710926.620091200 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 553, "value": 0.09831111111111111}

:::MLPv0.5.0 ssd 1541710926.714284658 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 554, "value": 0.09848888888888889}

:::MLPv0.5.0 ssd 1541710926.810425043 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 555, "value": 0.09866666666666667}

:::MLPv0.5.0 ssd 1541710926.904153824 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 556, "value": 0.09884444444444444}

:::MLPv0.5.0 ssd 1541710926.999696970 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 557, "value": 0.09902222222222222}

:::MLPv0.5.0 ssd 1541710927.094814062 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 558, "value": 0.09920000000000001}

:::MLPv0.5.0 ssd 1541710927.192403316 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 559, "value": 0.09937777777777779}

:::MLPv0.5.0 ssd 1541710927.287454605 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 560, "value": 0.09955555555555556}
Iteration:    560, Loss function: 5.904, Average Loss: 3.500, avg. samples / sec: 21490.39
Iteration:    560, Loss function: 4.434, Average Loss: 3.501, avg. samples / sec: 21491.29
Iteration:    560, Loss function: 5.608, Average Loss: 3.495, avg. samples / sec: 21484.24
Iteration:    560, Loss function: 5.834, Average Loss: 3.496, avg. samples / sec: 21484.00
Iteration:    560, Loss function: 5.128, Average Loss: 3.499, avg. samples / sec: 21482.71
Iteration:    560, Loss function: 5.407, Average Loss: 3.503, avg. samples / sec: 21484.16
Iteration:    560, Loss function: 5.606, Average Loss: 3.499, avg. samples / sec: 21481.25
Iteration:    560, Loss function: 6.303, Average Loss: 3.508, avg. samples / sec: 21473.82

:::MLPv0.5.0 ssd 1541710927.382269859 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 561, "value": 0.09973333333333334}

:::MLPv0.5.0 ssd 1541710927.477648973 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 562, "value": 0.09991111111111112}

:::MLPv0.5.0 ssd 1541710927.575072050 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 563, "value": 0.1000888888888889}

:::MLPv0.5.0 ssd 1541710927.669460535 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 564, "value": 0.10026666666666667}

:::MLPv0.5.0 ssd 1541710927.763473511 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 565, "value": 0.10044444444444445}

:::MLPv0.5.0 ssd 1541710927.857324600 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 566, "value": 0.10062222222222222}

:::MLPv0.5.0 ssd 1541710927.952089548 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 567, "value": 0.1008}

:::MLPv0.5.0 ssd 1541710928.047173738 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 568, "value": 0.10097777777777778}

:::MLPv0.5.0 ssd 1541710928.143227339 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 569, "value": 0.10115555555555555}

:::MLPv0.5.0 ssd 1541710928.238384962 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 570, "value": 0.10133333333333333}

:::MLPv0.5.0 ssd 1541710928.332610130 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 571, "value": 0.10151111111111111}

:::MLPv0.5.0 ssd 1541710928.427991867 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 572, "value": 0.10168888888888888}

:::MLPv0.5.0 ssd 1541710928.524579287 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 573, "value": 0.10186666666666666}

:::MLPv0.5.0 ssd 1541710928.620553255 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 574, "value": 0.10204444444444444}

:::MLPv0.5.0 ssd 1541710928.715713739 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 575, "value": 0.10222222222222221}

:::MLPv0.5.0 ssd 1541710928.810467958 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 576, "value": 0.10239999999999999}

:::MLPv0.5.0 ssd 1541710928.908296585 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 577, "value": 0.10257777777777778}

:::MLPv0.5.0 ssd 1541710928.999823332 (train.py:553) train_epoch: 10

:::MLPv0.5.0 ssd 1541710929.005358934 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 578, "value": 0.10275555555555556}

:::MLPv0.5.0 ssd 1541710929.100420713 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 579, "value": 0.10293333333333334}

:::MLPv0.5.0 ssd 1541710929.195364952 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 580, "value": 0.10311111111111111}
Iteration:    580, Loss function: 5.364, Average Loss: 3.551, avg. samples / sec: 21482.40
Iteration:    580, Loss function: 5.215, Average Loss: 3.539, avg. samples / sec: 21475.81
Iteration:    580, Loss function: 5.757, Average Loss: 3.543, avg. samples / sec: 21475.57
Iteration:    580, Loss function: 6.068, Average Loss: 3.542, avg. samples / sec: 21475.27
Iteration:    580, Loss function: 5.928, Average Loss: 3.538, avg. samples / sec: 21471.14
Iteration:    580, Loss function: 5.197, Average Loss: 3.541, avg. samples / sec: 21468.54
Iteration:    580, Loss function: 5.626, Average Loss: 3.543, avg. samples / sec: 21467.97
Iteration:    580, Loss function: 5.635, Average Loss: 3.544, avg. samples / sec: 21464.80

:::MLPv0.5.0 ssd 1541710929.290760994 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 581, "value": 0.10328888888888889}

:::MLPv0.5.0 ssd 1541710929.384913921 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 582, "value": 0.10346666666666667}

:::MLPv0.5.0 ssd 1541710929.480413914 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 583, "value": 0.10364444444444444}

:::MLPv0.5.0 ssd 1541710929.575277090 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 584, "value": 0.10382222222222223}

:::MLPv0.5.0 ssd 1541710929.670570135 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 585, "value": 0.10400000000000001}

:::MLPv0.5.0 ssd 1541710929.765794754 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 586, "value": 0.10417777777777779}

:::MLPv0.5.0 ssd 1541710929.861249685 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 587, "value": 0.10435555555555556}

:::MLPv0.5.0 ssd 1541710929.955365419 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 588, "value": 0.10453333333333334}

:::MLPv0.5.0 ssd 1541710930.053634167 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 589, "value": 0.10471111111111112}

:::MLPv0.5.0 ssd 1541710930.149453163 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 590, "value": 0.10488888888888889}

:::MLPv0.5.0 ssd 1541710930.244176626 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 591, "value": 0.10506666666666667}

:::MLPv0.5.0 ssd 1541710930.338906050 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 592, "value": 0.10524444444444445}

:::MLPv0.5.0 ssd 1541710930.434632301 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 593, "value": 0.10542222222222222}

:::MLPv0.5.0 ssd 1541710930.530536413 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 594, "value": 0.1056}

:::MLPv0.5.0 ssd 1541710930.625030518 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 595, "value": 0.10577777777777778}

:::MLPv0.5.0 ssd 1541710930.721503973 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 596, "value": 0.10595555555555555}

:::MLPv0.5.0 ssd 1541710930.818377495 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 597, "value": 0.10613333333333333}

:::MLPv0.5.0 ssd 1541710930.912014484 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 598, "value": 0.1063111111111111}

:::MLPv0.5.0 ssd 1541710931.005980730 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 599, "value": 0.10648888888888888}

:::MLPv0.5.0 ssd 1541710931.100609541 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 600, "value": 0.10666666666666666}
Iteration:    600, Loss function: 5.642, Average Loss: 3.599, avg. samples / sec: 21498.31
Iteration:    600, Loss function: 6.797, Average Loss: 3.586, avg. samples / sec: 21500.49
Iteration:    600, Loss function: 6.096, Average Loss: 3.587, avg. samples / sec: 21508.40
Iteration:    600, Loss function: 6.128, Average Loss: 3.588, avg. samples / sec: 21492.10
Iteration:    600, Loss function: 5.818, Average Loss: 3.591, avg. samples / sec: 21496.95
Iteration:    600, Loss function: 5.664, Average Loss: 3.591, avg. samples / sec: 21492.99
Iteration:    600, Loss function: 5.675, Average Loss: 3.589, avg. samples / sec: 21493.28
Iteration:    600, Loss function: 5.838, Average Loss: 3.588, avg. samples / sec: 21492.48

:::MLPv0.5.0 ssd 1541710931.195627451 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 601, "value": 0.10684444444444444}

:::MLPv0.5.0 ssd 1541710931.290354490 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 602, "value": 0.10702222222222221}

:::MLPv0.5.0 ssd 1541710931.385219574 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 603, "value": 0.1072}

:::MLPv0.5.0 ssd 1541710931.482653379 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 604, "value": 0.10737777777777778}

:::MLPv0.5.0 ssd 1541710931.577988148 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 605, "value": 0.10755555555555556}

:::MLPv0.5.0 ssd 1541710931.673231125 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 606, "value": 0.10773333333333333}

:::MLPv0.5.0 ssd 1541710931.768053293 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 607, "value": 0.10791111111111111}

:::MLPv0.5.0 ssd 1541710931.864069700 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 608, "value": 0.10808888888888889}

:::MLPv0.5.0 ssd 1541710931.959285736 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 609, "value": 0.10826666666666668}

:::MLPv0.5.0 ssd 1541710932.053309917 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 610, "value": 0.10844444444444445}

:::MLPv0.5.0 ssd 1541710932.147742987 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 611, "value": 0.10862222222222223}

:::MLPv0.5.0 ssd 1541710932.246959448 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 612, "value": 0.10880000000000001}

:::MLPv0.5.0 ssd 1541710932.345361948 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 613, "value": 0.10897777777777778}

:::MLPv0.5.0 ssd 1541710932.439434290 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 614, "value": 0.10915555555555556}

:::MLPv0.5.0 ssd 1541710932.534063578 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 615, "value": 0.10933333333333334}

:::MLPv0.5.0 ssd 1541710932.627615929 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 616, "value": 0.10951111111111111}

:::MLPv0.5.0 ssd 1541710932.722409964 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 617, "value": 0.10968888888888889}

:::MLPv0.5.0 ssd 1541710932.816415071 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 618, "value": 0.10986666666666667}

:::MLPv0.5.0 ssd 1541710932.912297964 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 619, "value": 0.11004444444444444}

:::MLPv0.5.0 ssd 1541710933.006059408 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 620, "value": 0.11022222222222222}
Iteration:    620, Loss function: 5.100, Average Loss: 3.629, avg. samples / sec: 21510.53
Iteration:    620, Loss function: 5.189, Average Loss: 3.628, avg. samples / sec: 21500.87
Iteration:    620, Loss function: 5.164, Average Loss: 3.641, avg. samples / sec: 21496.62
Iteration:    620, Loss function: 5.458, Average Loss: 3.632, avg. samples / sec: 21504.50
Iteration:    620, Loss function: 5.617, Average Loss: 3.630, avg. samples / sec: 21499.86
Iteration:    620, Loss function: 5.570, Average Loss: 3.627, avg. samples / sec: 21489.97
Iteration:    620, Loss function: 5.547, Average Loss: 3.629, avg. samples / sec: 21494.60
Iteration:    620, Loss function: 5.816, Average Loss: 3.633, avg. samples / sec: 21492.03

:::MLPv0.5.0 ssd 1541710933.100078344 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 621, "value": 0.1104}

:::MLPv0.5.0 ssd 1541710933.194372177 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 622, "value": 0.11057777777777777}

:::MLPv0.5.0 ssd 1541710933.291160107 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 623, "value": 0.11075555555555555}

:::MLPv0.5.0 ssd 1541710933.385237455 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 624, "value": 0.11093333333333333}

:::MLPv0.5.0 ssd 1541710933.478924274 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 625, "value": 0.1111111111111111}

:::MLPv0.5.0 ssd 1541710933.572962046 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 626, "value": 0.11128888888888888}

:::MLPv0.5.0 ssd 1541710933.667497635 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 627, "value": 0.11146666666666666}

:::MLPv0.5.0 ssd 1541710933.761829138 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 628, "value": 0.11164444444444445}

:::MLPv0.5.0 ssd 1541710933.856848478 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 629, "value": 0.11182222222222223}

:::MLPv0.5.0 ssd 1541710933.950495720 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 630, "value": 0.112}

:::MLPv0.5.0 ssd 1541710934.043833494 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 631, "value": 0.11217777777777778}

:::MLPv0.5.0 ssd 1541710934.139640093 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 632, "value": 0.11235555555555556}

:::MLPv0.5.0 ssd 1541710934.234685183 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 633, "value": 0.11253333333333333}

:::MLPv0.5.0 ssd 1541710934.329489470 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 634, "value": 0.11271111111111111}

:::MLPv0.5.0 ssd 1541710934.423953295 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 635, "value": 0.1128888888888889}

:::MLPv0.5.0 ssd 1541710934.514084101 (train.py:553) train_epoch: 11

:::MLPv0.5.0 ssd 1541710934.519073963 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 636, "value": 0.11306666666666668}

:::MLPv0.5.0 ssd 1541710934.613427639 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 637, "value": 0.11324444444444445}

:::MLPv0.5.0 ssd 1541710934.707201242 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 638, "value": 0.11342222222222223}

:::MLPv0.5.0 ssd 1541710934.801473856 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 639, "value": 0.1136}

:::MLPv0.5.0 ssd 1541710934.895706415 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 640, "value": 0.11377777777777778}
Iteration:    640, Loss function: 5.692, Average Loss: 3.670, avg. samples / sec: 21679.45
Iteration:    640, Loss function: 5.573, Average Loss: 3.665, avg. samples / sec: 21670.60
Iteration:    640, Loss function: 5.927, Average Loss: 3.678, avg. samples / sec: 21672.83
Iteration:    640, Loss function: 5.820, Average Loss: 3.666, avg. samples / sec: 21685.13
Iteration:    640, Loss function: 5.153, Average Loss: 3.671, avg. samples / sec: 21684.29
Iteration:    640, Loss function: 5.547, Average Loss: 3.669, avg. samples / sec: 21673.11
Iteration:    640, Loss function: 5.466, Average Loss: 3.665, avg. samples / sec: 21674.65
Iteration:    640, Loss function: 5.679, Average Loss: 3.665, avg. samples / sec: 21644.30

:::MLPv0.5.0 ssd 1541710934.991919041 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 641, "value": 0.11395555555555556}

:::MLPv0.5.0 ssd 1541710935.085923433 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 642, "value": 0.11413333333333334}

:::MLPv0.5.0 ssd 1541710935.179099798 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 643, "value": 0.11431111111111111}

:::MLPv0.5.0 ssd 1541710935.274484873 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 644, "value": 0.11448888888888889}

:::MLPv0.5.0 ssd 1541710935.368206739 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 645, "value": 0.11466666666666667}

:::MLPv0.5.0 ssd 1541710935.462828636 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 646, "value": 0.11484444444444444}

:::MLPv0.5.0 ssd 1541710935.556330919 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 647, "value": 0.11502222222222222}

:::MLPv0.5.0 ssd 1541710935.649717331 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 648, "value": 0.1152}

:::MLPv0.5.0 ssd 1541710935.742997169 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 649, "value": 0.11537777777777777}

:::MLPv0.5.0 ssd 1541710935.836829901 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 650, "value": 0.11555555555555555}

:::MLPv0.5.0 ssd 1541710935.930641890 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 651, "value": 0.11573333333333333}

:::MLPv0.5.0 ssd 1541710936.023996353 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 652, "value": 0.1159111111111111}

:::MLPv0.5.0 ssd 1541710936.118809938 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 653, "value": 0.11608888888888888}

:::MLPv0.5.0 ssd 1541710936.214499474 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 654, "value": 0.11626666666666667}

:::MLPv0.5.0 ssd 1541710936.308297157 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 655, "value": 0.11644444444444445}

:::MLPv0.5.0 ssd 1541710936.403393030 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 656, "value": 0.11662222222222222}

:::MLPv0.5.0 ssd 1541710936.496752977 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 657, "value": 0.1168}

:::MLPv0.5.0 ssd 1541710936.590111494 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 658, "value": 0.11697777777777778}

:::MLPv0.5.0 ssd 1541710936.685597181 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 659, "value": 0.11715555555555555}

:::MLPv0.5.0 ssd 1541710936.781002760 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 660, "value": 0.11733333333333333}
Iteration:    660, Loss function: 5.677, Average Loss: 3.701, avg. samples / sec: 21759.00
Iteration:    660, Loss function: 5.014, Average Loss: 3.703, avg. samples / sec: 21735.36
Iteration:    660, Loss function: 5.064, Average Loss: 3.706, avg. samples / sec: 21724.20
Iteration:    660, Loss function: 4.921, Average Loss: 3.699, avg. samples / sec: 21726.59
Iteration:    660, Loss function: 5.235, Average Loss: 3.705, avg. samples / sec: 21726.57
Iteration:    660, Loss function: 5.433, Average Loss: 3.703, avg. samples / sec: 21722.76
Iteration:    660, Loss function: 5.088, Average Loss: 3.701, avg. samples / sec: 21724.24
Iteration:    660, Loss function: 5.514, Average Loss: 3.713, avg. samples / sec: 21708.03

:::MLPv0.5.0 ssd 1541710936.876897097 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 661, "value": 0.11751111111111112}

:::MLPv0.5.0 ssd 1541710936.971354008 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 662, "value": 0.1176888888888889}

:::MLPv0.5.0 ssd 1541710937.065197706 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 663, "value": 0.11786666666666668}

:::MLPv0.5.0 ssd 1541710937.159141302 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 664, "value": 0.11804444444444445}

:::MLPv0.5.0 ssd 1541710937.253532410 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 665, "value": 0.11822222222222223}

:::MLPv0.5.0 ssd 1541710937.347710371 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 666, "value": 0.1184}

:::MLPv0.5.0 ssd 1541710937.443679333 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 667, "value": 0.11857777777777778}

:::MLPv0.5.0 ssd 1541710937.538440228 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 668, "value": 0.11875555555555556}

:::MLPv0.5.0 ssd 1541710937.636768579 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 669, "value": 0.11893333333333334}

:::MLPv0.5.0 ssd 1541710937.730307817 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 670, "value": 0.11911111111111111}

:::MLPv0.5.0 ssd 1541710937.823974609 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 671, "value": 0.11928888888888889}

:::MLPv0.5.0 ssd 1541710937.917243958 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 672, "value": 0.11946666666666667}

:::MLPv0.5.0 ssd 1541710938.011844397 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 673, "value": 0.11964444444444444}

:::MLPv0.5.0 ssd 1541710938.105990171 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 674, "value": 0.11982222222222222}

:::MLPv0.5.0 ssd 1541710938.200123549 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 675, "value": 0.12}

:::MLPv0.5.0 ssd 1541710938.294749022 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 676, "value": 0.12017777777777777}

:::MLPv0.5.0 ssd 1541710938.390062809 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 677, "value": 0.12035555555555555}

:::MLPv0.5.0 ssd 1541710938.484565735 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 678, "value": 0.12053333333333333}

:::MLPv0.5.0 ssd 1541710938.577996016 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 679, "value": 0.1207111111111111}

:::MLPv0.5.0 ssd 1541710938.676741838 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 680, "value": 0.12088888888888889}
Iteration:    680, Loss function: 6.351, Average Loss: 3.736, avg. samples / sec: 21609.83
Iteration:    680, Loss function: 5.319, Average Loss: 3.752, avg. samples / sec: 21626.98
Iteration:    680, Loss function: 6.153, Average Loss: 3.742, avg. samples / sec: 21609.63
Iteration:    680, Loss function: 6.115, Average Loss: 3.741, avg. samples / sec: 21607.15
Iteration:    680, Loss function: 5.428, Average Loss: 3.742, avg. samples / sec: 21601.76
Iteration:    680, Loss function: 5.553, Average Loss: 3.744, avg. samples / sec: 21599.56
Iteration:    680, Loss function: 5.981, Average Loss: 3.740, avg. samples / sec: 21577.61
Iteration:    680, Loss function: 5.723, Average Loss: 3.739, avg. samples / sec: 21590.45

:::MLPv0.5.0 ssd 1541710938.771562338 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 681, "value": 0.12106666666666667}

:::MLPv0.5.0 ssd 1541710938.866585970 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 682, "value": 0.12124444444444445}

:::MLPv0.5.0 ssd 1541710938.961805582 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 683, "value": 0.12142222222222222}

:::MLPv0.5.0 ssd 1541710939.056813478 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 684, "value": 0.1216}

:::MLPv0.5.0 ssd 1541710939.150680065 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 685, "value": 0.12177777777777778}

:::MLPv0.5.0 ssd 1541710939.248294592 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 686, "value": 0.12195555555555557}

:::MLPv0.5.0 ssd 1541710939.342544317 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 687, "value": 0.12213333333333334}

:::MLPv0.5.0 ssd 1541710939.438037395 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 688, "value": 0.12231111111111112}

:::MLPv0.5.0 ssd 1541710939.533779860 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 689, "value": 0.1224888888888889}

:::MLPv0.5.0 ssd 1541710939.628879070 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 690, "value": 0.12266666666666667}

:::MLPv0.5.0 ssd 1541710939.722682714 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 691, "value": 0.12284444444444445}

:::MLPv0.5.0 ssd 1541710939.816951036 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 692, "value": 0.12302222222222223}

:::MLPv0.5.0 ssd 1541710939.911548138 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 693, "value": 0.1232}

:::MLPv0.5.0 ssd 1541710940.002241373 (train.py:553) train_epoch: 12

:::MLPv0.5.0 ssd 1541710940.007740021 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 694, "value": 0.12337777777777778}

:::MLPv0.5.0 ssd 1541710940.102308750 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 695, "value": 0.12355555555555556}

:::MLPv0.5.0 ssd 1541710940.197981596 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 696, "value": 0.12373333333333333}

:::MLPv0.5.0 ssd 1541710940.294825315 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 697, "value": 0.12391111111111111}

:::MLPv0.5.0 ssd 1541710940.389198542 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 698, "value": 0.12408888888888889}

:::MLPv0.5.0 ssd 1541710940.484541416 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 699, "value": 0.12426666666666666}

:::MLPv0.5.0 ssd 1541710940.580870152 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 700, "value": 0.12444444444444444}
Iteration:    700, Loss function: 5.867, Average Loss: 3.776, avg. samples / sec: 21518.39
Iteration:    700, Loss function: 5.843, Average Loss: 3.775, avg. samples / sec: 21543.83
Iteration:    700, Loss function: 5.533, Average Loss: 3.778, avg. samples / sec: 21522.90
Iteration:    700, Loss function: 5.060, Average Loss: 3.787, avg. samples / sec: 21513.19
Iteration:    700, Loss function: 5.743, Average Loss: 3.778, avg. samples / sec: 21517.06
Iteration:    700, Loss function: 5.334, Average Loss: 3.774, avg. samples / sec: 21515.53
Iteration:    700, Loss function: 5.248, Average Loss: 3.769, avg. samples / sec: 21508.08
Iteration:    700, Loss function: 5.553, Average Loss: 3.774, avg. samples / sec: 21533.87

:::MLPv0.5.0 ssd 1541710940.677378416 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 701, "value": 0.12462222222222222}

:::MLPv0.5.0 ssd 1541710940.772004366 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 702, "value": 0.1248}

:::MLPv0.5.0 ssd 1541710940.866498709 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 703, "value": 0.12497777777777777}

:::MLPv0.5.0 ssd 1541710940.960199833 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 704, "value": 0.12515555555555555}

:::MLPv0.5.0 ssd 1541710941.055480480 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 705, "value": 0.12533333333333335}

:::MLPv0.5.0 ssd 1541710941.149969578 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 706, "value": 0.12551111111111113}

:::MLPv0.5.0 ssd 1541710941.244194746 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 707, "value": 0.1256888888888889}

:::MLPv0.5.0 ssd 1541710941.338627338 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 708, "value": 0.12586666666666668}

:::MLPv0.5.0 ssd 1541710941.433012962 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 709, "value": 0.12604444444444446}

:::MLPv0.5.0 ssd 1541710941.529608250 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 710, "value": 0.12622222222222224}

:::MLPv0.5.0 ssd 1541710941.624382973 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 711, "value": 0.1264}

:::MLPv0.5.0 ssd 1541710941.718708515 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 712, "value": 0.1265777777777778}

:::MLPv0.5.0 ssd 1541710941.812986851 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 713, "value": 0.12675555555555557}

:::MLPv0.5.0 ssd 1541710941.906908751 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 714, "value": 0.12693333333333334}

:::MLPv0.5.0 ssd 1541710942.001295328 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 715, "value": 0.12711111111111112}

:::MLPv0.5.0 ssd 1541710942.096415997 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 716, "value": 0.1272888888888889}

:::MLPv0.5.0 ssd 1541710942.190965176 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 717, "value": 0.12746666666666667}

:::MLPv0.5.0 ssd 1541710942.285246611 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 718, "value": 0.12764444444444445}

:::MLPv0.5.0 ssd 1541710942.388388157 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 719, "value": 0.12782222222222223}

:::MLPv0.5.0 ssd 1541710942.484219790 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 720, "value": 0.128}
Iteration:    720, Loss function: 4.686, Average Loss: 3.817, avg. samples / sec: 21524.90
Iteration:    720, Loss function: 5.594, Average Loss: 3.811, avg. samples / sec: 21521.44
Iteration:    720, Loss function: 4.743, Average Loss: 3.805, avg. samples / sec: 21515.25
Iteration:    720, Loss function: 5.486, Average Loss: 3.807, avg. samples / sec: 21513.39
Iteration:    720, Loss function: 5.586, Average Loss: 3.809, avg. samples / sec: 21516.76
Iteration:    720, Loss function: 5.501, Average Loss: 3.800, avg. samples / sec: 21518.65
Iteration:    720, Loss function: 5.477, Average Loss: 3.807, avg. samples / sec: 21517.02
Iteration:    720, Loss function: 5.247, Average Loss: 3.806, avg. samples / sec: 21516.90

:::MLPv0.5.0 ssd 1541710942.578617334 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 721, "value": 0.12817777777777778}

:::MLPv0.5.0 ssd 1541710942.674619675 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 722, "value": 0.12835555555555556}

:::MLPv0.5.0 ssd 1541710942.768070459 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 723, "value": 0.12853333333333333}

:::MLPv0.5.0 ssd 1541710942.863353491 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 724, "value": 0.1287111111111111}

:::MLPv0.5.0 ssd 1541710942.959128618 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 725, "value": 0.1288888888888889}

:::MLPv0.5.0 ssd 1541710943.053385496 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 726, "value": 0.12906666666666666}

:::MLPv0.5.0 ssd 1541710943.147350073 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 727, "value": 0.12924444444444444}

:::MLPv0.5.0 ssd 1541710943.242190599 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 728, "value": 0.12942222222222222}

:::MLPv0.5.0 ssd 1541710943.335878372 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 729, "value": 0.1296}

:::MLPv0.5.0 ssd 1541710943.430560827 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 730, "value": 0.12977777777777777}

:::MLPv0.5.0 ssd 1541710943.524690866 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 731, "value": 0.12995555555555555}

:::MLPv0.5.0 ssd 1541710943.619923115 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 732, "value": 0.13013333333333332}

:::MLPv0.5.0 ssd 1541710943.714710712 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 733, "value": 0.1303111111111111}

:::MLPv0.5.0 ssd 1541710943.808794260 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 734, "value": 0.13048888888888888}

:::MLPv0.5.0 ssd 1541710943.902699232 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 735, "value": 0.13066666666666665}

:::MLPv0.5.0 ssd 1541710943.996599197 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 736, "value": 0.13084444444444446}

:::MLPv0.5.0 ssd 1541710944.090210915 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 737, "value": 0.13102222222222223}

:::MLPv0.5.0 ssd 1541710944.184732676 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 738, "value": 0.1312}

:::MLPv0.5.0 ssd 1541710944.279863596 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 739, "value": 0.1313777777777778}

:::MLPv0.5.0 ssd 1541710944.374483109 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 740, "value": 0.13155555555555556}
Iteration:    740, Loss function: 5.404, Average Loss: 3.846, avg. samples / sec: 21670.34
Iteration:    740, Loss function: 5.013, Average Loss: 3.829, avg. samples / sec: 21677.55
Iteration:    740, Loss function: 4.955, Average Loss: 3.839, avg. samples / sec: 21673.63
Iteration:    740, Loss function: 5.283, Average Loss: 3.842, avg. samples / sec: 21667.77
Iteration:    740, Loss function: 4.987, Average Loss: 3.837, avg. samples / sec: 21671.57
Iteration:    740, Loss function: 5.762, Average Loss: 3.836, avg. samples / sec: 21667.03
Iteration:    740, Loss function: 5.553, Average Loss: 3.836, avg. samples / sec: 21662.39
Iteration:    740, Loss function: 5.850, Average Loss: 3.835, avg. samples / sec: 21667.90

:::MLPv0.5.0 ssd 1541710944.470922947 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 741, "value": 0.13173333333333334}

:::MLPv0.5.0 ssd 1541710944.565082312 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 742, "value": 0.13191111111111112}

:::MLPv0.5.0 ssd 1541710944.659677982 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 743, "value": 0.1320888888888889}

:::MLPv0.5.0 ssd 1541710944.753803253 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 744, "value": 0.13226666666666667}

:::MLPv0.5.0 ssd 1541710944.848707438 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 745, "value": 0.13244444444444445}

:::MLPv0.5.0 ssd 1541710944.945184708 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 746, "value": 0.13262222222222222}

:::MLPv0.5.0 ssd 1541710945.040483952 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 747, "value": 0.1328}

:::MLPv0.5.0 ssd 1541710945.136508703 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 748, "value": 0.13297777777777778}

:::MLPv0.5.0 ssd 1541710945.229832411 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 749, "value": 0.13315555555555555}

:::MLPv0.5.0 ssd 1541710945.325908184 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 750, "value": 0.13333333333333333}

:::MLPv0.5.0 ssd 1541710945.415870190 (train.py:553) train_epoch: 13

:::MLPv0.5.0 ssd 1541710945.422484636 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 751, "value": 0.1335111111111111}

:::MLPv0.5.0 ssd 1541710945.517240763 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 752, "value": 0.13368888888888888}

:::MLPv0.5.0 ssd 1541710945.612072468 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 753, "value": 0.13386666666666666}

:::MLPv0.5.0 ssd 1541710945.706595659 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 754, "value": 0.13404444444444444}

:::MLPv0.5.0 ssd 1541710945.800738096 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 755, "value": 0.13422222222222221}

:::MLPv0.5.0 ssd 1541710945.894682169 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 756, "value": 0.1344}

:::MLPv0.5.0 ssd 1541710945.992378473 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 757, "value": 0.13457777777777777}

:::MLPv0.5.0 ssd 1541710946.087123871 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 758, "value": 0.13475555555555557}

:::MLPv0.5.0 ssd 1541710946.182026148 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 759, "value": 0.13493333333333335}

:::MLPv0.5.0 ssd 1541710946.279170275 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 760, "value": 0.13511111111111113}
Iteration:    760, Loss function: 4.851, Average Loss: 3.863, avg. samples / sec: 21520.96
Iteration:    760, Loss function: 4.945, Average Loss: 3.856, avg. samples / sec: 21503.38
Iteration:    760, Loss function: 4.683, Average Loss: 3.874, avg. samples / sec: 21501.88
Iteration:    760, Loss function: 4.408, Average Loss: 3.868, avg. samples / sec: 21507.16
Iteration:    760, Loss function: 5.187, Average Loss: 3.866, avg. samples / sec: 21503.25
Iteration:    760, Loss function: 5.229, Average Loss: 3.864, avg. samples / sec: 21505.56
Iteration:    760, Loss function: 4.844, Average Loss: 3.866, avg. samples / sec: 21505.64
Iteration:    760, Loss function: 5.240, Average Loss: 3.865, avg. samples / sec: 21505.14

:::MLPv0.5.0 ssd 1541710946.374158382 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 761, "value": 0.1352888888888889}

:::MLPv0.5.0 ssd 1541710946.469010115 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 762, "value": 0.13546666666666668}

:::MLPv0.5.0 ssd 1541710946.563346386 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 763, "value": 0.13564444444444446}

:::MLPv0.5.0 ssd 1541710946.657676458 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 764, "value": 0.13582222222222223}

:::MLPv0.5.0 ssd 1541710946.751888037 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 765, "value": 0.136}

:::MLPv0.5.0 ssd 1541710946.846365690 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 766, "value": 0.1361777777777778}

:::MLPv0.5.0 ssd 1541710946.940871477 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 767, "value": 0.13635555555555556}

:::MLPv0.5.0 ssd 1541710947.034912825 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 768, "value": 0.13653333333333334}

:::MLPv0.5.0 ssd 1541710947.129379749 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 769, "value": 0.13671111111111112}

:::MLPv0.5.0 ssd 1541710947.223440886 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 770, "value": 0.1368888888888889}

:::MLPv0.5.0 ssd 1541710947.317968130 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 771, "value": 0.13706666666666667}

:::MLPv0.5.0 ssd 1541710947.412596226 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 772, "value": 0.13724444444444445}

:::MLPv0.5.0 ssd 1541710947.507344723 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 773, "value": 0.13742222222222222}

:::MLPv0.5.0 ssd 1541710947.601622343 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 774, "value": 0.1376}

:::MLPv0.5.0 ssd 1541710947.696282387 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 775, "value": 0.13777777777777778}

:::MLPv0.5.0 ssd 1541710947.791521788 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 776, "value": 0.13795555555555555}

:::MLPv0.5.0 ssd 1541710947.886972427 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 777, "value": 0.13813333333333333}

:::MLPv0.5.0 ssd 1541710947.981207371 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 778, "value": 0.1383111111111111}

:::MLPv0.5.0 ssd 1541710948.075590134 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 779, "value": 0.13848888888888888}

:::MLPv0.5.0 ssd 1541710948.170739889 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 780, "value": 0.13866666666666666}
Iteration:    780, Loss function: 5.584, Average Loss: 3.898, avg. samples / sec: 21657.75
Iteration:    780, Loss function: 4.888, Average Loss: 3.892, avg. samples / sec: 21659.73
Iteration:    780, Loss function: 4.827, Average Loss: 3.881, avg. samples / sec: 21655.28
Iteration:    780, Loss function: 5.269, Average Loss: 3.891, avg. samples / sec: 21659.82
Iteration:    780, Loss function: 4.847, Average Loss: 3.891, avg. samples / sec: 21655.37
Iteration:    780, Loss function: 4.918, Average Loss: 3.886, avg. samples / sec: 21641.89
Iteration:    780, Loss function: 4.671, Average Loss: 3.888, avg. samples / sec: 21653.50
Iteration:    780, Loss function: 4.981, Average Loss: 3.891, avg. samples / sec: 21659.09

:::MLPv0.5.0 ssd 1541710948.265592098 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 781, "value": 0.13884444444444444}

:::MLPv0.5.0 ssd 1541710948.360988617 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 782, "value": 0.1390222222222222}

:::MLPv0.5.0 ssd 1541710948.456962824 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 783, "value": 0.1392}

:::MLPv0.5.0 ssd 1541710948.551635265 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 784, "value": 0.13937777777777777}

:::MLPv0.5.0 ssd 1541710948.646533489 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 785, "value": 0.13955555555555554}

:::MLPv0.5.0 ssd 1541710948.740516901 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 786, "value": 0.13973333333333332}

:::MLPv0.5.0 ssd 1541710948.834883213 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 787, "value": 0.13991111111111112}

:::MLPv0.5.0 ssd 1541710948.930490971 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 788, "value": 0.1400888888888889}

:::MLPv0.5.0 ssd 1541710949.024815559 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 789, "value": 0.14026666666666668}

:::MLPv0.5.0 ssd 1541710949.119082212 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 790, "value": 0.14044444444444446}

:::MLPv0.5.0 ssd 1541710949.213594675 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 791, "value": 0.14062222222222223}

:::MLPv0.5.0 ssd 1541710949.309038639 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 792, "value": 0.1408}

:::MLPv0.5.0 ssd 1541710949.403507471 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 793, "value": 0.14097777777777779}

:::MLPv0.5.0 ssd 1541710949.499346495 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 794, "value": 0.14115555555555556}

:::MLPv0.5.0 ssd 1541710949.594809294 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 795, "value": 0.14133333333333334}

:::MLPv0.5.0 ssd 1541710949.690311432 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 796, "value": 0.14151111111111112}

:::MLPv0.5.0 ssd 1541710949.785240889 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 797, "value": 0.1416888888888889}

:::MLPv0.5.0 ssd 1541710949.879541397 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 798, "value": 0.14186666666666667}

:::MLPv0.5.0 ssd 1541710949.973637104 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 799, "value": 0.14204444444444445}

:::MLPv0.5.0 ssd 1541710950.068003893 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 800, "value": 0.14222222222222222}
Iteration:    800, Loss function: 6.426, Average Loss: 3.925, avg. samples / sec: 21587.14
Iteration:    800, Loss function: 5.235, Average Loss: 3.912, avg. samples / sec: 21595.00
Iteration:    800, Loss function: 5.292, Average Loss: 3.915, avg. samples / sec: 21588.42
Iteration:    800, Loss function: 5.457, Average Loss: 3.918, avg. samples / sec: 21582.80
Iteration:    800, Loss function: 5.618, Average Loss: 3.915, avg. samples / sec: 21592.65
Iteration:    800, Loss function: 5.093, Average Loss: 3.905, avg. samples / sec: 21581.63
Iteration:    800, Loss function: 5.610, Average Loss: 3.917, avg. samples / sec: 21580.24
Iteration:    800, Loss function: 5.749, Average Loss: 3.918, avg. samples / sec: 21587.04

:::MLPv0.5.0 ssd 1541710950.164174795 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 801, "value": 0.1424}

:::MLPv0.5.0 ssd 1541710950.258927584 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 802, "value": 0.14257777777777778}

:::MLPv0.5.0 ssd 1541710950.352280855 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 803, "value": 0.14275555555555555}

:::MLPv0.5.0 ssd 1541710950.446585178 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 804, "value": 0.14293333333333333}

:::MLPv0.5.0 ssd 1541710950.543063641 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 805, "value": 0.1431111111111111}

:::MLPv0.5.0 ssd 1541710950.638095379 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 806, "value": 0.14328888888888888}

:::MLPv0.5.0 ssd 1541710950.732432604 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 807, "value": 0.14346666666666666}

:::MLPv0.5.0 ssd 1541710950.826855421 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 808, "value": 0.14364444444444444}

:::MLPv0.5.0 ssd 1541710950.918080807 (train.py:553) train_epoch: 14

:::MLPv0.5.0 ssd 1541710950.923552513 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 809, "value": 0.14382222222222224}

:::MLPv0.5.0 ssd 1541710951.018907785 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 810, "value": 0.14400000000000002}

:::MLPv0.5.0 ssd 1541710951.113434553 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 811, "value": 0.1441777777777778}

:::MLPv0.5.0 ssd 1541710951.207826138 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 812, "value": 0.14435555555555557}

:::MLPv0.5.0 ssd 1541710951.302563667 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 813, "value": 0.14453333333333335}

:::MLPv0.5.0 ssd 1541710951.397945642 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 814, "value": 0.14471111111111112}

:::MLPv0.5.0 ssd 1541710951.492810488 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 815, "value": 0.1448888888888889}

:::MLPv0.5.0 ssd 1541710951.587477922 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 816, "value": 0.14506666666666668}

:::MLPv0.5.0 ssd 1541710951.682001829 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 817, "value": 0.14524444444444445}

:::MLPv0.5.0 ssd 1541710951.777270555 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 818, "value": 0.14542222222222223}

:::MLPv0.5.0 ssd 1541710951.871543407 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 819, "value": 0.1456}

:::MLPv0.5.0 ssd 1541710951.966616631 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 820, "value": 0.14577777777777778}
Iteration:    820, Loss function: 5.566, Average Loss: 3.947, avg. samples / sec: 21582.59
Iteration:    820, Loss function: 4.943, Average Loss: 3.954, avg. samples / sec: 21575.35
Iteration:    820, Loss function: 4.534, Average Loss: 3.949, avg. samples / sec: 21583.11
Iteration:    820, Loss function: 4.853, Average Loss: 3.942, avg. samples / sec: 21573.01
Iteration:    820, Loss function: 5.282, Average Loss: 3.950, avg. samples / sec: 21584.40
Iteration:    820, Loss function: 4.897, Average Loss: 3.945, avg. samples / sec: 21573.11
Iteration:    820, Loss function: 5.343, Average Loss: 3.948, avg. samples / sec: 21574.03
Iteration:    820, Loss function: 5.497, Average Loss: 3.937, avg. samples / sec: 21575.40

:::MLPv0.5.0 ssd 1541710952.061249495 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 821, "value": 0.14595555555555556}

:::MLPv0.5.0 ssd 1541710952.155846834 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 822, "value": 0.14613333333333334}

:::MLPv0.5.0 ssd 1541710952.250313759 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 823, "value": 0.14631111111111111}

:::MLPv0.5.0 ssd 1541710952.344623804 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 824, "value": 0.1464888888888889}

:::MLPv0.5.0 ssd 1541710952.439798594 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 825, "value": 0.14666666666666667}

:::MLPv0.5.0 ssd 1541710952.533175707 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 826, "value": 0.14684444444444444}

:::MLPv0.5.0 ssd 1541710952.630002499 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 827, "value": 0.14702222222222222}

:::MLPv0.5.0 ssd 1541710952.724119663 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 828, "value": 0.1472}

:::MLPv0.5.0 ssd 1541710952.819057941 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 829, "value": 0.14737777777777777}

:::MLPv0.5.0 ssd 1541710952.913132429 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 830, "value": 0.14755555555555555}

:::MLPv0.5.0 ssd 1541710953.007320642 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 831, "value": 0.14773333333333333}

:::MLPv0.5.0 ssd 1541710953.103134632 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 832, "value": 0.1479111111111111}

:::MLPv0.5.0 ssd 1541710953.197652578 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 833, "value": 0.14808888888888888}

:::MLPv0.5.0 ssd 1541710953.291888475 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 834, "value": 0.14826666666666666}

:::MLPv0.5.0 ssd 1541710953.386295080 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 835, "value": 0.14844444444444443}

:::MLPv0.5.0 ssd 1541710953.481128454 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 836, "value": 0.1486222222222222}

:::MLPv0.5.0 ssd 1541710953.575211048 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 837, "value": 0.14880000000000002}

:::MLPv0.5.0 ssd 1541710953.669773340 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 838, "value": 0.1489777777777778}

:::MLPv0.5.0 ssd 1541710953.763929129 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 839, "value": 0.14915555555555557}

:::MLPv0.5.0 ssd 1541710953.859456301 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 840, "value": 0.14933333333333335}
Iteration:    840, Loss function: 4.999, Average Loss: 3.974, avg. samples / sec: 21642.82
Iteration:    840, Loss function: 4.754, Average Loss: 3.962, avg. samples / sec: 21640.13
Iteration:    840, Loss function: 5.058, Average Loss: 3.968, avg. samples / sec: 21632.95
Iteration:    840, Loss function: 5.012, Average Loss: 3.972, avg. samples / sec: 21638.47
Iteration:    840, Loss function: 4.873, Average Loss: 3.966, avg. samples / sec: 21639.77
Iteration:    840, Loss function: 4.975, Average Loss: 3.969, avg. samples / sec: 21639.46
Iteration:    840, Loss function: 4.635, Average Loss: 3.971, avg. samples / sec: 21636.52
Iteration:    840, Loss function: 5.150, Average Loss: 3.959, avg. samples / sec: 21633.95

:::MLPv0.5.0 ssd 1541710953.953001738 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 841, "value": 0.14951111111111112}

:::MLPv0.5.0 ssd 1541710954.047911882 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 842, "value": 0.1496888888888889}

:::MLPv0.5.0 ssd 1541710954.142422438 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 843, "value": 0.14986666666666668}

:::MLPv0.5.0 ssd 1541710954.236075401 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 844, "value": 0.15004444444444445}

:::MLPv0.5.0 ssd 1541710954.331683874 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 845, "value": 0.15022222222222223}

:::MLPv0.5.0 ssd 1541710954.425906420 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 846, "value": 0.1504}

:::MLPv0.5.0 ssd 1541710954.519015074 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 847, "value": 0.15057777777777778}

:::MLPv0.5.0 ssd 1541710954.614017725 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 848, "value": 0.15075555555555556}

:::MLPv0.5.0 ssd 1541710954.708148956 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 849, "value": 0.15093333333333334}

:::MLPv0.5.0 ssd 1541710954.805946589 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 850, "value": 0.1511111111111111}

:::MLPv0.5.0 ssd 1541710954.900161028 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 851, "value": 0.1512888888888889}

:::MLPv0.5.0 ssd 1541710954.994265079 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 852, "value": 0.15146666666666667}

:::MLPv0.5.0 ssd 1541710955.089348316 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 853, "value": 0.15164444444444444}

:::MLPv0.5.0 ssd 1541710955.183884621 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 854, "value": 0.15182222222222222}

:::MLPv0.5.0 ssd 1541710955.277950525 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 855, "value": 0.152}

:::MLPv0.5.0 ssd 1541710955.372867346 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 856, "value": 0.15217777777777777}

:::MLPv0.5.0 ssd 1541710955.468667984 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 857, "value": 0.15235555555555555}

:::MLPv0.5.0 ssd 1541710955.563298225 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 858, "value": 0.15253333333333333}

:::MLPv0.5.0 ssd 1541710955.657231808 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 859, "value": 0.1527111111111111}

:::MLPv0.5.0 ssd 1541710955.752772808 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 860, "value": 0.15288888888888888}
Iteration:    860, Loss function: 4.804, Average Loss: 4.000, avg. samples / sec: 21632.94
Iteration:    860, Loss function: 4.617, Average Loss: 3.998, avg. samples / sec: 21635.71
Iteration:    860, Loss function: 4.743, Average Loss: 3.994, avg. samples / sec: 21635.62
Iteration:    860, Loss function: 5.634, Average Loss: 3.990, avg. samples / sec: 21636.39
Iteration:    860, Loss function: 5.410, Average Loss: 3.983, avg. samples / sec: 21642.75
Iteration:    860, Loss function: 4.830, Average Loss: 3.989, avg. samples / sec: 21632.34
Iteration:    860, Loss function: 5.128, Average Loss: 3.997, avg. samples / sec: 21631.36
Iteration:    860, Loss function: 5.160, Average Loss: 3.993, avg. samples / sec: 21626.65

:::MLPv0.5.0 ssd 1541710955.849680901 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 861, "value": 0.15306666666666666}

:::MLPv0.5.0 ssd 1541710955.945431471 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 862, "value": 0.15324444444444446}

:::MLPv0.5.0 ssd 1541710956.039546013 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 863, "value": 0.15342222222222224}

:::MLPv0.5.0 ssd 1541710956.133888721 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 864, "value": 0.15360000000000001}

:::MLPv0.5.0 ssd 1541710956.229136944 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 865, "value": 0.1537777777777778}

:::MLPv0.5.0 ssd 1541710956.324278593 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 866, "value": 0.15395555555555557}

:::MLPv0.5.0 ssd 1541710956.414958000 (train.py:553) train_epoch: 15

:::MLPv0.5.0 ssd 1541710956.421441078 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 867, "value": 0.15413333333333334}

:::MLPv0.5.0 ssd 1541710956.515588760 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 868, "value": 0.15431111111111112}

:::MLPv0.5.0 ssd 1541710956.610930204 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 869, "value": 0.1544888888888889}

:::MLPv0.5.0 ssd 1541710956.707489491 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 870, "value": 0.15466666666666667}

:::MLPv0.5.0 ssd 1541710956.801728964 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 871, "value": 0.15484444444444445}

:::MLPv0.5.0 ssd 1541710956.895417452 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 872, "value": 0.15502222222222223}

:::MLPv0.5.0 ssd 1541710956.993087053 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 873, "value": 0.1552}

:::MLPv0.5.0 ssd 1541710957.088205814 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 874, "value": 0.15537777777777778}

:::MLPv0.5.0 ssd 1541710957.182761192 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 875, "value": 0.15555555555555556}

:::MLPv0.5.0 ssd 1541710957.278401852 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 876, "value": 0.15573333333333333}

:::MLPv0.5.0 ssd 1541710957.374644995 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 877, "value": 0.1559111111111111}

:::MLPv0.5.0 ssd 1541710957.469799995 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 878, "value": 0.1560888888888889}

:::MLPv0.5.0 ssd 1541710957.565019846 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 879, "value": 0.15626666666666666}

:::MLPv0.5.0 ssd 1541710957.659122944 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 880, "value": 0.15644444444444444}
Iteration:    880, Loss function: 4.366, Average Loss: 4.013, avg. samples / sec: 21494.60
Iteration:    880, Loss function: 4.717, Average Loss: 4.009, avg. samples / sec: 21496.37
Iteration:    880, Loss function: 5.151, Average Loss: 4.012, avg. samples / sec: 21493.49
Iteration:    880, Loss function: 4.637, Average Loss: 4.020, avg. samples / sec: 21484.14
Iteration:    880, Loss function: 5.016, Average Loss: 4.014, avg. samples / sec: 21497.50
Iteration:    880, Loss function: 4.689, Average Loss: 4.006, avg. samples / sec: 21486.94
Iteration:    880, Loss function: 5.054, Average Loss: 4.016, avg. samples / sec: 21484.96
Iteration:    880, Loss function: 5.030, Average Loss: 4.020, avg. samples / sec: 21485.51

:::MLPv0.5.0 ssd 1541710957.754487991 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 881, "value": 0.15662222222222222}

:::MLPv0.5.0 ssd 1541710957.855172634 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 882, "value": 0.1568}

:::MLPv0.5.0 ssd 1541710957.949682951 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 883, "value": 0.15697777777777777}

:::MLPv0.5.0 ssd 1541710958.043754578 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 884, "value": 0.15715555555555555}

:::MLPv0.5.0 ssd 1541710958.137681723 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 885, "value": 0.15733333333333333}

:::MLPv0.5.0 ssd 1541710958.232416630 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 886, "value": 0.1575111111111111}

:::MLPv0.5.0 ssd 1541710958.327080488 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 887, "value": 0.15768888888888888}

:::MLPv0.5.0 ssd 1541710958.421863079 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 888, "value": 0.15786666666666668}

:::MLPv0.5.0 ssd 1541710958.518825769 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 889, "value": 0.15804444444444446}

:::MLPv0.5.0 ssd 1541710958.622683048 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 890, "value": 0.15822222222222224}

:::MLPv0.5.0 ssd 1541710958.716071844 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 891, "value": 0.1584}

:::MLPv0.5.0 ssd 1541710958.809989691 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 892, "value": 0.1585777777777778}

:::MLPv0.5.0 ssd 1541710958.904339790 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 893, "value": 0.15875555555555557}

:::MLPv0.5.0 ssd 1541710958.999469995 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 894, "value": 0.15893333333333334}

:::MLPv0.5.0 ssd 1541710959.094009638 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 895, "value": 0.15911111111111112}

:::MLPv0.5.0 ssd 1541710959.189807892 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 896, "value": 0.1592888888888889}

:::MLPv0.5.0 ssd 1541710959.285002947 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 897, "value": 0.15946666666666667}

:::MLPv0.5.0 ssd 1541710959.379712582 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 898, "value": 0.15964444444444445}

:::MLPv0.5.0 ssd 1541710959.476343632 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 899, "value": 0.15982222222222223}
Iteration:    900, Loss function: 5.832, Average Loss: 4.035, avg. samples / sec: 21446.49
Iteration:    900, Loss function: 6.544, Average Loss: 4.045, avg. samples / sec: 21455.08
Iteration:    900, Loss function: 5.013, Average Loss: 4.030, avg. samples / sec: 21436.57
Iteration:    900, Loss function: 5.790, Average Loss: 4.034, avg. samples / sec: 21435.50
Iteration:    900, Loss function: 5.696, Average Loss: 4.043, avg. samples / sec: 21439.57
Iteration:    900, Loss function: 5.452, Average Loss: 4.027, avg. samples / sec: 21441.58
Iteration:    900, Loss function: 5.662, Average Loss: 4.036, avg. samples / sec: 21440.98
Iteration:    900, Loss function: 5.846, Average Loss: 4.038, avg. samples / sec: 21434.37
Iteration:    920, Loss function: 4.928, Average Loss: 4.072, avg. samples / sec: 21899.09
Iteration:    920, Loss function: 5.406, Average Loss: 4.077, avg. samples / sec: 21906.53
Iteration:    920, Loss function: 5.265, Average Loss: 4.080, avg. samples / sec: 21900.56
Iteration:    920, Loss function: 5.388, Average Loss: 4.066, avg. samples / sec: 21903.90
Iteration:    920, Loss function: 5.732, Average Loss: 4.066, avg. samples / sec: 21900.89
Iteration:    920, Loss function: 5.368, Average Loss: 4.062, avg. samples / sec: 21903.16
Iteration:    920, Loss function: 6.037, Average Loss: 4.072, avg. samples / sec: 21903.71
Iteration:    920, Loss function: 5.099, Average Loss: 4.073, avg. samples / sec: 21905.84

:::MLPv0.5.0 ssd 1541710961.903478622 (train.py:553) train_epoch: 16
Iteration:    940, Loss function: 5.091, Average Loss: 4.087, avg. samples / sec: 21887.72
Iteration:    940, Loss function: 4.898, Average Loss: 4.097, avg. samples / sec: 21885.04
Iteration:    940, Loss function: 5.386, Average Loss: 4.091, avg. samples / sec: 21876.76
Iteration:    940, Loss function: 5.600, Average Loss: 4.088, avg. samples / sec: 21885.52
Iteration:    940, Loss function: 5.329, Average Loss: 4.093, avg. samples / sec: 21891.17
Iteration:    940, Loss function: 5.335, Average Loss: 4.100, avg. samples / sec: 21882.27
Iteration:    940, Loss function: 5.052, Average Loss: 4.094, avg. samples / sec: 21884.36
Iteration:    940, Loss function: 4.853, Average Loss: 4.085, avg. samples / sec: 21878.77
Iteration:    960, Loss function: 4.841, Average Loss: 4.102, avg. samples / sec: 21884.00
Iteration:    960, Loss function: 4.522, Average Loss: 4.107, avg. samples / sec: 21886.44
Iteration:    960, Loss function: 4.921, Average Loss: 4.101, avg. samples / sec: 21891.18
Iteration:    960, Loss function: 5.511, Average Loss: 4.102, avg. samples / sec: 21880.26
Iteration:    960, Loss function: 4.611, Average Loss: 4.110, avg. samples / sec: 21880.32
Iteration:    960, Loss function: 5.014, Average Loss: 4.108, avg. samples / sec: 21882.43
Iteration:    960, Loss function: 4.754, Average Loss: 4.114, avg. samples / sec: 21883.78
Iteration:    960, Loss function: 4.825, Average Loss: 4.107, avg. samples / sec: 21879.43
Iteration:    980, Loss function: 5.327, Average Loss: 4.126, avg. samples / sec: 21890.00
Iteration:    980, Loss function: 4.657, Average Loss: 4.129, avg. samples / sec: 21889.54
Iteration:    980, Loss function: 5.096, Average Loss: 4.126, avg. samples / sec: 21886.30
Iteration:    980, Loss function: 4.868, Average Loss: 4.118, avg. samples / sec: 21886.48
Iteration:    980, Loss function: 4.850, Average Loss: 4.123, avg. samples / sec: 21890.47
Iteration:    980, Loss function: 5.413, Average Loss: 4.120, avg. samples / sec: 21885.29
Iteration:    980, Loss function: 4.603, Average Loss: 4.119, avg. samples / sec: 21885.48
Iteration:    980, Loss function: 5.291, Average Loss: 4.124, avg. samples / sec: 21881.65

:::MLPv0.5.0 ssd 1541710967.238609076 (train.py:553) train_epoch: 17
Iteration:   1000, Loss function: 4.896, Average Loss: 4.144, avg. samples / sec: 21876.77
Iteration:   1000, Loss function: 4.709, Average Loss: 4.135, avg. samples / sec: 21872.27
Iteration:   1000, Loss function: 4.381, Average Loss: 4.139, avg. samples / sec: 21878.03
Iteration:   1000, Loss function: 4.721, Average Loss: 4.145, avg. samples / sec: 21869.81
Iteration:   1000, Loss function: 4.693, Average Loss: 4.137, avg. samples / sec: 21872.28
Iteration:   1000, Loss function: 4.765, Average Loss: 4.135, avg. samples / sec: 21870.03
Iteration:   1000, Loss function: 4.694, Average Loss: 4.138, avg. samples / sec: 21865.91
Iteration:   1000, Loss function: 4.802, Average Loss: 4.142, avg. samples / sec: 21863.42
Iteration:   1020, Loss function: 5.094, Average Loss: 4.158, avg. samples / sec: 21812.20
Iteration:   1020, Loss function: 4.647, Average Loss: 4.150, avg. samples / sec: 21820.10
Iteration:   1020, Loss function: 4.846, Average Loss: 4.146, avg. samples / sec: 21815.28
Iteration:   1020, Loss function: 4.721, Average Loss: 4.157, avg. samples / sec: 21810.46
Iteration:   1020, Loss function: 4.168, Average Loss: 4.149, avg. samples / sec: 21810.78
Iteration:   1020, Loss function: 4.799, Average Loss: 4.150, avg. samples / sec: 21806.94
Iteration:   1020, Loss function: 4.596, Average Loss: 4.153, avg. samples / sec: 21815.76
Iteration:   1020, Loss function: 4.244, Average Loss: 4.146, avg. samples / sec: 21807.44

:::MLPv0.5.0 ssd 1541710972.681018353 (train.py:553) train_epoch: 18
Iteration:   1040, Loss function: 4.744, Average Loss: 4.174, avg. samples / sec: 21791.23
Iteration:   1040, Loss function: 4.660, Average Loss: 4.163, avg. samples / sec: 21799.50
Iteration:   1040, Loss function: 4.650, Average Loss: 4.167, avg. samples / sec: 21795.97
Iteration:   1040, Loss function: 5.347, Average Loss: 4.162, avg. samples / sec: 21791.87
Iteration:   1040, Loss function: 4.653, Average Loss: 4.160, avg. samples / sec: 21786.86
Iteration:   1040, Loss function: 5.177, Average Loss: 4.159, avg. samples / sec: 21792.69
Iteration:   1040, Loss function: 4.868, Average Loss: 4.162, avg. samples / sec: 21784.77
Iteration:   1040, Loss function: 4.906, Average Loss: 4.169, avg. samples / sec: 21785.90
Iteration:   1060, Loss function: 4.477, Average Loss: 4.180, avg. samples / sec: 21895.53
Iteration:   1060, Loss function: 4.662, Average Loss: 4.173, avg. samples / sec: 21887.25
Iteration:   1060, Loss function: 4.700, Average Loss: 4.174, avg. samples / sec: 21879.40
Iteration:   1060, Loss function: 4.572, Average Loss: 4.181, avg. samples / sec: 21881.92
Iteration:   1060, Loss function: 4.551, Average Loss: 4.171, avg. samples / sec: 21883.90
Iteration:   1060, Loss function: 4.542, Average Loss: 4.173, avg. samples / sec: 21882.62
Iteration:   1060, Loss function: 5.151, Average Loss: 4.185, avg. samples / sec: 21869.50
Iteration:   1060, Loss function: 5.174, Average Loss: 4.172, avg. samples / sec: 21880.89
Iteration:   1080, Loss function: 4.290, Average Loss: 4.187, avg. samples / sec: 21880.65
Iteration:   1080, Loss function: 4.525, Average Loss: 4.193, avg. samples / sec: 21884.37
Iteration:   1080, Loss function: 5.303, Average Loss: 4.191, avg. samples / sec: 21872.72
Iteration:   1080, Loss function: 5.042, Average Loss: 4.182, avg. samples / sec: 21876.46
Iteration:   1080, Loss function: 4.708, Average Loss: 4.189, avg. samples / sec: 21872.84
Iteration:   1080, Loss function: 4.577, Average Loss: 4.179, avg. samples / sec: 21875.80
Iteration:   1080, Loss function: 5.250, Average Loss: 4.185, avg. samples / sec: 21871.49
Iteration:   1080, Loss function: 4.507, Average Loss: 4.186, avg. samples / sec: 21867.20

:::MLPv0.5.0 ssd 1541710978.112714767 (train.py:553) train_epoch: 19
Iteration:   1100, Loss function: 4.681, Average Loss: 4.199, avg. samples / sec: 21872.11
Iteration:   1100, Loss function: 4.855, Average Loss: 4.198, avg. samples / sec: 21868.60
Iteration:   1100, Loss function: 4.898, Average Loss: 4.192, avg. samples / sec: 21872.22
Iteration:   1100, Loss function: 4.847, Average Loss: 4.201, avg. samples / sec: 21865.81
Iteration:   1100, Loss function: 5.006, Average Loss: 4.196, avg. samples / sec: 21876.29
Iteration:   1100, Loss function: 4.216, Average Loss: 4.187, avg. samples / sec: 21871.02
Iteration:   1100, Loss function: 4.870, Average Loss: 4.195, avg. samples / sec: 21867.32
Iteration:   1100, Loss function: 4.992, Average Loss: 4.197, avg. samples / sec: 21861.81
Iteration:   1120, Loss function: 4.517, Average Loss: 4.207, avg. samples / sec: 21859.13
Iteration:   1120, Loss function: 5.086, Average Loss: 4.209, avg. samples / sec: 21856.77
Iteration:   1120, Loss function: 4.962, Average Loss: 4.215, avg. samples / sec: 21860.82
Iteration:   1120, Loss function: 4.833, Average Loss: 4.205, avg. samples / sec: 21859.68
Iteration:   1120, Loss function: 4.328, Average Loss: 4.206, avg. samples / sec: 21866.07
Iteration:   1120, Loss function: 4.885, Average Loss: 4.209, avg. samples / sec: 21867.50
Iteration:   1120, Loss function: 4.740, Average Loss: 4.195, avg. samples / sec: 21858.81
Iteration:   1120, Loss function: 4.743, Average Loss: 4.203, avg. samples / sec: 21853.63
Iteration:   1140, Loss function: 4.418, Average Loss: 4.216, avg. samples / sec: 21905.84
Iteration:   1140, Loss function: 4.386, Average Loss: 4.219, avg. samples / sec: 21914.16
Iteration:   1140, Loss function: 4.676, Average Loss: 4.225, avg. samples / sec: 21907.84
Iteration:   1140, Loss function: 4.280, Average Loss: 4.206, avg. samples / sec: 21911.84
Iteration:   1140, Loss function: 4.661, Average Loss: 4.214, avg. samples / sec: 21906.32
Iteration:   1140, Loss function: 4.914, Average Loss: 4.213, avg. samples / sec: 21909.16
Iteration:   1140, Loss function: 5.282, Average Loss: 4.221, avg. samples / sec: 21898.64
Iteration:   1140, Loss function: 4.642, Average Loss: 4.215, avg. samples / sec: 21901.36

:::MLPv0.5.0 ssd 1541710983.538736582 (train.py:553) train_epoch: 20
Iteration:   1160, Loss function: 4.270, Average Loss: 4.231, avg. samples / sec: 21863.82
Iteration:   1160, Loss function: 5.247, Average Loss: 4.224, avg. samples / sec: 21850.16
Iteration:   1160, Loss function: 4.901, Average Loss: 4.224, avg. samples / sec: 21855.66
Iteration:   1160, Loss function: 5.064, Average Loss: 4.234, avg. samples / sec: 21850.54
Iteration:   1160, Loss function: 4.893, Average Loss: 4.225, avg. samples / sec: 21858.77
Iteration:   1160, Loss function: 4.449, Average Loss: 4.221, avg. samples / sec: 21852.86
Iteration:   1160, Loss function: 4.666, Average Loss: 4.216, avg. samples / sec: 21848.69
Iteration:   1160, Loss function: 5.083, Average Loss: 4.230, avg. samples / sec: 21844.04
Iteration:   1180, Loss function: 4.455, Average Loss: 4.241, avg. samples / sec: 21728.16
Iteration:   1180, Loss function: 4.223, Average Loss: 4.233, avg. samples / sec: 21728.78
Iteration:   1180, Loss function: 4.326, Average Loss: 4.228, avg. samples / sec: 21726.91
Iteration:   1180, Loss function: 4.522, Average Loss: 4.240, avg. samples / sec: 21724.16
Iteration:   1180, Loss function: 4.817, Average Loss: 4.233, avg. samples / sec: 21726.07
Iteration:   1180, Loss function: 4.319, Average Loss: 4.225, avg. samples / sec: 21726.36
Iteration:   1180, Loss function: 4.400, Average Loss: 4.239, avg. samples / sec: 21725.15
Iteration:   1180, Loss function: 4.181, Average Loss: 4.229, avg. samples / sec: 21715.91
Iteration:   1200, Loss function: 4.392, Average Loss: 4.247, avg. samples / sec: 21920.22
Iteration:   1200, Loss function: 4.964, Average Loss: 4.240, avg. samples / sec: 21923.80
Iteration:   1200, Loss function: 4.759, Average Loss: 4.231, avg. samples / sec: 21929.64
Iteration:   1200, Loss function: 4.901, Average Loss: 4.235, avg. samples / sec: 21931.16
Iteration:   1200, Loss function: 4.349, Average Loss: 4.233, avg. samples / sec: 21921.89
Iteration:   1200, Loss function: 4.463, Average Loss: 4.245, avg. samples / sec: 21919.55
Iteration:   1200, Loss function: 5.064, Average Loss: 4.247, avg. samples / sec: 21922.42
Iteration:   1200, Loss function: 4.226, Average Loss: 4.237, avg. samples / sec: 21916.16

:::MLPv0.5.0 ssd 1541710988.893379927 (train.py:553) train_epoch: 21
Iteration:   1220, Loss function: 4.591, Average Loss: 4.252, avg. samples / sec: 21855.18
Iteration:   1220, Loss function: 4.428, Average Loss: 4.253, avg. samples / sec: 21843.63
Iteration:   1220, Loss function: 4.705, Average Loss: 4.239, avg. samples / sec: 21851.28
Iteration:   1220, Loss function: 4.920, Average Loss: 4.242, avg. samples / sec: 21846.03
Iteration:   1220, Loss function: 4.464, Average Loss: 4.245, avg. samples / sec: 21839.93
Iteration:   1220, Loss function: 4.283, Average Loss: 4.246, avg. samples / sec: 21854.02
Iteration:   1220, Loss function: 4.514, Average Loss: 4.255, avg. samples / sec: 21848.95
Iteration:   1220, Loss function: 4.777, Average Loss: 4.238, avg. samples / sec: 21836.49
Iteration:   1240, Loss function: 4.134, Average Loss: 4.244, avg. samples / sec: 21919.48
Iteration:   1240, Loss function: 4.676, Average Loss: 4.261, avg. samples / sec: 21907.68
Iteration:   1240, Loss function: 4.142, Average Loss: 4.250, avg. samples / sec: 21911.68
Iteration:   1240, Loss function: 4.684, Average Loss: 4.246, avg. samples / sec: 21909.73
Iteration:   1240, Loss function: 5.083, Average Loss: 4.259, avg. samples / sec: 21909.88
Iteration:   1240, Loss function: 4.353, Average Loss: 4.250, avg. samples / sec: 21907.49
Iteration:   1240, Loss function: 4.914, Average Loss: 4.258, avg. samples / sec: 21898.97
Iteration:   1240, Loss function: 4.038, Average Loss: 4.243, avg. samples / sec: 21891.66
Iteration:   1260, Loss function: 5.217, Average Loss: 4.270, avg. samples / sec: 21875.52
Iteration:   1260, Loss function: 4.712, Average Loss: 4.250, avg. samples / sec: 21876.01
Iteration:   1260, Loss function: 4.873, Average Loss: 4.264, avg. samples / sec: 21881.80
Iteration:   1260, Loss function: 4.436, Average Loss: 4.258, avg. samples / sec: 21877.96
Iteration:   1260, Loss function: 4.775, Average Loss: 4.249, avg. samples / sec: 21888.90
Iteration:   1260, Loss function: 4.601, Average Loss: 4.254, avg. samples / sec: 21869.66
Iteration:   1260, Loss function: 4.838, Average Loss: 4.249, avg. samples / sec: 21866.51
Iteration:   1260, Loss function: 4.479, Average Loss: 4.265, avg. samples / sec: 21870.06

:::MLPv0.5.0 ssd 1541710994.320366621 (train.py:553) train_epoch: 22
Iteration:   1280, Loss function: 4.900, Average Loss: 4.276, avg. samples / sec: 21842.72
Iteration:   1280, Loss function: 4.354, Average Loss: 4.256, avg. samples / sec: 21841.45
Iteration:   1280, Loss function: 4.350, Average Loss: 4.269, avg. samples / sec: 21840.42
Iteration:   1280, Loss function: 4.228, Average Loss: 4.254, avg. samples / sec: 21842.53
Iteration:   1280, Loss function: 4.536, Average Loss: 4.264, avg. samples / sec: 21839.55
Iteration:   1280, Loss function: 4.723, Average Loss: 4.256, avg. samples / sec: 21841.45
Iteration:   1280, Loss function: 4.126, Average Loss: 4.259, avg. samples / sec: 21840.69
Iteration:   1280, Loss function: 4.377, Average Loss: 4.269, avg. samples / sec: 21844.49
Iteration:   1300, Loss function: 4.781, Average Loss: 4.273, avg. samples / sec: 21824.62
Iteration:   1300, Loss function: 4.650, Average Loss: 4.261, avg. samples / sec: 21817.57
Iteration:   1300, Loss function: 4.724, Average Loss: 4.275, avg. samples / sec: 21821.22
Iteration:   1300, Loss function: 4.665, Average Loss: 4.266, avg. samples / sec: 21818.74
Iteration:   1300, Loss function: 5.088, Average Loss: 4.262, avg. samples / sec: 21819.87
Iteration:   1300, Loss function: 4.353, Average Loss: 4.261, avg. samples / sec: 21813.99
Iteration:   1300, Loss function: 4.793, Average Loss: 4.281, avg. samples / sec: 21809.87
Iteration:   1300, Loss function: 4.410, Average Loss: 4.265, avg. samples / sec: 21801.51
Iteration:   1320, Loss function: 4.636, Average Loss: 4.286, avg. samples / sec: 21899.15
Iteration:   1320, Loss function: 4.372, Average Loss: 4.279, avg. samples / sec: 21888.08
Iteration:   1320, Loss function: 4.200, Average Loss: 4.268, avg. samples / sec: 21891.34
Iteration:   1320, Loss function: 3.996, Average Loss: 4.267, avg. samples / sec: 21892.48
Iteration:   1320, Loss function: 4.448, Average Loss: 4.266, avg. samples / sec: 21890.17
Iteration:   1320, Loss function: 4.569, Average Loss: 4.271, avg. samples / sec: 21908.82
Iteration:   1320, Loss function: 4.534, Average Loss: 4.280, avg. samples / sec: 21886.37
Iteration:   1320, Loss function: 4.515, Average Loss: 4.271, avg. samples / sec: 21881.55

:::MLPv0.5.0 ssd 1541710999.757025003 (train.py:553) train_epoch: 23
Iteration:   1340, Loss function: 4.623, Average Loss: 4.287, avg. samples / sec: 21848.94
Iteration:   1340, Loss function: 4.898, Average Loss: 4.282, avg. samples / sec: 21847.30
Iteration:   1340, Loss function: 4.754, Average Loss: 4.274, avg. samples / sec: 21848.32
Iteration:   1340, Loss function: 4.506, Average Loss: 4.275, avg. samples / sec: 21849.31
Iteration:   1340, Loss function: 4.623, Average Loss: 4.283, avg. samples / sec: 21849.98
Iteration:   1340, Loss function: 4.373, Average Loss: 4.268, avg. samples / sec: 21846.27
Iteration:   1340, Loss function: 4.470, Average Loss: 4.273, avg. samples / sec: 21854.00
Iteration:   1340, Loss function: 4.948, Average Loss: 4.270, avg. samples / sec: 21840.63
Iteration:   1360, Loss function: 4.116, Average Loss: 4.292, avg. samples / sec: 21847.60
Iteration:   1360, Loss function: 3.888, Average Loss: 4.288, avg. samples / sec: 21850.07
Iteration:   1360, Loss function: 3.908, Average Loss: 4.279, avg. samples / sec: 21856.16
Iteration:   1360, Loss function: 4.716, Average Loss: 4.281, avg. samples / sec: 21849.61
Iteration:   1360, Loss function: 4.723, Average Loss: 4.276, avg. samples / sec: 21858.65
Iteration:   1360, Loss function: 5.184, Average Loss: 4.284, avg. samples / sec: 21850.57
Iteration:   1360, Loss function: 5.080, Average Loss: 4.290, avg. samples / sec: 21849.34
Iteration:   1360, Loss function: 4.901, Average Loss: 4.278, avg. samples / sec: 21848.34
Iteration:   1380, Loss function: 4.712, Average Loss: 4.287, avg. samples / sec: 21823.97
Iteration:   1380, Loss function: 4.358, Average Loss: 4.295, avg. samples / sec: 21814.81
Iteration:   1380, Loss function: 4.526, Average Loss: 4.289, avg. samples / sec: 21814.43
Iteration:   1380, Loss function: 4.121, Average Loss: 4.279, avg. samples / sec: 21816.73
Iteration:   1380, Loss function: 4.085, Average Loss: 4.283, avg. samples / sec: 21815.93
Iteration:   1380, Loss function: 4.142, Average Loss: 4.281, avg. samples / sec: 21820.42
Iteration:   1380, Loss function: 4.462, Average Loss: 4.283, avg. samples / sec: 21808.97
Iteration:   1380, Loss function: 3.686, Average Loss: 4.292, avg. samples / sec: 21812.53

:::MLPv0.5.0 ssd 1541711005.197623730 (train.py:553) train_epoch: 24
Iteration:   1400, Loss function: 4.721, Average Loss: 4.289, avg. samples / sec: 21808.47
Iteration:   1400, Loss function: 3.684, Average Loss: 4.279, avg. samples / sec: 21808.48
Iteration:   1400, Loss function: 4.232, Average Loss: 4.286, avg. samples / sec: 21811.67
Iteration:   1400, Loss function: 4.215, Average Loss: 4.289, avg. samples / sec: 21805.90
Iteration:   1400, Loss function: 4.670, Average Loss: 4.282, avg. samples / sec: 21807.41
Iteration:   1400, Loss function: 4.063, Average Loss: 4.294, avg. samples / sec: 21811.71
Iteration:   1400, Loss function: 4.091, Average Loss: 4.290, avg. samples / sec: 21798.45
Iteration:   1400, Loss function: 4.427, Average Loss: 4.297, avg. samples / sec: 21792.87
Iteration:   1420, Loss function: 4.275, Average Loss: 4.291, avg. samples / sec: 21889.27
Iteration:   1420, Loss function: 3.879, Average Loss: 4.298, avg. samples / sec: 21900.13
Iteration:   1420, Loss function: 4.396, Average Loss: 4.290, avg. samples / sec: 21888.17
Iteration:   1420, Loss function: 4.875, Average Loss: 4.289, avg. samples / sec: 21885.39
Iteration:   1420, Loss function: 4.650, Average Loss: 4.295, avg. samples / sec: 21886.47
Iteration:   1420, Loss function: 4.378, Average Loss: 4.291, avg. samples / sec: 21886.93
Iteration:   1420, Loss function: 4.373, Average Loss: 4.287, avg. samples / sec: 21882.07
Iteration:   1420, Loss function: 4.393, Average Loss: 4.283, avg. samples / sec: 21878.32
Iteration:   1440, Loss function: 4.463, Average Loss: 4.294, avg. samples / sec: 21817.36
Iteration:   1440, Loss function: 4.110, Average Loss: 4.288, avg. samples / sec: 21821.84
Iteration:   1440, Loss function: 3.971, Average Loss: 4.299, avg. samples / sec: 21819.27
Iteration:   1440, Loss function: 4.212, Average Loss: 4.287, avg. samples / sec: 21822.67
Iteration:   1440, Loss function: 4.752, Average Loss: 4.284, avg. samples / sec: 21821.81
Iteration:   1440, Loss function: 4.579, Average Loss: 4.296, avg. samples / sec: 21817.49
Iteration:   1440, Loss function: 4.890, Average Loss: 4.293, avg. samples / sec: 21814.73
Iteration:   1440, Loss function: 4.884, Average Loss: 4.292, avg. samples / sec: 21814.59

:::MLPv0.5.0 ssd 1541711010.542676926 (train.py:553) train_epoch: 25
Iteration:   1460, Loss function: 4.103, Average Loss: 4.295, avg. samples / sec: 21850.87
Iteration:   1460, Loss function: 4.029, Average Loss: 4.300, avg. samples / sec: 21853.14
Iteration:   1460, Loss function: 4.192, Average Loss: 4.295, avg. samples / sec: 21856.74
Iteration:   1460, Loss function: 4.477, Average Loss: 4.291, avg. samples / sec: 21853.15
Iteration:   1460, Loss function: 4.554, Average Loss: 4.289, avg. samples / sec: 21849.39
Iteration:   1460, Loss function: 4.251, Average Loss: 4.293, avg. samples / sec: 21858.12
Iteration:   1460, Loss function: 4.547, Average Loss: 4.287, avg. samples / sec: 21852.50
Iteration:   1460, Loss function: 4.040, Average Loss: 4.298, avg. samples / sec: 21847.83
Iteration:   1480, Loss function: 4.611, Average Loss: 4.292, avg. samples / sec: 21853.19
Iteration:   1480, Loss function: 4.629, Average Loss: 4.302, avg. samples / sec: 21849.03
Iteration:   1480, Loss function: 3.995, Average Loss: 4.291, avg. samples / sec: 21854.43
Iteration:   1480, Loss function: 4.677, Average Loss: 4.296, avg. samples / sec: 21846.49
Iteration:   1480, Loss function: 4.574, Average Loss: 4.293, avg. samples / sec: 21848.95
Iteration:   1480, Loss function: 4.954, Average Loss: 4.301, avg. samples / sec: 21855.05
Iteration:   1480, Loss function: 4.218, Average Loss: 4.296, avg. samples / sec: 21843.71
Iteration:   1480, Loss function: 4.702, Average Loss: 4.289, avg. samples / sec: 21840.17
Iteration:   1500, Loss function: 4.629, Average Loss: 4.299, avg. samples / sec: 21913.35
Iteration:   1500, Loss function: 4.291, Average Loss: 4.304, avg. samples / sec: 21910.68
Iteration:   1500, Loss function: 3.895, Average Loss: 4.292, avg. samples / sec: 21904.84
Iteration:   1500, Loss function: 4.711, Average Loss: 4.298, avg. samples / sec: 21911.11
Iteration:   1500, Loss function: 3.916, Average Loss: 4.295, avg. samples / sec: 21902.75
Iteration:   1500, Loss function: 4.455, Average Loss: 4.291, avg. samples / sec: 21916.60
Iteration:   1500, Loss function: 4.725, Average Loss: 4.305, avg. samples / sec: 21907.15
Iteration:   1500, Loss function: 4.567, Average Loss: 4.295, avg. samples / sec: 21901.86

:::MLPv0.5.0 ssd 1541711015.974047661 (train.py:553) train_epoch: 26
Iteration:   1520, Loss function: 4.676, Average Loss: 4.312, avg. samples / sec: 21873.40
Iteration:   1520, Loss function: 4.574, Average Loss: 4.307, avg. samples / sec: 21869.61
Iteration:   1520, Loss function: 4.034, Average Loss: 4.299, avg. samples / sec: 21870.68
Iteration:   1520, Loss function: 4.342, Average Loss: 4.302, avg. samples / sec: 21876.85
Iteration:   1520, Loss function: 4.340, Average Loss: 4.302, avg. samples / sec: 21870.39
Iteration:   1520, Loss function: 4.592, Average Loss: 4.303, avg. samples / sec: 21868.59
Iteration:   1520, Loss function: 4.878, Average Loss: 4.312, avg. samples / sec: 21870.02
Iteration:   1520, Loss function: 4.783, Average Loss: 4.299, avg. samples / sec: 21868.45
Iteration:   1540, Loss function: 4.665, Average Loss: 4.315, avg. samples / sec: 21819.80
Iteration:   1540, Loss function: 4.408, Average Loss: 4.312, avg. samples / sec: 21822.84
Iteration:   1540, Loss function: 4.126, Average Loss: 4.301, avg. samples / sec: 21825.88
Iteration:   1540, Loss function: 4.401, Average Loss: 4.301, avg. samples / sec: 21832.45
Iteration:   1540, Loss function: 4.309, Average Loss: 4.307, avg. samples / sec: 21826.15
Iteration:   1540, Loss function: 4.445, Average Loss: 4.303, avg. samples / sec: 21825.25
Iteration:   1540, Loss function: 3.866, Average Loss: 4.302, avg. samples / sec: 21823.14
Iteration:   1540, Loss function: 4.234, Average Loss: 4.316, avg. samples / sec: 21819.87

:::MLPv0.5.0 ssd 1541711021.410615206 (train.py:553) train_epoch: 27
Iteration:   1560, Loss function: 4.239, Average Loss: 4.306, avg. samples / sec: 21847.21
Iteration:   1560, Loss function: 4.565, Average Loss: 4.312, avg. samples / sec: 21842.16
Iteration:   1560, Loss function: 4.003, Average Loss: 4.300, avg. samples / sec: 21838.84
Iteration:   1560, Loss function: 4.011, Average Loss: 4.299, avg. samples / sec: 21842.01
Iteration:   1560, Loss function: 3.961, Average Loss: 4.305, avg. samples / sec: 21843.33
Iteration:   1560, Loss function: 4.062, Average Loss: 4.314, avg. samples / sec: 21847.83
Iteration:   1560, Loss function: 4.262, Average Loss: 4.301, avg. samples / sec: 21834.62
Iteration:   1560, Loss function: 4.374, Average Loss: 4.314, avg. samples / sec: 21826.31
Iteration:   1580, Loss function: 4.600, Average Loss: 4.300, avg. samples / sec: 21818.91
Iteration:   1580, Loss function: 4.520, Average Loss: 4.314, avg. samples / sec: 21814.39
Iteration:   1580, Loss function: 4.997, Average Loss: 4.307, avg. samples / sec: 21819.16
Iteration:   1580, Loss function: 4.031, Average Loss: 4.316, avg. samples / sec: 21826.54
Iteration:   1580, Loss function: 4.140, Average Loss: 4.314, avg. samples / sec: 21817.44
Iteration:   1580, Loss function: 4.656, Average Loss: 4.301, avg. samples / sec: 21812.10
Iteration:   1580, Loss function: 4.514, Average Loss: 4.306, avg. samples / sec: 21805.21
Iteration:   1580, Loss function: 4.466, Average Loss: 4.301, avg. samples / sec: 21813.24
Iteration:   1600, Loss function: 3.961, Average Loss: 4.306, avg. samples / sec: 21699.70
Iteration:   1600, Loss function: 4.190, Average Loss: 4.301, avg. samples / sec: 21700.37
Iteration:   1600, Loss function: 4.061, Average Loss: 4.301, avg. samples / sec: 21703.24
Iteration:   1600, Loss function: 3.993, Average Loss: 4.306, avg. samples / sec: 21700.00
Iteration:   1600, Loss function: 4.284, Average Loss: 4.315, avg. samples / sec: 21692.21
Iteration:   1600, Loss function: 4.688, Average Loss: 4.317, avg. samples / sec: 21693.84
Iteration:   1600, Loss function: 4.597, Average Loss: 4.300, avg. samples / sec: 21689.03
Iteration:   1600, Loss function: 4.594, Average Loss: 4.314, avg. samples / sec: 21686.23

:::MLPv0.5.0 ssd 1541711026.868120432 (train.py:553) train_epoch: 28
Iteration:   1620, Loss function: 4.668, Average Loss: 4.316, avg. samples / sec: 21812.79
Iteration:   1620, Loss function: 4.510, Average Loss: 4.300, avg. samples / sec: 21806.89
Iteration:   1620, Loss function: 4.752, Average Loss: 4.302, avg. samples / sec: 21810.74
Iteration:   1620, Loss function: 4.021, Average Loss: 4.308, avg. samples / sec: 21805.07
Iteration:   1620, Loss function: 4.572, Average Loss: 4.302, avg. samples / sec: 21802.95
Iteration:   1620, Loss function: 4.643, Average Loss: 4.316, avg. samples / sec: 21814.72
Iteration:   1620, Loss function: 4.123, Average Loss: 4.319, avg. samples / sec: 21806.39
Iteration:   1620, Loss function: 4.599, Average Loss: 4.306, avg. samples / sec: 21796.25
Iteration:   1640, Loss function: 4.022, Average Loss: 4.317, avg. samples / sec: 21885.45
Iteration:   1640, Loss function: 4.154, Average Loss: 4.305, avg. samples / sec: 21888.76
Iteration:   1640, Loss function: 4.399, Average Loss: 4.321, avg. samples / sec: 21884.09
Iteration:   1640, Loss function: 4.534, Average Loss: 4.310, avg. samples / sec: 21883.23
Iteration:   1640, Loss function: 4.035, Average Loss: 4.317, avg. samples / sec: 21883.70
Iteration:   1640, Loss function: 4.609, Average Loss: 4.305, avg. samples / sec: 21880.38
Iteration:   1640, Loss function: 3.816, Average Loss: 4.307, avg. samples / sec: 21882.76
Iteration:   1640, Loss function: 4.098, Average Loss: 4.299, avg. samples / sec: 21874.12
Iteration:   1660, Loss function: 4.584, Average Loss: 4.296, avg. samples / sec: 21838.76
Iteration:   1660, Loss function: 4.523, Average Loss: 4.320, avg. samples / sec: 21830.23
Iteration:   1660, Loss function: 3.920, Average Loss: 4.303, avg. samples / sec: 21828.20
Iteration:   1660, Loss function: 3.879, Average Loss: 4.305, avg. samples / sec: 21825.72
Iteration:   1660, Loss function: 4.203, Average Loss: 4.316, avg. samples / sec: 21813.19
Iteration:   1660, Loss function: 4.277, Average Loss: 4.304, avg. samples / sec: 21816.71
Iteration:   1660, Loss function: 4.044, Average Loss: 4.304, avg. samples / sec: 21826.62
Iteration:   1660, Loss function: 4.523, Average Loss: 4.316, avg. samples / sec: 21821.05

:::MLPv0.5.0 ssd 1541711032.210862160 (train.py:553) train_epoch: 29
Iteration:   1680, Loss function: 4.040, Average Loss: 4.300, avg. samples / sec: 21861.60
Iteration:   1680, Loss function: 4.333, Average Loss: 4.295, avg. samples / sec: 21851.08
Iteration:   1680, Loss function: 4.641, Average Loss: 4.316, avg. samples / sec: 21851.70
Iteration:   1680, Loss function: 4.376, Average Loss: 4.302, avg. samples / sec: 21857.81
Iteration:   1680, Loss function: 4.466, Average Loss: 4.304, avg. samples / sec: 21853.17
Iteration:   1680, Loss function: 4.987, Average Loss: 4.312, avg. samples / sec: 21852.73
Iteration:   1680, Loss function: 4.406, Average Loss: 4.313, avg. samples / sec: 21852.58
Iteration:   1680, Loss function: 4.208, Average Loss: 4.302, avg. samples / sec: 21848.20
Iteration:   1700, Loss function: 4.112, Average Loss: 4.300, avg. samples / sec: 21770.97
Iteration:   1700, Loss function: 4.255, Average Loss: 4.297, avg. samples / sec: 21762.09
Iteration:   1700, Loss function: 4.013, Average Loss: 4.295, avg. samples / sec: 21758.83
Iteration:   1700, Loss function: 3.736, Average Loss: 4.302, avg. samples / sec: 21763.02
Iteration:   1700, Loss function: 4.097, Average Loss: 4.315, avg. samples / sec: 21757.41
Iteration:   1700, Loss function: 3.985, Average Loss: 4.310, avg. samples / sec: 21763.49
Iteration:   1700, Loss function: 4.315, Average Loss: 4.308, avg. samples / sec: 21764.00
Iteration:   1700, Loss function: 4.040, Average Loss: 4.302, avg. samples / sec: 21754.19
Iteration:   1720, Loss function: 4.850, Average Loss: 4.316, avg. samples / sec: 21813.36
Iteration:   1720, Loss function: 4.302, Average Loss: 4.294, avg. samples / sec: 21807.88
Iteration:   1720, Loss function: 3.888, Average Loss: 4.296, avg. samples / sec: 21802.03
Iteration:   1720, Loss function: 3.742, Average Loss: 4.305, avg. samples / sec: 21805.90
Iteration:   1720, Loss function: 3.830, Average Loss: 4.300, avg. samples / sec: 21800.32
Iteration:   1720, Loss function: 3.355, Average Loss: 4.307, avg. samples / sec: 21808.51
Iteration:   1720, Loss function: 4.404, Average Loss: 4.308, avg. samples / sec: 21806.26
Iteration:   1720, Loss function: 4.359, Average Loss: 4.304, avg. samples / sec: 21808.60

:::MLPv0.5.0 ssd 1541711037.658047199 (train.py:553) train_epoch: 30
Iteration:   1740, Loss function: 4.278, Average Loss: 4.295, avg. samples / sec: 21792.80
Iteration:   1740, Loss function: 4.175, Average Loss: 4.314, avg. samples / sec: 21788.17
Iteration:   1740, Loss function: 3.510, Average Loss: 4.298, avg. samples / sec: 21794.69
Iteration:   1740, Loss function: 3.877, Average Loss: 4.302, avg. samples / sec: 21789.55
Iteration:   1740, Loss function: 4.162, Average Loss: 4.301, avg. samples / sec: 21785.71
Iteration:   1740, Loss function: 4.439, Average Loss: 4.295, avg. samples / sec: 21781.15
Iteration:   1740, Loss function: 4.380, Average Loss: 4.305, avg. samples / sec: 21779.82
Iteration:   1740, Loss function: 4.304, Average Loss: 4.306, avg. samples / sec: 21781.13
Iteration:   1760, Loss function: 4.011, Average Loss: 4.310, avg. samples / sec: 21886.22
Iteration:   1760, Loss function: 4.523, Average Loss: 4.294, avg. samples / sec: 21882.67
Iteration:   1760, Loss function: 4.021, Average Loss: 4.297, avg. samples / sec: 21880.23
Iteration:   1760, Loss function: 3.757, Average Loss: 4.304, avg. samples / sec: 21894.94
Iteration:   1760, Loss function: 4.879, Average Loss: 4.301, avg. samples / sec: 21884.49
Iteration:   1760, Loss function: 4.318, Average Loss: 4.293, avg. samples / sec: 21887.48
Iteration:   1760, Loss function: 4.029, Average Loss: 4.299, avg. samples / sec: 21883.23
Iteration:   1760, Loss function: 4.302, Average Loss: 4.303, avg. samples / sec: 21880.16
Iteration:   1780, Loss function: 4.739, Average Loss: 4.309, avg. samples / sec: 21802.82
Iteration:   1780, Loss function: 4.536, Average Loss: 4.304, avg. samples / sec: 21822.21
Iteration:   1780, Loss function: 3.973, Average Loss: 4.293, avg. samples / sec: 21799.73
Iteration:   1780, Loss function: 3.812, Average Loss: 4.297, avg. samples / sec: 21811.69
Iteration:   1780, Loss function: 4.714, Average Loss: 4.294, avg. samples / sec: 21810.12
Iteration:   1780, Loss function: 4.661, Average Loss: 4.299, avg. samples / sec: 21806.44
Iteration:   1780, Loss function: 4.456, Average Loss: 4.304, avg. samples / sec: 21803.29
Iteration:   1780, Loss function: 4.317, Average Loss: 4.298, avg. samples / sec: 21801.61

:::MLPv0.5.0 ssd 1541711043.101093292 (train.py:553) train_epoch: 31
Iteration:   1800, Loss function: 4.113, Average Loss: 4.303, avg. samples / sec: 21824.48
Iteration:   1800, Loss function: 4.505, Average Loss: 4.307, avg. samples / sec: 21814.68
Iteration:   1800, Loss function: 4.067, Average Loss: 4.293, avg. samples / sec: 21818.90
Iteration:   1800, Loss function: 3.676, Average Loss: 4.292, avg. samples / sec: 21816.08
Iteration:   1800, Loss function: 4.343, Average Loss: 4.297, avg. samples / sec: 21815.64
Iteration:   1800, Loss function: 4.172, Average Loss: 4.297, avg. samples / sec: 21818.22
Iteration:   1800, Loss function: 3.926, Average Loss: 4.302, avg. samples / sec: 21810.11
Iteration:   1800, Loss function: 4.163, Average Loss: 4.300, avg. samples / sec: 21806.44
Iteration:   1820, Loss function: 3.779, Average Loss: 4.295, avg. samples / sec: 21736.91
Iteration:   1820, Loss function: 4.561, Average Loss: 4.306, avg. samples / sec: 21728.42
Iteration:   1820, Loss function: 3.760, Average Loss: 4.294, avg. samples / sec: 21731.28
Iteration:   1820, Loss function: 4.081, Average Loss: 4.302, avg. samples / sec: 21724.65
Iteration:   1820, Loss function: 3.352, Average Loss: 4.290, avg. samples / sec: 21725.93
Iteration:   1820, Loss function: 3.966, Average Loss: 4.299, avg. samples / sec: 21732.49
Iteration:   1820, Loss function: 3.668, Average Loss: 4.292, avg. samples / sec: 21726.43
Iteration:   1820, Loss function: 3.938, Average Loss: 4.295, avg. samples / sec: 21739.21
Iteration:   1840, Loss function: 3.871, Average Loss: 4.303, avg. samples / sec: 21881.84
Iteration:   1840, Loss function: 4.406, Average Loss: 4.304, avg. samples / sec: 21884.19
Iteration:   1840, Loss function: 4.155, Average Loss: 4.290, avg. samples / sec: 21882.03
Iteration:   1840, Loss function: 3.591, Average Loss: 4.299, avg. samples / sec: 21882.13
Iteration:   1840, Loss function: 4.252, Average Loss: 4.293, avg. samples / sec: 21882.01
Iteration:   1840, Loss function: 4.171, Average Loss: 4.295, avg. samples / sec: 21873.70
Iteration:   1840, Loss function: 3.430, Average Loss: 4.292, avg. samples / sec: 21870.48
Iteration:   1840, Loss function: 4.549, Average Loss: 4.294, avg. samples / sec: 21874.75

:::MLPv0.5.0 ssd 1541711048.547736645 (train.py:553) train_epoch: 32
Iteration:   1860, Loss function: 4.228, Average Loss: 4.302, avg. samples / sec: 21846.28
Iteration:   1860, Loss function: 4.166, Average Loss: 4.289, avg. samples / sec: 21842.22
Iteration:   1860, Loss function: 4.512, Average Loss: 4.301, avg. samples / sec: 21838.10
Iteration:   1860, Loss function: 4.507, Average Loss: 4.293, avg. samples / sec: 21844.18
Iteration:   1860, Loss function: 4.081, Average Loss: 4.294, avg. samples / sec: 21842.08
Iteration:   1860, Loss function: 3.751, Average Loss: 4.291, avg. samples / sec: 21849.56
Iteration:   1860, Loss function: 4.420, Average Loss: 4.292, avg. samples / sec: 21848.41
Iteration:   1860, Loss function: 3.918, Average Loss: 4.297, avg. samples / sec: 21837.41

































































:::MLPv0.5.0 ssd 1541711051.084611654 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711051.085446119 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711051.086224794 (train.py:220) eval_start: 32
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.70 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.70 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.70 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.70 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.70 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.69 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.70 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.70 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(333666, 7)
(333666, 7)
(333666, 7)
0/333666
0/333666
0/333666
Loading and preparing results...
Converting ndarray to lists...
(333666, 7)
Loading and preparing results...
0/333666
Loading and preparing results...
Converting ndarray to lists...
(333666, 7)
Converting ndarray to lists...
0/333666
(333666, 7)
0/333666
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(333666, 7)
(333666, 7)
0/333666
0/333666
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(333666, 7)
Loading and preparing results...
Converting ndarray to lists...
0/333666
(333666, 7)
Converting ndarray to lists...
0/333666
Loading and preparing results...
(333666, 7)
0/333666
Converting ndarray to lists...
(333666, 7)
Loading and preparing results...
0/333666
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(333666, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/333666
Converting ndarray to lists...
(333666, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(333666, 7)
Converting ndarray to lists...
(333666, 7)
0/333666
(333666, 7)
(333666, 7)
0/333666
(333666, 7)
0/333666
0/333666
0/333666
0/333666
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(333666, 7)
Loading and preparing results...
Converting ndarray to lists...
0/333666
(333666, 7)
Converting ndarray to lists...
(333666, 7)
0/333666
(333666, 7)
0/333666
0/333666
Loading and preparing results...
Converting ndarray to lists...
(333666, 7)
0/333666
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(333666, 7)
(333666, 7)
Converting ndarray to lists...
0/333666
0/333666
(333666, 7)
0/333666
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(333666, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(333666, 7)
0/333666
Converting ndarray to lists...
Loading and preparing results...
0/333666
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(333666, 7)
Converting ndarray to lists...
(333666, 7)
Loading and preparing results...
Loading and preparing results...
0/333666
Converting ndarray to lists...
Loading and preparing results...
0/333666
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(333666, 7)
Converting ndarray to lists...
(333666, 7)
Converting ndarray to lists...
(333666, 7)
(333666, 7)
Converting ndarray to lists...
0/333666
0/333666
Loading and preparing results...
Loading and preparing results...
(333666, 7)
Converting ndarray to lists...
Loading and preparing results...
0/333666
Loading and preparing results...
0/333666
Converting ndarray to lists...
(333666, 7)
Loading and preparing results...
(333666, 7)
Converting ndarray to lists...
(333666, 7)
Loading and preparing results...
Loading and preparing results...
0/333666
0/333666
Converting ndarray to lists...
(333666, 7)
Loading and preparing results...
0/333666
Converting ndarray to lists...
(333666, 7)
Loading and preparing results...
0/333666
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/333666
Loading and preparing results...
Loading and preparing results...
0/333666
Loading and preparing results...
Loading and preparing results...
(333666, 7)
(333666, 7)
Loading and preparing results...
0/333666
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(333666, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(333666, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/333666
(333666, 7)
(333666, 7)
Converting ndarray to lists...
(333666, 7)
Converting ndarray to lists...
(333666, 7)
Loading and preparing results...
(333666, 7)
(333666, 7)
0/333666
Loading and preparing results...
Converting ndarray to lists...
(333666, 7)
0/333666
Loading and preparing results...
Converting ndarray to lists...
(333666, 7)
(333666, 7)
0/333666
0/333666
0/333666
(333666, 7)
0/333666
0/333666
0/333666
Converting ndarray to lists...
Loading and preparing results...
(333666, 7)
0/333666
Loading and preparing results...
Converting ndarray to lists...
0/333666
0/333666
(333666, 7)
Loading and preparing results...
0/333666
0/333666
Loading and preparing results...
(333666, 7)
0/333666
Converting ndarray to lists...
Converting ndarray to lists...
(333666, 7)
Converting ndarray to lists...
0/333666
Converting ndarray to lists...
(333666, 7)
0/333666
Converting ndarray to lists...
0/333666
(333666, 7)
(333666, 7)
(333666, 7)
(333666, 7)
0/333666
0/333666
0/333666
0/333666
DONE (t=2.21s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.32s)
creating index...
index created!
DONE (t=2.38s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
DONE (t=3.80s).
Accumulating evaluation results...
DONE (t=3.78s).
Accumulating evaluation results...
DONE (t=3.82s).
Accumulating evaluation results...
DONE (t=3.83s).
Accumulating evaluation results...
DONE (t=3.82s).
Accumulating evaluation results...
DONE (t=3.83s).
Accumulating evaluation results...
DONE (t=3.85s).
Accumulating evaluation results...
DONE (t=3.84s).
Accumulating evaluation results...
DONE (t=1.15s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.137
DONE (t=1.19s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.137
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
DONE (t=1.18s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.137
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
DONE (t=1.21s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.13669 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
DONE (t=1.19s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.209
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.137
DONE (t=1.25s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.137
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.13669 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
DONE (t=1.20s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.137
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.209
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.137
DONE (t=1.21s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.13669 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.137
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.13669 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.13669 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.13669 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.13669 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.13669 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711065.492586613 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711065.493512869 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.13668656785889827}

:::MLPv0.5.0 ssd 1541711065.494290113 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.13668656785889827}

:::MLPv0.5.0 ssd 1541711065.495043993 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711065.495802402 (train.py:338) eval_stop: 32
Iteration:   1880, Loss function: 3.926, Average Loss: 4.298, avg. samples / sec: 2437.67
Iteration:   1880, Loss function: 3.889, Average Loss: 4.298, avg. samples / sec: 2437.51
Iteration:   1880, Loss function: 4.583, Average Loss: 4.289, avg. samples / sec: 2437.64
Iteration:   1880, Loss function: 3.804, Average Loss: 4.290, avg. samples / sec: 2437.60
Iteration:   1880, Loss function: 4.613, Average Loss: 4.284, avg. samples / sec: 2437.56
Iteration:   1880, Loss function: 4.052, Average Loss: 4.294, avg. samples / sec: 2437.62
Iteration:   1880, Loss function: 4.113, Average Loss: 4.292, avg. samples / sec: 2437.54
Iteration:   1880, Loss function: 4.400, Average Loss: 4.287, avg. samples / sec: 2437.52
Iteration:   1900, Loss function: 4.641, Average Loss: 4.299, avg. samples / sec: 22008.83
Iteration:   1900, Loss function: 4.207, Average Loss: 4.291, avg. samples / sec: 22011.41
Iteration:   1900, Loss function: 4.809, Average Loss: 4.293, avg. samples / sec: 22017.28
Iteration:   1900, Loss function: 4.056, Average Loss: 4.291, avg. samples / sec: 22012.35
Iteration:   1900, Loss function: 4.331, Average Loss: 4.299, avg. samples / sec: 22007.71
Iteration:   1900, Loss function: 4.158, Average Loss: 4.283, avg. samples / sec: 22013.06
Iteration:   1900, Loss function: 4.304, Average Loss: 4.287, avg. samples / sec: 22015.28
Iteration:   1900, Loss function: 4.503, Average Loss: 4.295, avg. samples / sec: 22006.61

:::MLPv0.5.0 ssd 1541711068.805095911 (train.py:553) train_epoch: 33
Iteration:   1920, Loss function: 3.739, Average Loss: 4.294, avg. samples / sec: 21930.89
Iteration:   1920, Loss function: 3.737, Average Loss: 4.293, avg. samples / sec: 21932.39
Iteration:   1920, Loss function: 4.147, Average Loss: 4.300, avg. samples / sec: 21933.85
Iteration:   1920, Loss function: 3.792, Average Loss: 4.289, avg. samples / sec: 21930.56
Iteration:   1920, Loss function: 3.961, Average Loss: 4.288, avg. samples / sec: 21934.63
Iteration:   1920, Loss function: 3.535, Average Loss: 4.290, avg. samples / sec: 21928.22
Iteration:   1920, Loss function: 4.399, Average Loss: 4.283, avg. samples / sec: 21928.36
Iteration:   1920, Loss function: 4.272, Average Loss: 4.294, avg. samples / sec: 21931.04
Iteration:   1940, Loss function: 3.852, Average Loss: 4.291, avg. samples / sec: 21965.95
Iteration:   1940, Loss function: 3.918, Average Loss: 4.289, avg. samples / sec: 21963.74
Iteration:   1940, Loss function: 3.930, Average Loss: 4.285, avg. samples / sec: 21962.93
Iteration:   1940, Loss function: 4.183, Average Loss: 4.299, avg. samples / sec: 21961.30
Iteration:   1940, Loss function: 4.446, Average Loss: 4.289, avg. samples / sec: 21963.99
Iteration:   1940, Loss function: 4.220, Average Loss: 4.288, avg. samples / sec: 21960.45
Iteration:   1940, Loss function: 4.423, Average Loss: 4.280, avg. samples / sec: 21962.78
Iteration:   1940, Loss function: 4.352, Average Loss: 4.291, avg. samples / sec: 21965.37
Iteration:   1960, Loss function: 3.879, Average Loss: 4.292, avg. samples / sec: 21926.77
Iteration:   1960, Loss function: 4.287, Average Loss: 4.285, avg. samples / sec: 21927.05
Iteration:   1960, Loss function: 4.374, Average Loss: 4.282, avg. samples / sec: 21926.22
Iteration:   1960, Loss function: 4.103, Average Loss: 4.285, avg. samples / sec: 21925.98
Iteration:   1960, Loss function: 4.621, Average Loss: 4.298, avg. samples / sec: 21924.17
Iteration:   1960, Loss function: 4.298, Average Loss: 4.288, avg. samples / sec: 21927.02
Iteration:   1960, Loss function: 4.383, Average Loss: 4.287, avg. samples / sec: 21922.66
Iteration:   1960, Loss function: 4.267, Average Loss: 4.278, avg. samples / sec: 21923.36

:::MLPv0.5.0 ssd 1541711074.221792936 (train.py:553) train_epoch: 34
Iteration:   1980, Loss function: 4.136, Average Loss: 4.297, avg. samples / sec: 21837.55
Iteration:   1980, Loss function: 4.425, Average Loss: 4.290, avg. samples / sec: 21826.38
Iteration:   1980, Loss function: 4.091, Average Loss: 4.283, avg. samples / sec: 21832.18
Iteration:   1980, Loss function: 4.365, Average Loss: 4.279, avg. samples / sec: 21825.53
Iteration:   1980, Loss function: 4.308, Average Loss: 4.285, avg. samples / sec: 21829.61
Iteration:   1980, Loss function: 4.704, Average Loss: 4.277, avg. samples / sec: 21829.91
Iteration:   1980, Loss function: 3.805, Average Loss: 4.286, avg. samples / sec: 21825.49
Iteration:   1980, Loss function: 4.108, Average Loss: 4.281, avg. samples / sec: 21819.90
Iteration:   2000, Loss function: 4.548, Average Loss: 4.276, avg. samples / sec: 21871.65
Iteration:   2000, Loss function: 4.649, Average Loss: 4.289, avg. samples / sec: 21867.49
Iteration:   2000, Loss function: 4.468, Average Loss: 4.279, avg. samples / sec: 21864.97
Iteration:   2000, Loss function: 4.395, Average Loss: 4.274, avg. samples / sec: 21873.65
Iteration:   2000, Loss function: 4.771, Average Loss: 4.277, avg. samples / sec: 21874.62
Iteration:   2000, Loss function: 4.203, Average Loss: 4.284, avg. samples / sec: 21871.70
Iteration:   2000, Loss function: 4.317, Average Loss: 4.282, avg. samples / sec: 21869.43
Iteration:   2000, Loss function: 4.214, Average Loss: 4.291, avg. samples / sec: 21856.89
Iteration:   2020, Loss function: 4.915, Average Loss: 4.286, avg. samples / sec: 21914.59
Iteration:   2020, Loss function: 3.969, Average Loss: 4.289, avg. samples / sec: 21922.15
Iteration:   2020, Loss function: 4.277, Average Loss: 4.274, avg. samples / sec: 21916.64
Iteration:   2020, Loss function: 3.729, Average Loss: 4.279, avg. samples / sec: 21913.48
Iteration:   2020, Loss function: 4.128, Average Loss: 4.276, avg. samples / sec: 21908.64
Iteration:   2020, Loss function: 4.159, Average Loss: 4.282, avg. samples / sec: 21912.11
Iteration:   2020, Loss function: 3.829, Average Loss: 4.271, avg. samples / sec: 21900.41
Iteration:   2020, Loss function: 4.319, Average Loss: 4.271, avg. samples / sec: 21905.90

:::MLPv0.5.0 ssd 1541711079.652666330 (train.py:553) train_epoch: 35
Iteration:   2040, Loss function: 3.562, Average Loss: 4.273, avg. samples / sec: 21883.97
Iteration:   2040, Loss function: 3.787, Average Loss: 4.269, avg. samples / sec: 21875.88
Iteration:   2040, Loss function: 3.828, Average Loss: 4.280, avg. samples / sec: 21878.16
Iteration:   2040, Loss function: 4.029, Average Loss: 4.277, avg. samples / sec: 21875.56
Iteration:   2040, Loss function: 3.827, Average Loss: 4.265, avg. samples / sec: 21878.06
Iteration:   2040, Loss function: 3.788, Average Loss: 4.280, avg. samples / sec: 21861.37
Iteration:   2040, Loss function: 4.528, Average Loss: 4.286, avg. samples / sec: 21858.21
Iteration:   2040, Loss function: 3.678, Average Loss: 4.266, avg. samples / sec: 21864.39
Iteration:   2060, Loss function: 4.270, Average Loss: 4.283, avg. samples / sec: 21885.71
Iteration:   2060, Loss function: 4.394, Average Loss: 4.278, avg. samples / sec: 21880.38
Iteration:   2060, Loss function: 3.842, Average Loss: 4.267, avg. samples / sec: 21865.64
Iteration:   2060, Loss function: 4.332, Average Loss: 4.268, avg. samples / sec: 21862.09
Iteration:   2060, Loss function: 4.469, Average Loss: 4.263, avg. samples / sec: 21869.46
Iteration:   2060, Loss function: 4.226, Average Loss: 4.274, avg. samples / sec: 21866.86
Iteration:   2060, Loss function: 4.012, Average Loss: 4.278, avg. samples / sec: 21866.02
Iteration:   2060, Loss function: 4.291, Average Loss: 4.265, avg. samples / sec: 21881.14

:::MLPv0.5.0 ssd 1541711085.077423573 (train.py:553) train_epoch: 36
Iteration:   2080, Loss function: 4.198, Average Loss: 4.274, avg. samples / sec: 21926.53
Iteration:   2080, Loss function: 4.480, Average Loss: 4.260, avg. samples / sec: 21929.15
Iteration:   2080, Loss function: 4.349, Average Loss: 4.274, avg. samples / sec: 21929.38
Iteration:   2080, Loss function: 3.823, Average Loss: 4.263, avg. samples / sec: 21923.31
Iteration:   2080, Loss function: 4.377, Average Loss: 4.272, avg. samples / sec: 21924.65
Iteration:   2080, Loss function: 3.924, Average Loss: 4.262, avg. samples / sec: 21924.49
Iteration:   2080, Loss function: 4.526, Average Loss: 4.265, avg. samples / sec: 21920.59
Iteration:   2080, Loss function: 4.145, Average Loss: 4.284, avg. samples / sec: 21907.17
Iteration:   2100, Loss function: 3.884, Average Loss: 4.281, avg. samples / sec: 21945.14
Iteration:   2100, Loss function: 3.717, Average Loss: 4.258, avg. samples / sec: 21932.42
Iteration:   2100, Loss function: 4.649, Average Loss: 4.272, avg. samples / sec: 21921.55
Iteration:   2100, Loss function: 4.505, Average Loss: 4.270, avg. samples / sec: 21931.00
Iteration:   2100, Loss function: 3.870, Average Loss: 4.260, avg. samples / sec: 21927.26
Iteration:   2100, Loss function: 4.254, Average Loss: 4.262, avg. samples / sec: 21931.24
Iteration:   2100, Loss function: 4.115, Average Loss: 4.273, avg. samples / sec: 21919.64
Iteration:   2100, Loss function: 4.597, Average Loss: 4.259, avg. samples / sec: 21924.00
Iteration:   2120, Loss function: 4.302, Average Loss: 4.257, avg. samples / sec: 21863.81
Iteration:   2120, Loss function: 4.096, Average Loss: 4.267, avg. samples / sec: 21867.51
Iteration:   2120, Loss function: 3.852, Average Loss: 4.276, avg. samples / sec: 21860.30
Iteration:   2120, Loss function: 4.484, Average Loss: 4.260, avg. samples / sec: 21866.27
Iteration:   2120, Loss function: 4.063, Average Loss: 4.254, avg. samples / sec: 21865.62
Iteration:   2120, Loss function: 3.999, Average Loss: 4.268, avg. samples / sec: 21868.88
Iteration:   2120, Loss function: 3.972, Average Loss: 4.267, avg. samples / sec: 21859.26
Iteration:   2120, Loss function: 3.753, Average Loss: 4.253, avg. samples / sec: 21837.43

:::MLPv0.5.0 ssd 1541711090.505281687 (train.py:553) train_epoch: 37
Iteration:   2140, Loss function: 4.292, Average Loss: 4.251, avg. samples / sec: 21873.37
Iteration:   2140, Loss function: 4.342, Average Loss: 4.264, avg. samples / sec: 21870.80
Iteration:   2140, Loss function: 3.409, Average Loss: 4.270, avg. samples / sec: 21872.36
Iteration:   2140, Loss function: 3.997, Average Loss: 4.264, avg. samples / sec: 21873.80
Iteration:   2140, Loss function: 3.941, Average Loss: 4.264, avg. samples / sec: 21869.23
Iteration:   2140, Loss function: 4.253, Average Loss: 4.251, avg. samples / sec: 21867.06
Iteration:   2140, Loss function: 4.788, Average Loss: 4.256, avg. samples / sec: 21865.34
Iteration:   2140, Loss function: 3.334, Average Loss: 4.249, avg. samples / sec: 21901.46
Iteration:   2160, Loss function: 3.800, Average Loss: 4.259, avg. samples / sec: 21975.36
Iteration:   2160, Loss function: 4.121, Average Loss: 4.248, avg. samples / sec: 21967.08
Iteration:   2160, Loss function: 3.928, Average Loss: 4.266, avg. samples / sec: 21969.39
Iteration:   2160, Loss function: 3.796, Average Loss: 4.245, avg. samples / sec: 21976.69
Iteration:   2160, Loss function: 4.081, Average Loss: 4.254, avg. samples / sec: 21977.08
Iteration:   2160, Loss function: 3.977, Average Loss: 4.243, avg. samples / sec: 21971.50
Iteration:   2160, Loss function: 3.598, Average Loss: 4.259, avg. samples / sec: 21968.72
Iteration:   2160, Loss function: 3.684, Average Loss: 4.261, avg. samples / sec: 21961.57
Iteration:   2180, Loss function: 3.734, Average Loss: 4.255, avg. samples / sec: 21874.74
Iteration:   2180, Loss function: 4.010, Average Loss: 4.237, avg. samples / sec: 21878.87
Iteration:   2180, Loss function: 4.477, Average Loss: 4.246, avg. samples / sec: 21874.24
Iteration:   2180, Loss function: 4.532, Average Loss: 4.240, avg. samples / sec: 21883.56
Iteration:   2180, Loss function: 4.820, Average Loss: 4.263, avg. samples / sec: 21874.37
Iteration:   2180, Loss function: 3.871, Average Loss: 4.256, avg. samples / sec: 21886.22
Iteration:   2180, Loss function: 3.924, Average Loss: 4.249, avg. samples / sec: 21873.42
Iteration:   2180, Loss function: 4.181, Average Loss: 4.253, avg. samples / sec: 21879.18

:::MLPv0.5.0 ssd 1541711095.829242468 (train.py:553) train_epoch: 38
Iteration:   2200, Loss function: 3.709, Average Loss: 4.254, avg. samples / sec: 21918.26
Iteration:   2200, Loss function: 3.860, Average Loss: 4.250, avg. samples / sec: 21926.87
Iteration:   2200, Loss function: 4.126, Average Loss: 4.245, avg. samples / sec: 21917.57
Iteration:   2200, Loss function: 4.024, Average Loss: 4.238, avg. samples / sec: 21915.72
Iteration:   2200, Loss function: 3.873, Average Loss: 4.240, avg. samples / sec: 21916.12
Iteration:   2200, Loss function: 4.155, Average Loss: 4.254, avg. samples / sec: 21920.61
Iteration:   2200, Loss function: 4.046, Average Loss: 4.255, avg. samples / sec: 21916.95
Iteration:   2200, Loss function: 4.103, Average Loss: 4.259, avg. samples / sec: 21908.86
Iteration:   2220, Loss function: 4.213, Average Loss: 4.254, avg. samples / sec: 21941.57
Iteration:   2220, Loss function: 3.872, Average Loss: 4.251, avg. samples / sec: 21940.74
Iteration:   2220, Loss function: 3.788, Average Loss: 4.256, avg. samples / sec: 21955.14
Iteration:   2220, Loss function: 4.118, Average Loss: 4.239, avg. samples / sec: 21940.42
Iteration:   2220, Loss function: 4.391, Average Loss: 4.251, avg. samples / sec: 21943.73
Iteration:   2220, Loss function: 4.599, Average Loss: 4.251, avg. samples / sec: 21938.80
Iteration:   2220, Loss function: 4.264, Average Loss: 4.244, avg. samples / sec: 21929.99
Iteration:   2220, Loss function: 4.478, Average Loss: 4.234, avg. samples / sec: 21925.56
Iteration:   2240, Loss function: 4.074, Average Loss: 4.249, avg. samples / sec: 21846.28
Iteration:   2240, Loss function: 4.264, Average Loss: 4.252, avg. samples / sec: 21842.68
Iteration:   2240, Loss function: 4.311, Average Loss: 4.250, avg. samples / sec: 21846.79
Iteration:   2240, Loss function: 4.019, Average Loss: 4.235, avg. samples / sec: 21847.11
Iteration:   2240, Loss function: 4.008, Average Loss: 4.231, avg. samples / sec: 21859.63
Iteration:   2240, Loss function: 4.083, Average Loss: 4.249, avg. samples / sec: 21848.93
Iteration:   2240, Loss function: 4.639, Average Loss: 4.246, avg. samples / sec: 21836.64
Iteration:   2240, Loss function: 4.152, Average Loss: 4.240, avg. samples / sec: 21839.18

:::MLPv0.5.0 ssd 1541711101.264049053 (train.py:553) train_epoch: 39
Iteration:   2260, Loss function: 4.303, Average Loss: 4.245, avg. samples / sec: 21791.30
Iteration:   2260, Loss function: 3.741, Average Loss: 4.239, avg. samples / sec: 21813.80
Iteration:   2260, Loss function: 4.427, Average Loss: 4.245, avg. samples / sec: 21793.13
Iteration:   2260, Loss function: 4.305, Average Loss: 4.233, avg. samples / sec: 21793.16
Iteration:   2260, Loss function: 4.789, Average Loss: 4.228, avg. samples / sec: 21793.55
Iteration:   2260, Loss function: 3.675, Average Loss: 4.249, avg. samples / sec: 21789.52
Iteration:   2260, Loss function: 4.307, Average Loss: 4.246, avg. samples / sec: 21789.07
Iteration:   2260, Loss function: 3.514, Average Loss: 4.243, avg. samples / sec: 21790.80
Iteration:   2280, Loss function: 3.734, Average Loss: 4.240, avg. samples / sec: 21825.34
Iteration:   2280, Loss function: 3.942, Average Loss: 4.248, avg. samples / sec: 21828.36
Iteration:   2280, Loss function: 4.068, Average Loss: 4.239, avg. samples / sec: 21831.45
Iteration:   2280, Loss function: 4.232, Average Loss: 4.231, avg. samples / sec: 21824.14
Iteration:   2280, Loss function: 3.873, Average Loss: 4.243, avg. samples / sec: 21825.17
Iteration:   2280, Loss function: 3.969, Average Loss: 4.236, avg. samples / sec: 21815.90
Iteration:   2280, Loss function: 4.147, Average Loss: 4.227, avg. samples / sec: 21821.96
Iteration:   2280, Loss function: 4.116, Average Loss: 4.243, avg. samples / sec: 21819.47
Iteration:   2300, Loss function: 3.585, Average Loss: 4.242, avg. samples / sec: 21930.37
Iteration:   2300, Loss function: 3.724, Average Loss: 4.235, avg. samples / sec: 21924.09
Iteration:   2300, Loss function: 3.757, Average Loss: 4.224, avg. samples / sec: 21932.97
Iteration:   2300, Loss function: 4.491, Average Loss: 4.237, avg. samples / sec: 21927.36
Iteration:   2300, Loss function: 4.302, Average Loss: 4.241, avg. samples / sec: 21931.09
Iteration:   2300, Loss function: 3.790, Average Loss: 4.238, avg. samples / sec: 21931.27
Iteration:   2300, Loss function: 4.392, Average Loss: 4.233, avg. samples / sec: 21928.97
Iteration:   2300, Loss function: 4.492, Average Loss: 4.228, avg. samples / sec: 21923.57

:::MLPv0.5.0 ssd 1541711106.697390556 (train.py:553) train_epoch: 40
Iteration:   2320, Loss function: 3.876, Average Loss: 4.221, avg. samples / sec: 21790.16
Iteration:   2320, Loss function: 3.771, Average Loss: 4.230, avg. samples / sec: 21786.11
Iteration:   2320, Loss function: 4.329, Average Loss: 4.231, avg. samples / sec: 21789.08
Iteration:   2320, Loss function: 4.733, Average Loss: 4.239, avg. samples / sec: 21782.88
Iteration:   2320, Loss function: 3.986, Average Loss: 4.228, avg. samples / sec: 21788.53
Iteration:   2320, Loss function: 4.657, Average Loss: 4.229, avg. samples / sec: 21788.97
Iteration:   2320, Loss function: 4.432, Average Loss: 4.237, avg. samples / sec: 21782.46
Iteration:   2320, Loss function: 4.495, Average Loss: 4.233, avg. samples / sec: 21779.95
Iteration:   2340, Loss function: 3.708, Average Loss: 4.225, avg. samples / sec: 21907.16
Iteration:   2340, Loss function: 4.094, Average Loss: 4.217, avg. samples / sec: 21897.60
Iteration:   2340, Loss function: 4.011, Average Loss: 4.230, avg. samples / sec: 21900.22
Iteration:   2340, Loss function: 3.894, Average Loss: 4.224, avg. samples / sec: 21901.62
Iteration:   2340, Loss function: 4.806, Average Loss: 4.235, avg. samples / sec: 21899.05
Iteration:   2340, Loss function: 3.817, Average Loss: 4.229, avg. samples / sec: 21907.02
Iteration:   2340, Loss function: 3.974, Average Loss: 4.228, avg. samples / sec: 21895.30
Iteration:   2340, Loss function: 3.243, Average Loss: 4.234, avg. samples / sec: 21900.73
Iteration:   2360, Loss function: 3.913, Average Loss: 4.227, avg. samples / sec: 21869.69
Iteration:   2360, Loss function: 4.263, Average Loss: 4.221, avg. samples / sec: 21866.49
Iteration:   2360, Loss function: 4.231, Average Loss: 4.227, avg. samples / sec: 21861.52
Iteration:   2360, Loss function: 3.629, Average Loss: 4.234, avg. samples / sec: 21862.89
Iteration:   2360, Loss function: 3.946, Average Loss: 4.232, avg. samples / sec: 21867.45
Iteration:   2360, Loss function: 3.846, Average Loss: 4.223, avg. samples / sec: 21857.03
Iteration:   2360, Loss function: 4.150, Average Loss: 4.220, avg. samples / sec: 21856.64
Iteration:   2360, Loss function: 4.128, Average Loss: 4.230, avg. samples / sec: 21860.44

:::MLPv0.5.0 ssd 1541711112.130817175 (train.py:553) train_epoch: 41
Iteration:   2380, Loss function: 4.104, Average Loss: 4.226, avg. samples / sec: 21870.41
Iteration:   2380, Loss function: 4.070, Average Loss: 4.230, avg. samples / sec: 21872.65
Iteration:   2380, Loss function: 4.272, Average Loss: 4.231, avg. samples / sec: 21868.18
Iteration:   2380, Loss function: 3.835, Average Loss: 4.217, avg. samples / sec: 21872.20
Iteration:   2380, Loss function: 4.102, Average Loss: 4.221, avg. samples / sec: 21869.35
Iteration:   2380, Loss function: 3.769, Average Loss: 4.221, avg. samples / sec: 21861.05
Iteration:   2380, Loss function: 3.655, Average Loss: 4.225, avg. samples / sec: 21864.15
Iteration:   2380, Loss function: 4.260, Average Loss: 4.227, avg. samples / sec: 21866.27
Iteration:   2400, Loss function: 3.939, Average Loss: 4.221, avg. samples / sec: 21931.20
Iteration:   2400, Loss function: 4.382, Average Loss: 4.225, avg. samples / sec: 21923.48
Iteration:   2400, Loss function: 4.009, Average Loss: 4.227, avg. samples / sec: 21930.81
Iteration:   2400, Loss function: 4.053, Average Loss: 4.216, avg. samples / sec: 21927.14
Iteration:   2400, Loss function: 3.975, Average Loss: 4.213, avg. samples / sec: 21923.51
Iteration:   2400, Loss function: 3.940, Average Loss: 4.226, avg. samples / sec: 21923.73
Iteration:   2400, Loss function: 3.739, Average Loss: 4.218, avg. samples / sec: 21922.73
Iteration:   2400, Loss function: 3.872, Average Loss: 4.221, avg. samples / sec: 21905.84
Iteration:   2420, Loss function: 3.674, Average Loss: 4.211, avg. samples / sec: 21767.42
Iteration:   2420, Loss function: 3.796, Average Loss: 4.222, avg. samples / sec: 21766.41
Iteration:   2420, Loss function: 3.781, Average Loss: 4.207, avg. samples / sec: 21765.32
Iteration:   2420, Loss function: 4.461, Average Loss: 4.220, avg. samples / sec: 21760.06
Iteration:   2420, Loss function: 4.030, Average Loss: 4.225, avg. samples / sec: 21762.50
Iteration:   2420, Loss function: 3.816, Average Loss: 4.219, avg. samples / sec: 21774.73
Iteration:   2420, Loss function: 3.858, Average Loss: 4.212, avg. samples / sec: 21763.79
Iteration:   2420, Loss function: 4.563, Average Loss: 4.221, avg. samples / sec: 21754.51

:::MLPv0.5.0 ssd 1541711117.476331472 (train.py:553) train_epoch: 42
Iteration:   2440, Loss function: 4.313, Average Loss: 4.215, avg. samples / sec: 21909.70
Iteration:   2440, Loss function: 4.008, Average Loss: 4.207, avg. samples / sec: 21899.48
Iteration:   2440, Loss function: 3.818, Average Loss: 4.221, avg. samples / sec: 21903.69
Iteration:   2440, Loss function: 4.173, Average Loss: 4.204, avg. samples / sec: 21901.21
Iteration:   2440, Loss function: 3.746, Average Loss: 4.219, avg. samples / sec: 21896.14
Iteration:   2440, Loss function: 3.702, Average Loss: 4.217, avg. samples / sec: 21903.46
Iteration:   2440, Loss function: 4.259, Average Loss: 4.208, avg. samples / sec: 21898.95
Iteration:   2440, Loss function: 3.698, Average Loss: 4.212, avg. samples / sec: 21889.00
Iteration:   2460, Loss function: 3.956, Average Loss: 4.213, avg. samples / sec: 21955.92
Iteration:   2460, Loss function: 3.792, Average Loss: 4.202, avg. samples / sec: 21954.29
Iteration:   2460, Loss function: 3.379, Average Loss: 4.201, avg. samples / sec: 21960.71
Iteration:   2460, Loss function: 4.569, Average Loss: 4.217, avg. samples / sec: 21956.04
Iteration:   2460, Loss function: 4.064, Average Loss: 4.200, avg. samples / sec: 21952.65
Iteration:   2460, Loss function: 3.969, Average Loss: 4.210, avg. samples / sec: 21941.49
Iteration:   2460, Loss function: 4.032, Average Loss: 4.207, avg. samples / sec: 21962.75
Iteration:   2460, Loss function: 4.227, Average Loss: 4.213, avg. samples / sec: 21952.27
Iteration:   2480, Loss function: 4.286, Average Loss: 4.215, avg. samples / sec: 21892.81
Iteration:   2480, Loss function: 3.984, Average Loss: 4.208, avg. samples / sec: 21892.28
Iteration:   2480, Loss function: 5.032, Average Loss: 4.213, avg. samples / sec: 21883.71
Iteration:   2480, Loss function: 4.708, Average Loss: 4.205, avg. samples / sec: 21890.15
Iteration:   2480, Loss function: 4.138, Average Loss: 4.201, avg. samples / sec: 21884.38
Iteration:   2480, Loss function: 3.833, Average Loss: 4.197, avg. samples / sec: 21884.84
Iteration:   2480, Loss function: 3.885, Average Loss: 4.196, avg. samples / sec: 21879.62
Iteration:   2480, Loss function: 3.480, Average Loss: 4.208, avg. samples / sec: 21883.47

:::MLPv0.5.0 ssd 1541711122.895039797 (train.py:553) train_epoch: 43
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1

:::MLPv0.5.0 ssd 1541711124.394222260 (train.py:578) opt_learning_rate: 0.016
Iteration:   2500, Loss function: 3.899, Average Loss: 4.205, avg. samples / sec: 21899.53
Iteration:   2500, Loss function: 3.630, Average Loss: 4.199, avg. samples / sec: 21898.04
Iteration:   2500, Loss function: 4.110, Average Loss: 4.210, avg. samples / sec: 21893.78
Iteration:   2500, Loss function: 3.706, Average Loss: 4.192, avg. samples / sec: 21900.53
Iteration:   2500, Loss function: 4.211, Average Loss: 4.202, avg. samples / sec: 21895.20
Iteration:   2500, Loss function: 4.043, Average Loss: 4.195, avg. samples / sec: 21896.30
Iteration:   2500, Loss function: 4.472, Average Loss: 4.207, avg. samples / sec: 21897.71
Iteration:   2500, Loss function: 3.810, Average Loss: 4.211, avg. samples / sec: 21877.47

































































:::MLPv0.5.0 ssd 1541711124.486266136 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711124.487093925 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711124.487828255 (train.py:220) eval_start: 43
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 46
Predicting Ended, total time: 5.94 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.94 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.94 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.94 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.94 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.94 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.94 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.94 s
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(337417, 7)
Converting ndarray to lists...
(337417, 7)
0/337417
Loading and preparing results...
(337417, 7)
Loading and preparing results...
0/337417
0/337417
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(337417, 7)
Loading and preparing results...
0/337417
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(337417, 7)
(337417, 7)
(337417, 7)
0/337417
0/337417
Loading and preparing results...
0/337417
Converting ndarray to lists...
(337417, 7)
0/337417
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(337417, 7)
0/337417
Converting ndarray to lists...
(337417, 7)
Loading and preparing results...
0/337417
Converting ndarray to lists...
Loading and preparing results...
(337417, 7)
Converting ndarray to lists...
0/337417
(337417, 7)
0/337417
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(337417, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/337417
(337417, 7)
Converting ndarray to lists...
(337417, 7)
Loading and preparing results...
(337417, 7)
(337417, 7)
(337417, 7)
Loading and preparing results...
0/337417
Converting ndarray to lists...
0/337417
0/337417
0/337417
Converting ndarray to lists...
0/337417
(337417, 7)
Loading and preparing results...
(337417, 7)
Loading and preparing results...
0/337417
Loading and preparing results...
0/337417
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(337417, 7)
(337417, 7)
(337417, 7)
Loading and preparing results...
0/337417
0/337417
0/337417
Converting ndarray to lists...
(337417, 7)
0/337417
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(337417, 7)
(337417, 7)
(337417, 7)
(337417, 7)
0/337417
0/337417
0/337417
0/337417
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(337417, 7)
(337417, 7)
Converting ndarray to lists...
(337417, 7)
0/337417
0/337417
(337417, 7)
Loading and preparing results...
0/337417
0/337417
Converting ndarray to lists...
(337417, 7)
Loading and preparing results...
0/337417
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(337417, 7)
(337417, 7)
0/337417
Converting ndarray to lists...
Loading and preparing results...
(337417, 7)
0/337417
Loading and preparing results...
0/337417
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(337417, 7)
Loading and preparing results...
(337417, 7)
(337417, 7)
0/337417
Loading and preparing results...
0/337417
Converting ndarray to lists...
Loading and preparing results...
0/337417
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(337417, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/337417
Converting ndarray to lists...
(337417, 7)
(337417, 7)
(337417, 7)
0/337417
0/337417
Loading and preparing results...
0/337417
Converting ndarray to lists...
(337417, 7)
Loading and preparing results...
(337417, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/337417
(337417, 7)
0/337417
Converting ndarray to lists...
(337417, 7)
0/337417
0/337417
(337417, 7)
0/337417
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(337417, 7)
(337417, 7)
Converting ndarray to lists...
0/337417
0/337417
(337417, 7)
0/337417
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(337417, 7)
Converting ndarray to lists...
Loading and preparing results...
0/337417
(337417, 7)
(337417, 7)
Loading and preparing results...
0/337417
0/337417
Converting ndarray to lists...
(337417, 7)
Converting ndarray to lists...
0/337417
(337417, 7)
0/337417
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(337417, 7)
(337417, 7)
(337417, 7)
0/337417
0/337417
0/337417
Loading and preparing results...
Converting ndarray to lists...
(337417, 7)
Loading and preparing results...
0/337417
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(337417, 7)
(337417, 7)
(337417, 7)
0/337417
0/337417
0/337417
Converting ndarray to lists...
(337417, 7)
0/337417
DONE (t=1.84s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.97s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=2.10s)
creating index...
DONE (t=2.13s)
creating index...
index created!
DONE (t=2.13s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.31s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
DONE (t=3.73s).
Accumulating evaluation results...
DONE (t=3.73s).
Accumulating evaluation results...
DONE (t=3.74s).
Accumulating evaluation results...
DONE (t=3.70s).
Accumulating evaluation results...
DONE (t=3.72s).
Accumulating evaluation results...
DONE (t=3.72s).
Accumulating evaluation results...
DONE (t=3.70s).
Accumulating evaluation results...
DONE (t=3.75s).
Accumulating evaluation results...
DONE (t=1.17s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.291
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.164
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.243
DONE (t=1.24s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.249
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.278
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.404
Current AP: 0.15152 AP goal: 0.21200
DONE (t=1.22s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.291
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.291
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.164
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.243
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.164
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.249
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.278
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.404
Current AP: 0.15152 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.249
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.278
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.404
Current AP: 0.15152 AP goal: 0.21200
DONE (t=1.26s).
DONE (t=1.23s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
DONE (t=1.25s).
DONE (t=1.25s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.291
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.291
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.291
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.291
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.164
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.164
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.243
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.249
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.278
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.404
Current AP: 0.15152 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.249
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.164
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.164
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.278
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.404
Current AP: 0.15152 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.243
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.249
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.249
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.278
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.404
Current AP: 0.15152 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.278
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.404
Current AP: 0.15152 AP goal: 0.21200
DONE (t=1.28s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.291
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.164
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.249
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.278
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.404
Current AP: 0.15152 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711138.027405739 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711138.028343201 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.15151854237589552}

:::MLPv0.5.0 ssd 1541711138.029104471 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.15151854237589552}

:::MLPv0.5.0 ssd 1541711138.029840469 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711138.030630112 (train.py:338) eval_stop: 43
Iteration:   2520, Loss function: 3.606, Average Loss: 4.182, avg. samples / sec: 2581.83
Iteration:   2520, Loss function: 3.509, Average Loss: 4.201, avg. samples / sec: 2581.82
Iteration:   2520, Loss function: 3.765, Average Loss: 4.190, avg. samples / sec: 2581.78
Iteration:   2520, Loss function: 3.551, Average Loss: 4.196, avg. samples / sec: 2581.68
Iteration:   2520, Loss function: 3.949, Average Loss: 4.203, avg. samples / sec: 2581.95
Iteration:   2520, Loss function: 3.682, Average Loss: 4.192, avg. samples / sec: 2581.79
Iteration:   2520, Loss function: 3.467, Average Loss: 4.196, avg. samples / sec: 2581.79
Iteration:   2520, Loss function: 3.521, Average Loss: 4.184, avg. samples / sec: 2581.74
Iteration:   2540, Loss function: 3.565, Average Loss: 4.170, avg. samples / sec: 21876.78
Iteration:   2540, Loss function: 4.014, Average Loss: 4.190, avg. samples / sec: 21878.36
Iteration:   2540, Loss function: 2.990, Average Loss: 4.181, avg. samples / sec: 21881.65
Iteration:   2540, Loss function: 3.768, Average Loss: 4.178, avg. samples / sec: 21873.29
Iteration:   2540, Loss function: 3.489, Average Loss: 4.170, avg. samples / sec: 21879.59
Iteration:   2540, Loss function: 3.007, Average Loss: 4.177, avg. samples / sec: 21876.32
Iteration:   2540, Loss function: 3.104, Average Loss: 4.186, avg. samples / sec: 21870.72
Iteration:   2540, Loss function: 3.649, Average Loss: 4.183, avg. samples / sec: 21874.03

:::MLPv0.5.0 ssd 1541711142.314718723 (train.py:553) train_epoch: 44
Iteration:   2560, Loss function: 3.573, Average Loss: 4.160, avg. samples / sec: 21970.45
Iteration:   2560, Loss function: 3.296, Average Loss: 4.169, avg. samples / sec: 21971.32
Iteration:   2560, Loss function: 3.683, Average Loss: 4.164, avg. samples / sec: 21967.49
Iteration:   2560, Loss function: 3.122, Average Loss: 4.155, avg. samples / sec: 21970.47
Iteration:   2560, Loss function: 3.147, Average Loss: 4.171, avg. samples / sec: 21968.43
Iteration:   2560, Loss function: 3.347, Average Loss: 4.157, avg. samples / sec: 21960.41
Iteration:   2560, Loss function: 4.233, Average Loss: 4.174, avg. samples / sec: 21959.69
Iteration:   2560, Loss function: 3.569, Average Loss: 4.169, avg. samples / sec: 21961.56
Iteration:   2580, Loss function: 2.666, Average Loss: 4.148, avg. samples / sec: 21967.63
Iteration:   2580, Loss function: 3.164, Average Loss: 4.143, avg. samples / sec: 21961.10
Iteration:   2580, Loss function: 3.296, Average Loss: 4.161, avg. samples / sec: 21965.34
Iteration:   2580, Loss function: 2.633, Average Loss: 4.155, avg. samples / sec: 21956.67
Iteration:   2580, Loss function: 3.294, Average Loss: 4.139, avg. samples / sec: 21953.70
Iteration:   2580, Loss function: 3.139, Average Loss: 4.143, avg. samples / sec: 21956.61
Iteration:   2580, Loss function: 3.364, Average Loss: 4.156, avg. samples / sec: 21952.36
Iteration:   2580, Loss function: 3.894, Average Loss: 4.154, avg. samples / sec: 21957.30

:::MLPv0.5.0 ssd 1541711147.721355438 (train.py:553) train_epoch: 45
Iteration:   2600, Loss function: 3.251, Average Loss: 4.132, avg. samples / sec: 21979.22
Iteration:   2600, Loss function: 3.726, Average Loss: 4.125, avg. samples / sec: 21990.96
Iteration:   2600, Loss function: 3.369, Average Loss: 4.127, avg. samples / sec: 21974.40
Iteration:   2600, Loss function: 3.530, Average Loss: 4.137, avg. samples / sec: 21982.80
Iteration:   2600, Loss function: 2.934, Average Loss: 4.123, avg. samples / sec: 21979.33
Iteration:   2600, Loss function: 3.256, Average Loss: 4.139, avg. samples / sec: 21983.60
Iteration:   2600, Loss function: 3.095, Average Loss: 4.143, avg. samples / sec: 21968.39
Iteration:   2600, Loss function: 3.602, Average Loss: 4.141, avg. samples / sec: 21970.92
Iteration:   2620, Loss function: 3.166, Average Loss: 4.127, avg. samples / sec: 21773.38
Iteration:   2620, Loss function: 3.450, Average Loss: 4.118, avg. samples / sec: 21753.76
Iteration:   2620, Loss function: 3.659, Average Loss: 4.126, avg. samples / sec: 21770.91
Iteration:   2620, Loss function: 3.355, Average Loss: 4.123, avg. samples / sec: 21763.52
Iteration:   2620, Loss function: 3.519, Average Loss: 4.110, avg. samples / sec: 21752.24
Iteration:   2620, Loss function: 3.709, Average Loss: 4.109, avg. samples / sec: 21762.02
Iteration:   2620, Loss function: 3.304, Average Loss: 4.122, avg. samples / sec: 21758.26
Iteration:   2620, Loss function: 3.309, Average Loss: 4.110, avg. samples / sec: 21753.65
Iteration:   2640, Loss function: 3.514, Average Loss: 4.095, avg. samples / sec: 21959.23
Iteration:   2640, Loss function: 3.202, Average Loss: 4.108, avg. samples / sec: 21954.38
Iteration:   2640, Loss function: 3.627, Average Loss: 4.103, avg. samples / sec: 21943.60
Iteration:   2640, Loss function: 3.548, Average Loss: 4.094, avg. samples / sec: 21945.72
Iteration:   2640, Loss function: 3.403, Average Loss: 4.112, avg. samples / sec: 21938.90
Iteration:   2640, Loss function: 3.541, Average Loss: 4.096, avg. samples / sec: 21944.80
Iteration:   2640, Loss function: 3.647, Average Loss: 4.109, avg. samples / sec: 21937.59
Iteration:   2640, Loss function: 3.513, Average Loss: 4.111, avg. samples / sec: 21937.59

:::MLPv0.5.0 ssd 1541711153.059825659 (train.py:553) train_epoch: 46
Iteration:   2660, Loss function: 3.592, Average Loss: 4.089, avg. samples / sec: 21904.75
Iteration:   2660, Loss function: 3.371, Average Loss: 4.077, avg. samples / sec: 21898.18
Iteration:   2660, Loss function: 3.208, Average Loss: 4.092, avg. samples / sec: 21892.79
Iteration:   2660, Loss function: 3.448, Average Loss: 4.078, avg. samples / sec: 21898.51
Iteration:   2660, Loss function: 2.992, Average Loss: 4.080, avg. samples / sec: 21899.16
Iteration:   2660, Loss function: 3.182, Average Loss: 4.092, avg. samples / sec: 21903.16
Iteration:   2660, Loss function: 3.702, Average Loss: 4.098, avg. samples / sec: 21897.63
Iteration:   2660, Loss function: 3.600, Average Loss: 4.094, avg. samples / sec: 21899.75
Iteration:   2680, Loss function: 3.222, Average Loss: 4.062, avg. samples / sec: 21853.72
Iteration:   2680, Loss function: 3.564, Average Loss: 4.082, avg. samples / sec: 21855.66
Iteration:   2680, Loss function: 3.491, Average Loss: 4.080, avg. samples / sec: 21857.10
Iteration:   2680, Loss function: 3.092, Average Loss: 4.074, avg. samples / sec: 21844.29
Iteration:   2680, Loss function: 3.199, Average Loss: 4.077, avg. samples / sec: 21852.20
Iteration:   2680, Loss function: 3.544, Average Loss: 4.067, avg. samples / sec: 21851.59
Iteration:   2680, Loss function: 3.304, Average Loss: 4.076, avg. samples / sec: 21852.16
Iteration:   2680, Loss function: 3.139, Average Loss: 4.064, avg. samples / sec: 21847.75
Iteration:   2700, Loss function: 3.680, Average Loss: 4.048, avg. samples / sec: 21864.18
Iteration:   2700, Loss function: 2.629, Average Loss: 4.064, avg. samples / sec: 21865.53
Iteration:   2700, Loss function: 3.032, Average Loss: 4.063, avg. samples / sec: 21864.95
Iteration:   2700, Loss function: 2.881, Average Loss: 4.059, avg. samples / sec: 21863.47
Iteration:   2700, Loss function: 3.344, Average Loss: 4.060, avg. samples / sec: 21864.17
Iteration:   2700, Loss function: 3.309, Average Loss: 4.050, avg. samples / sec: 21868.38
Iteration:   2700, Loss function: 3.375, Average Loss: 4.067, avg. samples / sec: 21858.86
Iteration:   2700, Loss function: 3.123, Average Loss: 4.053, avg. samples / sec: 21860.82

:::MLPv0.5.0 ssd 1541711158.494133949 (train.py:553) train_epoch: 47
Iteration:   2720, Loss function: 3.618, Average Loss: 4.045, avg. samples / sec: 21888.84
Iteration:   2720, Loss function: 3.627, Average Loss: 4.051, avg. samples / sec: 21883.99
Iteration:   2720, Loss function: 2.903, Average Loss: 4.033, avg. samples / sec: 21875.12
Iteration:   2720, Loss function: 3.267, Average Loss: 4.037, avg. samples / sec: 21882.27
Iteration:   2720, Loss function: 3.311, Average Loss: 4.045, avg. samples / sec: 21880.75
Iteration:   2720, Loss function: 3.013, Average Loss: 4.050, avg. samples / sec: 21882.48
Iteration:   2720, Loss function: 2.708, Average Loss: 4.047, avg. samples / sec: 21876.56
Iteration:   2720, Loss function: 3.461, Average Loss: 4.038, avg. samples / sec: 21882.67
Iteration:   2740, Loss function: 3.619, Average Loss: 4.019, avg. samples / sec: 21918.61
Iteration:   2740, Loss function: 3.111, Average Loss: 4.036, avg. samples / sec: 21916.65
Iteration:   2740, Loss function: 3.044, Average Loss: 4.032, avg. samples / sec: 21919.67
Iteration:   2740, Loss function: 3.572, Average Loss: 4.026, avg. samples / sec: 21917.99
Iteration:   2740, Loss function: 3.288, Average Loss: 4.029, avg. samples / sec: 21916.22
Iteration:   2740, Loss function: 3.803, Average Loss: 4.031, avg. samples / sec: 21908.93
Iteration:   2740, Loss function: 3.639, Average Loss: 4.036, avg. samples / sec: 21916.82
Iteration:   2740, Loss function: 3.271, Average Loss: 4.023, avg. samples / sec: 21917.94
Iteration:   2760, Loss function: 2.952, Average Loss: 4.014, avg. samples / sec: 21874.00
Iteration:   2760, Loss function: 3.470, Average Loss: 4.003, avg. samples / sec: 21866.36
Iteration:   2760, Loss function: 3.150, Average Loss: 4.022, avg. samples / sec: 21869.98
Iteration:   2760, Loss function: 3.057, Average Loss: 4.020, avg. samples / sec: 21863.64
Iteration:   2760, Loss function: 3.184, Average Loss: 4.009, avg. samples / sec: 21869.25
Iteration:   2760, Loss function: 2.904, Average Loss: 4.011, avg. samples / sec: 21863.37
Iteration:   2760, Loss function: 2.978, Average Loss: 4.018, avg. samples / sec: 21863.10
Iteration:   2760, Loss function: 3.581, Average Loss: 4.014, avg. samples / sec: 21860.77

:::MLPv0.5.0 ssd 1541711163.926266432 (train.py:553) train_epoch: 48
Iteration:   2780, Loss function: 2.977, Average Loss: 3.989, avg. samples / sec: 21851.48
Iteration:   2780, Loss function: 3.313, Average Loss: 4.008, avg. samples / sec: 21849.29
Iteration:   2780, Loss function: 3.146, Average Loss: 3.998, avg. samples / sec: 21843.11
Iteration:   2780, Loss function: 3.302, Average Loss: 4.003, avg. samples / sec: 21850.16
Iteration:   2780, Loss function: 3.247, Average Loss: 3.993, avg. samples / sec: 21848.05
Iteration:   2780, Loss function: 3.672, Average Loss: 3.997, avg. samples / sec: 21849.23
Iteration:   2780, Loss function: 3.348, Average Loss: 4.004, avg. samples / sec: 21843.08
Iteration:   2780, Loss function: 3.700, Average Loss: 4.001, avg. samples / sec: 21851.67
Iteration:   2800, Loss function: 3.795, Average Loss: 3.982, avg. samples / sec: 21887.47
Iteration:   2800, Loss function: 3.199, Average Loss: 3.975, avg. samples / sec: 21878.94
Iteration:   2800, Loss function: 3.633, Average Loss: 3.989, avg. samples / sec: 21886.64
Iteration:   2800, Loss function: 3.412, Average Loss: 3.980, avg. samples / sec: 21885.00
Iteration:   2800, Loss function: 3.454, Average Loss: 3.994, avg. samples / sec: 21880.88
Iteration:   2800, Loss function: 2.808, Average Loss: 3.986, avg. samples / sec: 21884.01
Iteration:   2800, Loss function: 3.416, Average Loss: 3.984, avg. samples / sec: 21880.75
Iteration:   2800, Loss function: 2.994, Average Loss: 3.988, avg. samples / sec: 21882.80

































































:::MLPv0.5.0 ssd 1541711167.671772957 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711167.672595501 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711167.673331261 (train.py:220) eval_start: 48
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 22
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 30
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.65 s
Predicting Ended, total time: 5.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.65 s
Predicting Ended, total time: 5.65 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.65 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(313893, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(313893, 7)
(313893, 7)
Converting ndarray to lists...
(313893, 7)
Converting ndarray to lists...
0/313893
(313893, 7)
Converting ndarray to lists...
(313893, 7)
(313893, 7)
Converting ndarray to lists...
Loading and preparing results...
0/313893
0/313893
Converting ndarray to lists...
0/313893
(313893, 7)
0/313893
0/313893
(313893, 7)
0/313893
(313893, 7)
(313893, 7)
Loading and preparing results...
Converting ndarray to lists...
0/313893
Converting ndarray to lists...
0/313893
0/313893
(313893, 7)
(313893, 7)
0/313893
0/313893
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(313893, 7)
Loading and preparing results...
0/313893
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(313893, 7)
Loading and preparing results...
Loading and preparing results...
0/313893
Converting ndarray to lists...
0/313893
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(313893, 7)
(313893, 7)
(313893, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(313893, 7)
0/313893
Loading and preparing results...
0/313893
(313893, 7)
Converting ndarray to lists...
0/313893
0/313893
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(313893, 7)
(313893, 7)
(313893, 7)
0/313893
0/313893
0/313893
0/313893
(313893, 7)
0/313893
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(313893, 7)
Converting ndarray to lists...
(313893, 7)
Loading and preparing results...
(313893, 7)
0/313893
Loading and preparing results...
0/313893
0/313893
Converting ndarray to lists...
Converting ndarray to lists...
(313893, 7)
(313893, 7)
0/313893
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/313893
(313893, 7)
Loading and preparing results...
0/313893
Converting ndarray to lists...
Loading and preparing results...
(313893, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/313893
Loading and preparing results...
(313893, 7)
Converting ndarray to lists...
0/313893
(313893, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(313893, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(313893, 7)
0/313893
Loading and preparing results...
Loading and preparing results...
(313893, 7)
(313893, 7)
(313893, 7)
Converting ndarray to lists...
0/313893
0/313893
(313893, 7)
Loading and preparing results...
(313893, 7)
0/313893
0/313893
0/313893
Converting ndarray to lists...
Converting ndarray to lists...
0/313893
Converting ndarray to lists...
0/313893
(313893, 7)
(313893, 7)
(313893, 7)
0/313893
0/313893
Loading and preparing results...
0/313893
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(313893, 7)
(313893, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/313893
0/313893
(313893, 7)
(313893, 7)
(313893, 7)
0/313893
0/313893
0/313893
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(313893, 7)
(313893, 7)
Converting ndarray to lists...
0/313893
0/313893
(313893, 7)
0/313893
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(313893, 7)
0/313893
(313893, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/313893
Converting ndarray to lists...
Loading and preparing results...
(313893, 7)
Converting ndarray to lists...
(313893, 7)
Converting ndarray to lists...
0/313893
0/313893
Converting ndarray to lists...
Loading and preparing results...
(313893, 7)
Loading and preparing results...
(313893, 7)
Converting ndarray to lists...
(313893, 7)
Loading and preparing results...
0/313893
(313893, 7)
0/313893
Converting ndarray to lists...
0/313893
0/313893
(313893, 7)
Converting ndarray to lists...
0/313893
Loading and preparing results...
(313893, 7)
Loading and preparing results...
0/313893
Converting ndarray to lists...
Converting ndarray to lists...
(313893, 7)
(313893, 7)
Loading and preparing results...
0/313893
0/313893
Converting ndarray to lists...
(313893, 7)
0/313893
DONE (t=1.76s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.91s)
creating index...
DONE (t=1.91s)
creating index...
DONE (t=1.91s)
creating index...
index created!
DONE (t=1.91s)
creating index...
DONE (t=1.93s)
creating index...
index created!
index created!
index created!
index created!
DONE (t=1.94s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=2.01s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.04s)
creating index...
index created!
index created!
index created!
DONE (t=2.05s)
creating index...
index created!
DONE (t=2.05s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.07s)
creating index...
index created!
DONE (t=2.07s)
creating index...
DONE (t=2.08s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.09s)
creating index...
DONE (t=2.11s)
creating index...
DONE (t=2.12s)
creating index...
DONE (t=2.13s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.62s).
Accumulating evaluation results...
DONE (t=3.61s).
Accumulating evaluation results...
DONE (t=3.61s).
Accumulating evaluation results...
DONE (t=3.62s).
Accumulating evaluation results...
DONE (t=3.62s).
Accumulating evaluation results...
DONE (t=3.61s).
Accumulating evaluation results...
DONE (t=3.65s).
Accumulating evaluation results...
DONE (t=3.66s).
Accumulating evaluation results...
DONE (t=1.14s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
DONE (t=1.17s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.364
DONE (t=1.15s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.364
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.364
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
DONE (t=1.16s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.500
Current AP: 0.21129 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.364
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.500
Current AP: 0.21129 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.500
Current AP: 0.21129 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
DONE (t=1.17s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
DONE (t=1.15s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.500
Current AP: 0.21129 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.364
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.364
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.500
Current AP: 0.21129 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.500
Current AP: 0.21129 AP goal: 0.21200
DONE (t=1.09s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.364
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.500
Current AP: 0.21129 AP goal: 0.21200
DONE (t=1.21s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.364
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.500
Current AP: 0.21129 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711180.506880283 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711180.507773876 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.2112911635301769}

:::MLPv0.5.0 ssd 1541711180.508538008 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.2112911635301769}

:::MLPv0.5.0 ssd 1541711180.509293318 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711180.510104656 (train.py:338) eval_stop: 48
Iteration:   2820, Loss function: 2.910, Average Loss: 3.968, avg. samples / sec: 2694.08
Iteration:   2820, Loss function: 3.179, Average Loss: 3.961, avg. samples / sec: 2694.09
Iteration:   2820, Loss function: 2.815, Average Loss: 3.979, avg. samples / sec: 2694.12
Iteration:   2820, Loss function: 3.032, Average Loss: 3.965, avg. samples / sec: 2694.04
Iteration:   2820, Loss function: 3.263, Average Loss: 3.974, avg. samples / sec: 2694.15
Iteration:   2820, Loss function: 3.425, Average Loss: 3.975, avg. samples / sec: 2693.98
Iteration:   2820, Loss function: 3.262, Average Loss: 3.972, avg. samples / sec: 2694.06
Iteration:   2820, Loss function: 2.894, Average Loss: 3.970, avg. samples / sec: 2693.99

:::MLPv0.5.0 ssd 1541711182.679345846 (train.py:553) train_epoch: 49
Iteration:   2840, Loss function: 2.929, Average Loss: 3.952, avg. samples / sec: 21927.74
Iteration:   2840, Loss function: 3.275, Average Loss: 3.964, avg. samples / sec: 21929.62
Iteration:   2840, Loss function: 3.824, Average Loss: 3.950, avg. samples / sec: 21931.29
Iteration:   2840, Loss function: 3.574, Average Loss: 3.960, avg. samples / sec: 21925.20
Iteration:   2840, Loss function: 2.863, Average Loss: 3.944, avg. samples / sec: 21919.77
Iteration:   2840, Loss function: 2.880, Average Loss: 3.958, avg. samples / sec: 21928.22
Iteration:   2840, Loss function: 3.505, Average Loss: 3.955, avg. samples / sec: 21925.35
Iteration:   2840, Loss function: 2.916, Average Loss: 3.956, avg. samples / sec: 21920.41
Iteration:   2860, Loss function: 3.073, Average Loss: 3.930, avg. samples / sec: 21953.97
Iteration:   2860, Loss function: 3.519, Average Loss: 3.950, avg. samples / sec: 21942.82
Iteration:   2860, Loss function: 2.797, Average Loss: 3.939, avg. samples / sec: 21936.38
Iteration:   2860, Loss function: 2.873, Average Loss: 3.940, avg. samples / sec: 21957.55
Iteration:   2860, Loss function: 3.326, Average Loss: 3.944, avg. samples / sec: 21944.19
Iteration:   2860, Loss function: 3.094, Average Loss: 3.936, avg. samples / sec: 21938.04
Iteration:   2860, Loss function: 3.444, Average Loss: 3.947, avg. samples / sec: 21942.98
Iteration:   2860, Loss function: 3.049, Average Loss: 3.943, avg. samples / sec: 21946.30
Iteration:   2880, Loss function: 3.499, Average Loss: 3.924, avg. samples / sec: 21943.32
Iteration:   2880, Loss function: 3.791, Average Loss: 3.915, avg. samples / sec: 21934.75
Iteration:   2880, Loss function: 2.845, Average Loss: 3.936, avg. samples / sec: 21936.66
Iteration:   2880, Loss function: 3.473, Average Loss: 3.935, avg. samples / sec: 21937.97
Iteration:   2880, Loss function: 3.127, Average Loss: 3.926, avg. samples / sec: 21937.55
Iteration:   2880, Loss function: 2.708, Average Loss: 3.928, avg. samples / sec: 21935.25
Iteration:   2880, Loss function: 2.904, Average Loss: 3.922, avg. samples / sec: 21935.40
Iteration:   2880, Loss function: 3.311, Average Loss: 3.926, avg. samples / sec: 21925.59

:::MLPv0.5.0 ssd 1541711188.001978636 (train.py:553) train_epoch: 50
Iteration:   2900, Loss function: 3.334, Average Loss: 3.901, avg. samples / sec: 21956.68
Iteration:   2900, Loss function: 3.256, Average Loss: 3.923, avg. samples / sec: 21962.06
Iteration:   2900, Loss function: 3.536, Average Loss: 3.921, avg. samples / sec: 21948.93
Iteration:   2900, Loss function: 3.408, Average Loss: 3.914, avg. samples / sec: 21947.25
Iteration:   2900, Loss function: 3.371, Average Loss: 3.911, avg. samples / sec: 21955.70
Iteration:   2900, Loss function: 3.032, Average Loss: 3.913, avg. samples / sec: 21964.31
Iteration:   2900, Loss function: 3.192, Average Loss: 3.915, avg. samples / sec: 21952.98
Iteration:   2900, Loss function: 3.352, Average Loss: 3.907, avg. samples / sec: 21950.02
Iteration:   2920, Loss function: 3.740, Average Loss: 3.890, avg. samples / sec: 21992.13
Iteration:   2920, Loss function: 3.251, Average Loss: 3.908, avg. samples / sec: 21992.83
Iteration:   2920, Loss function: 3.462, Average Loss: 3.899, avg. samples / sec: 21996.70
Iteration:   2920, Loss function: 3.620, Average Loss: 3.898, avg. samples / sec: 21994.05
Iteration:   2920, Loss function: 3.042, Average Loss: 3.892, avg. samples / sec: 21999.57
Iteration:   2920, Loss function: 3.310, Average Loss: 3.895, avg. samples / sec: 21991.70
Iteration:   2920, Loss function: 3.313, Average Loss: 3.903, avg. samples / sec: 21992.45
Iteration:   2920, Loss function: 3.334, Average Loss: 3.907, avg. samples / sec: 21986.46
Iteration:   2940, Loss function: 3.655, Average Loss: 3.876, avg. samples / sec: 21983.26
Iteration:   2940, Loss function: 2.826, Average Loss: 3.888, avg. samples / sec: 21985.41
Iteration:   2940, Loss function: 2.543, Average Loss: 3.879, avg. samples / sec: 21984.75
Iteration:   2940, Loss function: 3.431, Average Loss: 3.894, avg. samples / sec: 21977.59
Iteration:   2940, Loss function: 3.102, Average Loss: 3.881, avg. samples / sec: 21981.75
Iteration:   2940, Loss function: 3.175, Average Loss: 3.885, avg. samples / sec: 21977.13
Iteration:   2940, Loss function: 3.417, Average Loss: 3.894, avg. samples / sec: 21979.42
Iteration:   2940, Loss function: 3.007, Average Loss: 3.890, avg. samples / sec: 21973.85

:::MLPv0.5.0 ssd 1541711193.409592390 (train.py:553) train_epoch: 51
Iteration:   2960, Loss function: 3.239, Average Loss: 3.882, avg. samples / sec: 21941.33
Iteration:   2960, Loss function: 2.746, Average Loss: 3.875, avg. samples / sec: 21933.51
Iteration:   2960, Loss function: 3.676, Average Loss: 3.865, avg. samples / sec: 21938.15
Iteration:   2960, Loss function: 2.828, Average Loss: 3.879, avg. samples / sec: 21946.98
Iteration:   2960, Loss function: 3.099, Average Loss: 3.863, avg. samples / sec: 21929.89
Iteration:   2960, Loss function: 3.513, Average Loss: 3.876, avg. samples / sec: 21948.80
Iteration:   2960, Loss function: 3.582, Average Loss: 3.865, avg. samples / sec: 21938.34
Iteration:   2960, Loss function: 3.038, Average Loss: 3.871, avg. samples / sec: 21936.38
Iteration:   2980, Loss function: 3.501, Average Loss: 3.848, avg. samples / sec: 21953.45
Iteration:   2980, Loss function: 2.775, Average Loss: 3.852, avg. samples / sec: 21950.52
Iteration:   2980, Loss function: 2.825, Average Loss: 3.865, avg. samples / sec: 21948.14
Iteration:   2980, Loss function: 3.357, Average Loss: 3.867, avg. samples / sec: 21949.88
Iteration:   2980, Loss function: 2.765, Average Loss: 3.866, avg. samples / sec: 21941.15
Iteration:   2980, Loss function: 3.134, Average Loss: 3.852, avg. samples / sec: 21947.08
Iteration:   2980, Loss function: 3.362, Average Loss: 3.857, avg. samples / sec: 21949.91
Iteration:   2980, Loss function: 3.377, Average Loss: 3.860, avg. samples / sec: 21937.41
Iteration:   3000, Loss function: 3.202, Average Loss: 3.833, avg. samples / sec: 21859.94
Iteration:   3000, Loss function: 3.406, Average Loss: 3.848, avg. samples / sec: 21871.90
Iteration:   3000, Loss function: 3.281, Average Loss: 3.851, avg. samples / sec: 21860.81
Iteration:   3000, Loss function: 2.982, Average Loss: 3.853, avg. samples / sec: 21861.19
Iteration:   3000, Loss function: 3.259, Average Loss: 3.841, avg. samples / sec: 21861.08
Iteration:   3000, Loss function: 2.868, Average Loss: 3.839, avg. samples / sec: 21850.30
Iteration:   3000, Loss function: 3.360, Average Loss: 3.852, avg. samples / sec: 21853.92
Iteration:   3000, Loss function: 3.007, Average Loss: 3.845, avg. samples / sec: 21857.44

:::MLPv0.5.0 ssd 1541711198.829017878 (train.py:553) train_epoch: 52
Iteration:   3020, Loss function: 3.441, Average Loss: 3.837, avg. samples / sec: 21852.23
Iteration:   3020, Loss function: 3.082, Average Loss: 3.838, avg. samples / sec: 21856.01
Iteration:   3020, Loss function: 3.548, Average Loss: 3.827, avg. samples / sec: 21855.27
Iteration:   3020, Loss function: 3.647, Average Loss: 3.820, avg. samples / sec: 21841.76
Iteration:   3020, Loss function: 3.447, Average Loss: 3.826, avg. samples / sec: 21850.63
Iteration:   3020, Loss function: 3.181, Average Loss: 3.838, avg. samples / sec: 21846.94
Iteration:   3020, Loss function: 2.971, Average Loss: 3.829, avg. samples / sec: 21853.46
Iteration:   3020, Loss function: 2.832, Average Loss: 3.838, avg. samples / sec: 21841.26
Iteration:   3040, Loss function: 2.920, Average Loss: 3.823, avg. samples / sec: 21928.18
Iteration:   3040, Loss function: 3.133, Average Loss: 3.826, avg. samples / sec: 21939.93
Iteration:   3040, Loss function: 3.046, Average Loss: 3.826, avg. samples / sec: 21936.14
Iteration:   3040, Loss function: 2.541, Average Loss: 3.804, avg. samples / sec: 21934.37
Iteration:   3040, Loss function: 3.518, Average Loss: 3.815, avg. samples / sec: 21933.43
Iteration:   3040, Loss function: 3.075, Average Loss: 3.813, avg. samples / sec: 21928.56
Iteration:   3040, Loss function: 3.072, Average Loss: 3.815, avg. samples / sec: 21930.20
Iteration:   3040, Loss function: 3.314, Average Loss: 3.825, avg. samples / sec: 21922.07
Iteration:   3060, Loss function: 3.079, Average Loss: 3.812, avg. samples / sec: 21900.86
Iteration:   3060, Loss function: 2.998, Average Loss: 3.811, avg. samples / sec: 21897.96
Iteration:   3060, Loss function: 3.071, Average Loss: 3.792, avg. samples / sec: 21897.11
Iteration:   3060, Loss function: 3.232, Average Loss: 3.800, avg. samples / sec: 21900.92
Iteration:   3060, Loss function: 2.789, Average Loss: 3.801, avg. samples / sec: 21898.86
Iteration:   3060, Loss function: 3.235, Average Loss: 3.810, avg. samples / sec: 21889.57
Iteration:   3060, Loss function: 3.111, Average Loss: 3.801, avg. samples / sec: 21894.21
Iteration:   3060, Loss function: 3.318, Average Loss: 3.811, avg. samples / sec: 21892.19

:::MLPv0.5.0 ssd 1541711204.257975340 (train.py:553) train_epoch: 53
Iteration:   3080, Loss function: 3.383, Average Loss: 3.780, avg. samples / sec: 21873.39
Iteration:   3080, Loss function: 2.631, Average Loss: 3.787, avg. samples / sec: 21878.49
Iteration:   3080, Loss function: 3.399, Average Loss: 3.799, avg. samples / sec: 21874.09
Iteration:   3080, Loss function: 2.532, Average Loss: 3.797, avg. samples / sec: 21865.40
Iteration:   3080, Loss function: 3.494, Average Loss: 3.799, avg. samples / sec: 21861.13
Iteration:   3080, Loss function: 3.330, Average Loss: 3.797, avg. samples / sec: 21879.65
Iteration:   3080, Loss function: 3.914, Average Loss: 3.790, avg. samples / sec: 21866.65
Iteration:   3080, Loss function: 3.206, Average Loss: 3.787, avg. samples / sec: 21860.93
Iteration:   3100, Loss function: 2.379, Average Loss: 3.786, avg. samples / sec: 21909.24
Iteration:   3100, Loss function: 2.743, Average Loss: 3.765, avg. samples / sec: 21901.23
Iteration:   3100, Loss function: 3.395, Average Loss: 3.787, avg. samples / sec: 21902.78
Iteration:   3100, Loss function: 3.008, Average Loss: 3.773, avg. samples / sec: 21901.75
Iteration:   3100, Loss function: 2.970, Average Loss: 3.783, avg. samples / sec: 21906.82
Iteration:   3100, Loss function: 3.022, Average Loss: 3.774, avg. samples / sec: 21914.40
Iteration:   3100, Loss function: 3.127, Average Loss: 3.786, avg. samples / sec: 21905.70
Iteration:   3100, Loss function: 4.158, Average Loss: 3.779, avg. samples / sec: 21902.32

:::MLPv0.5.0 ssd 1541711209.596092939 (train.py:553) train_epoch: 54
Iteration:   3120, Loss function: 2.963, Average Loss: 3.772, avg. samples / sec: 21788.25
Iteration:   3120, Loss function: 2.934, Average Loss: 3.760, avg. samples / sec: 21783.98
Iteration:   3120, Loss function: 3.340, Average Loss: 3.753, avg. samples / sec: 21782.48
Iteration:   3120, Loss function: 3.006, Average Loss: 3.772, avg. samples / sec: 21783.70
Iteration:   3120, Loss function: 3.268, Average Loss: 3.762, avg. samples / sec: 21785.02
Iteration:   3120, Loss function: 3.001, Average Loss: 3.771, avg. samples / sec: 21782.71
Iteration:   3120, Loss function: 3.363, Average Loss: 3.773, avg. samples / sec: 21779.19
Iteration:   3120, Loss function: 2.956, Average Loss: 3.767, avg. samples / sec: 21783.65
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2

:::MLPv0.5.0 ssd 1541711210.167665958 (train.py:586) opt_learning_rate: 0.0016

































































:::MLPv0.5.0 ssd 1541711210.259979725 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711210.260837555 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711210.261589527 (train.py:220) eval_start: 54
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 25
Predicting Ended, total time: 5.57 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.57 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.57 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.57 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.57 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.57 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.57 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.57 s
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(286478, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/286478
(286478, 7)
Converting ndarray to lists...
(286478, 7)
0/286478
(286478, 7)
0/286478
0/286478
Loading and preparing results...
Converting ndarray to lists...
(286478, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/286478
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(286478, 7)
(286478, 7)
(286478, 7)
0/286478
0/286478
0/286478
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(286478, 7)
Converting ndarray to lists...
0/286478
Loading and preparing results...
(286478, 7)
Loading and preparing results...
0/286478
Converting ndarray to lists...
Converting ndarray to lists...
(286478, 7)
(286478, 7)
0/286478
0/286478
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(286478, 7)
(286478, 7)
(286478, 7)
Converting ndarray to lists...
0/286478
0/286478
0/286478
(286478, 7)
0/286478
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(286478, 7)
Converting ndarray to lists...
Loading and preparing results...
0/286478
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(286478, 7)
Converting ndarray to lists...
(286478, 7)
Converting ndarray to lists...
0/286478
(286478, 7)
Loading and preparing results...
0/286478
(286478, 7)
Converting ndarray to lists...
(286478, 7)
(286478, 7)
0/286478
0/286478
0/286478
0/286478
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(286478, 7)
Converting ndarray to lists...
0/286478
(286478, 7)
Loading and preparing results...
Loading and preparing results...
0/286478
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(286478, 7)
Loading and preparing results...
Loading and preparing results...
(286478, 7)
Loading and preparing results...
(286478, 7)
Converting ndarray to lists...
0/286478
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/286478
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/286478
(286478, 7)
(286478, 7)
0/286478
Converting ndarray to lists...
(286478, 7)
0/286478
Loading and preparing results...
(286478, 7)
Converting ndarray to lists...
0/286478
(286478, 7)
Loading and preparing results...
Converting ndarray to lists...
(286478, 7)
(286478, 7)
Loading and preparing results...
0/286478
0/286478
(286478, 7)
Converting ndarray to lists...
(286478, 7)
Loading and preparing results...
0/286478
Converting ndarray to lists...
0/286478
0/286478
0/286478
Loading and preparing results...
(286478, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/286478
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(286478, 7)
(286478, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(286478, 7)
0/286478
Converting ndarray to lists...
(286478, 7)
(286478, 7)
(286478, 7)
(286478, 7)
Loading and preparing results...
(286478, 7)
0/286478
Loading and preparing results...
(286478, 7)
(286478, 7)
Loading and preparing results...
Converting ndarray to lists...
0/286478
(286478, 7)
0/286478
0/286478
0/286478
0/286478
0/286478
(286478, 7)
Converting ndarray to lists...
0/286478
0/286478
Converting ndarray to lists...
Converting ndarray to lists...
0/286478
(286478, 7)
Converting ndarray to lists...
Loading and preparing results...
(286478, 7)
0/286478
(286478, 7)
Converting ndarray to lists...
0/286478
(286478, 7)
0/286478
0/286478
(286478, 7)
Loading and preparing results...
Loading and preparing results...
0/286478
Loading and preparing results...
Loading and preparing results...
0/286478
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(286478, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
0/286478
(286478, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(286478, 7)
(286478, 7)
(286478, 7)
0/286478
0/286478
Converting ndarray to lists...
0/286478
0/286478
Converting ndarray to lists...
(286478, 7)
Converting ndarray to lists...
Loading and preparing results...
(286478, 7)
0/286478
(286478, 7)
Converting ndarray to lists...
0/286478
(286478, 7)
0/286478
0/286478
DONE (t=1.64s)
creating index...
DONE (t=1.67s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
index created!
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.80s)
creating index...
index created!
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.81s)
creating index...
index created!
DONE (t=1.82s)
creating index...
index created!
DONE (t=1.83s)
creating index...
DONE (t=1.84s)
creating index...
index created!
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.85s)
creating index...
index created!
DONE (t=1.85s)
creating index...
DONE (t=1.85s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.87s)
creating index...
index created!
index created!
DONE (t=1.88s)
creating index...
index created!
DONE (t=1.89s)
creating index...
index created!
index created!
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
index created!
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
index created!
DONE (t=1.91s)
creating index...
index created!
DONE (t=1.91s)
creating index...
DONE (t=1.91s)
creating index...
index created!
index created!
DONE (t=1.92s)
creating index...
DONE (t=1.92s)
creating index...
DONE (t=1.92s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.93s)
creating index...
index created!
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
index created!
index created!
DONE (t=1.93s)
creating index...
index created!
DONE (t=1.94s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=1.94s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.94s)
creating index...
index created!
DONE (t=1.95s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.97s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
DONE (t=3.43s).
Accumulating evaluation results...
DONE (t=3.44s).
Accumulating evaluation results...
DONE (t=3.41s).
Accumulating evaluation results...
DONE (t=3.46s).
Accumulating evaluation results...
DONE (t=3.45s).
Accumulating evaluation results...
DONE (t=3.44s).
Accumulating evaluation results...
DONE (t=3.45s).
Accumulating evaluation results...
DONE (t=3.81s).
Accumulating evaluation results...
DONE (t=1.06s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501
Current AP: 0.21239 AP goal: 0.21200
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501
Current AP: 0.21239 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501
Current AP: 0.21239 AP goal: 0.21200
DONE (t=1.09s).
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
DONE (t=1.08s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501
Current AP: 0.21239 AP goal: 0.21200
DONE (t=1.09s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501
Current AP: 0.21239 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501
Current AP: 0.21239 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501
Current AP: 0.21239 AP goal: 0.21200
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501
Current AP: 0.21239 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711222.698061943 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711222.698948383 (train.py:333) eval_accuracy: {"epoch": 54, "value": 0.2123943755067403}

:::MLPv0.5.0 ssd 1541711222.699757576 (train.py:336) eval_iteration_accuracy: {"epoch": 54, "value": 0.2123943755067403}

:::MLPv0.5.0 ssd 1541711222.700629234 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711222.701458216 (train.py:338) eval_stop: 54

:::MLPv0.5.0 ssd 1541711223.749389648 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1541711223.750130415 (train.py:707) run_final
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2018-11-08 09:07:08 PM
RESULT,OBJECT_DETECTION,,407,nvidia,2018-11-08 09:00:21 PM
ENDING TIMING RUN AT 2018-11-08 09:07:08 PM
RESULT,OBJECT_DETECTION,,407,nvidia,2018-11-08 09:00:21 PM
ENDING TIMING RUN AT 2018-11-08 09:07:08 PM
RESULT,OBJECT_DETECTION,,407,nvidia,2018-11-08 09:00:21 PM
ENDING TIMING RUN AT 2018-11-08 09:07:08 PM
RESULT,OBJECT_DETECTION,,407,nvidia,2018-11-08 09:00:21 PM
ENDING TIMING RUN AT 2018-11-08 09:07:08 PM
RESULT,OBJECT_DETECTION,,407,nvidia,2018-11-08 09:00:21 PM
ENDING TIMING RUN AT 2018-11-08 09:07:08 PM
RESULT,OBJECT_DETECTION,,407,nvidia,2018-11-08 09:00:21 PM
ENDING TIMING RUN AT 2018-11-08 09:07:08 PM
RESULT,OBJECT_DETECTION,,407,nvidia,2018-11-08 09:00:21 PM
ENDING TIMING RUN AT 2018-11-08 09:07:08 PM
RESULT,OBJECT_DETECTION,,407,nvidia,2018-11-08 09:00:21 PM
