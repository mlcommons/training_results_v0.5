Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3

:::MLPv0.5.0 ssd 1541710824.702882767 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.721375942 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.787059307 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.840207815 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.887329102 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.904003143 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.910029650 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.912245512 (<string>:1) run_clear_caches
Launching on node sc-sdgx-737
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-745
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-746
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-747
++ eval echo srun -N 1 -n 1 -w '$hostn'
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-745
+++ echo srun -N 1 -n 1 -w sc-sdgx-737
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-752
+ srun -N 1 -n 1 -w sc-sdgx-745 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=1 --master_addr=172.22.2.26 --master_port=4242' -e SLURM_JOB_ID=155375 -e SLURM_NTASKS_PER_NODE=8 cont_155375 ./run_and_time.sh
+ srun -N 1 -n 1 -w sc-sdgx-737 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=0 --master_addr=172.22.2.26 --master_port=4242' -e SLURM_JOB_ID=155375 -e SLURM_NTASKS_PER_NODE=8 cont_155375 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-746
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-753
++ eval echo srun -N 1 -n 1 -w '$hostn'
+ srun -N 1 -n 1 -w sc-sdgx-746 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=2 --master_addr=172.22.2.26 --master_port=4242' -e SLURM_JOB_ID=155375 -e SLURM_NTASKS_PER_NODE=8 cont_155375 ./run_and_time.sh
+++ echo srun -N 1 -n 1 -w sc-sdgx-747
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-754
++ eval echo srun -N 1 -n 1 -w '$hostn'
+ srun -N 1 -n 1 -w sc-sdgx-747 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=3 --master_addr=172.22.2.26 --master_port=4242' -e SLURM_JOB_ID=155375 -e SLURM_NTASKS_PER_NODE=8 cont_155375 ./run_and_time.sh
+++ echo srun -N 1 -n 1 -w sc-sdgx-752
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-755
+ srun -N 1 -n 1 -w sc-sdgx-752 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=4 --master_addr=172.22.2.26 --master_port=4242' -e SLURM_JOB_ID=155375 -e SLURM_NTASKS_PER_NODE=8 cont_155375 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+ pids+=($!)
+ set +x
+++ echo srun -N 1 -n 1 -w sc-sdgx-753
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-754
+ srun -N 1 -n 1 -w sc-sdgx-753 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=5 --master_addr=172.22.2.26 --master_port=4242' -e SLURM_JOB_ID=155375 -e SLURM_NTASKS_PER_NODE=8 cont_155375 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-755
+ srun -N 1 -n 1 -w sc-sdgx-754 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=6 --master_addr=172.22.2.26 --master_port=4242' -e SLURM_JOB_ID=155375 -e SLURM_NTASKS_PER_NODE=8 cont_155375 ./run_and_time.sh
+ srun -N 1 -n 1 -w sc-sdgx-755 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=7 --master_addr=172.22.2.26 --master_port=4242' -e SLURM_JOB_ID=155375 -e SLURM_NTASKS_PER_NODE=8 cont_155375 ./run_and_time.sh
Run vars: id 155375 gpus 8 mparams  --nnodes=8 --node_rank=0 --master_addr=172.22.2.26 --master_port=4242
Run vars: id 155375 gpus 8 mparams  --nnodes=8 --node_rank=1 --master_addr=172.22.2.26 --master_port=4242
Run vars: id 155375 gpus 8 mparams  --nnodes=8 --node_rank=2 --master_addr=172.22.2.26 --master_port=4242
Run vars: id 155375 gpus 8 mparams  --nnodes=8 --node_rank=3 --master_addr=172.22.2.26 --master_port=4242
Run vars: id 155375 gpus 8 mparams  --nnodes=8 --node_rank=5 --master_addr=172.22.2.26 --master_port=4242
Run vars: id 155375 gpus 8 mparams  --nnodes=8 --node_rank=6 --master_addr=172.22.2.26 --master_port=4242
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=0 --master_addr=172.22.2.26 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
Run vars: id 155375 gpus 8 mparams  --nnodes=8 --node_rank=4 --master_addr=172.22.2.26 --master_port=4242
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=1 --master_addr=172.22.2.26 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
Run vars: id 155375 gpus 8 mparams  --nnodes=8 --node_rank=7 --master_addr=172.22.2.26 --master_port=4242
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=2 --master_addr=172.22.2.26 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=3 --master_addr=172.22.2.26 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=5 --master_addr=172.22.2.26 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=6 --master_addr=172.22.2.26 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=4 --master_addr=172.22.2.26 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=7 --master_addr=172.22.2.26 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
0 Using seed = 4225792618
1 Using seed = 4225792619
3 Using seed = 4225792621
5 Using seed = 4225792623
7 Using seed = 4225792625
4 Using seed = 4225792622
14 Using seed = 4225792632
13 Using seed = 4225792631
12 Using seed = 4225792630
15 Using seed = 4225792633
9 Using seed = 4225792627
11 Using seed = 4225792629
10 Using seed = 4225792628
8 Using seed = 4225792626
16 Using seed = 4225792634
18 Using seed = 4225792636
17 Using seed = 4225792635
19 Using seed = 4225792637
21 Using seed = 4225792639
22 Using seed = 4225792640
20 Using seed = 4225792638
23 Using seed = 4225792641
31 Using seed = 4225792649
29 Using seed = 4225792647
28 Using seed = 4225792646
30 Using seed = 4225792648
26 Using seed = 4225792644
25 Using seed = 4225792643
24 Using seed = 4225792642
27 Using seed = 4225792645
32 Using seed = 4225792650
33 Using seed = 4225792651
34 Using seed = 4225792652
35 Using seed = 4225792653
39 Using seed = 4225792657
36 Using seed = 4225792654
37 Using seed = 4225792655
38 Using seed = 4225792656
44 Using seed = 4225792662
46 Using seed = 4225792664
45 Using seed = 4225792663
47 Using seed = 4225792665
40 Using seed = 4225792658
42 Using seed = 4225792660
41 Using seed = 4225792659
43 Using seed = 4225792661
54 Using seed = 4225792672
53 Using seed = 4225792671
55 Using seed = 4225792673
52 Using seed = 4225792670
49 Using seed = 4225792667
48 Using seed = 4225792666
50 Using seed = 4225792668
51 Using seed = 4225792669
56 Using seed = 4225792674
58 Using seed = 4225792676
59 Using seed = 4225792677
57 Using seed = 4225792675
63 Using seed = 4225792681
62 Using seed = 4225792680
61 Using seed = 4225792679
60 Using seed = 4225792678
2 Using seed = 4225792620
6 Using seed = 4225792624

:::MLPv0.5.0 ssd 1541710837.325950861 (train.py:371) run_start

:::MLPv0.5.0 ssd 1541710837.328720331 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1541710837.341771603 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1541710837.354774237 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1541710837.355584145 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1541710837.405467987 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1541710837.418764353 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1541710837.432084322 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1541710837.437990904 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1541710837.441312551 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1541710837.454281330 (train.py:382) input_size: 300
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
index created!
index created!
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.51s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.52s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Done (t=0.54s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
time_check a: 1541710838.446840048
time_check a: 1541710838.446676254
time_check a: 1541710838.447100878
time_check a: 1541710838.448817968
time_check a: 1541710838.449775457
time_check a: 1541710838.452966928
time_check a: 1541710838.459213018
time_check a: 1541710838.467355967
time_check b: 1541710862.054076672
time_check b: 1541710862.078302145
time_check b: 1541710862.085818291
time_check b: 1541710862.095993280
time_check b: 1541710862.143336296
time_check b: 1541710862.175490379
time_check b: 1541710862.186625481
time_check b: 1541710862.240084410

:::MLPv0.5.0 ssd 1541710863.603452206 (train.py:413) input_order

:::MLPv0.5.0 ssd 1541710863.609508514 (train.py:414) input_batch_size: 32

:::MLPv0.5.0 ssd 1541710867.728929520 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710867.729811430 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710867.759799480 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1541710868.858490944 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1541710868.859288692 (train.py:477) opt_learning_rate: 0.16

:::MLPv0.5.0 ssd 1541710868.860008717 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1541710868.860686302 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1541710868.861392021 (train.py:483) opt_learning_rate_warmup_steps: 900

:::MLPv0.5.0 ssd 1541710872.952101469 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710872.952963114 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710872.983408451 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss

:::MLPv0.5.0 ssd 1541710875.818527460 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1541710875.819327593 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1541710875.822927713 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 22.730, Average Loss: 0.023, avg. samples / sec: 22064.74
Iteration:      0, Loss function: 23.158, Average Loss: 0.023, avg. samples / sec: 29473.30
Iteration:      0, Loss function: 23.093, Average Loss: 0.023, avg. samples / sec: 15160.76
Iteration:      0, Loss function: 22.876, Average Loss: 0.023, avg. samples / sec: 19680.88
Iteration:      0, Loss function: 23.018, Average Loss: 0.023, avg. samples / sec: 40144.95
Iteration:      0, Loss function: 23.568, Average Loss: 0.024, avg. samples / sec: 19039.87
Iteration:      0, Loss function: 23.135, Average Loss: 0.023, avg. samples / sec: 36179.72
Iteration:      0, Loss function: 22.822, Average Loss: 0.023, avg. samples / sec: 23357.76

:::MLPv0.5.0 ssd 1541710877.718773365 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.0001777777777777767}

:::MLPv0.5.0 ssd 1541710877.957985163 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0003555555555555534}

:::MLPv0.5.0 ssd 1541710878.071287870 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0005333333333333301}

:::MLPv0.5.0 ssd 1541710878.175099850 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0007111111111111068}

:::MLPv0.5.0 ssd 1541710878.302815914 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0008888888888888835}

:::MLPv0.5.0 ssd 1541710878.407440424 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0010666666666666602}

:::MLPv0.5.0 ssd 1541710878.508487463 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.001244444444444437}

:::MLPv0.5.0 ssd 1541710878.621194601 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0014222222222222136}

:::MLPv0.5.0 ssd 1541710878.728318214 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0015999999999999903}

:::MLPv0.5.0 ssd 1541710878.831203222 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.001777777777777767}

:::MLPv0.5.0 ssd 1541710878.936208963 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0019555555555555437}

:::MLPv0.5.0 ssd 1541710879.036797523 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.0021333333333333204}

:::MLPv0.5.0 ssd 1541710879.138257742 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.002311111111111097}

:::MLPv0.5.0 ssd 1541710879.241649151 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.002488888888888874}

:::MLPv0.5.0 ssd 1541710879.340618134 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.0026666666666666505}

:::MLPv0.5.0 ssd 1541710879.440604925 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.0028444444444444272}

:::MLPv0.5.0 ssd 1541710879.544133663 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.0030222222222222317}

:::MLPv0.5.0 ssd 1541710879.641665936 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.0032000000000000084}

:::MLPv0.5.0 ssd 1541710879.746414185 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.003377777777777785}

:::MLPv0.5.0 ssd 1541710879.845468044 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.003555555555555562}
Iteration:     20, Loss function: 20.270, Average Loss: 0.441, avg. samples / sec: 10222.02
Iteration:     20, Loss function: 19.936, Average Loss: 0.441, avg. samples / sec: 10223.22
Iteration:     20, Loss function: 19.391, Average Loss: 0.442, avg. samples / sec: 10221.92
Iteration:     20, Loss function: 20.740, Average Loss: 0.442, avg. samples / sec: 10216.59
Iteration:     20, Loss function: 20.345, Average Loss: 0.443, avg. samples / sec: 10213.30
Iteration:     20, Loss function: 21.104, Average Loss: 0.445, avg. samples / sec: 10207.21
Iteration:     20, Loss function: 20.585, Average Loss: 0.444, avg. samples / sec: 10213.05
Iteration:     20, Loss function: 19.925, Average Loss: 0.443, avg. samples / sec: 10202.22

:::MLPv0.5.0 ssd 1541710879.945280790 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.0037333333333333385}

:::MLPv0.5.0 ssd 1541710880.044188261 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.003911111111111115}

:::MLPv0.5.0 ssd 1541710880.142019510 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.004088888888888892}

:::MLPv0.5.0 ssd 1541710880.251393318 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.004266666666666669}

:::MLPv0.5.0 ssd 1541710880.354543924 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.004444444444444445}

:::MLPv0.5.0 ssd 1541710880.456748009 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.004622222222222222}

:::MLPv0.5.0 ssd 1541710880.557642698 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.004799999999999999}

:::MLPv0.5.0 ssd 1541710880.678569794 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.004977777777777775}

:::MLPv0.5.0 ssd 1541710880.778367043 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.005155555555555552}

:::MLPv0.5.0 ssd 1541710880.875922918 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.005333333333333329}

:::MLPv0.5.0 ssd 1541710880.975916147 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.0055111111111111055}

:::MLPv0.5.0 ssd 1541710881.077868938 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.005688888888888882}

:::MLPv0.5.0 ssd 1541710881.175419569 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.005866666666666659}

:::MLPv0.5.0 ssd 1541710881.283671379 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.006044444444444436}

:::MLPv0.5.0 ssd 1541710881.383835316 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.006222222222222212}

:::MLPv0.5.0 ssd 1541710881.481425047 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.006399999999999989}

:::MLPv0.5.0 ssd 1541710881.584119558 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.006577777777777766}

:::MLPv0.5.0 ssd 1541710881.684217930 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.0067555555555555424}

:::MLPv0.5.0 ssd 1541710881.784592628 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.006933333333333319}

:::MLPv0.5.0 ssd 1541710881.886307478 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.007111111111111096}
Iteration:     40, Loss function: 16.723, Average Loss: 0.819, avg. samples / sec: 20115.01
Iteration:     40, Loss function: 17.127, Average Loss: 0.818, avg. samples / sec: 20110.46
Iteration:     40, Loss function: 16.652, Average Loss: 0.815, avg. samples / sec: 20069.20
Iteration:     40, Loss function: 17.279, Average Loss: 0.818, avg. samples / sec: 20089.61
Iteration:     40, Loss function: 16.047, Average Loss: 0.815, avg. samples / sec: 20076.13
Iteration:     40, Loss function: 16.654, Average Loss: 0.816, avg. samples / sec: 20080.80
Iteration:     40, Loss function: 16.579, Average Loss: 0.812, avg. samples / sec: 20050.57
Iteration:     40, Loss function: 16.625, Average Loss: 0.817, avg. samples / sec: 20046.32

:::MLPv0.5.0 ssd 1541710881.999899626 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.0072888888888888725}

:::MLPv0.5.0 ssd 1541710882.096590996 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.007466666666666649}

:::MLPv0.5.0 ssd 1541710882.198255777 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.007644444444444454}

:::MLPv0.5.0 ssd 1541710882.309266329 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.00782222222222223}

:::MLPv0.5.0 ssd 1541710882.418842077 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.008000000000000007}

:::MLPv0.5.0 ssd 1541710882.520188570 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.008177777777777784}

:::MLPv0.5.0 ssd 1541710882.614130259 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.00835555555555556}

:::MLPv0.5.0 ssd 1541710882.712536097 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.008533333333333337}

:::MLPv0.5.0 ssd 1541710882.809705019 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.008711111111111114}

:::MLPv0.5.0 ssd 1541710882.908960819 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.00888888888888889}

:::MLPv0.5.0 ssd 1541710883.006280899 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.009066666666666667}

:::MLPv0.5.0 ssd 1541710883.104317904 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.009244444444444444}

:::MLPv0.5.0 ssd 1541710883.206749201 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.00942222222222222}

:::MLPv0.5.0 ssd 1541710883.302841902 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.009599999999999997}

:::MLPv0.5.0 ssd 1541710883.405391693 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.009777777777777774}

:::MLPv0.5.0 ssd 1541710883.502169132 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.00995555555555555}

:::MLPv0.5.0 ssd 1541710883.607979536 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.010133333333333328}

:::MLPv0.5.0 ssd 1541710883.701042414 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1541710883.706050873 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 58, "value": 0.010311111111111104}

:::MLPv0.5.0 ssd 1541710883.805295467 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 59, "value": 0.010488888888888881}

:::MLPv0.5.0 ssd 1541710883.899749994 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 60, "value": 0.010666666666666658}
Iteration:     60, Loss function: 11.656, Average Loss: 1.065, avg. samples / sec: 20347.93
Iteration:     60, Loss function: 11.627, Average Loss: 1.056, avg. samples / sec: 20372.65
Iteration:     60, Loss function: 11.026, Average Loss: 1.066, avg. samples / sec: 20351.55
Iteration:     60, Loss function: 11.410, Average Loss: 1.066, avg. samples / sec: 20344.68
Iteration:     60, Loss function: 10.925, Average Loss: 1.060, avg. samples / sec: 20347.65
Iteration:     60, Loss function: 11.548, Average Loss: 1.058, avg. samples / sec: 20338.96
Iteration:     60, Loss function: 11.182, Average Loss: 1.066, avg. samples / sec: 20362.11
Iteration:     60, Loss function: 11.376, Average Loss: 1.061, avg. samples / sec: 20327.45

:::MLPv0.5.0 ssd 1541710883.996705294 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 61, "value": 0.010844444444444434}

:::MLPv0.5.0 ssd 1541710884.096472502 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 62, "value": 0.011022222222222211}

:::MLPv0.5.0 ssd 1541710884.193696022 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 63, "value": 0.011199999999999988}

:::MLPv0.5.0 ssd 1541710884.293082714 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 64, "value": 0.011377777777777764}

:::MLPv0.5.0 ssd 1541710884.393634796 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 65, "value": 0.011555555555555541}

:::MLPv0.5.0 ssd 1541710884.496900797 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 66, "value": 0.011733333333333318}

:::MLPv0.5.0 ssd 1541710884.598381758 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 67, "value": 0.011911111111111095}

:::MLPv0.5.0 ssd 1541710884.695512533 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 68, "value": 0.012088888888888899}

:::MLPv0.5.0 ssd 1541710884.792228699 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 69, "value": 0.012266666666666676}

:::MLPv0.5.0 ssd 1541710884.888934851 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 70, "value": 0.012444444444444452}

:::MLPv0.5.0 ssd 1541710884.988207579 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 71, "value": 0.012622222222222229}

:::MLPv0.5.0 ssd 1541710885.088344097 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 72, "value": 0.012800000000000006}

:::MLPv0.5.0 ssd 1541710885.184715033 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 73, "value": 0.012977777777777783}

:::MLPv0.5.0 ssd 1541710885.280467272 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 74, "value": 0.01315555555555556}

:::MLPv0.5.0 ssd 1541710885.378349781 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 75, "value": 0.013333333333333336}

:::MLPv0.5.0 ssd 1541710885.475432158 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 76, "value": 0.013511111111111113}

:::MLPv0.5.0 ssd 1541710885.574821711 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 77, "value": 0.01368888888888889}

:::MLPv0.5.0 ssd 1541710885.671722889 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 78, "value": 0.013866666666666666}

:::MLPv0.5.0 ssd 1541710885.770035982 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 79, "value": 0.014044444444444443}

:::MLPv0.5.0 ssd 1541710885.866166592 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 80, "value": 0.01422222222222222}
Iteration:     80, Loss function: 11.284, Average Loss: 1.268, avg. samples / sec: 20858.68
Iteration:     80, Loss function: 10.402, Average Loss: 1.263, avg. samples / sec: 20830.11
Iteration:     80, Loss function: 9.973, Average Loss: 1.265, avg. samples / sec: 20839.17
Iteration:     80, Loss function: 11.306, Average Loss: 1.269, avg. samples / sec: 20833.36
Iteration:     80, Loss function: 11.105, Average Loss: 1.274, avg. samples / sec: 20822.29
Iteration:     80, Loss function: 11.018, Average Loss: 1.272, avg. samples / sec: 20837.36
Iteration:     80, Loss function: 10.962, Average Loss: 1.276, avg. samples / sec: 20824.04
Iteration:     80, Loss function: 11.138, Average Loss: 1.271, avg. samples / sec: 20779.30

:::MLPv0.5.0 ssd 1541710885.964974642 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 81, "value": 0.014399999999999996}

:::MLPv0.5.0 ssd 1541710886.061630726 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 82, "value": 0.014577777777777773}

:::MLPv0.5.0 ssd 1541710886.157529593 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 83, "value": 0.01475555555555555}

:::MLPv0.5.0 ssd 1541710886.253951311 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 84, "value": 0.014933333333333326}

:::MLPv0.5.0 ssd 1541710886.350762606 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 85, "value": 0.015111111111111103}

:::MLPv0.5.0 ssd 1541710886.446878910 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 86, "value": 0.01528888888888888}

:::MLPv0.5.0 ssd 1541710886.542166948 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 87, "value": 0.015466666666666656}

:::MLPv0.5.0 ssd 1541710886.641330481 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 88, "value": 0.015644444444444433}

:::MLPv0.5.0 ssd 1541710886.743346930 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 89, "value": 0.01582222222222221}

:::MLPv0.5.0 ssd 1541710886.841703892 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 90, "value": 0.015999999999999986}

:::MLPv0.5.0 ssd 1541710886.938091040 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 91, "value": 0.016177777777777763}

:::MLPv0.5.0 ssd 1541710887.037595510 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 92, "value": 0.01635555555555554}

:::MLPv0.5.0 ssd 1541710887.131891251 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 93, "value": 0.016533333333333317}

:::MLPv0.5.0 ssd 1541710887.226832628 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 94, "value": 0.01671111111111112}

:::MLPv0.5.0 ssd 1541710887.323828459 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 95, "value": 0.016888888888888898}

:::MLPv0.5.0 ssd 1541710887.428091049 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 96, "value": 0.017066666666666674}

:::MLPv0.5.0 ssd 1541710887.522576809 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 97, "value": 0.01724444444444445}

:::MLPv0.5.0 ssd 1541710887.618403673 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 98, "value": 0.017422222222222228}

:::MLPv0.5.0 ssd 1541710887.721163034 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 99, "value": 0.017600000000000005}

:::MLPv0.5.0 ssd 1541710887.817419529 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 100, "value": 0.01777777777777778}
Iteration:    100, Loss function: 8.959, Average Loss: 1.444, avg. samples / sec: 21003.48
Iteration:    100, Loss function: 9.368, Average Loss: 1.438, avg. samples / sec: 20995.12
Iteration:    100, Loss function: 8.562, Average Loss: 1.431, avg. samples / sec: 20989.49
Iteration:    100, Loss function: 9.431, Average Loss: 1.444, avg. samples / sec: 20998.19
Iteration:    100, Loss function: 9.298, Average Loss: 1.435, avg. samples / sec: 20980.59
Iteration:    100, Loss function: 8.910, Average Loss: 1.437, avg. samples / sec: 20975.88
Iteration:    100, Loss function: 9.318, Average Loss: 1.444, avg. samples / sec: 20987.21
Iteration:    100, Loss function: 9.385, Average Loss: 1.439, avg. samples / sec: 21009.20

:::MLPv0.5.0 ssd 1541710887.913322687 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 101, "value": 0.017955555555555558}

:::MLPv0.5.0 ssd 1541710888.008821726 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 102, "value": 0.018133333333333335}

:::MLPv0.5.0 ssd 1541710888.104312181 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 103, "value": 0.01831111111111111}

:::MLPv0.5.0 ssd 1541710888.201789379 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 104, "value": 0.018488888888888888}

:::MLPv0.5.0 ssd 1541710888.297392607 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 105, "value": 0.018666666666666665}

:::MLPv0.5.0 ssd 1541710888.394341230 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 106, "value": 0.01884444444444444}

:::MLPv0.5.0 ssd 1541710888.493839502 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 107, "value": 0.019022222222222218}

:::MLPv0.5.0 ssd 1541710888.588782787 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 108, "value": 0.019199999999999995}

:::MLPv0.5.0 ssd 1541710888.688037395 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 109, "value": 0.01937777777777777}

:::MLPv0.5.0 ssd 1541710888.784074545 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 110, "value": 0.019555555555555548}

:::MLPv0.5.0 ssd 1541710888.879049301 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 111, "value": 0.019733333333333325}

:::MLPv0.5.0 ssd 1541710888.976980448 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 112, "value": 0.0199111111111111}

:::MLPv0.5.0 ssd 1541710889.077226639 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 113, "value": 0.02008888888888888}

:::MLPv0.5.0 ssd 1541710889.176445961 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 114, "value": 0.020266666666666655}

:::MLPv0.5.0 ssd 1541710889.273921490 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 115, "value": 0.020444444444444432}

:::MLPv0.5.0 ssd 1541710889.368495941 (train.py:553) train_epoch: 2

:::MLPv0.5.0 ssd 1541710889.373662710 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 116, "value": 0.02062222222222221}

:::MLPv0.5.0 ssd 1541710889.468807697 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 117, "value": 0.020799999999999985}

:::MLPv0.5.0 ssd 1541710889.567344427 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 118, "value": 0.020977777777777762}

:::MLPv0.5.0 ssd 1541710889.663714409 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 119, "value": 0.02115555555555554}

:::MLPv0.5.0 ssd 1541710889.759029150 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 120, "value": 0.021333333333333343}
Iteration:    120, Loss function: 9.137, Average Loss: 1.588, avg. samples / sec: 21108.55
Iteration:    120, Loss function: 8.822, Average Loss: 1.598, avg. samples / sec: 21093.57
Iteration:    120, Loss function: 8.842, Average Loss: 1.594, avg. samples / sec: 21082.37
Iteration:    120, Loss function: 8.889, Average Loss: 1.590, avg. samples / sec: 21086.67
Iteration:    120, Loss function: 9.097, Average Loss: 1.589, avg. samples / sec: 21100.21
Iteration:    120, Loss function: 9.314, Average Loss: 1.583, avg. samples / sec: 21078.52
Iteration:    120, Loss function: 8.888, Average Loss: 1.591, avg. samples / sec: 21088.23
Iteration:    120, Loss function: 8.669, Average Loss: 1.594, avg. samples / sec: 21053.39

:::MLPv0.5.0 ssd 1541710889.855303049 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 121, "value": 0.02151111111111112}

:::MLPv0.5.0 ssd 1541710889.975159883 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 122, "value": 0.021688888888888896}

:::MLPv0.5.0 ssd 1541710890.076636553 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 123, "value": 0.021866666666666673}

:::MLPv0.5.0 ssd 1541710890.172876596 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 124, "value": 0.02204444444444445}

:::MLPv0.5.0 ssd 1541710890.267917156 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 125, "value": 0.022222222222222227}

:::MLPv0.5.0 ssd 1541710890.364977121 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 126, "value": 0.022400000000000003}

:::MLPv0.5.0 ssd 1541710890.459371328 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 127, "value": 0.02257777777777778}

:::MLPv0.5.0 ssd 1541710890.554135323 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 128, "value": 0.022755555555555557}

:::MLPv0.5.0 ssd 1541710890.650213718 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 129, "value": 0.022933333333333333}

:::MLPv0.5.0 ssd 1541710890.745644569 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 130, "value": 0.02311111111111111}

:::MLPv0.5.0 ssd 1541710890.840596199 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 131, "value": 0.023288888888888887}

:::MLPv0.5.0 ssd 1541710890.935736179 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 132, "value": 0.023466666666666663}

:::MLPv0.5.0 ssd 1541710891.034364700 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 133, "value": 0.02364444444444444}

:::MLPv0.5.0 ssd 1541710891.133960247 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 134, "value": 0.023822222222222217}

:::MLPv0.5.0 ssd 1541710891.229059935 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 135, "value": 0.023999999999999994}

:::MLPv0.5.0 ssd 1541710891.324408531 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 136, "value": 0.02417777777777777}

:::MLPv0.5.0 ssd 1541710891.419874907 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 137, "value": 0.024355555555555547}

:::MLPv0.5.0 ssd 1541710891.514992952 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 138, "value": 0.024533333333333324}

:::MLPv0.5.0 ssd 1541710891.619553328 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 139, "value": 0.0247111111111111}

:::MLPv0.5.0 ssd 1541710891.714601040 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 140, "value": 0.024888888888888877}
Iteration:    140, Loss function: 8.550, Average Loss: 1.730, avg. samples / sec: 20955.31
Iteration:    140, Loss function: 7.794, Average Loss: 1.734, avg. samples / sec: 20982.97
Iteration:    140, Loss function: 8.549, Average Loss: 1.737, avg. samples / sec: 20951.77
Iteration:    140, Loss function: 8.660, Average Loss: 1.734, avg. samples / sec: 20997.95
Iteration:    140, Loss function: 8.873, Average Loss: 1.725, avg. samples / sec: 20956.49
Iteration:    140, Loss function: 8.378, Average Loss: 1.737, avg. samples / sec: 20943.88
Iteration:    140, Loss function: 8.889, Average Loss: 1.731, avg. samples / sec: 20945.87
Iteration:    140, Loss function: 8.624, Average Loss: 1.729, avg. samples / sec: 20920.09

:::MLPv0.5.0 ssd 1541710891.817638159 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 141, "value": 0.025066666666666654}

:::MLPv0.5.0 ssd 1541710891.915022850 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 142, "value": 0.02524444444444443}

:::MLPv0.5.0 ssd 1541710892.008685112 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 143, "value": 0.025422222222222207}

:::MLPv0.5.0 ssd 1541710892.106460571 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 144, "value": 0.025599999999999984}

:::MLPv0.5.0 ssd 1541710892.203545332 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 145, "value": 0.02577777777777779}

:::MLPv0.5.0 ssd 1541710892.299023390 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 146, "value": 0.025955555555555565}

:::MLPv0.5.0 ssd 1541710892.396340370 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 147, "value": 0.026133333333333342}

:::MLPv0.5.0 ssd 1541710892.491435289 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 148, "value": 0.02631111111111112}

:::MLPv0.5.0 ssd 1541710892.586286783 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 149, "value": 0.026488888888888895}

:::MLPv0.5.0 ssd 1541710892.681799889 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 150, "value": 0.026666666666666672}

:::MLPv0.5.0 ssd 1541710892.779264688 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 151, "value": 0.02684444444444445}

:::MLPv0.5.0 ssd 1541710892.875795126 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 152, "value": 0.027022222222222225}

:::MLPv0.5.0 ssd 1541710892.972669363 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 153, "value": 0.027200000000000002}

:::MLPv0.5.0 ssd 1541710893.067952871 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 154, "value": 0.02737777777777778}

:::MLPv0.5.0 ssd 1541710893.166687965 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 155, "value": 0.027555555555555555}

:::MLPv0.5.0 ssd 1541710893.264017105 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 156, "value": 0.027733333333333332}

:::MLPv0.5.0 ssd 1541710893.361520052 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 157, "value": 0.02791111111111111}

:::MLPv0.5.0 ssd 1541710893.461591721 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 158, "value": 0.028088888888888885}

:::MLPv0.5.0 ssd 1541710893.559256554 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 159, "value": 0.028266666666666662}

:::MLPv0.5.0 ssd 1541710893.653678417 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 160, "value": 0.02844444444444444}
Iteration:    160, Loss function: 8.183, Average Loss: 1.864, avg. samples / sec: 21139.05
Iteration:    160, Loss function: 8.346, Average Loss: 1.870, avg. samples / sec: 21129.65
Iteration:    160, Loss function: 8.256, Average Loss: 1.870, avg. samples / sec: 21123.66
Iteration:    160, Loss function: 8.204, Average Loss: 1.867, avg. samples / sec: 21124.62
Iteration:    160, Loss function: 8.066, Average Loss: 1.864, avg. samples / sec: 21118.50
Iteration:    160, Loss function: 8.247, Average Loss: 1.868, avg. samples / sec: 21117.45
Iteration:    160, Loss function: 8.830, Average Loss: 1.858, avg. samples / sec: 21119.53
Iteration:    160, Loss function: 8.272, Average Loss: 1.864, avg. samples / sec: 21134.44

:::MLPv0.5.0 ssd 1541710893.749450922 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 161, "value": 0.028622222222222216}

:::MLPv0.5.0 ssd 1541710893.844511986 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 162, "value": 0.028799999999999992}

:::MLPv0.5.0 ssd 1541710893.943797350 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 163, "value": 0.02897777777777777}

:::MLPv0.5.0 ssd 1541710894.040064096 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 164, "value": 0.029155555555555546}

:::MLPv0.5.0 ssd 1541710894.137522697 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 165, "value": 0.029333333333333322}

:::MLPv0.5.0 ssd 1541710894.242063761 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 166, "value": 0.0295111111111111}

:::MLPv0.5.0 ssd 1541710894.336663008 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 167, "value": 0.029688888888888876}

:::MLPv0.5.0 ssd 1541710894.431482077 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 168, "value": 0.029866666666666652}

:::MLPv0.5.0 ssd 1541710894.528323650 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 169, "value": 0.03004444444444443}

:::MLPv0.5.0 ssd 1541710894.625250101 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 170, "value": 0.030222222222222206}

:::MLPv0.5.0 ssd 1541710894.720000744 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 171, "value": 0.03040000000000001}

:::MLPv0.5.0 ssd 1541710894.815604687 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 172, "value": 0.030577777777777787}

:::MLPv0.5.0 ssd 1541710894.912770033 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 173, "value": 0.030755555555555564}

:::MLPv0.5.0 ssd 1541710895.006529331 (train.py:553) train_epoch: 3

:::MLPv0.5.0 ssd 1541710895.015260458 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 174, "value": 0.03093333333333334}

:::MLPv0.5.0 ssd 1541710895.110733509 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 175, "value": 0.031111111111111117}

:::MLPv0.5.0 ssd 1541710895.207163811 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 176, "value": 0.031288888888888894}

:::MLPv0.5.0 ssd 1541710895.302951097 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 177, "value": 0.03146666666666667}

:::MLPv0.5.0 ssd 1541710895.400202274 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 178, "value": 0.03164444444444445}

:::MLPv0.5.0 ssd 1541710895.495889664 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 179, "value": 0.031822222222222224}

:::MLPv0.5.0 ssd 1541710895.592765331 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 180, "value": 0.032}
Iteration:    180, Loss function: 8.202, Average Loss: 1.990, avg. samples / sec: 21119.79
Iteration:    180, Loss function: 7.819, Average Loss: 1.988, avg. samples / sec: 21129.20
Iteration:    180, Loss function: 8.348, Average Loss: 1.991, avg. samples / sec: 21126.84
Iteration:    180, Loss function: 7.804, Average Loss: 1.982, avg. samples / sec: 21133.59
Iteration:    180, Loss function: 8.335, Average Loss: 1.993, avg. samples / sec: 21129.99
Iteration:    180, Loss function: 8.521, Average Loss: 1.996, avg. samples / sec: 21123.54
Iteration:    180, Loss function: 8.386, Average Loss: 1.989, avg. samples / sec: 21135.12
Iteration:    180, Loss function: 8.516, Average Loss: 1.993, avg. samples / sec: 21122.34

:::MLPv0.5.0 ssd 1541710895.687045097 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 181, "value": 0.03217777777777778}

:::MLPv0.5.0 ssd 1541710895.785241842 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 182, "value": 0.032355555555555554}

:::MLPv0.5.0 ssd 1541710895.882960081 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 183, "value": 0.03253333333333333}

:::MLPv0.5.0 ssd 1541710895.978453875 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 184, "value": 0.03271111111111111}

:::MLPv0.5.0 ssd 1541710896.071392775 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 185, "value": 0.032888888888888884}

:::MLPv0.5.0 ssd 1541710896.166100740 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 186, "value": 0.03306666666666666}

:::MLPv0.5.0 ssd 1541710896.260771751 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 187, "value": 0.03324444444444444}

:::MLPv0.5.0 ssd 1541710896.360283136 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 188, "value": 0.033422222222222214}

:::MLPv0.5.0 ssd 1541710896.455396414 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 189, "value": 0.03359999999999999}

:::MLPv0.5.0 ssd 1541710896.551997900 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 190, "value": 0.03377777777777777}

:::MLPv0.5.0 ssd 1541710896.646765232 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 191, "value": 0.033955555555555544}

:::MLPv0.5.0 ssd 1541710896.739527225 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 192, "value": 0.03413333333333332}

:::MLPv0.5.0 ssd 1541710896.834956884 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 193, "value": 0.0343111111111111}

:::MLPv0.5.0 ssd 1541710896.929780960 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 194, "value": 0.034488888888888874}

:::MLPv0.5.0 ssd 1541710897.023746014 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 195, "value": 0.03466666666666665}

:::MLPv0.5.0 ssd 1541710897.119267941 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 196, "value": 0.03484444444444443}

:::MLPv0.5.0 ssd 1541710897.215086937 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 197, "value": 0.03502222222222222}

:::MLPv0.5.0 ssd 1541710897.310867548 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 198, "value": 0.035199999999999995}

:::MLPv0.5.0 ssd 1541710897.404741049 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 199, "value": 0.03537777777777777}

:::MLPv0.5.0 ssd 1541710897.499359369 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 200, "value": 0.03555555555555555}
Iteration:    200, Loss function: 8.672, Average Loss: 2.123, avg. samples / sec: 21492.08
Iteration:    200, Loss function: 8.698, Average Loss: 2.110, avg. samples / sec: 21491.08
Iteration:    200, Loss function: 7.658, Average Loss: 2.118, avg. samples / sec: 21489.30
Iteration:    200, Loss function: 8.310, Average Loss: 2.121, avg. samples / sec: 21488.23
Iteration:    200, Loss function: 7.868, Average Loss: 2.116, avg. samples / sec: 21476.60
Iteration:    200, Loss function: 8.166, Average Loss: 2.116, avg. samples / sec: 21475.02
Iteration:    200, Loss function: 8.317, Average Loss: 2.123, avg. samples / sec: 21475.48
Iteration:    200, Loss function: 8.086, Average Loss: 2.118, avg. samples / sec: 21459.46

:::MLPv0.5.0 ssd 1541710897.596520185 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 201, "value": 0.035733333333333325}

:::MLPv0.5.0 ssd 1541710897.697946548 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 202, "value": 0.0359111111111111}

:::MLPv0.5.0 ssd 1541710897.793221235 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 203, "value": 0.03608888888888889}

:::MLPv0.5.0 ssd 1541710897.888670921 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 204, "value": 0.03626666666666667}

:::MLPv0.5.0 ssd 1541710897.984991312 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 205, "value": 0.036444444444444446}

:::MLPv0.5.0 ssd 1541710898.079632759 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 206, "value": 0.03662222222222222}

:::MLPv0.5.0 ssd 1541710898.175903082 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 207, "value": 0.0368}

:::MLPv0.5.0 ssd 1541710898.270007372 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 208, "value": 0.036977777777777776}

:::MLPv0.5.0 ssd 1541710898.364444017 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 209, "value": 0.03715555555555555}

:::MLPv0.5.0 ssd 1541710898.469799757 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 210, "value": 0.03733333333333333}

:::MLPv0.5.0 ssd 1541710898.565664768 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 211, "value": 0.037511111111111106}

:::MLPv0.5.0 ssd 1541710898.660767555 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 212, "value": 0.03768888888888888}

:::MLPv0.5.0 ssd 1541710898.763802767 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 213, "value": 0.03786666666666666}

:::MLPv0.5.0 ssd 1541710898.857818604 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 214, "value": 0.038044444444444436}

:::MLPv0.5.0 ssd 1541710898.954283476 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 215, "value": 0.03822222222222221}

:::MLPv0.5.0 ssd 1541710899.051167965 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 216, "value": 0.038400000000000004}

:::MLPv0.5.0 ssd 1541710899.145992041 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 217, "value": 0.03857777777777778}

:::MLPv0.5.0 ssd 1541710899.239667177 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 218, "value": 0.03875555555555556}

:::MLPv0.5.0 ssd 1541710899.333818674 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 219, "value": 0.038933333333333334}

:::MLPv0.5.0 ssd 1541710899.428735018 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 220, "value": 0.03911111111111111}
Iteration:    220, Loss function: 7.520, Average Loss: 2.229, avg. samples / sec: 21242.04
Iteration:    220, Loss function: 7.611, Average Loss: 2.235, avg. samples / sec: 21224.67
Iteration:    220, Loss function: 7.555, Average Loss: 2.224, avg. samples / sec: 21224.70
Iteration:    220, Loss function: 8.231, Average Loss: 2.236, avg. samples / sec: 21230.92
Iteration:    220, Loss function: 7.914, Average Loss: 2.233, avg. samples / sec: 21259.30
Iteration:    220, Loss function: 7.758, Average Loss: 2.233, avg. samples / sec: 21224.15
Iteration:    220, Loss function: 7.835, Average Loss: 2.238, avg. samples / sec: 21231.45
Iteration:    220, Loss function: 7.980, Average Loss: 2.230, avg. samples / sec: 21218.90

:::MLPv0.5.0 ssd 1541710899.523941517 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 221, "value": 0.03928888888888889}

:::MLPv0.5.0 ssd 1541710899.622797966 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 222, "value": 0.039466666666666664}

:::MLPv0.5.0 ssd 1541710899.724826574 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 223, "value": 0.03964444444444444}

:::MLPv0.5.0 ssd 1541710899.831796408 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 224, "value": 0.03982222222222222}

:::MLPv0.5.0 ssd 1541710899.931524038 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 225, "value": 0.039999999999999994}

:::MLPv0.5.0 ssd 1541710900.026842833 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 226, "value": 0.04017777777777777}

:::MLPv0.5.0 ssd 1541710900.121244431 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 227, "value": 0.04035555555555555}

:::MLPv0.5.0 ssd 1541710900.217119455 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 228, "value": 0.04053333333333334}

:::MLPv0.5.0 ssd 1541710900.312407970 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 229, "value": 0.040711111111111115}

:::MLPv0.5.0 ssd 1541710900.407135725 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 230, "value": 0.04088888888888889}

:::MLPv0.5.0 ssd 1541710900.504141569 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 231, "value": 0.04106666666666667}

:::MLPv0.5.0 ssd 1541710900.594802380 (train.py:553) train_epoch: 4

:::MLPv0.5.0 ssd 1541710900.600544214 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 232, "value": 0.041244444444444445}

:::MLPv0.5.0 ssd 1541710900.701478004 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 233, "value": 0.04142222222222222}

:::MLPv0.5.0 ssd 1541710900.798294306 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 234, "value": 0.0416}

:::MLPv0.5.0 ssd 1541710900.895713329 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 235, "value": 0.041777777777777775}

:::MLPv0.5.0 ssd 1541710900.991596699 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 236, "value": 0.04195555555555555}

:::MLPv0.5.0 ssd 1541710901.090043545 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 237, "value": 0.04213333333333333}

:::MLPv0.5.0 ssd 1541710901.189027548 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 238, "value": 0.042311111111111105}

:::MLPv0.5.0 ssd 1541710901.283065796 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 239, "value": 0.04248888888888888}

:::MLPv0.5.0 ssd 1541710901.378834724 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 240, "value": 0.04266666666666666}
Iteration:    240, Loss function: 7.780, Average Loss: 2.343, avg. samples / sec: 21006.87
Iteration:    240, Loss function: 7.116, Average Loss: 2.338, avg. samples / sec: 21025.44
Iteration:    240, Loss function: 8.291, Average Loss: 2.346, avg. samples / sec: 21008.35
Iteration:    240, Loss function: 7.518, Average Loss: 2.341, avg. samples / sec: 21007.17
Iteration:    240, Loss function: 7.845, Average Loss: 2.342, avg. samples / sec: 21006.13
Iteration:    240, Loss function: 7.771, Average Loss: 2.338, avg. samples / sec: 21000.53
Iteration:    240, Loss function: 7.697, Average Loss: 2.344, avg. samples / sec: 21010.14
Iteration:    240, Loss function: 7.599, Average Loss: 2.335, avg. samples / sec: 20998.83

:::MLPv0.5.0 ssd 1541710901.476923704 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 241, "value": 0.04284444444444445}

:::MLPv0.5.0 ssd 1541710901.570748091 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 242, "value": 0.043022222222222226}

:::MLPv0.5.0 ssd 1541710901.665498734 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 243, "value": 0.0432}

:::MLPv0.5.0 ssd 1541710901.763002872 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 244, "value": 0.04337777777777778}

:::MLPv0.5.0 ssd 1541710901.858869076 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 245, "value": 0.043555555555555556}

:::MLPv0.5.0 ssd 1541710901.957239151 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 246, "value": 0.04373333333333333}

:::MLPv0.5.0 ssd 1541710902.052842379 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 247, "value": 0.04391111111111111}

:::MLPv0.5.0 ssd 1541710902.146841288 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 248, "value": 0.044088888888888886}

:::MLPv0.5.0 ssd 1541710902.242034912 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 249, "value": 0.04426666666666666}

:::MLPv0.5.0 ssd 1541710902.336203098 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 250, "value": 0.04444444444444444}

:::MLPv0.5.0 ssd 1541710902.430880785 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 251, "value": 0.044622222222222216}

:::MLPv0.5.0 ssd 1541710902.527203321 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 252, "value": 0.04479999999999999}

:::MLPv0.5.0 ssd 1541710902.623789310 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 253, "value": 0.04497777777777777}

:::MLPv0.5.0 ssd 1541710902.718729019 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 254, "value": 0.04515555555555556}

:::MLPv0.5.0 ssd 1541710902.812651873 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 255, "value": 0.04533333333333334}

:::MLPv0.5.0 ssd 1541710902.909127474 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 256, "value": 0.04551111111111111}

:::MLPv0.5.0 ssd 1541710903.004435539 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 257, "value": 0.04568888888888889}

:::MLPv0.5.0 ssd 1541710903.099087000 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 258, "value": 0.04586666666666667}

:::MLPv0.5.0 ssd 1541710903.193894863 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 259, "value": 0.04604444444444444}

:::MLPv0.5.0 ssd 1541710903.288707256 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 260, "value": 0.04622222222222222}
Iteration:    260, Loss function: 7.011, Average Loss: 2.438, avg. samples / sec: 21454.36
Iteration:    260, Loss function: 7.485, Average Loss: 2.444, avg. samples / sec: 21446.86
Iteration:    260, Loss function: 7.099, Average Loss: 2.450, avg. samples / sec: 21442.31
Iteration:    260, Loss function: 7.235, Average Loss: 2.445, avg. samples / sec: 21440.75
Iteration:    260, Loss function: 7.459, Average Loss: 2.448, avg. samples / sec: 21438.74
Iteration:    260, Loss function: 7.508, Average Loss: 2.443, avg. samples / sec: 21441.48
Iteration:    260, Loss function: 7.720, Average Loss: 2.447, avg. samples / sec: 21442.64
Iteration:    260, Loss function: 7.431, Average Loss: 2.441, avg. samples / sec: 21426.24

:::MLPv0.5.0 ssd 1541710903.385627031 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 261, "value": 0.0464}

:::MLPv0.5.0 ssd 1541710903.482099533 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 262, "value": 0.046577777777777774}

:::MLPv0.5.0 ssd 1541710903.578627348 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 263, "value": 0.04675555555555555}

:::MLPv0.5.0 ssd 1541710903.672586203 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 264, "value": 0.04693333333333333}

:::MLPv0.5.0 ssd 1541710903.766515970 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 265, "value": 0.047111111111111104}

:::MLPv0.5.0 ssd 1541710903.871125460 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 266, "value": 0.04728888888888888}

:::MLPv0.5.0 ssd 1541710903.965707541 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 267, "value": 0.04746666666666667}

:::MLPv0.5.0 ssd 1541710904.059739828 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 268, "value": 0.04764444444444445}

:::MLPv0.5.0 ssd 1541710904.155985355 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 269, "value": 0.047822222222222224}

:::MLPv0.5.0 ssd 1541710904.251081944 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 270, "value": 0.048}

:::MLPv0.5.0 ssd 1541710904.349888563 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 271, "value": 0.04817777777777778}

:::MLPv0.5.0 ssd 1541710904.446681261 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 272, "value": 0.048355555555555554}

:::MLPv0.5.0 ssd 1541710904.540579557 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 273, "value": 0.04853333333333333}

:::MLPv0.5.0 ssd 1541710904.635378361 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 274, "value": 0.04871111111111111}

:::MLPv0.5.0 ssd 1541710904.732346058 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 275, "value": 0.048888888888888885}

:::MLPv0.5.0 ssd 1541710904.828578472 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 276, "value": 0.04906666666666666}

:::MLPv0.5.0 ssd 1541710904.928519249 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 277, "value": 0.04924444444444444}

:::MLPv0.5.0 ssd 1541710905.023008108 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 278, "value": 0.049422222222222215}

:::MLPv0.5.0 ssd 1541710905.117326736 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 279, "value": 0.04959999999999999}

:::MLPv0.5.0 ssd 1541710905.211993933 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 280, "value": 0.04977777777777778}
Iteration:    280, Loss function: 7.089, Average Loss: 2.534, avg. samples / sec: 21300.82
Iteration:    280, Loss function: 7.540, Average Loss: 2.543, avg. samples / sec: 21306.64
Iteration:    280, Loss function: 7.751, Average Loss: 2.538, avg. samples / sec: 21316.28
Iteration:    280, Loss function: 7.392, Average Loss: 2.538, avg. samples / sec: 21297.54
Iteration:    280, Loss function: 6.904, Average Loss: 2.543, avg. samples / sec: 21297.76
Iteration:    280, Loss function: 7.352, Average Loss: 2.540, avg. samples / sec: 21292.79
Iteration:    280, Loss function: 7.336, Average Loss: 2.539, avg. samples / sec: 21276.56
Iteration:    280, Loss function: 7.285, Average Loss: 2.544, avg. samples / sec: 21271.54

:::MLPv0.5.0 ssd 1541710905.307498693 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 281, "value": 0.04995555555555556}

:::MLPv0.5.0 ssd 1541710905.403302431 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 282, "value": 0.050133333333333335}

:::MLPv0.5.0 ssd 1541710905.506544352 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 283, "value": 0.05031111111111111}

:::MLPv0.5.0 ssd 1541710905.603061199 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 284, "value": 0.05048888888888889}

:::MLPv0.5.0 ssd 1541710905.699383259 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 285, "value": 0.050666666666666665}

:::MLPv0.5.0 ssd 1541710905.793312073 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 286, "value": 0.05084444444444444}

:::MLPv0.5.0 ssd 1541710905.886796713 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 287, "value": 0.05102222222222222}

:::MLPv0.5.0 ssd 1541710905.980343103 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 288, "value": 0.051199999999999996}

:::MLPv0.5.0 ssd 1541710906.070168018 (train.py:553) train_epoch: 5

:::MLPv0.5.0 ssd 1541710906.075487852 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 289, "value": 0.05137777777777777}

:::MLPv0.5.0 ssd 1541710906.172183990 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 290, "value": 0.05155555555555555}

:::MLPv0.5.0 ssd 1541710906.266171694 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 291, "value": 0.051733333333333326}

:::MLPv0.5.0 ssd 1541710906.361083746 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 292, "value": 0.0519111111111111}

:::MLPv0.5.0 ssd 1541710906.455838442 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 293, "value": 0.05208888888888889}

:::MLPv0.5.0 ssd 1541710906.550057888 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 294, "value": 0.05226666666666667}

:::MLPv0.5.0 ssd 1541710906.647741556 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 295, "value": 0.052444444444444446}

:::MLPv0.5.0 ssd 1541710906.743340015 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 296, "value": 0.05262222222222222}

:::MLPv0.5.0 ssd 1541710906.840532780 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 297, "value": 0.0528}

:::MLPv0.5.0 ssd 1541710906.935377836 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 298, "value": 0.052977777777777776}

:::MLPv0.5.0 ssd 1541710907.031728745 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 299, "value": 0.05315555555555555}

:::MLPv0.5.0 ssd 1541710907.126641035 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 300, "value": 0.05333333333333333}
Iteration:    300, Loss function: 7.312, Average Loss: 2.633, avg. samples / sec: 21421.60
Iteration:    300, Loss function: 7.301, Average Loss: 2.629, avg. samples / sec: 21394.87
Iteration:    300, Loss function: 7.063, Average Loss: 2.636, avg. samples / sec: 21391.10
Iteration:    300, Loss function: 7.217, Average Loss: 2.636, avg. samples / sec: 21423.24
Iteration:    300, Loss function: 6.866, Average Loss: 2.634, avg. samples / sec: 21396.82
Iteration:    300, Loss function: 7.292, Average Loss: 2.632, avg. samples / sec: 21398.21
Iteration:    300, Loss function: 6.606, Average Loss: 2.632, avg. samples / sec: 21407.04
Iteration:    300, Loss function: 7.127, Average Loss: 2.631, avg. samples / sec: 21390.32

:::MLPv0.5.0 ssd 1541710907.220469236 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 301, "value": 0.053511111111111107}

:::MLPv0.5.0 ssd 1541710907.317481279 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 302, "value": 0.05368888888888888}

:::MLPv0.5.0 ssd 1541710907.414120197 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 303, "value": 0.05386666666666666}

:::MLPv0.5.0 ssd 1541710907.509539843 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 304, "value": 0.05404444444444444}

:::MLPv0.5.0 ssd 1541710907.605932951 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 305, "value": 0.05422222222222223}

:::MLPv0.5.0 ssd 1541710907.704238176 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 306, "value": 0.054400000000000004}

:::MLPv0.5.0 ssd 1541710907.798916817 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 307, "value": 0.05457777777777778}

:::MLPv0.5.0 ssd 1541710907.894177198 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 308, "value": 0.05475555555555556}

:::MLPv0.5.0 ssd 1541710907.988735676 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 309, "value": 0.054933333333333334}

:::MLPv0.5.0 ssd 1541710908.083262920 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 310, "value": 0.05511111111111111}

:::MLPv0.5.0 ssd 1541710908.177528858 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 311, "value": 0.05528888888888889}

:::MLPv0.5.0 ssd 1541710908.272632599 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 312, "value": 0.055466666666666664}

:::MLPv0.5.0 ssd 1541710908.369715214 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 313, "value": 0.05564444444444444}

:::MLPv0.5.0 ssd 1541710908.464153290 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 314, "value": 0.05582222222222222}

:::MLPv0.5.0 ssd 1541710908.557702065 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 315, "value": 0.055999999999999994}

:::MLPv0.5.0 ssd 1541710908.651685238 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 316, "value": 0.05617777777777777}

:::MLPv0.5.0 ssd 1541710908.746904850 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 317, "value": 0.05635555555555555}

:::MLPv0.5.0 ssd 1541710908.841759205 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 318, "value": 0.05653333333333334}

:::MLPv0.5.0 ssd 1541710908.935616732 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 319, "value": 0.056711111111111115}

:::MLPv0.5.0 ssd 1541710909.030887842 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 320, "value": 0.05688888888888889}
Iteration:    320, Loss function: 7.297, Average Loss: 2.719, avg. samples / sec: 21514.02
Iteration:    320, Loss function: 6.902, Average Loss: 2.722, avg. samples / sec: 21513.07
Iteration:    320, Loss function: 6.862, Average Loss: 2.719, avg. samples / sec: 21516.77
Iteration:    320, Loss function: 6.919, Average Loss: 2.720, avg. samples / sec: 21513.49
Iteration:    320, Loss function: 6.472, Average Loss: 2.719, avg. samples / sec: 21510.55
Iteration:    320, Loss function: 7.183, Average Loss: 2.722, avg. samples / sec: 21504.60
Iteration:    320, Loss function: 7.209, Average Loss: 2.721, avg. samples / sec: 21484.78
Iteration:    320, Loss function: 7.046, Average Loss: 2.722, avg. samples / sec: 21493.83

:::MLPv0.5.0 ssd 1541710909.125399113 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 321, "value": 0.05706666666666667}

:::MLPv0.5.0 ssd 1541710909.220881224 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 322, "value": 0.057244444444444445}

:::MLPv0.5.0 ssd 1541710909.317611694 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 323, "value": 0.05742222222222222}

:::MLPv0.5.0 ssd 1541710909.414115429 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 324, "value": 0.0576}

:::MLPv0.5.0 ssd 1541710909.509397030 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 325, "value": 0.057777777777777775}

:::MLPv0.5.0 ssd 1541710909.604091167 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 326, "value": 0.05795555555555555}

:::MLPv0.5.0 ssd 1541710909.698097706 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 327, "value": 0.05813333333333333}

:::MLPv0.5.0 ssd 1541710909.792261839 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 328, "value": 0.058311111111111105}

:::MLPv0.5.0 ssd 1541710909.889209509 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 329, "value": 0.05848888888888888}

:::MLPv0.5.0 ssd 1541710909.984254837 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 330, "value": 0.05866666666666666}

:::MLPv0.5.0 ssd 1541710910.078862906 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 331, "value": 0.05884444444444445}

:::MLPv0.5.0 ssd 1541710910.174512863 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 332, "value": 0.059022222222222226}

:::MLPv0.5.0 ssd 1541710910.268983364 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 333, "value": 0.0592}

:::MLPv0.5.0 ssd 1541710910.363077402 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 334, "value": 0.05937777777777778}

:::MLPv0.5.0 ssd 1541710910.459900379 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 335, "value": 0.059555555555555556}

:::MLPv0.5.0 ssd 1541710910.555756330 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 336, "value": 0.05973333333333333}

:::MLPv0.5.0 ssd 1541710910.650287867 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 337, "value": 0.05991111111111111}

:::MLPv0.5.0 ssd 1541710910.744613886 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 338, "value": 0.060088888888888886}

:::MLPv0.5.0 ssd 1541710910.839370966 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 339, "value": 0.06026666666666666}

:::MLPv0.5.0 ssd 1541710910.933801174 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 340, "value": 0.06044444444444444}
Iteration:    340, Loss function: 7.087, Average Loss: 2.806, avg. samples / sec: 21534.31
Iteration:    340, Loss function: 7.220, Average Loss: 2.811, avg. samples / sec: 21530.41
Iteration:    340, Loss function: 7.024, Average Loss: 2.806, avg. samples / sec: 21523.16
Iteration:    340, Loss function: 6.794, Average Loss: 2.804, avg. samples / sec: 21526.18
Iteration:    340, Loss function: 7.242, Average Loss: 2.808, avg. samples / sec: 21533.37
Iteration:    340, Loss function: 6.887, Average Loss: 2.808, avg. samples / sec: 21543.77
Iteration:    340, Loss function: 7.873, Average Loss: 2.809, avg. samples / sec: 21541.66
Iteration:    340, Loss function: 7.359, Average Loss: 2.803, avg. samples / sec: 21523.30

:::MLPv0.5.0 ssd 1541710911.033038855 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 341, "value": 0.060622222222222216}

:::MLPv0.5.0 ssd 1541710911.127054453 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 342, "value": 0.06079999999999999}

:::MLPv0.5.0 ssd 1541710911.225529194 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 343, "value": 0.06097777777777777}

:::MLPv0.5.0 ssd 1541710911.320551157 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 344, "value": 0.06115555555555556}

:::MLPv0.5.0 ssd 1541710911.416791201 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 345, "value": 0.06133333333333334}

:::MLPv0.5.0 ssd 1541710911.511727571 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 346, "value": 0.061511111111111114}

:::MLPv0.5.0 ssd 1541710911.601758718 (train.py:553) train_epoch: 6

:::MLPv0.5.0 ssd 1541710911.608728170 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 347, "value": 0.06168888888888889}

:::MLPv0.5.0 ssd 1541710911.703507185 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 348, "value": 0.06186666666666667}

:::MLPv0.5.0 ssd 1541710911.798527956 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 349, "value": 0.062044444444444444}

:::MLPv0.5.0 ssd 1541710911.895563602 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 350, "value": 0.06222222222222222}

:::MLPv0.5.0 ssd 1541710911.989741564 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 351, "value": 0.0624}

:::MLPv0.5.0 ssd 1541710912.084297657 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 352, "value": 0.06257777777777777}

:::MLPv0.5.0 ssd 1541710912.178855896 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 353, "value": 0.06275555555555555}

:::MLPv0.5.0 ssd 1541710912.275712729 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 354, "value": 0.06293333333333333}

:::MLPv0.5.0 ssd 1541710912.369920015 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 355, "value": 0.0631111111111111}

:::MLPv0.5.0 ssd 1541710912.464731693 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 356, "value": 0.0632888888888889}

:::MLPv0.5.0 ssd 1541710912.560774088 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 357, "value": 0.06346666666666667}

:::MLPv0.5.0 ssd 1541710912.655545235 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 358, "value": 0.06364444444444445}

:::MLPv0.5.0 ssd 1541710912.750496149 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 359, "value": 0.06382222222222222}

:::MLPv0.5.0 ssd 1541710912.844486952 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 360, "value": 0.064}
Iteration:    360, Loss function: 6.797, Average Loss: 2.885, avg. samples / sec: 21437.26
Iteration:    360, Loss function: 6.523, Average Loss: 2.887, avg. samples / sec: 21440.05
Iteration:    360, Loss function: 6.487, Average Loss: 2.885, avg. samples / sec: 21444.44
Iteration:    360, Loss function: 6.257, Average Loss: 2.885, avg. samples / sec: 21438.07
Iteration:    360, Loss function: 6.407, Average Loss: 2.895, avg. samples / sec: 21442.60
Iteration:    360, Loss function: 6.688, Average Loss: 2.889, avg. samples / sec: 21429.63
Iteration:    360, Loss function: 6.331, Average Loss: 2.887, avg. samples / sec: 21432.82
Iteration:    360, Loss function: 7.231, Average Loss: 2.884, avg. samples / sec: 21439.18

:::MLPv0.5.0 ssd 1541710912.939949036 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 361, "value": 0.06417777777777778}

:::MLPv0.5.0 ssd 1541710913.034519434 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 362, "value": 0.06435555555555555}

:::MLPv0.5.0 ssd 1541710913.129977226 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 363, "value": 0.06453333333333333}

:::MLPv0.5.0 ssd 1541710913.224568129 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 364, "value": 0.06471111111111111}

:::MLPv0.5.0 ssd 1541710913.321253777 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 365, "value": 0.06488888888888888}

:::MLPv0.5.0 ssd 1541710913.416333675 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 366, "value": 0.06506666666666666}

:::MLPv0.5.0 ssd 1541710913.510790348 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 367, "value": 0.06524444444444444}

:::MLPv0.5.0 ssd 1541710913.611815691 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 368, "value": 0.06542222222222221}

:::MLPv0.5.0 ssd 1541710913.706307173 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 369, "value": 0.0656}

:::MLPv0.5.0 ssd 1541710913.807182789 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 370, "value": 0.06577777777777778}

:::MLPv0.5.0 ssd 1541710913.902714014 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 371, "value": 0.06595555555555556}

:::MLPv0.5.0 ssd 1541710913.997180700 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 372, "value": 0.06613333333333334}

:::MLPv0.5.0 ssd 1541710914.094503403 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 373, "value": 0.06631111111111111}

:::MLPv0.5.0 ssd 1541710914.192301273 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 374, "value": 0.06648888888888889}

:::MLPv0.5.0 ssd 1541710914.290503263 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 375, "value": 0.06666666666666667}

:::MLPv0.5.0 ssd 1541710914.387233496 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 376, "value": 0.06684444444444444}

:::MLPv0.5.0 ssd 1541710914.481730223 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 377, "value": 0.06702222222222222}

:::MLPv0.5.0 ssd 1541710914.576727390 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 378, "value": 0.0672}

:::MLPv0.5.0 ssd 1541710914.681670666 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 379, "value": 0.06737777777777777}

:::MLPv0.5.0 ssd 1541710914.777100086 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 380, "value": 0.06755555555555555}
Iteration:    380, Loss function: 6.727, Average Loss: 2.964, avg. samples / sec: 21194.48
Iteration:    380, Loss function: 6.351, Average Loss: 2.971, avg. samples / sec: 21195.96
Iteration:    380, Loss function: 6.735, Average Loss: 2.963, avg. samples / sec: 21194.71
Iteration:    380, Loss function: 6.804, Average Loss: 2.964, avg. samples / sec: 21200.31
Iteration:    380, Loss function: 6.798, Average Loss: 2.964, avg. samples / sec: 21188.58
Iteration:    380, Loss function: 6.423, Average Loss: 2.965, avg. samples / sec: 21196.55
Iteration:    380, Loss function: 6.861, Average Loss: 2.962, avg. samples / sec: 21186.79
Iteration:    380, Loss function: 6.997, Average Loss: 2.963, avg. samples / sec: 21191.03

:::MLPv0.5.0 ssd 1541710914.871879101 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 381, "value": 0.06773333333333333}

:::MLPv0.5.0 ssd 1541710914.965495348 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 382, "value": 0.06791111111111112}

:::MLPv0.5.0 ssd 1541710915.068509340 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 383, "value": 0.0680888888888889}

:::MLPv0.5.0 ssd 1541710915.162704945 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 384, "value": 0.06826666666666667}

:::MLPv0.5.0 ssd 1541710915.256915569 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 385, "value": 0.06844444444444445}

:::MLPv0.5.0 ssd 1541710915.353201389 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 386, "value": 0.06862222222222222}

:::MLPv0.5.0 ssd 1541710915.448318720 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 387, "value": 0.0688}

:::MLPv0.5.0 ssd 1541710915.543155670 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 388, "value": 0.06897777777777778}

:::MLPv0.5.0 ssd 1541710915.636670113 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 389, "value": 0.06915555555555555}

:::MLPv0.5.0 ssd 1541710915.731937170 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 390, "value": 0.06933333333333333}

:::MLPv0.5.0 ssd 1541710915.826910257 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 391, "value": 0.0695111111111111}

:::MLPv0.5.0 ssd 1541710915.921404600 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 392, "value": 0.06968888888888888}

:::MLPv0.5.0 ssd 1541710916.015297890 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 393, "value": 0.06986666666666666}

:::MLPv0.5.0 ssd 1541710916.108869314 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 394, "value": 0.07004444444444444}

:::MLPv0.5.0 ssd 1541710916.203747749 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 395, "value": 0.07022222222222223}

:::MLPv0.5.0 ssd 1541710916.298626661 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 396, "value": 0.0704}

:::MLPv0.5.0 ssd 1541710916.392756701 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 397, "value": 0.07057777777777778}

:::MLPv0.5.0 ssd 1541710916.489891291 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 398, "value": 0.07075555555555556}

:::MLPv0.5.0 ssd 1541710916.584504366 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 399, "value": 0.07093333333333333}

:::MLPv0.5.0 ssd 1541710916.679317236 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 400, "value": 0.07111111111111111}
Iteration:    400, Loss function: 7.461, Average Loss: 3.041, avg. samples / sec: 21540.33
Iteration:    400, Loss function: 7.263, Average Loss: 3.040, avg. samples / sec: 21533.90
Iteration:    400, Loss function: 7.829, Average Loss: 3.041, avg. samples / sec: 21539.57
Iteration:    400, Loss function: 7.077, Average Loss: 3.037, avg. samples / sec: 21534.74
Iteration:    400, Loss function: 6.900, Average Loss: 3.038, avg. samples / sec: 21542.02
Iteration:    400, Loss function: 7.341, Average Loss: 3.048, avg. samples / sec: 21532.86
Iteration:    400, Loss function: 7.339, Average Loss: 3.041, avg. samples / sec: 21532.63
Iteration:    400, Loss function: 7.076, Average Loss: 3.040, avg. samples / sec: 21534.04

:::MLPv0.5.0 ssd 1541710916.773347378 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 401, "value": 0.07128888888888889}

:::MLPv0.5.0 ssd 1541710916.868619442 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 402, "value": 0.07146666666666666}

:::MLPv0.5.0 ssd 1541710916.962907791 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 403, "value": 0.07164444444444444}

:::MLPv0.5.0 ssd 1541710917.058087349 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 404, "value": 0.07182222222222222}

:::MLPv0.5.0 ssd 1541710917.149481058 (train.py:553) train_epoch: 7

:::MLPv0.5.0 ssd 1541710917.155038118 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 405, "value": 0.072}

:::MLPv0.5.0 ssd 1541710917.249776602 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 406, "value": 0.07217777777777777}

:::MLPv0.5.0 ssd 1541710917.345864773 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 407, "value": 0.07235555555555555}

:::MLPv0.5.0 ssd 1541710917.442409992 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 408, "value": 0.07253333333333334}

:::MLPv0.5.0 ssd 1541710917.536696434 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 409, "value": 0.07271111111111112}

:::MLPv0.5.0 ssd 1541710917.631427288 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 410, "value": 0.07288888888888889}

:::MLPv0.5.0 ssd 1541710917.727308989 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 411, "value": 0.07306666666666667}

:::MLPv0.5.0 ssd 1541710917.823188543 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 412, "value": 0.07324444444444445}

:::MLPv0.5.0 ssd 1541710917.917799950 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 413, "value": 0.07342222222222222}

:::MLPv0.5.0 ssd 1541710918.013079405 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 414, "value": 0.0736}

:::MLPv0.5.0 ssd 1541710918.108178377 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 415, "value": 0.07377777777777778}

:::MLPv0.5.0 ssd 1541710918.203302860 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 416, "value": 0.07395555555555555}

:::MLPv0.5.0 ssd 1541710918.297748566 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 417, "value": 0.07413333333333333}

:::MLPv0.5.0 ssd 1541710918.392199516 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 418, "value": 0.0743111111111111}

:::MLPv0.5.0 ssd 1541710918.488171339 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 419, "value": 0.07448888888888888}

:::MLPv0.5.0 ssd 1541710918.583166838 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 420, "value": 0.07466666666666666}
Iteration:    420, Loss function: 7.277, Average Loss: 3.128, avg. samples / sec: 21519.09
Iteration:    420, Loss function: 6.707, Average Loss: 3.131, avg. samples / sec: 21512.15
Iteration:    420, Loss function: 6.462, Average Loss: 3.130, avg. samples / sec: 21510.08
Iteration:    420, Loss function: 7.316, Average Loss: 3.133, avg. samples / sec: 21515.82
Iteration:    420, Loss function: 7.177, Average Loss: 3.126, avg. samples / sec: 21511.86
Iteration:    420, Loss function: 6.682, Average Loss: 3.136, avg. samples / sec: 21511.85
Iteration:    420, Loss function: 7.007, Average Loss: 3.131, avg. samples / sec: 21518.66
Iteration:    420, Loss function: 7.129, Average Loss: 3.129, avg. samples / sec: 21500.58

:::MLPv0.5.0 ssd 1541710918.678105831 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 421, "value": 0.07484444444444445}

:::MLPv0.5.0 ssd 1541710918.776787758 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 422, "value": 0.07502222222222223}

:::MLPv0.5.0 ssd 1541710918.870538235 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 423, "value": 0.0752}

:::MLPv0.5.0 ssd 1541710918.965645552 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 424, "value": 0.07537777777777778}

:::MLPv0.5.0 ssd 1541710919.059993505 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 425, "value": 0.07555555555555556}

:::MLPv0.5.0 ssd 1541710919.156272650 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 426, "value": 0.07573333333333333}

:::MLPv0.5.0 ssd 1541710919.250303984 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 427, "value": 0.07591111111111111}

:::MLPv0.5.0 ssd 1541710919.343477726 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 428, "value": 0.07608888888888889}

:::MLPv0.5.0 ssd 1541710919.439005613 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 429, "value": 0.07626666666666666}

:::MLPv0.5.0 ssd 1541710919.533662319 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 430, "value": 0.07644444444444444}

:::MLPv0.5.0 ssd 1541710919.627778530 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 431, "value": 0.07662222222222222}

:::MLPv0.5.0 ssd 1541710919.721835852 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 432, "value": 0.0768}

:::MLPv0.5.0 ssd 1541710919.818124056 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 433, "value": 0.07697777777777778}

:::MLPv0.5.0 ssd 1541710919.912970781 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 434, "value": 0.07715555555555556}

:::MLPv0.5.0 ssd 1541710920.007139683 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 435, "value": 0.07733333333333334}

:::MLPv0.5.0 ssd 1541710920.101706982 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 436, "value": 0.07751111111111111}

:::MLPv0.5.0 ssd 1541710920.196517467 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 437, "value": 0.07768888888888889}

:::MLPv0.5.0 ssd 1541710920.291005135 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 438, "value": 0.07786666666666667}

:::MLPv0.5.0 ssd 1541710920.387573004 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 439, "value": 0.07804444444444444}

:::MLPv0.5.0 ssd 1541710920.483611345 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 440, "value": 0.07822222222222222}
Iteration:    440, Loss function: 6.384, Average Loss: 3.200, avg. samples / sec: 21567.56
Iteration:    440, Loss function: 7.090, Average Loss: 3.202, avg. samples / sec: 21556.71
Iteration:    440, Loss function: 6.179, Average Loss: 3.199, avg. samples / sec: 21550.65
Iteration:    440, Loss function: 6.712, Average Loss: 3.206, avg. samples / sec: 21556.51
Iteration:    440, Loss function: 6.509, Average Loss: 3.200, avg. samples / sec: 21545.88
Iteration:    440, Loss function: 6.421, Average Loss: 3.195, avg. samples / sec: 21551.41
Iteration:    440, Loss function: 6.426, Average Loss: 3.198, avg. samples / sec: 21542.49
Iteration:    440, Loss function: 6.760, Average Loss: 3.202, avg. samples / sec: 21550.97

:::MLPv0.5.0 ssd 1541710920.578661442 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 441, "value": 0.0784}

:::MLPv0.5.0 ssd 1541710920.675456047 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 442, "value": 0.07857777777777777}

:::MLPv0.5.0 ssd 1541710920.770288706 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 443, "value": 0.07875555555555555}

:::MLPv0.5.0 ssd 1541710920.865577936 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 444, "value": 0.07893333333333333}

:::MLPv0.5.0 ssd 1541710920.959689617 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 445, "value": 0.0791111111111111}

:::MLPv0.5.0 ssd 1541710921.057011127 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 446, "value": 0.0792888888888889}

:::MLPv0.5.0 ssd 1541710921.152070761 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 447, "value": 0.07946666666666667}

:::MLPv0.5.0 ssd 1541710921.249182463 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 448, "value": 0.07964444444444445}

:::MLPv0.5.0 ssd 1541710921.345108747 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 449, "value": 0.07982222222222222}

:::MLPv0.5.0 ssd 1541710921.442526102 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 450, "value": 0.08}

:::MLPv0.5.0 ssd 1541710921.537176609 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 451, "value": 0.08017777777777778}

:::MLPv0.5.0 ssd 1541710921.632125616 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 452, "value": 0.08035555555555556}

:::MLPv0.5.0 ssd 1541710921.726869345 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 453, "value": 0.08053333333333333}

:::MLPv0.5.0 ssd 1541710921.821124554 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 454, "value": 0.08071111111111111}

:::MLPv0.5.0 ssd 1541710921.915375948 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 455, "value": 0.08088888888888889}

:::MLPv0.5.0 ssd 1541710922.012015104 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 456, "value": 0.08106666666666666}

:::MLPv0.5.0 ssd 1541710922.105567694 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 457, "value": 0.08124444444444444}

:::MLPv0.5.0 ssd 1541710922.199832678 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 458, "value": 0.08142222222222222}

:::MLPv0.5.0 ssd 1541710922.294528484 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 459, "value": 0.0816}

:::MLPv0.5.0 ssd 1541710922.389492035 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 460, "value": 0.08177777777777778}
Iteration:    460, Loss function: 6.317, Average Loss: 3.265, avg. samples / sec: 21499.02
Iteration:    460, Loss function: 6.495, Average Loss: 3.264, avg. samples / sec: 21491.59
Iteration:    460, Loss function: 5.778, Average Loss: 3.262, avg. samples / sec: 21506.69
Iteration:    460, Loss function: 6.556, Average Loss: 3.267, avg. samples / sec: 21485.98
Iteration:    460, Loss function: 6.839, Average Loss: 3.263, avg. samples / sec: 21486.19
Iteration:    460, Loss function: 6.803, Average Loss: 3.261, avg. samples / sec: 21486.94
Iteration:    460, Loss function: 6.674, Average Loss: 3.269, avg. samples / sec: 21483.73
Iteration:    460, Loss function: 6.238, Average Loss: 3.266, avg. samples / sec: 21491.68

:::MLPv0.5.0 ssd 1541710922.483446121 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 461, "value": 0.08195555555555556}

:::MLPv0.5.0 ssd 1541710922.579454899 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 462, "value": 0.08213333333333334}

:::MLPv0.5.0 ssd 1541710922.670201063 (train.py:553) train_epoch: 8

:::MLPv0.5.0 ssd 1541710922.675924063 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 463, "value": 0.08231111111111111}

:::MLPv0.5.0 ssd 1541710922.770546675 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 464, "value": 0.08248888888888889}

:::MLPv0.5.0 ssd 1541710922.865248442 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 465, "value": 0.08266666666666667}

:::MLPv0.5.0 ssd 1541710922.959449053 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 466, "value": 0.08284444444444444}

:::MLPv0.5.0 ssd 1541710923.054206133 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 467, "value": 0.08302222222222222}

:::MLPv0.5.0 ssd 1541710923.147962093 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 468, "value": 0.0832}

:::MLPv0.5.0 ssd 1541710923.243486166 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 469, "value": 0.08337777777777777}

:::MLPv0.5.0 ssd 1541710923.338140965 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 470, "value": 0.08355555555555555}

:::MLPv0.5.0 ssd 1541710923.435509443 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 471, "value": 0.08373333333333333}

:::MLPv0.5.0 ssd 1541710923.532266855 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 472, "value": 0.08391111111111112}

:::MLPv0.5.0 ssd 1541710923.626475334 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 473, "value": 0.0840888888888889}

:::MLPv0.5.0 ssd 1541710923.724349499 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 474, "value": 0.08426666666666667}

:::MLPv0.5.0 ssd 1541710923.817877531 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 475, "value": 0.08444444444444445}

:::MLPv0.5.0 ssd 1541710923.913756847 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 476, "value": 0.08462222222222222}

:::MLPv0.5.0 ssd 1541710924.007603168 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 477, "value": 0.0848}

:::MLPv0.5.0 ssd 1541710924.102883816 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 478, "value": 0.08497777777777778}

:::MLPv0.5.0 ssd 1541710924.198732615 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 479, "value": 0.08515555555555555}

:::MLPv0.5.0 ssd 1541710924.293086290 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 480, "value": 0.08533333333333333}
Iteration:    480, Loss function: 6.193, Average Loss: 3.323, avg. samples / sec: 21522.49
Iteration:    480, Loss function: 6.160, Average Loss: 3.318, avg. samples / sec: 21535.08
Iteration:    480, Loss function: 6.360, Average Loss: 3.324, avg. samples / sec: 21525.56
Iteration:    480, Loss function: 6.128, Average Loss: 3.325, avg. samples / sec: 21527.30
Iteration:    480, Loss function: 6.523, Average Loss: 3.330, avg. samples / sec: 21527.75
Iteration:    480, Loss function: 5.695, Average Loss: 3.323, avg. samples / sec: 21511.63
Iteration:    480, Loss function: 5.908, Average Loss: 3.326, avg. samples / sec: 21527.21
Iteration:    480, Loss function: 5.699, Average Loss: 3.323, avg. samples / sec: 21506.79

:::MLPv0.5.0 ssd 1541710924.388225317 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 481, "value": 0.08551111111111111}

:::MLPv0.5.0 ssd 1541710924.483979225 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 482, "value": 0.08568888888888888}

:::MLPv0.5.0 ssd 1541710924.578362226 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 483, "value": 0.08586666666666666}

:::MLPv0.5.0 ssd 1541710924.675665855 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 484, "value": 0.08604444444444445}

:::MLPv0.5.0 ssd 1541710924.770194530 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 485, "value": 0.08622222222222223}

:::MLPv0.5.0 ssd 1541710924.865054607 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 486, "value": 0.0864}

:::MLPv0.5.0 ssd 1541710924.960258722 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 487, "value": 0.08657777777777778}

:::MLPv0.5.0 ssd 1541710925.055862427 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 488, "value": 0.08675555555555556}

:::MLPv0.5.0 ssd 1541710925.150766134 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 489, "value": 0.08693333333333333}

:::MLPv0.5.0 ssd 1541710925.246377945 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 490, "value": 0.08711111111111111}

:::MLPv0.5.0 ssd 1541710925.343025208 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 491, "value": 0.08728888888888889}

:::MLPv0.5.0 ssd 1541710925.438729048 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 492, "value": 0.08746666666666666}

:::MLPv0.5.0 ssd 1541710925.533323288 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 493, "value": 0.08764444444444444}

:::MLPv0.5.0 ssd 1541710925.628423691 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 494, "value": 0.08782222222222222}

:::MLPv0.5.0 ssd 1541710925.723082781 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 495, "value": 0.088}

:::MLPv0.5.0 ssd 1541710925.818184853 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 496, "value": 0.08817777777777777}

:::MLPv0.5.0 ssd 1541710925.913376331 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 497, "value": 0.08835555555555556}

:::MLPv0.5.0 ssd 1541710926.009471178 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 498, "value": 0.08853333333333334}

:::MLPv0.5.0 ssd 1541710926.103842020 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 499, "value": 0.08871111111111112}

:::MLPv0.5.0 ssd 1541710926.198550224 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 500, "value": 0.08888888888888889}
Iteration:    500, Loss function: 6.252, Average Loss: 3.380, avg. samples / sec: 21506.72
Iteration:    500, Loss function: 6.269, Average Loss: 3.381, avg. samples / sec: 21493.22
Iteration:    500, Loss function: 5.861, Average Loss: 3.380, avg. samples / sec: 21497.43
Iteration:    500, Loss function: 5.643, Average Loss: 3.379, avg. samples / sec: 21499.69
Iteration:    500, Loss function: 5.353, Average Loss: 3.384, avg. samples / sec: 21499.51
Iteration:    500, Loss function: 6.262, Average Loss: 3.374, avg. samples / sec: 21488.47
Iteration:    500, Loss function: 5.853, Average Loss: 3.380, avg. samples / sec: 21490.60
Iteration:    500, Loss function: 5.964, Average Loss: 3.378, avg. samples / sec: 21490.22

:::MLPv0.5.0 ssd 1541710926.299952030 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 501, "value": 0.08906666666666667}

:::MLPv0.5.0 ssd 1541710926.393700838 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 502, "value": 0.08924444444444445}

:::MLPv0.5.0 ssd 1541710926.490445137 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 503, "value": 0.08942222222222222}

:::MLPv0.5.0 ssd 1541710926.584422827 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 504, "value": 0.0896}

:::MLPv0.5.0 ssd 1541710926.685052156 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 505, "value": 0.08977777777777778}

:::MLPv0.5.0 ssd 1541710926.778493166 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 506, "value": 0.08995555555555555}

:::MLPv0.5.0 ssd 1541710926.874971628 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 507, "value": 0.09013333333333333}

:::MLPv0.5.0 ssd 1541710926.969510555 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 508, "value": 0.0903111111111111}

:::MLPv0.5.0 ssd 1541710927.069124460 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 509, "value": 0.09048888888888888}

:::MLPv0.5.0 ssd 1541710927.163404703 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 510, "value": 0.09066666666666667}

:::MLPv0.5.0 ssd 1541710927.264524698 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 511, "value": 0.09084444444444445}

:::MLPv0.5.0 ssd 1541710927.359715939 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 512, "value": 0.09102222222222223}

:::MLPv0.5.0 ssd 1541710927.454179525 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 513, "value": 0.0912}

:::MLPv0.5.0 ssd 1541710927.550003767 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 514, "value": 0.09137777777777778}

:::MLPv0.5.0 ssd 1541710927.644293308 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 515, "value": 0.09155555555555556}

:::MLPv0.5.0 ssd 1541710927.740116119 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 516, "value": 0.09173333333333333}

:::MLPv0.5.0 ssd 1541710927.834966898 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 517, "value": 0.09191111111111111}

:::MLPv0.5.0 ssd 1541710927.930018663 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 518, "value": 0.09208888888888889}

:::MLPv0.5.0 ssd 1541710928.025115967 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 519, "value": 0.09226666666666666}

:::MLPv0.5.0 ssd 1541710928.117832422 (train.py:553) train_epoch: 9

:::MLPv0.5.0 ssd 1541710928.123376846 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 520, "value": 0.09244444444444444}
Iteration:    520, Loss function: 6.734, Average Loss: 3.436, avg. samples / sec: 21281.99
Iteration:    520, Loss function: 6.140, Average Loss: 3.436, avg. samples / sec: 21279.60
Iteration:    520, Loss function: 6.144, Average Loss: 3.434, avg. samples / sec: 21280.43
Iteration:    520, Loss function: 6.062, Average Loss: 3.435, avg. samples / sec: 21294.89
Iteration:    520, Loss function: 6.285, Average Loss: 3.435, avg. samples / sec: 21273.98
Iteration:    520, Loss function: 5.838, Average Loss: 3.431, avg. samples / sec: 21276.44
Iteration:    520, Loss function: 6.483, Average Loss: 3.442, avg. samples / sec: 21272.01
Iteration:    520, Loss function: 5.742, Average Loss: 3.435, avg. samples / sec: 21279.02

:::MLPv0.5.0 ssd 1541710928.219305515 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 521, "value": 0.09262222222222222}

:::MLPv0.5.0 ssd 1541710928.314349890 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 522, "value": 0.0928}

:::MLPv0.5.0 ssd 1541710928.409969330 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 523, "value": 0.09297777777777778}

:::MLPv0.5.0 ssd 1541710928.507127285 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 524, "value": 0.09315555555555556}

:::MLPv0.5.0 ssd 1541710928.602010012 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 525, "value": 0.09333333333333334}

:::MLPv0.5.0 ssd 1541710928.696398735 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 526, "value": 0.09351111111111111}

:::MLPv0.5.0 ssd 1541710928.790206671 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 527, "value": 0.09368888888888889}

:::MLPv0.5.0 ssd 1541710928.884659052 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 528, "value": 0.09386666666666667}

:::MLPv0.5.0 ssd 1541710928.979150295 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 529, "value": 0.09404444444444444}

:::MLPv0.5.0 ssd 1541710929.075154781 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 530, "value": 0.09422222222222222}

:::MLPv0.5.0 ssd 1541710929.169497967 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 531, "value": 0.0944}

:::MLPv0.5.0 ssd 1541710929.264380217 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 532, "value": 0.09457777777777777}

:::MLPv0.5.0 ssd 1541710929.359494686 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 533, "value": 0.09475555555555555}

:::MLPv0.5.0 ssd 1541710929.455275536 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 534, "value": 0.09493333333333333}

:::MLPv0.5.0 ssd 1541710929.551707983 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 535, "value": 0.0951111111111111}

:::MLPv0.5.0 ssd 1541710929.651793957 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 536, "value": 0.0952888888888889}

:::MLPv0.5.0 ssd 1541710929.747030020 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 537, "value": 0.09546666666666667}

:::MLPv0.5.0 ssd 1541710929.841790438 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 538, "value": 0.09564444444444445}

:::MLPv0.5.0 ssd 1541710929.936496019 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 539, "value": 0.09582222222222223}

:::MLPv0.5.0 ssd 1541710930.032875538 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 540, "value": 0.096}
Iteration:    540, Loss function: 6.482, Average Loss: 3.485, avg. samples / sec: 21453.85
Iteration:    540, Loss function: 5.610, Average Loss: 3.493, avg. samples / sec: 21459.27
Iteration:    540, Loss function: 6.264, Average Loss: 3.482, avg. samples / sec: 21456.05
Iteration:    540, Loss function: 5.892, Average Loss: 3.487, avg. samples / sec: 21452.65
Iteration:    540, Loss function: 6.216, Average Loss: 3.486, avg. samples / sec: 21439.09
Iteration:    540, Loss function: 5.733, Average Loss: 3.486, avg. samples / sec: 21435.89
Iteration:    540, Loss function: 6.407, Average Loss: 3.485, avg. samples / sec: 21448.48
Iteration:    540, Loss function: 6.365, Average Loss: 3.485, avg. samples / sec: 21420.07

:::MLPv0.5.0 ssd 1541710930.126786470 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 541, "value": 0.09617777777777778}

:::MLPv0.5.0 ssd 1541710930.221997023 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 542, "value": 0.09635555555555556}

:::MLPv0.5.0 ssd 1541710930.317453384 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 543, "value": 0.09653333333333333}

:::MLPv0.5.0 ssd 1541710930.411996603 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 544, "value": 0.09671111111111111}

:::MLPv0.5.0 ssd 1541710930.507295370 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 545, "value": 0.09688888888888889}

:::MLPv0.5.0 ssd 1541710930.605099201 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 546, "value": 0.09706666666666666}

:::MLPv0.5.0 ssd 1541710930.698945045 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 547, "value": 0.09724444444444444}

:::MLPv0.5.0 ssd 1541710930.794472456 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 548, "value": 0.09742222222222222}

:::MLPv0.5.0 ssd 1541710930.889161825 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 549, "value": 0.09759999999999999}

:::MLPv0.5.0 ssd 1541710930.983768940 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 550, "value": 0.09777777777777777}

:::MLPv0.5.0 ssd 1541710931.079002619 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 551, "value": 0.09795555555555555}

:::MLPv0.5.0 ssd 1541710931.172931433 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 552, "value": 0.09813333333333334}

:::MLPv0.5.0 ssd 1541710931.267385483 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 553, "value": 0.09831111111111111}

:::MLPv0.5.0 ssd 1541710931.363882542 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 554, "value": 0.09848888888888889}

:::MLPv0.5.0 ssd 1541710931.459933996 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 555, "value": 0.09866666666666667}

:::MLPv0.5.0 ssd 1541710931.555343151 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 556, "value": 0.09884444444444444}

:::MLPv0.5.0 ssd 1541710931.650373936 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 557, "value": 0.09902222222222222}

:::MLPv0.5.0 ssd 1541710931.743936300 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 558, "value": 0.09920000000000001}

:::MLPv0.5.0 ssd 1541710931.839594126 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 559, "value": 0.09937777777777779}

:::MLPv0.5.0 ssd 1541710931.933655739 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 560, "value": 0.09955555555555556}
Iteration:    560, Loss function: 5.859, Average Loss: 3.537, avg. samples / sec: 21549.91
Iteration:    560, Loss function: 5.781, Average Loss: 3.536, avg. samples / sec: 21561.45
Iteration:    560, Loss function: 5.937, Average Loss: 3.536, avg. samples / sec: 21548.86
Iteration:    560, Loss function: 6.145, Average Loss: 3.536, avg. samples / sec: 21561.07
Iteration:    560, Loss function: 6.226, Average Loss: 3.536, avg. samples / sec: 21577.04
Iteration:    560, Loss function: 5.855, Average Loss: 3.544, avg. samples / sec: 21546.45
Iteration:    560, Loss function: 5.491, Average Loss: 3.539, avg. samples / sec: 21548.92
Iteration:    560, Loss function: 5.791, Average Loss: 3.537, avg. samples / sec: 21548.13

:::MLPv0.5.0 ssd 1541710932.028013706 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 561, "value": 0.09973333333333334}

:::MLPv0.5.0 ssd 1541710932.125432014 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 562, "value": 0.09991111111111112}

:::MLPv0.5.0 ssd 1541710932.219044685 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 563, "value": 0.1000888888888889}

:::MLPv0.5.0 ssd 1541710932.313053846 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 564, "value": 0.10026666666666667}

:::MLPv0.5.0 ssd 1541710932.409213781 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 565, "value": 0.10044444444444445}

:::MLPv0.5.0 ssd 1541710932.503319025 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 566, "value": 0.10062222222222222}

:::MLPv0.5.0 ssd 1541710932.597283363 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 567, "value": 0.1008}

:::MLPv0.5.0 ssd 1541710932.691665888 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 568, "value": 0.10097777777777778}

:::MLPv0.5.0 ssd 1541710932.785815954 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 569, "value": 0.10115555555555555}

:::MLPv0.5.0 ssd 1541710932.881613731 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 570, "value": 0.10133333333333333}

:::MLPv0.5.0 ssd 1541710932.975976944 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 571, "value": 0.10151111111111111}

:::MLPv0.5.0 ssd 1541710933.070209026 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 572, "value": 0.10168888888888888}

:::MLPv0.5.0 ssd 1541710933.164155722 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 573, "value": 0.10186666666666666}

:::MLPv0.5.0 ssd 1541710933.257604599 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 574, "value": 0.10204444444444444}

:::MLPv0.5.0 ssd 1541710933.351920366 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 575, "value": 0.10222222222222221}

:::MLPv0.5.0 ssd 1541710933.446260452 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 576, "value": 0.10239999999999999}

:::MLPv0.5.0 ssd 1541710933.541348934 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 577, "value": 0.10257777777777778}

:::MLPv0.5.0 ssd 1541710933.631925344 (train.py:553) train_epoch: 10

:::MLPv0.5.0 ssd 1541710933.637554884 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 578, "value": 0.10275555555555556}

:::MLPv0.5.0 ssd 1541710933.732004881 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 579, "value": 0.10293333333333334}

:::MLPv0.5.0 ssd 1541710933.826364756 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 580, "value": 0.10311111111111111}
Iteration:    580, Loss function: 6.242, Average Loss: 3.583, avg. samples / sec: 21635.85
Iteration:    580, Loss function: 5.553, Average Loss: 3.583, avg. samples / sec: 21640.55
Iteration:    580, Loss function: 6.268, Average Loss: 3.591, avg. samples / sec: 21641.98
Iteration:    580, Loss function: 5.983, Average Loss: 3.581, avg. samples / sec: 21643.23
Iteration:    580, Loss function: 6.127, Average Loss: 3.584, avg. samples / sec: 21639.81
Iteration:    580, Loss function: 6.303, Average Loss: 3.585, avg. samples / sec: 21640.33
Iteration:    580, Loss function: 6.028, Average Loss: 3.582, avg. samples / sec: 21638.87
Iteration:    580, Loss function: 6.207, Average Loss: 3.584, avg. samples / sec: 21637.94

:::MLPv0.5.0 ssd 1541710933.920472383 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 581, "value": 0.10328888888888889}

:::MLPv0.5.0 ssd 1541710934.014617682 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 582, "value": 0.10346666666666667}

:::MLPv0.5.0 ssd 1541710934.109565973 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 583, "value": 0.10364444444444444}

:::MLPv0.5.0 ssd 1541710934.204516172 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 584, "value": 0.10382222222222223}

:::MLPv0.5.0 ssd 1541710934.298587799 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 585, "value": 0.10400000000000001}

:::MLPv0.5.0 ssd 1541710934.392219067 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 586, "value": 0.10417777777777779}

:::MLPv0.5.0 ssd 1541710934.487139225 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 587, "value": 0.10435555555555556}

:::MLPv0.5.0 ssd 1541710934.581187725 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 588, "value": 0.10453333333333334}

:::MLPv0.5.0 ssd 1541710934.675760269 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 589, "value": 0.10471111111111112}

:::MLPv0.5.0 ssd 1541710934.769796133 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 590, "value": 0.10488888888888889}

:::MLPv0.5.0 ssd 1541710934.863812685 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 591, "value": 0.10506666666666667}

:::MLPv0.5.0 ssd 1541710934.959625959 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 592, "value": 0.10524444444444445}

:::MLPv0.5.0 ssd 1541710935.053483009 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 593, "value": 0.10542222222222222}

:::MLPv0.5.0 ssd 1541710935.147923231 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 594, "value": 0.1056}

:::MLPv0.5.0 ssd 1541710935.243109226 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 595, "value": 0.10577777777777778}

:::MLPv0.5.0 ssd 1541710935.337347984 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 596, "value": 0.10595555555555555}

:::MLPv0.5.0 ssd 1541710935.431233168 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 597, "value": 0.10613333333333333}

:::MLPv0.5.0 ssd 1541710935.526351452 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 598, "value": 0.1063111111111111}

:::MLPv0.5.0 ssd 1541710935.621096373 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 599, "value": 0.10648888888888888}

:::MLPv0.5.0 ssd 1541710935.715818644 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 600, "value": 0.10666666666666666}
Iteration:    600, Loss function: 5.175, Average Loss: 3.629, avg. samples / sec: 21685.52
Iteration:    600, Loss function: 5.360, Average Loss: 3.629, avg. samples / sec: 21684.35
Iteration:    600, Loss function: 5.661, Average Loss: 3.638, avg. samples / sec: 21679.59
Iteration:    600, Loss function: 5.526, Average Loss: 3.630, avg. samples / sec: 21676.27
Iteration:    600, Loss function: 6.057, Average Loss: 3.629, avg. samples / sec: 21679.54
Iteration:    600, Loss function: 5.549, Average Loss: 3.632, avg. samples / sec: 21681.50
Iteration:    600, Loss function: 5.546, Average Loss: 3.631, avg. samples / sec: 21681.17
Iteration:    600, Loss function: 5.656, Average Loss: 3.630, avg. samples / sec: 21677.39

:::MLPv0.5.0 ssd 1541710935.810711145 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 601, "value": 0.10684444444444444}

:::MLPv0.5.0 ssd 1541710935.905259848 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 602, "value": 0.10702222222222221}

:::MLPv0.5.0 ssd 1541710935.999353647 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 603, "value": 0.1072}

:::MLPv0.5.0 ssd 1541710936.094045162 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 604, "value": 0.10737777777777778}

:::MLPv0.5.0 ssd 1541710936.188741446 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 605, "value": 0.10755555555555556}

:::MLPv0.5.0 ssd 1541710936.283783913 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 606, "value": 0.10773333333333333}

:::MLPv0.5.0 ssd 1541710936.378459930 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 607, "value": 0.10791111111111111}

:::MLPv0.5.0 ssd 1541710936.473725557 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 608, "value": 0.10808888888888889}

:::MLPv0.5.0 ssd 1541710936.570410490 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 609, "value": 0.10826666666666668}

:::MLPv0.5.0 ssd 1541710936.666325569 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 610, "value": 0.10844444444444445}

:::MLPv0.5.0 ssd 1541710936.761080742 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 611, "value": 0.10862222222222223}

:::MLPv0.5.0 ssd 1541710936.857082844 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 612, "value": 0.10880000000000001}

:::MLPv0.5.0 ssd 1541710936.950911045 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 613, "value": 0.10897777777777778}

:::MLPv0.5.0 ssd 1541710937.046382189 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 614, "value": 0.10915555555555556}

:::MLPv0.5.0 ssd 1541710937.141516447 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 615, "value": 0.10933333333333334}

:::MLPv0.5.0 ssd 1541710937.235339165 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 616, "value": 0.10951111111111111}

:::MLPv0.5.0 ssd 1541710937.328948259 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 617, "value": 0.10968888888888889}

:::MLPv0.5.0 ssd 1541710937.423063517 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 618, "value": 0.10986666666666667}

:::MLPv0.5.0 ssd 1541710937.519853830 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 619, "value": 0.11004444444444444}

:::MLPv0.5.0 ssd 1541710937.615110874 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 620, "value": 0.11022222222222222}
Iteration:    620, Loss function: 5.229, Average Loss: 3.668, avg. samples / sec: 21571.49
Iteration:    620, Loss function: 5.778, Average Loss: 3.664, avg. samples / sec: 21571.09
Iteration:    620, Loss function: 6.271, Average Loss: 3.672, avg. samples / sec: 21572.81
Iteration:    620, Loss function: 5.704, Average Loss: 3.669, avg. samples / sec: 21564.05
Iteration:    620, Loss function: 4.940, Average Loss: 3.668, avg. samples / sec: 21569.33
Iteration:    620, Loss function: 5.286, Average Loss: 3.671, avg. samples / sec: 21566.78
Iteration:    620, Loss function: 5.954, Average Loss: 3.676, avg. samples / sec: 21564.21
Iteration:    620, Loss function: 6.031, Average Loss: 3.670, avg. samples / sec: 21565.03

:::MLPv0.5.0 ssd 1541710937.709235907 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 621, "value": 0.1104}

:::MLPv0.5.0 ssd 1541710937.802829742 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 622, "value": 0.11057777777777777}

:::MLPv0.5.0 ssd 1541710937.897382259 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 623, "value": 0.11075555555555555}

:::MLPv0.5.0 ssd 1541710937.993604422 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 624, "value": 0.11093333333333333}

:::MLPv0.5.0 ssd 1541710938.087803841 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 625, "value": 0.1111111111111111}

:::MLPv0.5.0 ssd 1541710938.182104111 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 626, "value": 0.11128888888888888}

:::MLPv0.5.0 ssd 1541710938.277309179 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 627, "value": 0.11146666666666666}

:::MLPv0.5.0 ssd 1541710938.371153116 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 628, "value": 0.11164444444444445}

:::MLPv0.5.0 ssd 1541710938.465214491 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 629, "value": 0.11182222222222223}

:::MLPv0.5.0 ssd 1541710938.560170650 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 630, "value": 0.112}

:::MLPv0.5.0 ssd 1541710938.656467915 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 631, "value": 0.11217777777777778}

:::MLPv0.5.0 ssd 1541710938.750352621 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 632, "value": 0.11235555555555556}

:::MLPv0.5.0 ssd 1541710938.844935894 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 633, "value": 0.11253333333333333}

:::MLPv0.5.0 ssd 1541710938.941013575 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 634, "value": 0.11271111111111111}

:::MLPv0.5.0 ssd 1541710939.035686970 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 635, "value": 0.1128888888888889}

:::MLPv0.5.0 ssd 1541710939.125458717 (train.py:553) train_epoch: 11

:::MLPv0.5.0 ssd 1541710939.130681753 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 636, "value": 0.11306666666666668}

:::MLPv0.5.0 ssd 1541710939.224583149 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 637, "value": 0.11324444444444445}

:::MLPv0.5.0 ssd 1541710939.319614172 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 638, "value": 0.11342222222222223}

:::MLPv0.5.0 ssd 1541710939.415535688 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 639, "value": 0.1136}

:::MLPv0.5.0 ssd 1541710939.509571075 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 640, "value": 0.11377777777777778}
Iteration:    640, Loss function: 5.551, Average Loss: 3.709, avg. samples / sec: 21618.78
Iteration:    640, Loss function: 5.769, Average Loss: 3.711, avg. samples / sec: 21621.08
Iteration:    640, Loss function: 6.016, Average Loss: 3.705, avg. samples / sec: 21611.03
Iteration:    640, Loss function: 5.595, Average Loss: 3.708, avg. samples / sec: 21617.77
Iteration:    640, Loss function: 5.813, Average Loss: 3.712, avg. samples / sec: 21618.34
Iteration:    640, Loss function: 5.667, Average Loss: 3.702, avg. samples / sec: 21610.64
Iteration:    640, Loss function: 5.592, Average Loss: 3.707, avg. samples / sec: 21611.49
Iteration:    640, Loss function: 5.934, Average Loss: 3.702, avg. samples / sec: 21610.35

:::MLPv0.5.0 ssd 1541710939.603431225 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 641, "value": 0.11395555555555556}

:::MLPv0.5.0 ssd 1541710939.697463512 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 642, "value": 0.11413333333333334}

:::MLPv0.5.0 ssd 1541710939.792754650 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 643, "value": 0.11431111111111111}

:::MLPv0.5.0 ssd 1541710939.886669159 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 644, "value": 0.11448888888888889}

:::MLPv0.5.0 ssd 1541710939.982332230 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 645, "value": 0.11466666666666667}

:::MLPv0.5.0 ssd 1541710940.076622009 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 646, "value": 0.11484444444444444}

:::MLPv0.5.0 ssd 1541710940.170659065 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 647, "value": 0.11502222222222222}

:::MLPv0.5.0 ssd 1541710940.266105413 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 648, "value": 0.1152}

:::MLPv0.5.0 ssd 1541710940.360668421 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 649, "value": 0.11537777777777777}

:::MLPv0.5.0 ssd 1541710940.454696655 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 650, "value": 0.11555555555555555}

:::MLPv0.5.0 ssd 1541710940.551829100 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 651, "value": 0.11573333333333333}

:::MLPv0.5.0 ssd 1541710940.646259785 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 652, "value": 0.1159111111111111}

:::MLPv0.5.0 ssd 1541710940.741020441 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 653, "value": 0.11608888888888888}

:::MLPv0.5.0 ssd 1541710940.834529877 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 654, "value": 0.11626666666666667}

:::MLPv0.5.0 ssd 1541710940.928225279 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 655, "value": 0.11644444444444445}

:::MLPv0.5.0 ssd 1541710941.023141861 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 656, "value": 0.11662222222222222}

:::MLPv0.5.0 ssd 1541710941.117180824 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 657, "value": 0.1168}

:::MLPv0.5.0 ssd 1541710941.211310863 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 658, "value": 0.11697777777777778}

:::MLPv0.5.0 ssd 1541710941.305284262 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 659, "value": 0.11715555555555555}

:::MLPv0.5.0 ssd 1541710941.399014711 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 660, "value": 0.11733333333333333}
Iteration:    660, Loss function: 5.662, Average Loss: 3.746, avg. samples / sec: 21689.25
Iteration:    660, Loss function: 5.638, Average Loss: 3.743, avg. samples / sec: 21682.57
Iteration:    660, Loss function: 5.359, Average Loss: 3.745, avg. samples / sec: 21679.91
Iteration:    660, Loss function: 5.514, Average Loss: 3.739, avg. samples / sec: 21683.45
Iteration:    660, Loss function: 5.670, Average Loss: 3.746, avg. samples / sec: 21674.33
Iteration:    660, Loss function: 5.232, Average Loss: 3.738, avg. samples / sec: 21685.37
Iteration:    660, Loss function: 5.934, Average Loss: 3.750, avg. samples / sec: 21676.07
Iteration:    660, Loss function: 6.386, Average Loss: 3.742, avg. samples / sec: 21682.93

:::MLPv0.5.0 ssd 1541710941.492911816 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 661, "value": 0.11751111111111112}

:::MLPv0.5.0 ssd 1541710941.588156462 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 662, "value": 0.1176888888888889}

:::MLPv0.5.0 ssd 1541710941.681730747 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 663, "value": 0.11786666666666668}

:::MLPv0.5.0 ssd 1541710941.777172565 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 664, "value": 0.11804444444444445}

:::MLPv0.5.0 ssd 1541710941.871608257 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 665, "value": 0.11822222222222223}

:::MLPv0.5.0 ssd 1541710941.966281176 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 666, "value": 0.1184}

:::MLPv0.5.0 ssd 1541710942.060486317 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 667, "value": 0.11857777777777778}

:::MLPv0.5.0 ssd 1541710942.154377699 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 668, "value": 0.11875555555555556}

:::MLPv0.5.0 ssd 1541710942.250703573 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 669, "value": 0.11893333333333334}

:::MLPv0.5.0 ssd 1541710942.344366312 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 670, "value": 0.11911111111111111}

:::MLPv0.5.0 ssd 1541710942.438766003 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 671, "value": 0.11928888888888889}

:::MLPv0.5.0 ssd 1541710942.533433199 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 672, "value": 0.11946666666666667}

:::MLPv0.5.0 ssd 1541710942.627404690 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 673, "value": 0.11964444444444444}

:::MLPv0.5.0 ssd 1541710942.724047184 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 674, "value": 0.11982222222222222}

:::MLPv0.5.0 ssd 1541710942.817921162 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 675, "value": 0.12}

:::MLPv0.5.0 ssd 1541710942.911854744 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 676, "value": 0.12017777777777777}

:::MLPv0.5.0 ssd 1541710943.005577564 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 677, "value": 0.12035555555555555}

:::MLPv0.5.0 ssd 1541710943.104714155 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 678, "value": 0.12053333333333333}

:::MLPv0.5.0 ssd 1541710943.199239016 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 679, "value": 0.1207111111111111}

:::MLPv0.5.0 ssd 1541710943.294640064 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 680, "value": 0.12088888888888889}
Iteration:    680, Loss function: 5.702, Average Loss: 3.787, avg. samples / sec: 21627.63
Iteration:    680, Loss function: 6.287, Average Loss: 3.781, avg. samples / sec: 21619.30
Iteration:    680, Loss function: 5.578, Average Loss: 3.777, avg. samples / sec: 21612.97
Iteration:    680, Loss function: 5.704, Average Loss: 3.782, avg. samples / sec: 21615.17
Iteration:    680, Loss function: 5.619, Average Loss: 3.776, avg. samples / sec: 21614.84
Iteration:    680, Loss function: 5.316, Average Loss: 3.782, avg. samples / sec: 21604.49
Iteration:    680, Loss function: 6.176, Average Loss: 3.781, avg. samples / sec: 21617.73
Iteration:    680, Loss function: 5.775, Average Loss: 3.778, avg. samples / sec: 21606.55

:::MLPv0.5.0 ssd 1541710943.388711214 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 681, "value": 0.12106666666666667}

:::MLPv0.5.0 ssd 1541710943.487718344 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 682, "value": 0.12124444444444445}

:::MLPv0.5.0 ssd 1541710943.582054138 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 683, "value": 0.12142222222222222}

:::MLPv0.5.0 ssd 1541710943.677082300 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 684, "value": 0.1216}

:::MLPv0.5.0 ssd 1541710943.770966768 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 685, "value": 0.12177777777777778}

:::MLPv0.5.0 ssd 1541710943.866024256 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 686, "value": 0.12195555555555557}

:::MLPv0.5.0 ssd 1541710943.960578442 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 687, "value": 0.12213333333333334}

:::MLPv0.5.0 ssd 1541710944.054398060 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 688, "value": 0.12231111111111112}

:::MLPv0.5.0 ssd 1541710944.147913456 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 689, "value": 0.1224888888888889}

:::MLPv0.5.0 ssd 1541710944.242958546 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 690, "value": 0.12266666666666667}

:::MLPv0.5.0 ssd 1541710944.336966753 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 691, "value": 0.12284444444444445}

:::MLPv0.5.0 ssd 1541710944.431671619 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 692, "value": 0.12302222222222223}

:::MLPv0.5.0 ssd 1541710944.527346134 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 693, "value": 0.1232}

:::MLPv0.5.0 ssd 1541710944.618032694 (train.py:553) train_epoch: 12

:::MLPv0.5.0 ssd 1541710944.623639345 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 694, "value": 0.12337777777777778}

:::MLPv0.5.0 ssd 1541710944.716577053 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 695, "value": 0.12355555555555556}

:::MLPv0.5.0 ssd 1541710944.811316252 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 696, "value": 0.12373333333333333}

:::MLPv0.5.0 ssd 1541710944.905323029 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 697, "value": 0.12391111111111111}

:::MLPv0.5.0 ssd 1541710944.998830557 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 698, "value": 0.12408888888888889}

:::MLPv0.5.0 ssd 1541710945.094266891 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 699, "value": 0.12426666666666666}

:::MLPv0.5.0 ssd 1541710945.188167095 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 700, "value": 0.12444444444444444}
Iteration:    700, Loss function: 5.669, Average Loss: 3.817, avg. samples / sec: 21635.03
Iteration:    700, Loss function: 5.444, Average Loss: 3.809, avg. samples / sec: 21635.27
Iteration:    700, Loss function: 5.552, Average Loss: 3.819, avg. samples / sec: 21636.70
Iteration:    700, Loss function: 5.359, Average Loss: 3.818, avg. samples / sec: 21632.87
Iteration:    700, Loss function: 5.528, Average Loss: 3.824, avg. samples / sec: 21622.34
Iteration:    700, Loss function: 6.135, Average Loss: 3.815, avg. samples / sec: 21631.53
Iteration:    700, Loss function: 5.913, Average Loss: 3.812, avg. samples / sec: 21622.53
Iteration:    700, Loss function: 6.707, Average Loss: 3.815, avg. samples / sec: 21627.05

:::MLPv0.5.0 ssd 1541710945.281699181 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 701, "value": 0.12462222222222222}

:::MLPv0.5.0 ssd 1541710945.375225306 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 702, "value": 0.1248}

:::MLPv0.5.0 ssd 1541710945.469510794 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 703, "value": 0.12497777777777777}

:::MLPv0.5.0 ssd 1541710945.563550234 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 704, "value": 0.12515555555555555}

:::MLPv0.5.0 ssd 1541710945.657318115 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 705, "value": 0.12533333333333335}

:::MLPv0.5.0 ssd 1541710945.751167536 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 706, "value": 0.12551111111111113}

:::MLPv0.5.0 ssd 1541710945.845443249 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 707, "value": 0.1256888888888889}

:::MLPv0.5.0 ssd 1541710945.938725233 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 708, "value": 0.12586666666666668}

:::MLPv0.5.0 ssd 1541710946.032759905 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 709, "value": 0.12604444444444446}

:::MLPv0.5.0 ssd 1541710946.126351833 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 710, "value": 0.12622222222222224}

:::MLPv0.5.0 ssd 1541710946.220281601 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 711, "value": 0.1264}

:::MLPv0.5.0 ssd 1541710946.314097404 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 712, "value": 0.1265777777777778}

:::MLPv0.5.0 ssd 1541710946.408064604 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 713, "value": 0.12675555555555557}

:::MLPv0.5.0 ssd 1541710946.501032352 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 714, "value": 0.12693333333333334}

:::MLPv0.5.0 ssd 1541710946.594902515 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 715, "value": 0.12711111111111112}

:::MLPv0.5.0 ssd 1541710946.693362713 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 716, "value": 0.1272888888888889}

:::MLPv0.5.0 ssd 1541710946.788524866 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 717, "value": 0.12746666666666667}

:::MLPv0.5.0 ssd 1541710946.882699013 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 718, "value": 0.12764444444444445}

:::MLPv0.5.0 ssd 1541710946.975411654 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 719, "value": 0.12782222222222223}

:::MLPv0.5.0 ssd 1541710947.069975853 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 720, "value": 0.128}
Iteration:    720, Loss function: 5.154, Average Loss: 3.842, avg. samples / sec: 21768.50
Iteration:    720, Loss function: 5.076, Average Loss: 3.849, avg. samples / sec: 21766.83
Iteration:    720, Loss function: 4.740, Average Loss: 3.844, avg. samples / sec: 21776.29
Iteration:    720, Loss function: 5.295, Average Loss: 3.849, avg. samples / sec: 21771.67
Iteration:    720, Loss function: 5.390, Average Loss: 3.851, avg. samples / sec: 21762.93
Iteration:    720, Loss function: 5.000, Average Loss: 3.848, avg. samples / sec: 21758.16
Iteration:    720, Loss function: 4.994, Average Loss: 3.856, avg. samples / sec: 21761.52
Iteration:    720, Loss function: 5.905, Average Loss: 3.848, avg. samples / sec: 21767.06

:::MLPv0.5.0 ssd 1541710947.162755489 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 721, "value": 0.12817777777777778}

:::MLPv0.5.0 ssd 1541710947.256276369 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 722, "value": 0.12835555555555556}

:::MLPv0.5.0 ssd 1541710947.350403070 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 723, "value": 0.12853333333333333}

:::MLPv0.5.0 ssd 1541710947.444659472 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 724, "value": 0.1287111111111111}

:::MLPv0.5.0 ssd 1541710947.538612366 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 725, "value": 0.1288888888888889}

:::MLPv0.5.0 ssd 1541710947.632308006 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 726, "value": 0.12906666666666666}

:::MLPv0.5.0 ssd 1541710947.726207733 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 727, "value": 0.12924444444444444}

:::MLPv0.5.0 ssd 1541710947.821050406 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 728, "value": 0.12942222222222222}

:::MLPv0.5.0 ssd 1541710947.915310144 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 729, "value": 0.1296}

:::MLPv0.5.0 ssd 1541710948.010390520 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 730, "value": 0.12977777777777777}

:::MLPv0.5.0 ssd 1541710948.104828835 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 731, "value": 0.12995555555555555}

:::MLPv0.5.0 ssd 1541710948.199074268 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 732, "value": 0.13013333333333332}

:::MLPv0.5.0 ssd 1541710948.293341637 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 733, "value": 0.1303111111111111}

:::MLPv0.5.0 ssd 1541710948.387209177 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 734, "value": 0.13048888888888888}

:::MLPv0.5.0 ssd 1541710948.481608629 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 735, "value": 0.13066666666666665}

:::MLPv0.5.0 ssd 1541710948.577047348 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 736, "value": 0.13084444444444446}

:::MLPv0.5.0 ssd 1541710948.671298742 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 737, "value": 0.13102222222222223}

:::MLPv0.5.0 ssd 1541710948.765260220 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 738, "value": 0.1312}

:::MLPv0.5.0 ssd 1541710948.861567974 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 739, "value": 0.1313777777777778}

:::MLPv0.5.0 ssd 1541710948.956898212 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 740, "value": 0.13155555555555556}
Iteration:    740, Loss function: 5.257, Average Loss: 3.875, avg. samples / sec: 21719.51
Iteration:    740, Loss function: 5.727, Average Loss: 3.882, avg. samples / sec: 21710.38
Iteration:    740, Loss function: 5.179, Average Loss: 3.879, avg. samples / sec: 21705.10
Iteration:    740, Loss function: 5.012, Average Loss: 3.872, avg. samples / sec: 21703.14
Iteration:    740, Loss function: 4.837, Average Loss: 3.878, avg. samples / sec: 21705.01
Iteration:    740, Loss function: 5.716, Average Loss: 3.887, avg. samples / sec: 21714.24
Iteration:    740, Loss function: 5.376, Average Loss: 3.872, avg. samples / sec: 21704.68
Iteration:    740, Loss function: 5.395, Average Loss: 3.877, avg. samples / sec: 21714.35

:::MLPv0.5.0 ssd 1541710949.050738335 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 741, "value": 0.13173333333333334}

:::MLPv0.5.0 ssd 1541710949.144843102 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 742, "value": 0.13191111111111112}

:::MLPv0.5.0 ssd 1541710949.239156008 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 743, "value": 0.1320888888888889}

:::MLPv0.5.0 ssd 1541710949.332896233 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 744, "value": 0.13226666666666667}

:::MLPv0.5.0 ssd 1541710949.428933382 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 745, "value": 0.13244444444444445}

:::MLPv0.5.0 ssd 1541710949.523992300 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 746, "value": 0.13262222222222222}

:::MLPv0.5.0 ssd 1541710949.618140459 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 747, "value": 0.1328}

:::MLPv0.5.0 ssd 1541710949.712573528 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 748, "value": 0.13297777777777778}

:::MLPv0.5.0 ssd 1541710949.807135344 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 749, "value": 0.13315555555555555}

:::MLPv0.5.0 ssd 1541710949.900993586 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 750, "value": 0.13333333333333333}

:::MLPv0.5.0 ssd 1541710949.991291761 (train.py:553) train_epoch: 13

:::MLPv0.5.0 ssd 1541710949.996666431 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 751, "value": 0.1335111111111111}

:::MLPv0.5.0 ssd 1541710950.093080759 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 752, "value": 0.13368888888888888}

:::MLPv0.5.0 ssd 1541710950.187304258 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 753, "value": 0.13386666666666666}

:::MLPv0.5.0 ssd 1541710950.281833410 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 754, "value": 0.13404444444444444}

:::MLPv0.5.0 ssd 1541710950.376049995 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 755, "value": 0.13422222222222221}

:::MLPv0.5.0 ssd 1541710950.470468283 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 756, "value": 0.1344}

:::MLPv0.5.0 ssd 1541710950.565284491 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 757, "value": 0.13457777777777777}

:::MLPv0.5.0 ssd 1541710950.662441254 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 758, "value": 0.13475555555555557}

:::MLPv0.5.0 ssd 1541710950.757306099 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 759, "value": 0.13493333333333335}

:::MLPv0.5.0 ssd 1541710950.852728605 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 760, "value": 0.13511111111111113}
Iteration:    760, Loss function: 5.027, Average Loss: 3.903, avg. samples / sec: 21607.11
Iteration:    760, Loss function: 5.553, Average Loss: 3.899, avg. samples / sec: 21607.60
Iteration:    760, Loss function: 5.139, Average Loss: 3.906, avg. samples / sec: 21617.17
Iteration:    760, Loss function: 5.586, Average Loss: 3.916, avg. samples / sec: 21608.48
Iteration:    760, Loss function: 5.320, Average Loss: 3.906, avg. samples / sec: 21600.88
Iteration:    760, Loss function: 5.099, Average Loss: 3.901, avg. samples / sec: 21596.57
Iteration:    760, Loss function: 4.994, Average Loss: 3.898, avg. samples / sec: 21603.54
Iteration:    760, Loss function: 5.261, Average Loss: 3.903, avg. samples / sec: 21580.96

:::MLPv0.5.0 ssd 1541710950.947885513 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 761, "value": 0.1352888888888889}

:::MLPv0.5.0 ssd 1541710951.042012691 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 762, "value": 0.13546666666666668}

:::MLPv0.5.0 ssd 1541710951.136350870 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 763, "value": 0.13564444444444446}

:::MLPv0.5.0 ssd 1541710951.231008768 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 764, "value": 0.13582222222222223}

:::MLPv0.5.0 ssd 1541710951.327692986 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 765, "value": 0.136}

:::MLPv0.5.0 ssd 1541710951.422004938 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 766, "value": 0.1361777777777778}

:::MLPv0.5.0 ssd 1541710951.517492294 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 767, "value": 0.13635555555555556}

:::MLPv0.5.0 ssd 1541710951.613072157 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 768, "value": 0.13653333333333334}

:::MLPv0.5.0 ssd 1541710951.709057093 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 769, "value": 0.13671111111111112}

:::MLPv0.5.0 ssd 1541710951.803044796 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 770, "value": 0.1368888888888889}

:::MLPv0.5.0 ssd 1541710951.897191286 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 771, "value": 0.13706666666666667}

:::MLPv0.5.0 ssd 1541710951.991523743 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 772, "value": 0.13724444444444445}

:::MLPv0.5.0 ssd 1541710952.085745573 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 773, "value": 0.13742222222222222}

:::MLPv0.5.0 ssd 1541710952.179765224 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 774, "value": 0.1376}

:::MLPv0.5.0 ssd 1541710952.273909569 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 775, "value": 0.13777777777777778}

:::MLPv0.5.0 ssd 1541710952.369055033 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 776, "value": 0.13795555555555555}

:::MLPv0.5.0 ssd 1541710952.462766647 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 777, "value": 0.13813333333333333}

:::MLPv0.5.0 ssd 1541710952.558135271 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 778, "value": 0.1383111111111111}

:::MLPv0.5.0 ssd 1541710952.652442455 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 779, "value": 0.13848888888888888}

:::MLPv0.5.0 ssd 1541710952.746506929 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 780, "value": 0.13866666666666666}
Iteration:    780, Loss function: 5.381, Average Loss: 3.928, avg. samples / sec: 21661.32
Iteration:    780, Loss function: 5.050, Average Loss: 3.927, avg. samples / sec: 21629.17
Iteration:    780, Loss function: 5.250, Average Loss: 3.933, avg. samples / sec: 21630.84
Iteration:    780, Loss function: 5.372, Average Loss: 3.930, avg. samples / sec: 21627.46
Iteration:    780, Loss function: 5.286, Average Loss: 3.926, avg. samples / sec: 21630.94
Iteration:    780, Loss function: 5.586, Average Loss: 3.925, avg. samples / sec: 21632.34
Iteration:    780, Loss function: 5.169, Average Loss: 3.941, avg. samples / sec: 21622.62
Iteration:    780, Loss function: 5.348, Average Loss: 3.929, avg. samples / sec: 21617.17

:::MLPv0.5.0 ssd 1541710952.840719700 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 781, "value": 0.13884444444444444}

:::MLPv0.5.0 ssd 1541710952.936716318 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 782, "value": 0.1390222222222222}

:::MLPv0.5.0 ssd 1541710953.031195641 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 783, "value": 0.1392}

:::MLPv0.5.0 ssd 1541710953.125333786 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 784, "value": 0.13937777777777777}

:::MLPv0.5.0 ssd 1541710953.220174789 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 785, "value": 0.13955555555555554}

:::MLPv0.5.0 ssd 1541710953.314107180 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 786, "value": 0.13973333333333332}

:::MLPv0.5.0 ssd 1541710953.408413172 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 787, "value": 0.13991111111111112}

:::MLPv0.5.0 ssd 1541710953.503240585 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 788, "value": 0.1400888888888889}

:::MLPv0.5.0 ssd 1541710953.597121239 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 789, "value": 0.14026666666666668}

:::MLPv0.5.0 ssd 1541710953.691227674 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 790, "value": 0.14044444444444446}

:::MLPv0.5.0 ssd 1541710953.787641525 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 791, "value": 0.14062222222222223}

:::MLPv0.5.0 ssd 1541710953.883754015 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 792, "value": 0.1408}

:::MLPv0.5.0 ssd 1541710953.978205442 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 793, "value": 0.14097777777777779}

:::MLPv0.5.0 ssd 1541710954.073398113 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 794, "value": 0.14115555555555556}

:::MLPv0.5.0 ssd 1541710954.168462276 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 795, "value": 0.14133333333333334}

:::MLPv0.5.0 ssd 1541710954.263262987 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 796, "value": 0.14151111111111112}

:::MLPv0.5.0 ssd 1541710954.357734919 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 797, "value": 0.1416888888888889}

:::MLPv0.5.0 ssd 1541710954.451351404 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 798, "value": 0.14186666666666667}

:::MLPv0.5.0 ssd 1541710954.546247959 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 799, "value": 0.14204444444444445}

:::MLPv0.5.0 ssd 1541710954.640495300 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 800, "value": 0.14222222222222222}
Iteration:    800, Loss function: 4.964, Average Loss: 3.957, avg. samples / sec: 21625.03
Iteration:    800, Loss function: 5.100, Average Loss: 3.959, avg. samples / sec: 21628.09
Iteration:    800, Loss function: 5.282, Average Loss: 3.957, avg. samples / sec: 21633.15
Iteration:    800, Loss function: 5.000, Average Loss: 3.961, avg. samples / sec: 21626.26
Iteration:    800, Loss function: 5.065, Average Loss: 3.952, avg. samples / sec: 21627.49
Iteration:    800, Loss function: 5.620, Average Loss: 3.953, avg. samples / sec: 21624.12
Iteration:    800, Loss function: 5.427, Average Loss: 3.969, avg. samples / sec: 21626.55
Iteration:    800, Loss function: 5.673, Average Loss: 3.958, avg. samples / sec: 21612.05

:::MLPv0.5.0 ssd 1541710954.736129522 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 801, "value": 0.1424}

:::MLPv0.5.0 ssd 1541710954.830244541 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 802, "value": 0.14257777777777778}

:::MLPv0.5.0 ssd 1541710954.924849033 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 803, "value": 0.14275555555555555}

:::MLPv0.5.0 ssd 1541710955.018777847 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 804, "value": 0.14293333333333333}

:::MLPv0.5.0 ssd 1541710955.113070488 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 805, "value": 0.1431111111111111}

:::MLPv0.5.0 ssd 1541710955.208313704 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 806, "value": 0.14328888888888888}

:::MLPv0.5.0 ssd 1541710955.303974628 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 807, "value": 0.14346666666666666}

:::MLPv0.5.0 ssd 1541710955.399951935 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 808, "value": 0.14364444444444444}

:::MLPv0.5.0 ssd 1541710955.490258217 (train.py:553) train_epoch: 14

:::MLPv0.5.0 ssd 1541710955.495433569 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 809, "value": 0.14382222222222224}

:::MLPv0.5.0 ssd 1541710955.594185591 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 810, "value": 0.14400000000000002}

:::MLPv0.5.0 ssd 1541710955.695063353 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 811, "value": 0.1441777777777778}

:::MLPv0.5.0 ssd 1541710955.789318323 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 812, "value": 0.14435555555555557}

:::MLPv0.5.0 ssd 1541710955.883832216 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 813, "value": 0.14453333333333335}

:::MLPv0.5.0 ssd 1541710955.978441477 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 814, "value": 0.14471111111111112}

:::MLPv0.5.0 ssd 1541710956.072162867 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 815, "value": 0.1448888888888889}

:::MLPv0.5.0 ssd 1541710956.166894436 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 816, "value": 0.14506666666666668}

:::MLPv0.5.0 ssd 1541710956.261470318 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 817, "value": 0.14524444444444445}

:::MLPv0.5.0 ssd 1541710956.356370687 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 818, "value": 0.14542222222222223}

:::MLPv0.5.0 ssd 1541710956.450860500 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 819, "value": 0.1456}

:::MLPv0.5.0 ssd 1541710956.545040607 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 820, "value": 0.14577777777777778}
Iteration:    820, Loss function: 4.779, Average Loss: 3.975, avg. samples / sec: 21510.72
Iteration:    820, Loss function: 4.991, Average Loss: 3.979, avg. samples / sec: 21512.27
Iteration:    820, Loss function: 4.875, Average Loss: 3.983, avg. samples / sec: 21506.53
Iteration:    820, Loss function: 5.309, Average Loss: 3.996, avg. samples / sec: 21509.19
Iteration:    820, Loss function: 5.067, Average Loss: 3.986, avg. samples / sec: 21501.89
Iteration:    820, Loss function: 5.178, Average Loss: 3.986, avg. samples / sec: 21502.02
Iteration:    820, Loss function: 5.160, Average Loss: 3.983, avg. samples / sec: 21511.12
Iteration:    820, Loss function: 5.131, Average Loss: 3.983, avg. samples / sec: 21496.60

:::MLPv0.5.0 ssd 1541710956.640228271 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 821, "value": 0.14595555555555556}

:::MLPv0.5.0 ssd 1541710956.734215975 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 822, "value": 0.14613333333333334}

:::MLPv0.5.0 ssd 1541710956.828814745 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 823, "value": 0.14631111111111111}

:::MLPv0.5.0 ssd 1541710956.923555374 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 824, "value": 0.1464888888888889}

:::MLPv0.5.0 ssd 1541710957.019661427 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 825, "value": 0.14666666666666667}

:::MLPv0.5.0 ssd 1541710957.113673449 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 826, "value": 0.14684444444444444}

:::MLPv0.5.0 ssd 1541710957.208541870 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 827, "value": 0.14702222222222222}

:::MLPv0.5.0 ssd 1541710957.302612305 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 828, "value": 0.1472}

:::MLPv0.5.0 ssd 1541710957.397538185 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 829, "value": 0.14737777777777777}

:::MLPv0.5.0 ssd 1541710957.491670609 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 830, "value": 0.14755555555555555}

:::MLPv0.5.0 ssd 1541710957.585837603 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 831, "value": 0.14773333333333333}

:::MLPv0.5.0 ssd 1541710957.681287527 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 832, "value": 0.1479111111111111}

:::MLPv0.5.0 ssd 1541710957.776419401 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 833, "value": 0.14808888888888888}

:::MLPv0.5.0 ssd 1541710957.872410059 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 834, "value": 0.14826666666666666}

:::MLPv0.5.0 ssd 1541710957.966683149 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 835, "value": 0.14844444444444443}

:::MLPv0.5.0 ssd 1541710958.062255144 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 836, "value": 0.1486222222222222}

:::MLPv0.5.0 ssd 1541710958.156866789 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 837, "value": 0.14880000000000002}

:::MLPv0.5.0 ssd 1541710958.251418352 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 838, "value": 0.1489777777777778}

:::MLPv0.5.0 ssd 1541710958.346873999 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 839, "value": 0.14915555555555557}

:::MLPv0.5.0 ssd 1541710958.444988966 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 840, "value": 0.14933333333333335}
Iteration:    840, Loss function: 5.134, Average Loss: 4.004, avg. samples / sec: 21567.83
Iteration:    840, Loss function: 5.319, Average Loss: 4.011, avg. samples / sec: 21576.48
Iteration:    840, Loss function: 5.142, Average Loss: 4.008, avg. samples / sec: 21573.15
Iteration:    840, Loss function: 5.526, Average Loss: 4.011, avg. samples / sec: 21568.97
Iteration:    840, Loss function: 5.179, Average Loss: 4.001, avg. samples / sec: 21560.01
Iteration:    840, Loss function: 5.103, Average Loss: 4.006, avg. samples / sec: 21563.93
Iteration:    840, Loss function: 5.044, Average Loss: 4.020, avg. samples / sec: 21561.36
Iteration:    840, Loss function: 5.359, Average Loss: 4.012, avg. samples / sec: 21551.51

:::MLPv0.5.0 ssd 1541710958.538883686 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 841, "value": 0.14951111111111112}

:::MLPv0.5.0 ssd 1541710958.633287430 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 842, "value": 0.1496888888888889}

:::MLPv0.5.0 ssd 1541710958.726700544 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 843, "value": 0.14986666666666668}

:::MLPv0.5.0 ssd 1541710958.823881149 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 844, "value": 0.15004444444444445}

:::MLPv0.5.0 ssd 1541710958.918798447 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 845, "value": 0.15022222222222223}

:::MLPv0.5.0 ssd 1541710959.014225721 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 846, "value": 0.1504}

:::MLPv0.5.0 ssd 1541710959.109407425 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 847, "value": 0.15057777777777778}

:::MLPv0.5.0 ssd 1541710959.206209421 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 848, "value": 0.15075555555555556}

:::MLPv0.5.0 ssd 1541710959.300420046 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 849, "value": 0.15093333333333334}

:::MLPv0.5.0 ssd 1541710959.395478487 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 850, "value": 0.1511111111111111}

:::MLPv0.5.0 ssd 1541710959.492288113 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 851, "value": 0.1512888888888889}

:::MLPv0.5.0 ssd 1541710959.586128950 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 852, "value": 0.15146666666666667}

:::MLPv0.5.0 ssd 1541710959.680782795 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 853, "value": 0.15164444444444444}

:::MLPv0.5.0 ssd 1541710959.774684668 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 854, "value": 0.15182222222222222}

:::MLPv0.5.0 ssd 1541710959.868919373 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 855, "value": 0.152}

:::MLPv0.5.0 ssd 1541710959.962785006 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 856, "value": 0.15217777777777777}

:::MLPv0.5.0 ssd 1541710960.057160616 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 857, "value": 0.15235555555555555}

:::MLPv0.5.0 ssd 1541710960.152251244 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 858, "value": 0.15253333333333333}

:::MLPv0.5.0 ssd 1541710960.246323824 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 859, "value": 0.1527111111111111}

:::MLPv0.5.0 ssd 1541710960.340547323 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 860, "value": 0.15288888888888888}
Iteration:    860, Loss function: 5.281, Average Loss: 4.031, avg. samples / sec: 21601.33
Iteration:    860, Loss function: 5.562, Average Loss: 4.031, avg. samples / sec: 21605.15
Iteration:    860, Loss function: 4.691, Average Loss: 4.025, avg. samples / sec: 21599.44
Iteration:    860, Loss function: 5.219, Average Loss: 4.023, avg. samples / sec: 21604.78
Iteration:    860, Loss function: 5.421, Average Loss: 4.028, avg. samples / sec: 21604.09
Iteration:    860, Loss function: 5.517, Average Loss: 4.042, avg. samples / sec: 21608.59
Iteration:    860, Loss function: 5.513, Average Loss: 4.032, avg. samples / sec: 21593.94
Iteration:    860, Loss function: 5.321, Average Loss: 4.035, avg. samples / sec: 21612.11

:::MLPv0.5.0 ssd 1541710960.434532404 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 861, "value": 0.15306666666666666}

:::MLPv0.5.0 ssd 1541710960.528981447 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 862, "value": 0.15324444444444446}

:::MLPv0.5.0 ssd 1541710960.622998953 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 863, "value": 0.15342222222222224}

:::MLPv0.5.0 ssd 1541710960.717612028 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 864, "value": 0.15360000000000001}

:::MLPv0.5.0 ssd 1541710960.813010216 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 865, "value": 0.1537777777777778}

:::MLPv0.5.0 ssd 1541710960.906716824 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 866, "value": 0.15395555555555557}

:::MLPv0.5.0 ssd 1541710960.997058630 (train.py:553) train_epoch: 15

:::MLPv0.5.0 ssd 1541710961.002465725 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 867, "value": 0.15413333333333334}

:::MLPv0.5.0 ssd 1541710961.096637726 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 868, "value": 0.15431111111111112}

:::MLPv0.5.0 ssd 1541710961.195945263 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 869, "value": 0.1544888888888889}

:::MLPv0.5.0 ssd 1541710961.291425467 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 870, "value": 0.15466666666666667}

:::MLPv0.5.0 ssd 1541710961.385356188 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 871, "value": 0.15484444444444445}

:::MLPv0.5.0 ssd 1541710961.482056141 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 872, "value": 0.15502222222222223}

:::MLPv0.5.0 ssd 1541710961.577441931 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 873, "value": 0.1552}

:::MLPv0.5.0 ssd 1541710961.671479702 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 874, "value": 0.15537777777777778}

:::MLPv0.5.0 ssd 1541710961.766008615 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 875, "value": 0.15555555555555556}

:::MLPv0.5.0 ssd 1541710961.861049175 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 876, "value": 0.15573333333333333}

:::MLPv0.5.0 ssd 1541710961.956522465 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 877, "value": 0.1559111111111111}

:::MLPv0.5.0 ssd 1541710962.053694010 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 878, "value": 0.1560888888888889}

:::MLPv0.5.0 ssd 1541710962.148426294 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 879, "value": 0.15626666666666666}

:::MLPv0.5.0 ssd 1541710962.243165016 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 880, "value": 0.15644444444444444}
Iteration:    880, Loss function: 5.311, Average Loss: 4.052, avg. samples / sec: 21538.04
Iteration:    880, Loss function: 5.056, Average Loss: 4.051, avg. samples / sec: 21535.13
Iteration:    880, Loss function: 4.191, Average Loss: 4.049, avg. samples / sec: 21536.48
Iteration:    880, Loss function: 4.663, Average Loss: 4.054, avg. samples / sec: 21539.71
Iteration:    880, Loss function: 5.424, Average Loss: 4.052, avg. samples / sec: 21529.75
Iteration:    880, Loss function: 5.141, Average Loss: 4.059, avg. samples / sec: 21538.20
Iteration:    880, Loss function: 4.912, Average Loss: 4.054, avg. samples / sec: 21527.41
Iteration:    880, Loss function: 5.432, Average Loss: 4.066, avg. samples / sec: 21527.86

:::MLPv0.5.0 ssd 1541710962.337557316 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 881, "value": 0.15662222222222222}

:::MLPv0.5.0 ssd 1541710962.434348583 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 882, "value": 0.1568}

:::MLPv0.5.0 ssd 1541710962.528069973 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 883, "value": 0.15697777777777777}

:::MLPv0.5.0 ssd 1541710962.621659994 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 884, "value": 0.15715555555555555}

:::MLPv0.5.0 ssd 1541710962.715365648 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 885, "value": 0.15733333333333333}

:::MLPv0.5.0 ssd 1541710962.809428215 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 886, "value": 0.1575111111111111}

:::MLPv0.5.0 ssd 1541710962.903532267 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 887, "value": 0.15768888888888888}

:::MLPv0.5.0 ssd 1541710962.997309923 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 888, "value": 0.15786666666666668}

:::MLPv0.5.0 ssd 1541710963.095030069 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 889, "value": 0.15804444444444446}

:::MLPv0.5.0 ssd 1541710963.190516710 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 890, "value": 0.15822222222222224}

:::MLPv0.5.0 ssd 1541710963.284358263 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 891, "value": 0.1584}

:::MLPv0.5.0 ssd 1541710963.378216028 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 892, "value": 0.1585777777777778}

:::MLPv0.5.0 ssd 1541710963.472259045 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 893, "value": 0.15875555555555557}

:::MLPv0.5.0 ssd 1541710963.566316605 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 894, "value": 0.15893333333333334}

:::MLPv0.5.0 ssd 1541710963.660732985 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 895, "value": 0.15911111111111112}

:::MLPv0.5.0 ssd 1541710963.755925655 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 896, "value": 0.1592888888888889}

:::MLPv0.5.0 ssd 1541710963.849785566 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 897, "value": 0.15946666666666667}

:::MLPv0.5.0 ssd 1541710963.948133230 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 898, "value": 0.15964444444444445}

:::MLPv0.5.0 ssd 1541710964.041999578 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 899, "value": 0.15982222222222223}
Iteration:    900, Loss function: 5.407, Average Loss: 4.074, avg. samples / sec: 21639.60
Iteration:    900, Loss function: 5.586, Average Loss: 4.080, avg. samples / sec: 21645.06
Iteration:    900, Loss function: 5.148, Average Loss: 4.087, avg. samples / sec: 21646.76
Iteration:    900, Loss function: 5.477, Average Loss: 4.075, avg. samples / sec: 21638.37
Iteration:    900, Loss function: 5.068, Average Loss: 4.083, avg. samples / sec: 21641.80
Iteration:    900, Loss function: 4.973, Average Loss: 4.073, avg. samples / sec: 21635.19
Iteration:    900, Loss function: 5.011, Average Loss: 4.076, avg. samples / sec: 21638.98
Iteration:    900, Loss function: 4.954, Average Loss: 4.074, avg. samples / sec: 21631.09
Iteration:    920, Loss function: 5.038, Average Loss: 4.100, avg. samples / sec: 21884.39
Iteration:    920, Loss function: 4.886, Average Loss: 4.093, avg. samples / sec: 21886.46
Iteration:    920, Loss function: 4.525, Average Loss: 4.092, avg. samples / sec: 21878.66
Iteration:    920, Loss function: 5.122, Average Loss: 4.102, avg. samples / sec: 21882.13
Iteration:    920, Loss function: 5.653, Average Loss: 4.096, avg. samples / sec: 21887.96
Iteration:    920, Loss function: 4.760, Average Loss: 4.104, avg. samples / sec: 21881.36
Iteration:    920, Loss function: 5.114, Average Loss: 4.093, avg. samples / sec: 21879.34
Iteration:    920, Loss function: 5.076, Average Loss: 4.094, avg. samples / sec: 21867.64

:::MLPv0.5.0 ssd 1541710966.479302168 (train.py:553) train_epoch: 16
Iteration:    940, Loss function: 5.134, Average Loss: 4.109, avg. samples / sec: 21775.75
Iteration:    940, Loss function: 4.370, Average Loss: 4.120, avg. samples / sec: 21778.53
Iteration:    940, Loss function: 4.697, Average Loss: 4.111, avg. samples / sec: 21793.83
Iteration:    940, Loss function: 4.744, Average Loss: 4.111, avg. samples / sec: 21773.49
Iteration:    940, Loss function: 4.925, Average Loss: 4.116, avg. samples / sec: 21769.19
Iteration:    940, Loss function: 4.439, Average Loss: 4.119, avg. samples / sec: 21772.21
Iteration:    940, Loss function: 5.235, Average Loss: 4.110, avg. samples / sec: 21774.08
Iteration:    940, Loss function: 4.650, Average Loss: 4.113, avg. samples / sec: 21770.89
Iteration:    960, Loss function: 4.391, Average Loss: 4.134, avg. samples / sec: 21911.81
Iteration:    960, Loss function: 5.090, Average Loss: 4.133, avg. samples / sec: 21913.42
Iteration:    960, Loss function: 4.717, Average Loss: 4.130, avg. samples / sec: 21915.82
Iteration:    960, Loss function: 4.810, Average Loss: 4.124, avg. samples / sec: 21909.92
Iteration:    960, Loss function: 5.041, Average Loss: 4.125, avg. samples / sec: 21905.09
Iteration:    960, Loss function: 4.874, Average Loss: 4.128, avg. samples / sec: 21904.71
Iteration:    960, Loss function: 4.836, Average Loss: 4.133, avg. samples / sec: 21907.45
Iteration:    960, Loss function: 5.030, Average Loss: 4.124, avg. samples / sec: 21906.87
Iteration:    980, Loss function: 4.846, Average Loss: 4.144, avg. samples / sec: 21902.14
Iteration:    980, Loss function: 4.890, Average Loss: 4.140, avg. samples / sec: 21903.30
Iteration:    980, Loss function: 4.523, Average Loss: 4.148, avg. samples / sec: 21894.75
Iteration:    980, Loss function: 4.523, Average Loss: 4.137, avg. samples / sec: 21896.92
Iteration:    980, Loss function: 4.755, Average Loss: 4.148, avg. samples / sec: 21901.76
Iteration:    980, Loss function: 4.957, Average Loss: 4.150, avg. samples / sec: 21894.16
Iteration:    980, Loss function: 4.825, Average Loss: 4.142, avg. samples / sec: 21897.51
Iteration:    980, Loss function: 4.691, Average Loss: 4.137, avg. samples / sec: 21899.84

:::MLPv0.5.0 ssd 1541710971.813430071 (train.py:553) train_epoch: 17
Iteration:   1000, Loss function: 5.084, Average Loss: 4.162, avg. samples / sec: 21918.74
Iteration:   1000, Loss function: 5.483, Average Loss: 4.155, avg. samples / sec: 21916.78
Iteration:   1000, Loss function: 4.666, Average Loss: 4.153, avg. samples / sec: 21927.11
Iteration:   1000, Loss function: 4.759, Average Loss: 4.154, avg. samples / sec: 21921.18
Iteration:   1000, Loss function: 4.854, Average Loss: 4.167, avg. samples / sec: 21914.90
Iteration:   1000, Loss function: 5.074, Average Loss: 4.167, avg. samples / sec: 21915.86
Iteration:   1000, Loss function: 4.362, Average Loss: 4.160, avg. samples / sec: 21915.72
Iteration:   1000, Loss function: 5.292, Average Loss: 4.163, avg. samples / sec: 21911.93
Iteration:   1020, Loss function: 5.385, Average Loss: 4.180, avg. samples / sec: 21841.85
Iteration:   1020, Loss function: 5.068, Average Loss: 4.172, avg. samples / sec: 21842.79
Iteration:   1020, Loss function: 4.498, Average Loss: 4.172, avg. samples / sec: 21834.83
Iteration:   1020, Loss function: 5.026, Average Loss: 4.182, avg. samples / sec: 21840.46
Iteration:   1020, Loss function: 5.378, Average Loss: 4.170, avg. samples / sec: 21832.88
Iteration:   1020, Loss function: 4.732, Average Loss: 4.177, avg. samples / sec: 21842.67
Iteration:   1020, Loss function: 5.311, Average Loss: 4.169, avg. samples / sec: 21830.22
Iteration:   1020, Loss function: 4.363, Average Loss: 4.175, avg. samples / sec: 21825.53

:::MLPv0.5.0 ssd 1541710977.247166395 (train.py:553) train_epoch: 18
Iteration:   1040, Loss function: 4.986, Average Loss: 4.186, avg. samples / sec: 21821.56
Iteration:   1040, Loss function: 5.086, Average Loss: 4.190, avg. samples / sec: 21823.48
Iteration:   1040, Loss function: 4.305, Average Loss: 4.186, avg. samples / sec: 21815.01
Iteration:   1040, Loss function: 4.814, Average Loss: 4.184, avg. samples / sec: 21819.86
Iteration:   1040, Loss function: 5.005, Average Loss: 4.190, avg. samples / sec: 21818.00
Iteration:   1040, Loss function: 4.518, Average Loss: 4.200, avg. samples / sec: 21813.11
Iteration:   1040, Loss function: 4.482, Average Loss: 4.185, avg. samples / sec: 21810.74
Iteration:   1040, Loss function: 5.050, Average Loss: 4.195, avg. samples / sec: 21806.72
Iteration:   1060, Loss function: 4.764, Average Loss: 4.210, avg. samples / sec: 21791.09
Iteration:   1060, Loss function: 4.487, Average Loss: 4.198, avg. samples / sec: 21781.44
Iteration:   1060, Loss function: 4.951, Average Loss: 4.205, avg. samples / sec: 21788.16
Iteration:   1060, Loss function: 4.723, Average Loss: 4.196, avg. samples / sec: 21778.40
Iteration:   1060, Loss function: 5.125, Average Loss: 4.196, avg. samples / sec: 21779.71
Iteration:   1060, Loss function: 4.622, Average Loss: 4.193, avg. samples / sec: 21779.71
Iteration:   1060, Loss function: 4.625, Average Loss: 4.196, avg. samples / sec: 21781.08
Iteration:   1060, Loss function: 5.224, Average Loss: 4.200, avg. samples / sec: 21772.87
Iteration:   1080, Loss function: 4.266, Average Loss: 4.215, avg. samples / sec: 21797.76
Iteration:   1080, Loss function: 4.715, Average Loss: 4.208, avg. samples / sec: 21797.62
Iteration:   1080, Loss function: 4.059, Average Loss: 4.204, avg. samples / sec: 21793.53
Iteration:   1080, Loss function: 4.506, Average Loss: 4.204, avg. samples / sec: 21796.62
Iteration:   1080, Loss function: 4.383, Average Loss: 4.213, avg. samples / sec: 21802.23
Iteration:   1080, Loss function: 4.962, Average Loss: 4.221, avg. samples / sec: 21785.17
Iteration:   1080, Loss function: 4.642, Average Loss: 4.209, avg. samples / sec: 21790.42
Iteration:   1080, Loss function: 4.844, Average Loss: 4.206, avg. samples / sec: 21785.87

:::MLPv0.5.0 ssd 1541710982.695390463 (train.py:553) train_epoch: 19
Iteration:   1100, Loss function: 5.476, Average Loss: 4.219, avg. samples / sec: 21828.29
Iteration:   1100, Loss function: 4.371, Average Loss: 4.222, avg. samples / sec: 21832.58
Iteration:   1100, Loss function: 4.870, Average Loss: 4.212, avg. samples / sec: 21827.69
Iteration:   1100, Loss function: 5.228, Average Loss: 4.234, avg. samples / sec: 21829.40
Iteration:   1100, Loss function: 4.578, Average Loss: 4.225, avg. samples / sec: 21824.43
Iteration:   1100, Loss function: 4.740, Average Loss: 4.212, avg. samples / sec: 21825.15
Iteration:   1100, Loss function: 5.317, Average Loss: 4.216, avg. samples / sec: 21836.38
Iteration:   1100, Loss function: 5.078, Average Loss: 4.222, avg. samples / sec: 21823.49
Iteration:   1120, Loss function: 4.852, Average Loss: 4.233, avg. samples / sec: 21853.58
Iteration:   1120, Loss function: 4.465, Average Loss: 4.233, avg. samples / sec: 21847.67
Iteration:   1120, Loss function: 4.742, Average Loss: 4.234, avg. samples / sec: 21853.11
Iteration:   1120, Loss function: 4.670, Average Loss: 4.232, avg. samples / sec: 21843.52
Iteration:   1120, Loss function: 4.919, Average Loss: 4.224, avg. samples / sec: 21847.64
Iteration:   1120, Loss function: 4.507, Average Loss: 4.225, avg. samples / sec: 21849.71
Iteration:   1120, Loss function: 4.650, Average Loss: 4.243, avg. samples / sec: 21845.54
Iteration:   1120, Loss function: 4.612, Average Loss: 4.222, avg. samples / sec: 21840.55
Iteration:   1140, Loss function: 4.487, Average Loss: 4.243, avg. samples / sec: 21825.64
Iteration:   1140, Loss function: 4.625, Average Loss: 4.232, avg. samples / sec: 21830.04
Iteration:   1140, Loss function: 5.178, Average Loss: 4.243, avg. samples / sec: 21814.18
Iteration:   1140, Loss function: 4.403, Average Loss: 4.241, avg. samples / sec: 21821.43
Iteration:   1140, Loss function: 4.444, Average Loss: 4.252, avg. samples / sec: 21823.02
Iteration:   1140, Loss function: 5.098, Average Loss: 4.234, avg. samples / sec: 21819.04
Iteration:   1140, Loss function: 4.901, Average Loss: 4.235, avg. samples / sec: 21818.11
Iteration:   1140, Loss function: 4.958, Average Loss: 4.244, avg. samples / sec: 21814.09

:::MLPv0.5.0 ssd 1541710988.136414289 (train.py:553) train_epoch: 20
Iteration:   1160, Loss function: 4.794, Average Loss: 4.249, avg. samples / sec: 21835.60
Iteration:   1160, Loss function: 4.767, Average Loss: 4.249, avg. samples / sec: 21839.09
Iteration:   1160, Loss function: 4.511, Average Loss: 4.239, avg. samples / sec: 21827.19
Iteration:   1160, Loss function: 4.427, Average Loss: 4.250, avg. samples / sec: 21830.49
Iteration:   1160, Loss function: 4.333, Average Loss: 4.249, avg. samples / sec: 21824.98
Iteration:   1160, Loss function: 4.347, Average Loss: 4.243, avg. samples / sec: 21830.97
Iteration:   1160, Loss function: 5.477, Average Loss: 4.241, avg. samples / sec: 21828.16
Iteration:   1160, Loss function: 4.647, Average Loss: 4.258, avg. samples / sec: 21823.06
Iteration:   1180, Loss function: 4.571, Average Loss: 4.256, avg. samples / sec: 21910.52
Iteration:   1180, Loss function: 4.656, Average Loss: 4.257, avg. samples / sec: 21907.81
Iteration:   1180, Loss function: 5.351, Average Loss: 4.247, avg. samples / sec: 21914.16
Iteration:   1180, Loss function: 5.088, Average Loss: 4.259, avg. samples / sec: 21906.25
Iteration:   1180, Loss function: 4.171, Average Loss: 4.251, avg. samples / sec: 21908.78
Iteration:   1180, Loss function: 4.659, Average Loss: 4.244, avg. samples / sec: 21905.19
Iteration:   1180, Loss function: 5.213, Average Loss: 4.264, avg. samples / sec: 21911.06
Iteration:   1180, Loss function: 4.933, Average Loss: 4.255, avg. samples / sec: 21901.63
Iteration:   1200, Loss function: 4.776, Average Loss: 4.263, avg. samples / sec: 21836.49
Iteration:   1200, Loss function: 4.915, Average Loss: 4.255, avg. samples / sec: 21839.07
Iteration:   1200, Loss function: 4.332, Average Loss: 4.272, avg. samples / sec: 21841.12
Iteration:   1200, Loss function: 4.376, Average Loss: 4.265, avg. samples / sec: 21838.39
Iteration:   1200, Loss function: 4.279, Average Loss: 4.257, avg. samples / sec: 21838.65
Iteration:   1200, Loss function: 4.159, Average Loss: 4.251, avg. samples / sec: 21836.58
Iteration:   1200, Loss function: 4.625, Average Loss: 4.263, avg. samples / sec: 21837.70
Iteration:   1200, Loss function: 4.991, Average Loss: 4.267, avg. samples / sec: 21822.84

:::MLPv0.5.0 ssd 1541710993.474910021 (train.py:553) train_epoch: 21
Iteration:   1220, Loss function: 4.442, Average Loss: 4.273, avg. samples / sec: 21888.17
Iteration:   1220, Loss function: 5.036, Average Loss: 4.268, avg. samples / sec: 21871.84
Iteration:   1220, Loss function: 4.858, Average Loss: 4.259, avg. samples / sec: 21876.39
Iteration:   1220, Loss function: 3.903, Average Loss: 4.270, avg. samples / sec: 21877.38
Iteration:   1220, Loss function: 4.940, Average Loss: 4.261, avg. samples / sec: 21876.41
Iteration:   1220, Loss function: 4.631, Average Loss: 4.255, avg. samples / sec: 21874.95
Iteration:   1220, Loss function: 4.556, Average Loss: 4.277, avg. samples / sec: 21873.10
Iteration:   1220, Loss function: 4.006, Average Loss: 4.264, avg. samples / sec: 21876.71
Iteration:   1240, Loss function: 5.141, Average Loss: 4.280, avg. samples / sec: 21932.50
Iteration:   1240, Loss function: 4.819, Average Loss: 4.284, avg. samples / sec: 21933.40
Iteration:   1240, Loss function: 4.558, Average Loss: 4.268, avg. samples / sec: 21930.23
Iteration:   1240, Loss function: 5.855, Average Loss: 4.269, avg. samples / sec: 21931.98
Iteration:   1240, Loss function: 4.770, Average Loss: 4.280, avg. samples / sec: 21925.09
Iteration:   1240, Loss function: 5.555, Average Loss: 4.272, avg. samples / sec: 21925.70
Iteration:   1240, Loss function: 4.801, Average Loss: 4.273, avg. samples / sec: 21930.11
Iteration:   1240, Loss function: 5.112, Average Loss: 4.285, avg. samples / sec: 21924.60
Iteration:   1260, Loss function: 4.170, Average Loss: 4.274, avg. samples / sec: 21811.68
Iteration:   1260, Loss function: 4.291, Average Loss: 4.286, avg. samples / sec: 21810.54
Iteration:   1260, Loss function: 4.901, Average Loss: 4.285, avg. samples / sec: 21811.85
Iteration:   1260, Loss function: 3.851, Average Loss: 4.275, avg. samples / sec: 21801.08
Iteration:   1260, Loss function: 4.461, Average Loss: 4.292, avg. samples / sec: 21811.43
Iteration:   1260, Loss function: 4.178, Average Loss: 4.288, avg. samples / sec: 21795.56
Iteration:   1260, Loss function: 4.139, Average Loss: 4.280, avg. samples / sec: 21805.99
Iteration:   1260, Loss function: 5.081, Average Loss: 4.293, avg. samples / sec: 21791.33

:::MLPv0.5.0 ssd 1541710998.911854267 (train.py:553) train_epoch: 22
Iteration:   1280, Loss function: 4.560, Average Loss: 4.290, avg. samples / sec: 21781.06
Iteration:   1280, Loss function: 4.233, Average Loss: 4.279, avg. samples / sec: 21783.83
Iteration:   1280, Loss function: 4.458, Average Loss: 4.299, avg. samples / sec: 21789.06
Iteration:   1280, Loss function: 4.740, Average Loss: 4.294, avg. samples / sec: 21783.06
Iteration:   1280, Loss function: 4.105, Average Loss: 4.290, avg. samples / sec: 21774.42
Iteration:   1280, Loss function: 4.639, Average Loss: 4.287, avg. samples / sec: 21779.45
Iteration:   1280, Loss function: 4.197, Average Loss: 4.295, avg. samples / sec: 21774.92
Iteration:   1280, Loss function: 4.641, Average Loss: 4.279, avg. samples / sec: 21762.80
Iteration:   1300, Loss function: 5.221, Average Loss: 4.299, avg. samples / sec: 21783.08
Iteration:   1300, Loss function: 4.543, Average Loss: 4.304, avg. samples / sec: 21778.98
Iteration:   1300, Loss function: 4.350, Average Loss: 4.285, avg. samples / sec: 21790.14
Iteration:   1300, Loss function: 4.091, Average Loss: 4.295, avg. samples / sec: 21775.04
Iteration:   1300, Loss function: 4.795, Average Loss: 4.298, avg. samples / sec: 21783.76
Iteration:   1300, Loss function: 4.924, Average Loss: 4.294, avg. samples / sec: 21781.31
Iteration:   1300, Loss function: 4.486, Average Loss: 4.282, avg. samples / sec: 21771.57
Iteration:   1300, Loss function: 4.464, Average Loss: 4.292, avg. samples / sec: 21779.44
Iteration:   1320, Loss function: 4.871, Average Loss: 4.297, avg. samples / sec: 21874.84
Iteration:   1320, Loss function: 4.615, Average Loss: 4.301, avg. samples / sec: 21866.42
Iteration:   1320, Loss function: 4.257, Average Loss: 4.298, avg. samples / sec: 21870.15
Iteration:   1320, Loss function: 4.784, Average Loss: 4.291, avg. samples / sec: 21868.52
Iteration:   1320, Loss function: 4.083, Average Loss: 4.306, avg. samples / sec: 21862.75
Iteration:   1320, Loss function: 4.734, Average Loss: 4.301, avg. samples / sec: 21865.08
Iteration:   1320, Loss function: 4.678, Average Loss: 4.286, avg. samples / sec: 21864.19
Iteration:   1320, Loss function: 4.618, Average Loss: 4.301, avg. samples / sec: 21864.85

:::MLPv0.5.0 ssd 1541711004.355162621 (train.py:553) train_epoch: 23
Iteration:   1340, Loss function: 4.896, Average Loss: 4.301, avg. samples / sec: 21900.21
Iteration:   1340, Loss function: 5.114, Average Loss: 4.294, avg. samples / sec: 21898.63
Iteration:   1340, Loss function: 3.856, Average Loss: 4.301, avg. samples / sec: 21896.34
Iteration:   1340, Loss function: 4.575, Average Loss: 4.303, avg. samples / sec: 21893.80
Iteration:   1340, Loss function: 4.538, Average Loss: 4.302, avg. samples / sec: 21898.62
Iteration:   1340, Loss function: 4.434, Average Loss: 4.304, avg. samples / sec: 21895.88
Iteration:   1340, Loss function: 4.488, Average Loss: 4.307, avg. samples / sec: 21893.35
Iteration:   1340, Loss function: 4.732, Average Loss: 4.289, avg. samples / sec: 21884.23
Iteration:   1360, Loss function: 4.610, Average Loss: 4.308, avg. samples / sec: 21902.27
Iteration:   1360, Loss function: 4.298, Average Loss: 4.305, avg. samples / sec: 21897.32
Iteration:   1360, Loss function: 4.451, Average Loss: 4.306, avg. samples / sec: 21892.03
Iteration:   1360, Loss function: 4.465, Average Loss: 4.306, avg. samples / sec: 21884.95
Iteration:   1360, Loss function: 4.607, Average Loss: 4.291, avg. samples / sec: 21907.30
Iteration:   1360, Loss function: 4.143, Average Loss: 4.305, avg. samples / sec: 21894.00
Iteration:   1360, Loss function: 4.215, Average Loss: 4.302, avg. samples / sec: 21885.89
Iteration:   1360, Loss function: 4.528, Average Loss: 4.298, avg. samples / sec: 21882.42
Iteration:   1380, Loss function: 4.612, Average Loss: 4.310, avg. samples / sec: 21831.33
Iteration:   1380, Loss function: 4.457, Average Loss: 4.309, avg. samples / sec: 21824.95
Iteration:   1380, Loss function: 4.773, Average Loss: 4.297, avg. samples / sec: 21831.18
Iteration:   1380, Loss function: 4.632, Average Loss: 4.312, avg. samples / sec: 21825.29
Iteration:   1380, Loss function: 4.662, Average Loss: 4.306, avg. samples / sec: 21829.35
Iteration:   1380, Loss function: 4.887, Average Loss: 4.309, avg. samples / sec: 21818.62
Iteration:   1380, Loss function: 4.034, Average Loss: 4.303, avg. samples / sec: 21828.68
Iteration:   1380, Loss function: 4.633, Average Loss: 4.305, avg. samples / sec: 21817.17

:::MLPv0.5.0 ssd 1541711009.784355402 (train.py:553) train_epoch: 24
Iteration:   1400, Loss function: 4.306, Average Loss: 4.315, avg. samples / sec: 21855.24
Iteration:   1400, Loss function: 4.539, Average Loss: 4.316, avg. samples / sec: 21854.15
Iteration:   1400, Loss function: 5.066, Average Loss: 4.318, avg. samples / sec: 21856.16
Iteration:   1400, Loss function: 4.282, Average Loss: 4.314, avg. samples / sec: 21857.77
Iteration:   1400, Loss function: 4.583, Average Loss: 4.311, avg. samples / sec: 21864.58
Iteration:   1400, Loss function: 4.208, Average Loss: 4.311, avg. samples / sec: 21853.71
Iteration:   1400, Loss function: 5.465, Average Loss: 4.302, avg. samples / sec: 21844.53
Iteration:   1400, Loss function: 5.106, Average Loss: 4.313, avg. samples / sec: 21835.41
Iteration:   1420, Loss function: 4.164, Average Loss: 4.316, avg. samples / sec: 21859.57
Iteration:   1420, Loss function: 4.535, Average Loss: 4.321, avg. samples / sec: 21860.02
Iteration:   1420, Loss function: 4.349, Average Loss: 4.311, avg. samples / sec: 21864.38
Iteration:   1420, Loss function: 4.827, Average Loss: 4.305, avg. samples / sec: 21871.41
Iteration:   1420, Loss function: 3.816, Average Loss: 4.314, avg. samples / sec: 21862.53
Iteration:   1420, Loss function: 4.150, Average Loss: 4.318, avg. samples / sec: 21858.94
Iteration:   1420, Loss function: 4.353, Average Loss: 4.317, avg. samples / sec: 21879.13
Iteration:   1420, Loss function: 4.428, Average Loss: 4.312, avg. samples / sec: 21860.99
Iteration:   1440, Loss function: 4.042, Average Loss: 4.317, avg. samples / sec: 21838.23
Iteration:   1440, Loss function: 4.149, Average Loss: 4.315, avg. samples / sec: 21831.54
Iteration:   1440, Loss function: 4.428, Average Loss: 4.313, avg. samples / sec: 21834.27
Iteration:   1440, Loss function: 3.905, Average Loss: 4.318, avg. samples / sec: 21833.72
Iteration:   1440, Loss function: 3.949, Average Loss: 4.316, avg. samples / sec: 21834.19
Iteration:   1440, Loss function: 4.082, Average Loss: 4.309, avg. samples / sec: 21830.42
Iteration:   1440, Loss function: 3.616, Average Loss: 4.313, avg. samples / sec: 21833.82
Iteration:   1440, Loss function: 4.220, Average Loss: 4.305, avg. samples / sec: 21826.51

:::MLPv0.5.0 ssd 1541711015.128465891 (train.py:553) train_epoch: 25
Iteration:   1460, Loss function: 4.669, Average Loss: 4.320, avg. samples / sec: 21883.97
Iteration:   1460, Loss function: 4.744, Average Loss: 4.321, avg. samples / sec: 21874.19
Iteration:   1460, Loss function: 4.893, Average Loss: 4.325, avg. samples / sec: 21870.62
Iteration:   1460, Loss function: 4.859, Average Loss: 4.323, avg. samples / sec: 21873.57
Iteration:   1460, Loss function: 4.800, Average Loss: 4.316, avg. samples / sec: 21874.93
Iteration:   1460, Loss function: 4.777, Average Loss: 4.319, avg. samples / sec: 21869.25
Iteration:   1460, Loss function: 4.729, Average Loss: 4.313, avg. samples / sec: 21874.55
Iteration:   1460, Loss function: 5.058, Average Loss: 4.322, avg. samples / sec: 21868.18
Iteration:   1480, Loss function: 4.396, Average Loss: 4.330, avg. samples / sec: 21898.63
Iteration:   1480, Loss function: 4.471, Average Loss: 4.326, avg. samples / sec: 21901.28
Iteration:   1480, Loss function: 4.222, Average Loss: 4.325, avg. samples / sec: 21892.86
Iteration:   1480, Loss function: 3.998, Average Loss: 4.330, avg. samples / sec: 21896.13
Iteration:   1480, Loss function: 4.501, Average Loss: 4.318, avg. samples / sec: 21901.60
Iteration:   1480, Loss function: 4.219, Average Loss: 4.329, avg. samples / sec: 21886.52
Iteration:   1480, Loss function: 4.450, Average Loss: 4.323, avg. samples / sec: 21887.41
Iteration:   1480, Loss function: 4.517, Average Loss: 4.328, avg. samples / sec: 21890.15
Iteration:   1500, Loss function: 4.764, Average Loss: 4.327, avg. samples / sec: 21903.35
Iteration:   1500, Loss function: 3.996, Average Loss: 4.328, avg. samples / sec: 21899.72
Iteration:   1500, Loss function: 4.692, Average Loss: 4.318, avg. samples / sec: 21893.74
Iteration:   1500, Loss function: 4.413, Average Loss: 4.330, avg. samples / sec: 21891.83
Iteration:   1500, Loss function: 4.884, Average Loss: 4.322, avg. samples / sec: 21903.90
Iteration:   1500, Loss function: 3.766, Average Loss: 4.323, avg. samples / sec: 21890.17
Iteration:   1500, Loss function: 4.272, Average Loss: 4.327, avg. samples / sec: 21897.86
Iteration:   1500, Loss function: 4.006, Average Loss: 4.329, avg. samples / sec: 21902.83

:::MLPv0.5.0 ssd 1541711020.560753345 (train.py:553) train_epoch: 26
Iteration:   1520, Loss function: 4.909, Average Loss: 4.331, avg. samples / sec: 21815.66
Iteration:   1520, Loss function: 4.552, Average Loss: 4.329, avg. samples / sec: 21829.51
Iteration:   1520, Loss function: 4.353, Average Loss: 4.330, avg. samples / sec: 21815.67
Iteration:   1520, Loss function: 4.304, Average Loss: 4.325, avg. samples / sec: 21820.63
Iteration:   1520, Loss function: 4.603, Average Loss: 4.319, avg. samples / sec: 21820.28
Iteration:   1520, Loss function: 4.487, Average Loss: 4.332, avg. samples / sec: 21817.78
Iteration:   1520, Loss function: 5.027, Average Loss: 4.325, avg. samples / sec: 21816.65
Iteration:   1520, Loss function: 5.077, Average Loss: 4.332, avg. samples / sec: 21820.41
Iteration:   1540, Loss function: 4.474, Average Loss: 4.334, avg. samples / sec: 21841.54
Iteration:   1540, Loss function: 4.332, Average Loss: 4.337, avg. samples / sec: 21838.92
Iteration:   1540, Loss function: 4.048, Average Loss: 4.328, avg. samples / sec: 21843.64
Iteration:   1540, Loss function: 4.381, Average Loss: 4.327, avg. samples / sec: 21841.89
Iteration:   1540, Loss function: 4.746, Average Loss: 4.339, avg. samples / sec: 21842.59
Iteration:   1540, Loss function: 4.644, Average Loss: 4.337, avg. samples / sec: 21831.82
Iteration:   1540, Loss function: 4.779, Average Loss: 4.330, avg. samples / sec: 21839.85
Iteration:   1540, Loss function: 4.635, Average Loss: 4.340, avg. samples / sec: 21836.37

:::MLPv0.5.0 ssd 1541711026.003439903 (train.py:553) train_epoch: 27
Iteration:   1560, Loss function: 4.521, Average Loss: 4.326, avg. samples / sec: 21769.01
Iteration:   1560, Loss function: 3.623, Average Loss: 4.337, avg. samples / sec: 21771.66
Iteration:   1560, Loss function: 4.542, Average Loss: 4.331, avg. samples / sec: 21768.93
Iteration:   1560, Loss function: 4.186, Average Loss: 4.336, avg. samples / sec: 21757.18
Iteration:   1560, Loss function: 3.931, Average Loss: 4.333, avg. samples / sec: 21754.76
Iteration:   1560, Loss function: 4.179, Average Loss: 4.340, avg. samples / sec: 21769.23
Iteration:   1560, Loss function: 4.570, Average Loss: 4.325, avg. samples / sec: 21758.01
Iteration:   1560, Loss function: 4.268, Average Loss: 4.339, avg. samples / sec: 21753.27
Iteration:   1580, Loss function: 4.716, Average Loss: 4.339, avg. samples / sec: 21847.66
Iteration:   1580, Loss function: 4.739, Average Loss: 4.328, avg. samples / sec: 21849.14
Iteration:   1580, Loss function: 4.288, Average Loss: 4.332, avg. samples / sec: 21843.78
Iteration:   1580, Loss function: 3.987, Average Loss: 4.332, avg. samples / sec: 21837.91
Iteration:   1580, Loss function: 4.307, Average Loss: 4.341, avg. samples / sec: 21849.35
Iteration:   1580, Loss function: 5.051, Average Loss: 4.327, avg. samples / sec: 21822.66
Iteration:   1580, Loss function: 4.343, Average Loss: 4.346, avg. samples / sec: 21830.04
Iteration:   1580, Loss function: 4.193, Average Loss: 4.336, avg. samples / sec: 21820.61
Iteration:   1600, Loss function: 4.441, Average Loss: 4.339, avg. samples / sec: 21926.55
Iteration:   1600, Loss function: 4.115, Average Loss: 4.327, avg. samples / sec: 21927.25
Iteration:   1600, Loss function: 4.051, Average Loss: 4.330, avg. samples / sec: 21930.96
Iteration:   1600, Loss function: 3.975, Average Loss: 4.334, avg. samples / sec: 21943.91
Iteration:   1600, Loss function: 4.623, Average Loss: 4.326, avg. samples / sec: 21939.61
Iteration:   1600, Loss function: 4.320, Average Loss: 4.334, avg. samples / sec: 21925.34
Iteration:   1600, Loss function: 4.274, Average Loss: 4.343, avg. samples / sec: 21935.72
Iteration:   1600, Loss function: 4.398, Average Loss: 4.341, avg. samples / sec: 21920.10

:::MLPv0.5.0 ssd 1541711031.431104183 (train.py:553) train_epoch: 28
Iteration:   1620, Loss function: 4.516, Average Loss: 4.338, avg. samples / sec: 21877.45
Iteration:   1620, Loss function: 4.806, Average Loss: 4.324, avg. samples / sec: 21881.01
Iteration:   1620, Loss function: 4.378, Average Loss: 4.334, avg. samples / sec: 21880.78
Iteration:   1620, Loss function: 4.415, Average Loss: 4.330, avg. samples / sec: 21877.44
Iteration:   1620, Loss function: 5.092, Average Loss: 4.342, avg. samples / sec: 21887.01
Iteration:   1620, Loss function: 4.410, Average Loss: 4.329, avg. samples / sec: 21872.17
Iteration:   1620, Loss function: 4.757, Average Loss: 4.345, avg. samples / sec: 21881.49
Iteration:   1620, Loss function: 4.484, Average Loss: 4.335, avg. samples / sec: 21875.92
Iteration:   1640, Loss function: 3.818, Average Loss: 4.329, avg. samples / sec: 21901.36
Iteration:   1640, Loss function: 4.173, Average Loss: 4.337, avg. samples / sec: 21895.36
Iteration:   1640, Loss function: 4.278, Average Loss: 4.334, avg. samples / sec: 21894.82
Iteration:   1640, Loss function: 3.558, Average Loss: 4.334, avg. samples / sec: 21899.08
Iteration:   1640, Loss function: 4.128, Average Loss: 4.344, avg. samples / sec: 21897.22
Iteration:   1640, Loss function: 3.574, Average Loss: 4.345, avg. samples / sec: 21896.39
Iteration:   1640, Loss function: 4.176, Average Loss: 4.323, avg. samples / sec: 21889.94
Iteration:   1640, Loss function: 4.299, Average Loss: 4.331, avg. samples / sec: 21891.83
Iteration:   1660, Loss function: 4.459, Average Loss: 4.326, avg. samples / sec: 21918.86
Iteration:   1660, Loss function: 4.036, Average Loss: 4.330, avg. samples / sec: 21917.62
Iteration:   1660, Loss function: 4.118, Average Loss: 4.343, avg. samples / sec: 21918.83
Iteration:   1660, Loss function: 4.042, Average Loss: 4.330, avg. samples / sec: 21913.75
Iteration:   1660, Loss function: 4.178, Average Loss: 4.325, avg. samples / sec: 21915.78
Iteration:   1660, Loss function: 4.146, Average Loss: 4.334, avg. samples / sec: 21908.27
Iteration:   1660, Loss function: 4.315, Average Loss: 4.317, avg. samples / sec: 21911.35
Iteration:   1660, Loss function: 4.298, Average Loss: 4.341, avg. samples / sec: 21895.02

:::MLPv0.5.0 ssd 1541711036.770814896 (train.py:553) train_epoch: 29
Iteration:   1680, Loss function: 4.755, Average Loss: 4.340, avg. samples / sec: 21790.68
Iteration:   1680, Loss function: 4.903, Average Loss: 4.331, avg. samples / sec: 21762.10
Iteration:   1680, Loss function: 4.620, Average Loss: 4.341, avg. samples / sec: 21764.80
Iteration:   1680, Loss function: 4.301, Average Loss: 4.325, avg. samples / sec: 21757.62
Iteration:   1680, Loss function: 4.519, Average Loss: 4.334, avg. samples / sec: 21762.15
Iteration:   1680, Loss function: 4.059, Average Loss: 4.318, avg. samples / sec: 21765.17
Iteration:   1680, Loss function: 4.375, Average Loss: 4.323, avg. samples / sec: 21759.79
Iteration:   1680, Loss function: 4.648, Average Loss: 4.328, avg. samples / sec: 21758.27
Iteration:   1700, Loss function: 4.792, Average Loss: 4.340, avg. samples / sec: 21841.27
Iteration:   1700, Loss function: 4.193, Average Loss: 4.331, avg. samples / sec: 21839.34
Iteration:   1700, Loss function: 4.160, Average Loss: 4.333, avg. samples / sec: 21846.62
Iteration:   1700, Loss function: 3.949, Average Loss: 4.332, avg. samples / sec: 21846.76
Iteration:   1700, Loss function: 4.462, Average Loss: 4.317, avg. samples / sec: 21843.43
Iteration:   1700, Loss function: 4.516, Average Loss: 4.325, avg. samples / sec: 21833.65
Iteration:   1700, Loss function: 4.230, Average Loss: 4.322, avg. samples / sec: 21839.17
Iteration:   1700, Loss function: 4.609, Average Loss: 4.340, avg. samples / sec: 21820.49
Iteration:   1720, Loss function: 4.259, Average Loss: 4.333, avg. samples / sec: 21954.38
Iteration:   1720, Loss function: 4.028, Average Loss: 4.337, avg. samples / sec: 21951.70
Iteration:   1720, Loss function: 4.289, Average Loss: 4.332, avg. samples / sec: 21950.93
Iteration:   1720, Loss function: 4.483, Average Loss: 4.338, avg. samples / sec: 21964.13
Iteration:   1720, Loss function: 4.228, Average Loss: 4.324, avg. samples / sec: 21956.48
Iteration:   1720, Loss function: 4.606, Average Loss: 4.332, avg. samples / sec: 21948.08
Iteration:   1720, Loss function: 4.309, Average Loss: 4.320, avg. samples / sec: 21950.69
Iteration:   1720, Loss function: 3.614, Average Loss: 4.314, avg. samples / sec: 21942.69

:::MLPv0.5.0 ssd 1541711042.200816393 (train.py:553) train_epoch: 30
Iteration:   1740, Loss function: 3.965, Average Loss: 4.328, avg. samples / sec: 21850.09
Iteration:   1740, Loss function: 3.401, Average Loss: 4.332, avg. samples / sec: 21859.22
Iteration:   1740, Loss function: 4.259, Average Loss: 4.331, avg. samples / sec: 21852.07
Iteration:   1740, Loss function: 4.232, Average Loss: 4.338, avg. samples / sec: 21849.65
Iteration:   1740, Loss function: 3.517, Average Loss: 4.313, avg. samples / sec: 21860.38
Iteration:   1740, Loss function: 3.937, Average Loss: 4.323, avg. samples / sec: 21849.91
Iteration:   1740, Loss function: 4.459, Average Loss: 4.335, avg. samples / sec: 21845.55
Iteration:   1740, Loss function: 4.014, Average Loss: 4.317, avg. samples / sec: 21851.84
Iteration:   1760, Loss function: 3.766, Average Loss: 4.334, avg. samples / sec: 21826.16
Iteration:   1760, Loss function: 4.194, Average Loss: 4.326, avg. samples / sec: 21820.88
Iteration:   1760, Loss function: 4.356, Average Loss: 4.328, avg. samples / sec: 21819.80
Iteration:   1760, Loss function: 4.471, Average Loss: 4.335, avg. samples / sec: 21822.43
Iteration:   1760, Loss function: 4.227, Average Loss: 4.329, avg. samples / sec: 21815.32
Iteration:   1760, Loss function: 4.207, Average Loss: 4.316, avg. samples / sec: 21827.05
Iteration:   1760, Loss function: 4.230, Average Loss: 4.322, avg. samples / sec: 21820.80
Iteration:   1760, Loss function: 4.099, Average Loss: 4.313, avg. samples / sec: 21813.70
Iteration:   1780, Loss function: 4.115, Average Loss: 4.332, avg. samples / sec: 21898.17
Iteration:   1780, Loss function: 3.970, Average Loss: 4.326, avg. samples / sec: 21898.89
Iteration:   1780, Loss function: 4.729, Average Loss: 4.326, avg. samples / sec: 21899.84
Iteration:   1780, Loss function: 4.462, Average Loss: 4.325, avg. samples / sec: 21893.16
Iteration:   1780, Loss function: 4.209, Average Loss: 4.321, avg. samples / sec: 21897.79
Iteration:   1780, Loss function: 3.824, Average Loss: 4.313, avg. samples / sec: 21896.31
Iteration:   1780, Loss function: 4.124, Average Loss: 4.311, avg. samples / sec: 21902.38
Iteration:   1780, Loss function: 4.051, Average Loss: 4.333, avg. samples / sec: 21893.99

:::MLPv0.5.0 ssd 1541711047.633970499 (train.py:553) train_epoch: 31
Iteration:   1800, Loss function: 4.343, Average Loss: 4.330, avg. samples / sec: 21950.39
Iteration:   1800, Loss function: 4.195, Average Loss: 4.322, avg. samples / sec: 21945.96
Iteration:   1800, Loss function: 4.184, Average Loss: 4.324, avg. samples / sec: 21940.02
Iteration:   1800, Loss function: 4.114, Average Loss: 4.313, avg. samples / sec: 21946.62
Iteration:   1800, Loss function: 4.163, Average Loss: 4.322, avg. samples / sec: 21942.31
Iteration:   1800, Loss function: 4.548, Average Loss: 4.321, avg. samples / sec: 21943.77
Iteration:   1800, Loss function: 4.620, Average Loss: 4.313, avg. samples / sec: 21940.91
Iteration:   1800, Loss function: 4.684, Average Loss: 4.332, avg. samples / sec: 21931.75
Iteration:   1820, Loss function: 4.438, Average Loss: 4.322, avg. samples / sec: 21922.78
Iteration:   1820, Loss function: 3.906, Average Loss: 4.321, avg. samples / sec: 21920.54
Iteration:   1820, Loss function: 4.091, Average Loss: 4.329, avg. samples / sec: 21915.70
Iteration:   1820, Loss function: 4.165, Average Loss: 4.329, avg. samples / sec: 21922.72
Iteration:   1820, Loss function: 4.513, Average Loss: 4.312, avg. samples / sec: 21922.73
Iteration:   1820, Loss function: 4.993, Average Loss: 4.313, avg. samples / sec: 21911.40
Iteration:   1820, Loss function: 4.199, Average Loss: 4.320, avg. samples / sec: 21906.72
Iteration:   1820, Loss function: 5.073, Average Loss: 4.323, avg. samples / sec: 21906.69
Iteration:   1840, Loss function: 4.327, Average Loss: 4.322, avg. samples / sec: 21890.33
Iteration:   1840, Loss function: 4.401, Average Loss: 4.319, avg. samples / sec: 21886.23
Iteration:   1840, Loss function: 4.032, Average Loss: 4.328, avg. samples / sec: 21875.98
Iteration:   1840, Loss function: 3.761, Average Loss: 4.320, avg. samples / sec: 21869.53
Iteration:   1840, Loss function: 4.334, Average Loss: 4.313, avg. samples / sec: 21872.03
Iteration:   1840, Loss function: 4.344, Average Loss: 4.320, avg. samples / sec: 21869.10
Iteration:   1840, Loss function: 4.068, Average Loss: 4.327, avg. samples / sec: 21865.74
Iteration:   1840, Loss function: 3.882, Average Loss: 4.313, avg. samples / sec: 21843.58

:::MLPv0.5.0 ssd 1541711053.053699493 (train.py:553) train_epoch: 32
Iteration:   1860, Loss function: 3.708, Average Loss: 4.316, avg. samples / sec: 21928.64
Iteration:   1860, Loss function: 3.824, Average Loss: 4.317, avg. samples / sec: 21915.01
Iteration:   1860, Loss function: 4.600, Average Loss: 4.309, avg. samples / sec: 21919.15
Iteration:   1860, Loss function: 3.805, Average Loss: 4.325, avg. samples / sec: 21914.02
Iteration:   1860, Loss function: 4.329, Average Loss: 4.317, avg. samples / sec: 21916.72
Iteration:   1860, Loss function: 4.212, Average Loss: 4.312, avg. samples / sec: 21952.93
Iteration:   1860, Loss function: 4.566, Average Loss: 4.325, avg. samples / sec: 21920.51
Iteration:   1860, Loss function: 4.544, Average Loss: 4.316, avg. samples / sec: 21904.81

































































:::MLPv0.5.0 ssd 1541711055.587309599 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711055.588110924 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711055.588834763 (train.py:220) eval_start: 32
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.90 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.90 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.90 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.90 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.90 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.90 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.90 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.90 s
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(284726, 7)
Loading and preparing results...
0/284726
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(284726, 7)
(284726, 7)
Converting ndarray to lists...
0/284726
0/284726
(284726, 7)
0/284726
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(284726, 7)
Converting ndarray to lists...
(284726, 7)
Loading and preparing results...
0/284726
(284726, 7)
0/284726
Converting ndarray to lists...
0/284726
(284726, 7)
0/284726
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(284726, 7)
Loading and preparing results...
Converting ndarray to lists...
(284726, 7)
Converting ndarray to lists...
(284726, 7)
(284726, 7)
(284726, 7)
Converting ndarray to lists...
0/284726
Converting ndarray to lists...
(284726, 7)
(284726, 7)
0/284726
(284726, 7)
(284726, 7)
0/284726
0/284726
(284726, 7)
0/284726
Loading and preparing results...
0/284726
0/284726
Loading and preparing results...
0/284726
0/284726
0/284726
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(284726, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(284726, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(284726, 7)
Loading and preparing results...
Converting ndarray to lists...
(284726, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/284726
0/284726
Loading and preparing results...
(284726, 7)
(284726, 7)
Converting ndarray to lists...
(284726, 7)
Loading and preparing results...
(284726, 7)
0/284726
(284726, 7)
(284726, 7)
Converting ndarray to lists...
0/284726
Converting ndarray to lists...
Converting ndarray to lists...
(284726, 7)
Loading and preparing results...
Loading and preparing results...
0/284726
(284726, 7)
0/284726
(284726, 7)
0/284726
Converting ndarray to lists...
Converting ndarray to lists...
0/284726
0/284726
Converting ndarray to lists...
Converting ndarray to lists...
(284726, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/284726
0/284726
(284726, 7)
Loading and preparing results...
(284726, 7)
0/284726
0/284726
0/284726
Loading and preparing results...
Loading and preparing results...
0/284726
Loading and preparing results...
(284726, 7)
0/284726
(284726, 7)
Loading and preparing results...
Loading and preparing results...
(284726, 7)
(284726, 7)
(284726, 7)
Converting ndarray to lists...
Loading and preparing results...
0/284726
Converting ndarray to lists...
0/284726
0/284726
Converting ndarray to lists...
Converting ndarray to lists...
0/284726
0/284726
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(284726, 7)
Converting ndarray to lists...
Loading and preparing results...
(284726, 7)
Loading and preparing results...
(284726, 7)
Converting ndarray to lists...
(284726, 7)
Loading and preparing results...
Converting ndarray to lists...
(284726, 7)
Converting ndarray to lists...
0/284726
Loading and preparing results...
(284726, 7)
Converting ndarray to lists...
0/284726
Loading and preparing results...
(284726, 7)
Converting ndarray to lists...
0/284726
Loading and preparing results...
0/284726
(284726, 7)
0/284726
Converting ndarray to lists...
Converting ndarray to lists...
(284726, 7)
0/284726
(284726, 7)
Converting ndarray to lists...
0/284726
Converting ndarray to lists...
(284726, 7)
(284726, 7)
Loading and preparing results...
Converting ndarray to lists...
(284726, 7)
0/284726
Loading and preparing results...
(284726, 7)
0/284726
0/284726
Converting ndarray to lists...
(284726, 7)
0/284726
Converting ndarray to lists...
(284726, 7)
0/284726
Loading and preparing results...
(284726, 7)
0/284726
(284726, 7)
0/284726
(284726, 7)
0/284726
0/284726
0/284726
0/284726
Converting ndarray to lists...
0/284726
(284726, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/284726
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(284726, 7)
Converting ndarray to lists...
0/284726
Converting ndarray to lists...
(284726, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(284726, 7)
0/284726
(284726, 7)
0/284726
(284726, 7)
0/284726
0/284726
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
DONE (t=1.82s)
creating index...
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.87s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
DONE (t=3.43s).
Accumulating evaluation results...
DONE (t=3.43s).
Accumulating evaluation results...
DONE (t=3.45s).
Accumulating evaluation results...
DONE (t=3.45s).
Accumulating evaluation results...
DONE (t=3.45s).
Accumulating evaluation results...
DONE (t=3.45s).
Accumulating evaluation results...
DONE (t=3.44s).
Accumulating evaluation results...
DONE (t=3.46s).
Accumulating evaluation results...
DONE (t=1.07s).
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.138
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.138
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.215
DONE (t=1.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.145
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
DONE (t=1.13s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.246
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.363
Current AP: 0.13763 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.215
DONE (t=1.09s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.138
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.138
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.226
DONE (t=1.10s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.246
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.363
Current AP: 0.13763 AP goal: 0.21200
DONE (t=1.10s).
DONE (t=1.12s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.138
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.138
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.138
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.138
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.264
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.246
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.363
Current AP: 0.13763 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.145
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.246
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.363
Current AP: 0.13763 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.145
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.246
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.363
Current AP: 0.13763 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.246
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.363
Current AP: 0.13763 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.246
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.363
Current AP: 0.13763 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.246
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.363
Current AP: 0.13763 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711069.216438293 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711069.217372894 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.13762831743730353}

:::MLPv0.5.0 ssd 1541711069.218119144 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.13762831743730353}

:::MLPv0.5.0 ssd 1541711069.218868494 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711069.219593287 (train.py:338) eval_stop: 32
Iteration:   1880, Loss function: 3.839, Average Loss: 4.307, avg. samples / sec: 2559.67
Iteration:   1880, Loss function: 3.946, Average Loss: 4.312, avg. samples / sec: 2559.52
Iteration:   1880, Loss function: 3.686, Average Loss: 4.307, avg. samples / sec: 2559.58
Iteration:   1880, Loss function: 4.408, Average Loss: 4.320, avg. samples / sec: 2559.66
Iteration:   1880, Loss function: 4.275, Average Loss: 4.313, avg. samples / sec: 2559.67
Iteration:   1880, Loss function: 4.502, Average Loss: 4.313, avg. samples / sec: 2559.58
Iteration:   1880, Loss function: 4.085, Average Loss: 4.320, avg. samples / sec: 2559.49
Iteration:   1880, Loss function: 4.347, Average Loss: 4.315, avg. samples / sec: 2559.45
Iteration:   1900, Loss function: 4.181, Average Loss: 4.303, avg. samples / sec: 22044.72
Iteration:   1900, Loss function: 4.563, Average Loss: 4.311, avg. samples / sec: 22044.78
Iteration:   1900, Loss function: 4.187, Average Loss: 4.319, avg. samples / sec: 22041.76
Iteration:   1900, Loss function: 4.595, Average Loss: 4.312, avg. samples / sec: 22037.49
Iteration:   1900, Loss function: 4.740, Average Loss: 4.318, avg. samples / sec: 22047.73
Iteration:   1900, Loss function: 4.624, Average Loss: 4.306, avg. samples / sec: 22029.63
Iteration:   1900, Loss function: 4.184, Average Loss: 4.313, avg. samples / sec: 22042.69
Iteration:   1900, Loss function: 3.896, Average Loss: 4.311, avg. samples / sec: 22036.26

:::MLPv0.5.0 ssd 1541711072.506347895 (train.py:553) train_epoch: 33
Iteration:   1920, Loss function: 4.116, Average Loss: 4.309, avg. samples / sec: 21960.50
Iteration:   1920, Loss function: 4.088, Average Loss: 4.312, avg. samples / sec: 21962.63
Iteration:   1920, Loss function: 3.939, Average Loss: 4.318, avg. samples / sec: 21963.36
Iteration:   1920, Loss function: 4.044, Average Loss: 4.311, avg. samples / sec: 21966.04
Iteration:   1920, Loss function: 4.201, Average Loss: 4.301, avg. samples / sec: 21956.02
Iteration:   1920, Loss function: 3.904, Average Loss: 4.316, avg. samples / sec: 21959.10
Iteration:   1920, Loss function: 4.155, Average Loss: 4.311, avg. samples / sec: 21961.47
Iteration:   1920, Loss function: 3.795, Average Loss: 4.305, avg. samples / sec: 21958.93
Iteration:   1940, Loss function: 4.423, Average Loss: 4.308, avg. samples / sec: 21986.89
Iteration:   1940, Loss function: 4.494, Average Loss: 4.307, avg. samples / sec: 21992.89
Iteration:   1940, Loss function: 4.239, Average Loss: 4.300, avg. samples / sec: 21984.26
Iteration:   1940, Loss function: 4.933, Average Loss: 4.316, avg. samples / sec: 21980.56
Iteration:   1940, Loss function: 4.311, Average Loss: 4.309, avg. samples / sec: 21979.40
Iteration:   1940, Loss function: 4.181, Average Loss: 4.307, avg. samples / sec: 21980.03
Iteration:   1940, Loss function: 4.790, Average Loss: 4.305, avg. samples / sec: 21987.08
Iteration:   1940, Loss function: 4.290, Average Loss: 4.315, avg. samples / sec: 21978.26
Iteration:   1960, Loss function: 3.581, Average Loss: 4.306, avg. samples / sec: 22018.97
Iteration:   1960, Loss function: 4.428, Average Loss: 4.304, avg. samples / sec: 22019.71
Iteration:   1960, Loss function: 4.687, Average Loss: 4.317, avg. samples / sec: 22023.62
Iteration:   1960, Loss function: 3.351, Average Loss: 4.304, avg. samples / sec: 22024.17
Iteration:   1960, Loss function: 3.936, Average Loss: 4.305, avg. samples / sec: 22021.17
Iteration:   1960, Loss function: 4.172, Average Loss: 4.312, avg. samples / sec: 22025.21
Iteration:   1960, Loss function: 3.801, Average Loss: 4.298, avg. samples / sec: 22016.28
Iteration:   1960, Loss function: 4.209, Average Loss: 4.303, avg. samples / sec: 22014.20

:::MLPv0.5.0 ssd 1541711077.909459829 (train.py:553) train_epoch: 34
Iteration:   1980, Loss function: 4.578, Average Loss: 4.306, avg. samples / sec: 21922.82
Iteration:   1980, Loss function: 3.890, Average Loss: 4.317, avg. samples / sec: 21920.98
Iteration:   1980, Loss function: 4.293, Average Loss: 4.296, avg. samples / sec: 21925.29
Iteration:   1980, Loss function: 4.115, Average Loss: 4.302, avg. samples / sec: 21918.86
Iteration:   1980, Loss function: 4.377, Average Loss: 4.303, avg. samples / sec: 21918.39
Iteration:   1980, Loss function: 4.385, Average Loss: 4.305, avg. samples / sec: 21919.12
Iteration:   1980, Loss function: 4.410, Average Loss: 4.310, avg. samples / sec: 21920.33
Iteration:   1980, Loss function: 4.352, Average Loss: 4.298, avg. samples / sec: 21926.25
Iteration:   2000, Loss function: 4.089, Average Loss: 4.313, avg. samples / sec: 21909.86
Iteration:   2000, Loss function: 3.997, Average Loss: 4.304, avg. samples / sec: 21905.12
Iteration:   2000, Loss function: 3.709, Average Loss: 4.303, avg. samples / sec: 21910.70
Iteration:   2000, Loss function: 3.951, Average Loss: 4.291, avg. samples / sec: 21904.27
Iteration:   2000, Loss function: 4.085, Average Loss: 4.302, avg. samples / sec: 21906.68
Iteration:   2000, Loss function: 3.955, Average Loss: 4.297, avg. samples / sec: 21900.47
Iteration:   2000, Loss function: 3.991, Average Loss: 4.296, avg. samples / sec: 21904.81
Iteration:   2000, Loss function: 3.988, Average Loss: 4.308, avg. samples / sec: 21900.72
Iteration:   2020, Loss function: 4.428, Average Loss: 4.294, avg. samples / sec: 21929.93
Iteration:   2020, Loss function: 4.170, Average Loss: 4.300, avg. samples / sec: 21918.73
Iteration:   2020, Loss function: 4.073, Average Loss: 4.301, avg. samples / sec: 21917.17
Iteration:   2020, Loss function: 3.874, Average Loss: 4.307, avg. samples / sec: 21913.83
Iteration:   2020, Loss function: 4.047, Average Loss: 4.289, avg. samples / sec: 21920.71
Iteration:   2020, Loss function: 4.639, Average Loss: 4.307, avg. samples / sec: 21924.83
Iteration:   2020, Loss function: 4.543, Average Loss: 4.291, avg. samples / sec: 21923.40
Iteration:   2020, Loss function: 4.872, Average Loss: 4.299, avg. samples / sec: 21912.98

:::MLPv0.5.0 ssd 1541711083.330371380 (train.py:553) train_epoch: 35
Iteration:   2040, Loss function: 3.931, Average Loss: 4.298, avg. samples / sec: 21920.02
Iteration:   2040, Loss function: 3.649, Average Loss: 4.292, avg. samples / sec: 21916.28
Iteration:   2040, Loss function: 4.322, Average Loss: 4.296, avg. samples / sec: 21921.12
Iteration:   2040, Loss function: 4.455, Average Loss: 4.287, avg. samples / sec: 21917.77
Iteration:   2040, Loss function: 4.020, Average Loss: 4.290, avg. samples / sec: 21917.12
Iteration:   2040, Loss function: 3.895, Average Loss: 4.305, avg. samples / sec: 21916.90
Iteration:   2040, Loss function: 4.522, Average Loss: 4.307, avg. samples / sec: 21912.64
Iteration:   2040, Loss function: 4.478, Average Loss: 4.299, avg. samples / sec: 21918.57
Iteration:   2060, Loss function: 3.967, Average Loss: 4.296, avg. samples / sec: 21937.18
Iteration:   2060, Loss function: 4.165, Average Loss: 4.289, avg. samples / sec: 21935.23
Iteration:   2060, Loss function: 3.979, Average Loss: 4.305, avg. samples / sec: 21940.09
Iteration:   2060, Loss function: 4.306, Average Loss: 4.284, avg. samples / sec: 21936.87
Iteration:   2060, Loss function: 4.026, Average Loss: 4.294, avg. samples / sec: 21929.43
Iteration:   2060, Loss function: 3.680, Average Loss: 4.285, avg. samples / sec: 21935.27
Iteration:   2060, Loss function: 3.921, Average Loss: 4.303, avg. samples / sec: 21934.89
Iteration:   2060, Loss function: 4.409, Average Loss: 4.298, avg. samples / sec: 21932.71

:::MLPv0.5.0 ssd 1541711088.749734879 (train.py:553) train_epoch: 36
Iteration:   2080, Loss function: 4.547, Average Loss: 4.296, avg. samples / sec: 21878.33
Iteration:   2080, Loss function: 4.348, Average Loss: 4.292, avg. samples / sec: 21867.21
Iteration:   2080, Loss function: 3.929, Average Loss: 4.294, avg. samples / sec: 21852.97
Iteration:   2080, Loss function: 4.442, Average Loss: 4.283, avg. samples / sec: 21860.93
Iteration:   2080, Loss function: 4.359, Average Loss: 4.282, avg. samples / sec: 21862.71
Iteration:   2080, Loss function: 3.813, Average Loss: 4.300, avg. samples / sec: 21859.98
Iteration:   2080, Loss function: 4.667, Average Loss: 4.301, avg. samples / sec: 21851.25
Iteration:   2080, Loss function: 4.535, Average Loss: 4.289, avg. samples / sec: 21844.89
Iteration:   2100, Loss function: 3.948, Average Loss: 4.292, avg. samples / sec: 21869.06
Iteration:   2100, Loss function: 4.647, Average Loss: 4.293, avg. samples / sec: 21874.85
Iteration:   2100, Loss function: 3.906, Average Loss: 4.287, avg. samples / sec: 21887.12
Iteration:   2100, Loss function: 4.063, Average Loss: 4.278, avg. samples / sec: 21874.25
Iteration:   2100, Loss function: 4.552, Average Loss: 4.299, avg. samples / sec: 21884.15
Iteration:   2100, Loss function: 3.967, Average Loss: 4.280, avg. samples / sec: 21874.45
Iteration:   2100, Loss function: 4.357, Average Loss: 4.296, avg. samples / sec: 21876.72
Iteration:   2100, Loss function: 4.156, Average Loss: 4.287, avg. samples / sec: 21856.40
Iteration:   2120, Loss function: 3.390, Average Loss: 4.295, avg. samples / sec: 21917.00
Iteration:   2120, Loss function: 3.509, Average Loss: 4.281, avg. samples / sec: 21912.93
Iteration:   2120, Loss function: 4.159, Average Loss: 4.273, avg. samples / sec: 21910.53
Iteration:   2120, Loss function: 4.363, Average Loss: 4.282, avg. samples / sec: 21924.19
Iteration:   2120, Loss function: 4.449, Average Loss: 4.286, avg. samples / sec: 21906.83
Iteration:   2120, Loss function: 3.928, Average Loss: 4.289, avg. samples / sec: 21904.21
Iteration:   2120, Loss function: 4.152, Average Loss: 4.290, avg. samples / sec: 21907.04
Iteration:   2120, Loss function: 4.106, Average Loss: 4.277, avg. samples / sec: 21905.86

:::MLPv0.5.0 ssd 1541711094.176411390 (train.py:553) train_epoch: 37
Iteration:   2140, Loss function: 4.445, Average Loss: 4.280, avg. samples / sec: 21886.82
Iteration:   2140, Loss function: 4.464, Average Loss: 4.276, avg. samples / sec: 21886.34
Iteration:   2140, Loss function: 3.616, Average Loss: 4.281, avg. samples / sec: 21885.86
Iteration:   2140, Loss function: 4.466, Average Loss: 4.270, avg. samples / sec: 21881.02
Iteration:   2140, Loss function: 3.913, Average Loss: 4.291, avg. samples / sec: 21873.02
Iteration:   2140, Loss function: 4.640, Average Loss: 4.286, avg. samples / sec: 21883.66
Iteration:   2140, Loss function: 3.743, Average Loss: 4.285, avg. samples / sec: 21883.76
Iteration:   2140, Loss function: 4.325, Average Loss: 4.272, avg. samples / sec: 21883.27
Iteration:   2160, Loss function: 4.482, Average Loss: 4.276, avg. samples / sec: 21892.11
Iteration:   2160, Loss function: 4.409, Average Loss: 4.284, avg. samples / sec: 21900.54
Iteration:   2160, Loss function: 4.262, Average Loss: 4.278, avg. samples / sec: 21892.69
Iteration:   2160, Loss function: 4.578, Average Loss: 4.270, avg. samples / sec: 21897.38
Iteration:   2160, Loss function: 4.218, Average Loss: 4.271, avg. samples / sec: 21885.21
Iteration:   2160, Loss function: 4.468, Average Loss: 4.281, avg. samples / sec: 21894.96
Iteration:   2160, Loss function: 4.105, Average Loss: 4.287, avg. samples / sec: 21893.05
Iteration:   2160, Loss function: 4.496, Average Loss: 4.265, avg. samples / sec: 21890.71
Iteration:   2180, Loss function: 4.173, Average Loss: 4.279, avg. samples / sec: 21927.90
Iteration:   2180, Loss function: 4.093, Average Loss: 4.275, avg. samples / sec: 21930.01
Iteration:   2180, Loss function: 4.143, Average Loss: 4.274, avg. samples / sec: 21920.58
Iteration:   2180, Loss function: 4.106, Average Loss: 4.283, avg. samples / sec: 21930.43
Iteration:   2180, Loss function: 4.129, Average Loss: 4.267, avg. samples / sec: 21929.10
Iteration:   2180, Loss function: 3.937, Average Loss: 4.264, avg. samples / sec: 21928.07
Iteration:   2180, Loss function: 4.849, Average Loss: 4.279, avg. samples / sec: 21927.66
Iteration:   2180, Loss function: 4.887, Average Loss: 4.267, avg. samples / sec: 21922.06

:::MLPv0.5.0 ssd 1541711099.503426313 (train.py:553) train_epoch: 38
Iteration:   2200, Loss function: 3.241, Average Loss: 4.281, avg. samples / sec: 21878.21
Iteration:   2200, Loss function: 4.550, Average Loss: 4.263, avg. samples / sec: 21879.83
Iteration:   2200, Loss function: 3.695, Average Loss: 4.264, avg. samples / sec: 21877.53
Iteration:   2200, Loss function: 3.952, Average Loss: 4.277, avg. samples / sec: 21871.04
Iteration:   2200, Loss function: 3.693, Average Loss: 4.274, avg. samples / sec: 21867.03
Iteration:   2200, Loss function: 4.000, Average Loss: 4.275, avg. samples / sec: 21868.79
Iteration:   2200, Loss function: 3.850, Average Loss: 4.266, avg. samples / sec: 21875.10
Iteration:   2200, Loss function: 4.261, Average Loss: 4.276, avg. samples / sec: 21868.69
Iteration:   2220, Loss function: 4.141, Average Loss: 4.271, avg. samples / sec: 21913.21
Iteration:   2220, Loss function: 3.652, Average Loss: 4.259, avg. samples / sec: 21903.45
Iteration:   2220, Loss function: 4.464, Average Loss: 4.258, avg. samples / sec: 21898.64
Iteration:   2220, Loss function: 3.602, Average Loss: 4.275, avg. samples / sec: 21897.43
Iteration:   2220, Loss function: 4.171, Average Loss: 4.273, avg. samples / sec: 21900.41
Iteration:   2220, Loss function: 4.302, Average Loss: 4.273, avg. samples / sec: 21909.21
Iteration:   2220, Loss function: 4.127, Average Loss: 4.271, avg. samples / sec: 21904.21
Iteration:   2220, Loss function: 3.956, Average Loss: 4.264, avg. samples / sec: 21903.19
Iteration:   2240, Loss function: 3.760, Average Loss: 4.267, avg. samples / sec: 21909.83
Iteration:   2240, Loss function: 4.412, Average Loss: 4.269, avg. samples / sec: 21907.12
Iteration:   2240, Loss function: 4.502, Average Loss: 4.257, avg. samples / sec: 21906.08
Iteration:   2240, Loss function: 4.084, Average Loss: 4.265, avg. samples / sec: 21895.52
Iteration:   2240, Loss function: 3.720, Average Loss: 4.266, avg. samples / sec: 21903.74
Iteration:   2240, Loss function: 4.230, Average Loss: 4.270, avg. samples / sec: 21901.39
Iteration:   2240, Loss function: 4.094, Average Loss: 4.255, avg. samples / sec: 21895.51
Iteration:   2240, Loss function: 4.115, Average Loss: 4.260, avg. samples / sec: 21903.64

:::MLPv0.5.0 ssd 1541711104.930923939 (train.py:553) train_epoch: 39
Iteration:   2260, Loss function: 4.134, Average Loss: 4.262, avg. samples / sec: 21925.78
Iteration:   2260, Loss function: 3.965, Average Loss: 4.263, avg. samples / sec: 21920.83
Iteration:   2260, Loss function: 3.987, Average Loss: 4.255, avg. samples / sec: 21917.93
Iteration:   2260, Loss function: 4.096, Average Loss: 4.266, avg. samples / sec: 21917.90
Iteration:   2260, Loss function: 4.219, Average Loss: 4.267, avg. samples / sec: 21918.65
Iteration:   2260, Loss function: 4.029, Average Loss: 4.265, avg. samples / sec: 21912.22
Iteration:   2260, Loss function: 4.501, Average Loss: 4.254, avg. samples / sec: 21911.33
Iteration:   2260, Loss function: 4.566, Average Loss: 4.257, avg. samples / sec: 21910.18
Iteration:   2280, Loss function: 3.984, Average Loss: 4.259, avg. samples / sec: 21806.10
Iteration:   2280, Loss function: 4.510, Average Loss: 4.263, avg. samples / sec: 21806.48
Iteration:   2280, Loss function: 4.060, Average Loss: 4.265, avg. samples / sec: 21810.41
Iteration:   2280, Loss function: 4.217, Average Loss: 4.260, avg. samples / sec: 21798.81
Iteration:   2280, Loss function: 4.163, Average Loss: 4.252, avg. samples / sec: 21814.56
Iteration:   2280, Loss function: 4.047, Average Loss: 4.261, avg. samples / sec: 21810.84
Iteration:   2280, Loss function: 4.062, Average Loss: 4.253, avg. samples / sec: 21801.26
Iteration:   2280, Loss function: 4.657, Average Loss: 4.256, avg. samples / sec: 21813.34
Iteration:   2300, Loss function: 3.519, Average Loss: 4.256, avg. samples / sec: 21926.66
Iteration:   2300, Loss function: 4.627, Average Loss: 4.251, avg. samples / sec: 21933.95
Iteration:   2300, Loss function: 4.266, Average Loss: 4.259, avg. samples / sec: 21929.06
Iteration:   2300, Loss function: 4.830, Average Loss: 4.254, avg. samples / sec: 21931.87
Iteration:   2300, Loss function: 3.686, Average Loss: 4.262, avg. samples / sec: 21922.67
Iteration:   2300, Loss function: 4.420, Average Loss: 4.251, avg. samples / sec: 21927.13
Iteration:   2300, Loss function: 4.452, Average Loss: 4.261, avg. samples / sec: 21922.17
Iteration:   2300, Loss function: 3.861, Average Loss: 4.258, avg. samples / sec: 21919.56

:::MLPv0.5.0 ssd 1541711110.360610485 (train.py:553) train_epoch: 40
Iteration:   2320, Loss function: 4.754, Average Loss: 4.245, avg. samples / sec: 21855.01
Iteration:   2320, Loss function: 3.913, Average Loss: 4.257, avg. samples / sec: 21860.43
Iteration:   2320, Loss function: 4.340, Average Loss: 4.252, avg. samples / sec: 21854.30
Iteration:   2320, Loss function: 3.765, Average Loss: 4.256, avg. samples / sec: 21858.72
Iteration:   2320, Loss function: 3.867, Average Loss: 4.256, avg. samples / sec: 21863.64
Iteration:   2320, Loss function: 4.194, Average Loss: 4.258, avg. samples / sec: 21859.16
Iteration:   2320, Loss function: 3.775, Average Loss: 4.249, avg. samples / sec: 21855.87
Iteration:   2320, Loss function: 3.880, Average Loss: 4.251, avg. samples / sec: 21854.26
Iteration:   2340, Loss function: 4.193, Average Loss: 4.241, avg. samples / sec: 21819.66
Iteration:   2340, Loss function: 4.401, Average Loss: 4.245, avg. samples / sec: 21816.44
Iteration:   2340, Loss function: 3.780, Average Loss: 4.250, avg. samples / sec: 21818.65
Iteration:   2340, Loss function: 3.809, Average Loss: 4.252, avg. samples / sec: 21820.20
Iteration:   2340, Loss function: 3.986, Average Loss: 4.252, avg. samples / sec: 21815.99
Iteration:   2340, Loss function: 3.968, Average Loss: 4.245, avg. samples / sec: 21816.22
Iteration:   2340, Loss function: 4.126, Average Loss: 4.249, avg. samples / sec: 21809.81
Iteration:   2340, Loss function: 4.251, Average Loss: 4.247, avg. samples / sec: 21812.09
Iteration:   2360, Loss function: 4.114, Average Loss: 4.248, avg. samples / sec: 21825.64
Iteration:   2360, Loss function: 3.827, Average Loss: 4.247, avg. samples / sec: 21821.93
Iteration:   2360, Loss function: 4.039, Average Loss: 4.244, avg. samples / sec: 21816.91
Iteration:   2360, Loss function: 3.948, Average Loss: 4.244, avg. samples / sec: 21823.72
Iteration:   2360, Loss function: 3.959, Average Loss: 4.245, avg. samples / sec: 21819.97
Iteration:   2360, Loss function: 4.060, Average Loss: 4.250, avg. samples / sec: 21816.01
Iteration:   2360, Loss function: 4.107, Average Loss: 4.245, avg. samples / sec: 21816.88
Iteration:   2360, Loss function: 4.443, Average Loss: 4.239, avg. samples / sec: 21781.37

:::MLPv0.5.0 ssd 1541711115.805926800 (train.py:553) train_epoch: 41
Iteration:   2380, Loss function: 4.169, Average Loss: 4.241, avg. samples / sec: 21851.08
Iteration:   2380, Loss function: 4.661, Average Loss: 4.245, avg. samples / sec: 21848.70
Iteration:   2380, Loss function: 4.075, Average Loss: 4.239, avg. samples / sec: 21852.31
Iteration:   2380, Loss function: 4.025, Average Loss: 4.238, avg. samples / sec: 21851.95
Iteration:   2380, Loss function: 4.273, Average Loss: 4.248, avg. samples / sec: 21854.66
Iteration:   2380, Loss function: 4.832, Average Loss: 4.241, avg. samples / sec: 21860.89
Iteration:   2380, Loss function: 4.341, Average Loss: 4.241, avg. samples / sec: 21851.98
Iteration:   2380, Loss function: 4.148, Average Loss: 4.234, avg. samples / sec: 21878.27
Iteration:   2400, Loss function: 3.802, Average Loss: 4.237, avg. samples / sec: 21953.86
Iteration:   2400, Loss function: 4.372, Average Loss: 4.239, avg. samples / sec: 21956.50
Iteration:   2400, Loss function: 4.255, Average Loss: 4.243, avg. samples / sec: 21951.17
Iteration:   2400, Loss function: 4.381, Average Loss: 4.233, avg. samples / sec: 21955.50
Iteration:   2400, Loss function: 3.841, Average Loss: 4.237, avg. samples / sec: 21949.58
Iteration:   2400, Loss function: 3.870, Average Loss: 4.245, avg. samples / sec: 21949.81
Iteration:   2400, Loss function: 4.270, Average Loss: 4.238, avg. samples / sec: 21946.13
Iteration:   2400, Loss function: 4.478, Average Loss: 4.238, avg. samples / sec: 21950.84
Iteration:   2420, Loss function: 4.160, Average Loss: 4.232, avg. samples / sec: 21881.52
Iteration:   2420, Loss function: 3.606, Average Loss: 4.234, avg. samples / sec: 21871.53
Iteration:   2420, Loss function: 3.908, Average Loss: 4.242, avg. samples / sec: 21881.62
Iteration:   2420, Loss function: 3.542, Average Loss: 4.230, avg. samples / sec: 21879.25
Iteration:   2420, Loss function: 4.221, Average Loss: 4.242, avg. samples / sec: 21870.73
Iteration:   2420, Loss function: 4.253, Average Loss: 4.236, avg. samples / sec: 21873.84
Iteration:   2420, Loss function: 4.016, Average Loss: 4.234, avg. samples / sec: 21866.30
Iteration:   2420, Loss function: 4.273, Average Loss: 4.235, avg. samples / sec: 21869.14

:::MLPv0.5.0 ssd 1541711121.139156342 (train.py:553) train_epoch: 42
Iteration:   2440, Loss function: 3.707, Average Loss: 4.237, avg. samples / sec: 21848.19
Iteration:   2440, Loss function: 4.127, Average Loss: 4.231, avg. samples / sec: 21843.99
Iteration:   2440, Loss function: 3.853, Average Loss: 4.226, avg. samples / sec: 21839.15
Iteration:   2440, Loss function: 3.881, Average Loss: 4.225, avg. samples / sec: 21842.47
Iteration:   2440, Loss function: 4.604, Average Loss: 4.231, avg. samples / sec: 21854.90
Iteration:   2440, Loss function: 4.039, Average Loss: 4.228, avg. samples / sec: 21848.18
Iteration:   2440, Loss function: 3.912, Average Loss: 4.231, avg. samples / sec: 21845.83
Iteration:   2440, Loss function: 3.519, Average Loss: 4.239, avg. samples / sec: 21850.05
Iteration:   2460, Loss function: 4.258, Average Loss: 4.221, avg. samples / sec: 21807.75
Iteration:   2460, Loss function: 4.080, Average Loss: 4.233, avg. samples / sec: 21799.26
Iteration:   2460, Loss function: 4.333, Average Loss: 4.229, avg. samples / sec: 21797.00
Iteration:   2460, Loss function: 3.996, Average Loss: 4.227, avg. samples / sec: 21802.62
Iteration:   2460, Loss function: 4.032, Average Loss: 4.234, avg. samples / sec: 21830.18
Iteration:   2460, Loss function: 4.449, Average Loss: 4.228, avg. samples / sec: 21796.15
Iteration:   2460, Loss function: 3.949, Average Loss: 4.221, avg. samples / sec: 21795.27
Iteration:   2460, Loss function: 3.658, Average Loss: 4.223, avg. samples / sec: 21790.40
Iteration:   2480, Loss function: 4.147, Average Loss: 4.215, avg. samples / sec: 21860.13
Iteration:   2480, Loss function: 3.372, Average Loss: 4.230, avg. samples / sec: 21858.66
Iteration:   2480, Loss function: 3.706, Average Loss: 4.216, avg. samples / sec: 21865.27
Iteration:   2480, Loss function: 4.111, Average Loss: 4.225, avg. samples / sec: 21853.52
Iteration:   2480, Loss function: 4.521, Average Loss: 4.225, avg. samples / sec: 21858.08
Iteration:   2480, Loss function: 4.701, Average Loss: 4.230, avg. samples / sec: 21857.08
Iteration:   2480, Loss function: 4.351, Average Loss: 4.223, avg. samples / sec: 21854.98
Iteration:   2480, Loss function: 4.890, Average Loss: 4.219, avg. samples / sec: 21863.05

:::MLPv0.5.0 ssd 1541711126.577465773 (train.py:553) train_epoch: 43
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1

:::MLPv0.5.0 ssd 1541711128.081502199 (train.py:578) opt_learning_rate: 0.016
Iteration:   2500, Loss function: 4.133, Average Loss: 4.228, avg. samples / sec: 21838.43
Iteration:   2500, Loss function: 3.803, Average Loss: 4.218, avg. samples / sec: 21836.88
Iteration:   2500, Loss function: 3.972, Average Loss: 4.213, avg. samples / sec: 21821.57
Iteration:   2500, Loss function: 4.275, Average Loss: 4.231, avg. samples / sec: 21822.89
Iteration:   2500, Loss function: 3.873, Average Loss: 4.218, avg. samples / sec: 21836.94
Iteration:   2500, Loss function: 4.223, Average Loss: 4.224, avg. samples / sec: 21834.90
Iteration:   2500, Loss function: 3.754, Average Loss: 4.215, avg. samples / sec: 21826.32
Iteration:   2500, Loss function: 4.164, Average Loss: 4.221, avg. samples / sec: 21830.50

































































:::MLPv0.5.0 ssd 1541711128.173096895 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711128.173923254 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711128.174702883 (train.py:220) eval_start: 43
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 29
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 34
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.22 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.22 s
Predicting Ended, total time: 5.22 s
Predicting Ended, total time: 5.22 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.22 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.22 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.22 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.22 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(325859, 7)
(325859, 7)
(325859, 7)
0/325859
0/325859
0/325859
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(325859, 7)
(325859, 7)
(325859, 7)
(325859, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(325859, 7)
0/325859
0/325859
(325859, 7)
0/325859
(325859, 7)
0/325859
(325859, 7)
Loading and preparing results...
0/325859
0/325859
Loading and preparing results...
0/325859
Converting ndarray to lists...
0/325859
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(325859, 7)
Loading and preparing results...
Loading and preparing results...
(325859, 7)
0/325859
Converting ndarray to lists...
(325859, 7)
(325859, 7)
Converting ndarray to lists...
0/325859
Converting ndarray to lists...
Loading and preparing results...
0/325859
Converting ndarray to lists...
(325859, 7)
0/325859
0/325859
Converting ndarray to lists...
(325859, 7)
Loading and preparing results...
(325859, 7)
0/325859
(325859, 7)
Loading and preparing results...
0/325859
Loading and preparing results...
0/325859
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(325859, 7)
(325859, 7)
(325859, 7)
0/325859
0/325859
0/325859
Loading and preparing results...
Converting ndarray to lists...
(325859, 7)
0/325859
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(325859, 7)
(325859, 7)
(325859, 7)
0/325859
0/325859
0/325859
Converting ndarray to lists...
(325859, 7)
0/325859
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(325859, 7)
(325859, 7)
0/325859
0/325859
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(325859, 7)
Loading and preparing results...
Converting ndarray to lists...
(325859, 7)
(325859, 7)
0/325859
Converting ndarray to lists...
0/325859
Converting ndarray to lists...
0/325859
(325859, 7)
(325859, 7)
0/325859
0/325859
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(325859, 7)
Converting ndarray to lists...
(325859, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(325859, 7)
0/325859
Loading and preparing results...
0/325859
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(325859, 7)
Loading and preparing results...
Converting ndarray to lists...
0/325859
(325859, 7)
0/325859
(325859, 7)
(325859, 7)
(325859, 7)
0/325859
Loading and preparing results...
(325859, 7)
Converting ndarray to lists...
Loading and preparing results...
(325859, 7)
0/325859
Converting ndarray to lists...
0/325859
0/325859
(325859, 7)
0/325859
Converting ndarray to lists...
0/325859
0/325859
Loading and preparing results...
(325859, 7)
Loading and preparing results...
0/325859
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(325859, 7)
Converting ndarray to lists...
Loading and preparing results...
(325859, 7)
Converting ndarray to lists...
(325859, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(325859, 7)
Converting ndarray to lists...
0/325859
(325859, 7)
Converting ndarray to lists...
0/325859
(325859, 7)
Loading and preparing results...
(325859, 7)
(325859, 7)
(325859, 7)
(325859, 7)
0/325859
0/325859
Converting ndarray to lists...
0/325859
0/325859
0/325859
0/325859
(325859, 7)
0/325859
(325859, 7)
Loading and preparing results...
0/325859
Converting ndarray to lists...
Converting ndarray to lists...
(325859, 7)
(325859, 7)
0/325859
0/325859
Loading and preparing results...
0/325859
Loading and preparing results...
(325859, 7)
Converting ndarray to lists...
0/325859
Loading and preparing results...
0/325859
(325859, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/325859
(325859, 7)
(325859, 7)
0/325859
0/325859
DONE (t=2.11s)
creating index...
DONE (t=2.12s)
creating index...
DONE (t=2.13s)
creating index...
DONE (t=2.13s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
index created!
DONE (t=2.27s)
creating index...
index created!
index created!
index created!
DONE (t=2.30s)
creating index...
index created!
index created!
index created!
DONE (t=2.31s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.34s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=3.85s).
Accumulating evaluation results...
DONE (t=3.76s).
Accumulating evaluation results...
DONE (t=3.79s).
Accumulating evaluation results...
DONE (t=3.81s).
Accumulating evaluation results...
DONE (t=3.78s).
Accumulating evaluation results...
DONE (t=3.82s).
Accumulating evaluation results...
DONE (t=3.80s).
Accumulating evaluation results...
DONE (t=3.82s).
Accumulating evaluation results...
DONE (t=1.19s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.134
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.129
DONE (t=1.20s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.031
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.134
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.364
Current AP: 0.13384 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.031
DONE (t=1.22s).
DONE (t=1.21s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.134
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.134
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.211
DONE (t=1.22s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.364
Current AP: 0.13384 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.134
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.031
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.031
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.031
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.364
Current AP: 0.13384 AP goal: 0.21200
DONE (t=1.26s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.142
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.224
DONE (t=1.23s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.364
Current AP: 0.13384 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.134
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.134
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.364
Current AP: 0.13384 AP goal: 0.21200
DONE (t=1.26s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.134
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.031
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.031
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.031
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.364
Current AP: 0.13384 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.364
Current AP: 0.13384 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.364
Current AP: 0.13384 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711140.959696054 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711140.960660219 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.13383629577085301}

:::MLPv0.5.0 ssd 1541711140.961419582 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.13383629577085301}

:::MLPv0.5.0 ssd 1541711140.962162018 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711140.962907076 (train.py:338) eval_stop: 43
Iteration:   2520, Loss function: 3.885, Average Loss: 4.216, avg. samples / sec: 2700.85
Iteration:   2520, Loss function: 3.652, Average Loss: 4.205, avg. samples / sec: 2700.77
Iteration:   2520, Loss function: 3.886, Average Loss: 4.211, avg. samples / sec: 2700.80
Iteration:   2520, Loss function: 2.986, Average Loss: 4.221, avg. samples / sec: 2700.74
Iteration:   2520, Loss function: 3.988, Average Loss: 4.210, avg. samples / sec: 2700.72
Iteration:   2520, Loss function: 3.558, Average Loss: 4.219, avg. samples / sec: 2700.67
Iteration:   2520, Loss function: 3.949, Average Loss: 4.208, avg. samples / sec: 2700.71
Iteration:   2520, Loss function: 3.339, Average Loss: 4.206, avg. samples / sec: 2700.59
Iteration:   2540, Loss function: 3.737, Average Loss: 4.191, avg. samples / sec: 21911.95
Iteration:   2540, Loss function: 3.719, Average Loss: 4.199, avg. samples / sec: 21899.24
Iteration:   2540, Loss function: 3.591, Average Loss: 4.197, avg. samples / sec: 21898.64
Iteration:   2540, Loss function: 3.411, Average Loss: 4.208, avg. samples / sec: 21897.38
Iteration:   2540, Loss function: 3.745, Average Loss: 4.203, avg. samples / sec: 21889.58
Iteration:   2540, Loss function: 3.704, Average Loss: 4.206, avg. samples / sec: 21896.92
Iteration:   2540, Loss function: 3.695, Average Loss: 4.192, avg. samples / sec: 21889.89
Iteration:   2540, Loss function: 3.379, Average Loss: 4.195, avg. samples / sec: 21892.06

:::MLPv0.5.0 ssd 1541711145.302556515 (train.py:553) train_epoch: 44
Iteration:   2560, Loss function: 4.265, Average Loss: 4.192, avg. samples / sec: 21966.03
Iteration:   2560, Loss function: 3.146, Average Loss: 4.187, avg. samples / sec: 21962.64
Iteration:   2560, Loss function: 3.397, Average Loss: 4.190, avg. samples / sec: 21958.53
Iteration:   2560, Loss function: 3.695, Average Loss: 4.179, avg. samples / sec: 21957.48
Iteration:   2560, Loss function: 3.690, Average Loss: 4.185, avg. samples / sec: 21955.27
Iteration:   2560, Loss function: 3.314, Average Loss: 4.175, avg. samples / sec: 21952.67
Iteration:   2560, Loss function: 3.185, Average Loss: 4.177, avg. samples / sec: 21957.23
Iteration:   2560, Loss function: 3.834, Average Loss: 4.181, avg. samples / sec: 21959.75
Iteration:   2580, Loss function: 3.510, Average Loss: 4.171, avg. samples / sec: 21908.01
Iteration:   2580, Loss function: 3.000, Average Loss: 4.177, avg. samples / sec: 21902.83
Iteration:   2580, Loss function: 3.330, Average Loss: 4.158, avg. samples / sec: 21906.06
Iteration:   2580, Loss function: 3.513, Average Loss: 4.172, avg. samples / sec: 21900.09
Iteration:   2580, Loss function: 3.208, Average Loss: 4.174, avg. samples / sec: 21899.39
Iteration:   2580, Loss function: 3.498, Average Loss: 4.167, avg. samples / sec: 21905.54
Iteration:   2580, Loss function: 2.724, Average Loss: 4.162, avg. samples / sec: 21899.43
Iteration:   2580, Loss function: 3.092, Average Loss: 4.163, avg. samples / sec: 21903.45

:::MLPv0.5.0 ssd 1541711150.713833809 (train.py:553) train_epoch: 45
Iteration:   2600, Loss function: 2.924, Average Loss: 4.157, avg. samples / sec: 21982.78
Iteration:   2600, Loss function: 3.531, Average Loss: 4.143, avg. samples / sec: 21980.96
Iteration:   2600, Loss function: 3.378, Average Loss: 4.156, avg. samples / sec: 21973.74
Iteration:   2600, Loss function: 3.415, Average Loss: 4.162, avg. samples / sec: 21973.05
Iteration:   2600, Loss function: 3.880, Average Loss: 4.147, avg. samples / sec: 21983.14
Iteration:   2600, Loss function: 3.195, Average Loss: 4.147, avg. samples / sec: 21980.39
Iteration:   2600, Loss function: 3.201, Average Loss: 4.157, avg. samples / sec: 21978.07
Iteration:   2600, Loss function: 3.697, Average Loss: 4.154, avg. samples / sec: 21975.22
Iteration:   2620, Loss function: 3.465, Average Loss: 4.145, avg. samples / sec: 21960.73
Iteration:   2620, Loss function: 3.462, Average Loss: 4.142, avg. samples / sec: 21952.94
Iteration:   2620, Loss function: 3.176, Average Loss: 4.132, avg. samples / sec: 21957.62
Iteration:   2620, Loss function: 3.676, Average Loss: 4.143, avg. samples / sec: 21960.72
Iteration:   2620, Loss function: 3.405, Average Loss: 4.128, avg. samples / sec: 21952.41
Iteration:   2620, Loss function: 3.398, Average Loss: 4.131, avg. samples / sec: 21954.25
Iteration:   2620, Loss function: 4.084, Average Loss: 4.140, avg. samples / sec: 21950.40
Iteration:   2620, Loss function: 3.535, Average Loss: 4.139, avg. samples / sec: 21933.85
Iteration:   2640, Loss function: 3.768, Average Loss: 4.130, avg. samples / sec: 21997.78
Iteration:   2640, Loss function: 3.240, Average Loss: 4.112, avg. samples / sec: 21996.89
Iteration:   2640, Loss function: 3.479, Average Loss: 4.115, avg. samples / sec: 21992.97
Iteration:   2640, Loss function: 3.622, Average Loss: 4.124, avg. samples / sec: 22024.08
Iteration:   2640, Loss function: 4.160, Average Loss: 4.126, avg. samples / sec: 21998.02
Iteration:   2640, Loss function: 2.980, Average Loss: 4.127, avg. samples / sec: 21991.03
Iteration:   2640, Loss function: 3.935, Average Loss: 4.129, avg. samples / sec: 21991.52
Iteration:   2640, Loss function: 3.798, Average Loss: 4.116, avg. samples / sec: 21988.36

:::MLPv0.5.0 ssd 1541711156.027568102 (train.py:553) train_epoch: 46
Iteration:   2660, Loss function: 3.251, Average Loss: 4.103, avg. samples / sec: 21972.56
Iteration:   2660, Loss function: 2.893, Average Loss: 4.109, avg. samples / sec: 21968.79
Iteration:   2660, Loss function: 3.335, Average Loss: 4.098, avg. samples / sec: 21965.31
Iteration:   2660, Loss function: 3.431, Average Loss: 4.113, avg. samples / sec: 21959.69
Iteration:   2660, Loss function: 2.953, Average Loss: 4.112, avg. samples / sec: 21968.43
Iteration:   2660, Loss function: 3.259, Average Loss: 4.107, avg. samples / sec: 21966.87
Iteration:   2660, Loss function: 3.426, Average Loss: 4.098, avg. samples / sec: 21973.79
Iteration:   2660, Loss function: 3.669, Average Loss: 4.110, avg. samples / sec: 21958.91
Iteration:   2680, Loss function: 3.543, Average Loss: 4.096, avg. samples / sec: 21972.62
Iteration:   2680, Loss function: 3.876, Average Loss: 4.092, avg. samples / sec: 21973.26
Iteration:   2680, Loss function: 3.298, Average Loss: 4.082, avg. samples / sec: 21973.38
Iteration:   2680, Loss function: 3.021, Average Loss: 4.094, avg. samples / sec: 21968.41
Iteration:   2680, Loss function: 2.951, Average Loss: 4.089, avg. samples / sec: 21959.68
Iteration:   2680, Loss function: 3.476, Average Loss: 4.095, avg. samples / sec: 21971.93
Iteration:   2680, Loss function: 3.632, Average Loss: 4.098, avg. samples / sec: 21962.33
Iteration:   2680, Loss function: 3.546, Average Loss: 4.084, avg. samples / sec: 21949.19
Iteration:   2700, Loss function: 3.147, Average Loss: 4.073, avg. samples / sec: 21957.30
Iteration:   2700, Loss function: 2.831, Average Loss: 4.068, avg. samples / sec: 21970.09
Iteration:   2700, Loss function: 3.793, Average Loss: 4.080, avg. samples / sec: 21943.33
Iteration:   2700, Loss function: 3.416, Average Loss: 4.077, avg. samples / sec: 21939.81
Iteration:   2700, Loss function: 3.244, Average Loss: 4.083, avg. samples / sec: 21947.28
Iteration:   2700, Loss function: 2.854, Average Loss: 4.079, avg. samples / sec: 21945.47
Iteration:   2700, Loss function: 3.592, Average Loss: 4.069, avg. samples / sec: 21937.41
Iteration:   2700, Loss function: 3.425, Average Loss: 4.084, avg. samples / sec: 21934.57

:::MLPv0.5.0 ssd 1541711161.443863630 (train.py:553) train_epoch: 47
Iteration:   2720, Loss function: 3.701, Average Loss: 4.060, avg. samples / sec: 21838.18
Iteration:   2720, Loss function: 3.010, Average Loss: 4.053, avg. samples / sec: 21839.78
Iteration:   2720, Loss function: 3.084, Average Loss: 4.061, avg. samples / sec: 21849.82
Iteration:   2720, Loss function: 3.215, Average Loss: 4.068, avg. samples / sec: 21850.59
Iteration:   2720, Loss function: 3.347, Average Loss: 4.066, avg. samples / sec: 21851.49
Iteration:   2720, Loss function: 3.307, Average Loss: 4.053, avg. samples / sec: 21849.37
Iteration:   2720, Loss function: 2.939, Average Loss: 4.061, avg. samples / sec: 21844.16
Iteration:   2720, Loss function: 3.091, Average Loss: 4.066, avg. samples / sec: 21843.99
Iteration:   2740, Loss function: 3.265, Average Loss: 4.040, avg. samples / sec: 21970.45
Iteration:   2740, Loss function: 2.981, Average Loss: 4.050, avg. samples / sec: 21970.43
Iteration:   2740, Loss function: 3.466, Average Loss: 4.045, avg. samples / sec: 21966.22
Iteration:   2740, Loss function: 2.586, Average Loss: 4.046, avg. samples / sec: 21965.75
Iteration:   2740, Loss function: 3.797, Average Loss: 4.048, avg. samples / sec: 21965.83
Iteration:   2740, Loss function: 3.066, Average Loss: 4.050, avg. samples / sec: 21964.33
Iteration:   2740, Loss function: 3.645, Average Loss: 4.040, avg. samples / sec: 21960.71
Iteration:   2740, Loss function: 3.212, Average Loss: 4.054, avg. samples / sec: 21958.07
Iteration:   2760, Loss function: 3.398, Average Loss: 4.035, avg. samples / sec: 21916.78
Iteration:   2760, Loss function: 2.959, Average Loss: 4.036, avg. samples / sec: 21925.33
Iteration:   2760, Loss function: 3.195, Average Loss: 4.027, avg. samples / sec: 21912.88
Iteration:   2760, Loss function: 3.270, Average Loss: 4.032, avg. samples / sec: 21913.65
Iteration:   2760, Loss function: 3.298, Average Loss: 4.029, avg. samples / sec: 21920.98
Iteration:   2760, Loss function: 3.028, Average Loss: 4.030, avg. samples / sec: 21913.09
Iteration:   2760, Loss function: 3.218, Average Loss: 4.033, avg. samples / sec: 21914.84
Iteration:   2760, Loss function: 2.930, Average Loss: 4.040, avg. samples / sec: 21917.04

:::MLPv0.5.0 ssd 1541711166.857227802 (train.py:553) train_epoch: 48
Iteration:   2780, Loss function: 2.834, Average Loss: 4.022, avg. samples / sec: 21910.07
Iteration:   2780, Loss function: 2.903, Average Loss: 4.020, avg. samples / sec: 21908.13
Iteration:   2780, Loss function: 3.200, Average Loss: 4.013, avg. samples / sec: 21908.61
Iteration:   2780, Loss function: 2.967, Average Loss: 4.018, avg. samples / sec: 21915.00
Iteration:   2780, Loss function: 3.348, Average Loss: 4.015, avg. samples / sec: 21906.67
Iteration:   2780, Loss function: 3.547, Average Loss: 4.022, avg. samples / sec: 21910.71
Iteration:   2780, Loss function: 2.876, Average Loss: 4.012, avg. samples / sec: 21905.47
Iteration:   2780, Loss function: 3.736, Average Loss: 4.016, avg. samples / sec: 21896.33
Iteration:   2800, Loss function: 3.301, Average Loss: 4.004, avg. samples / sec: 21974.58
Iteration:   2800, Loss function: 3.246, Average Loss: 4.009, avg. samples / sec: 21980.52
Iteration:   2800, Loss function: 3.686, Average Loss: 4.001, avg. samples / sec: 21972.81
Iteration:   2800, Loss function: 3.361, Average Loss: 3.999, avg. samples / sec: 21988.15
Iteration:   2800, Loss function: 3.398, Average Loss: 4.002, avg. samples / sec: 21974.80
Iteration:   2800, Loss function: 3.208, Average Loss: 4.006, avg. samples / sec: 21964.64
Iteration:   2800, Loss function: 3.571, Average Loss: 3.997, avg. samples / sec: 21975.86
Iteration:   2800, Loss function: 4.196, Average Loss: 4.003, avg. samples / sec: 21964.72

































































:::MLPv0.5.0 ssd 1541711170.595708847 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711170.596516609 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711170.597249746 (train.py:220) eval_start: 48
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.68 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.68 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.68 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.68 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.68 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.68 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.68 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.68 s
Loading and preparing results...
Converting ndarray to lists...
(301241, 7)
0/301241
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(301241, 7)
Loading and preparing results...
Converting ndarray to lists...
(301241, 7)
0/301241
(301241, 7)
Converting ndarray to lists...
0/301241
0/301241
(301241, 7)
0/301241
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(301241, 7)
Converting ndarray to lists...
Loading and preparing results...
(301241, 7)
0/301241
Loading and preparing results...
Converting ndarray to lists...
0/301241
Converting ndarray to lists...
(301241, 7)
(301241, 7)
0/301241
0/301241
(301241, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(301241, 7)
(301241, 7)
0/301241
Loading and preparing results...
0/301241
0/301241
Converting ndarray to lists...
(301241, 7)
Loading and preparing results...
0/301241
Converting ndarray to lists...
Loading and preparing results...
(301241, 7)
Converting ndarray to lists...
Loading and preparing results...
0/301241
(301241, 7)
Converting ndarray to lists...
0/301241
(301241, 7)
0/301241
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(301241, 7)
Converting ndarray to lists...
Loading and preparing results...
(301241, 7)
0/301241
Converting ndarray to lists...
Loading and preparing results...
0/301241
(301241, 7)
Converting ndarray to lists...
0/301241
(301241, 7)
0/301241
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(301241, 7)
(301241, 7)
Loading and preparing results...
0/301241
0/301241
Converting ndarray to lists...
Converting ndarray to lists...
(301241, 7)
(301241, 7)
0/301241
Loading and preparing results...
0/301241
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(301241, 7)
Loading and preparing results...
(301241, 7)
Converting ndarray to lists...
0/301241
(301241, 7)
0/301241
Loading and preparing results...
Converting ndarray to lists...
0/301241
(301241, 7)
Converting ndarray to lists...
Loading and preparing results...
0/301241
(301241, 7)
0/301241
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(301241, 7)
Converting ndarray to lists...
(301241, 7)
0/301241
(301241, 7)
0/301241
0/301241
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(301241, 7)
Loading and preparing results...
(301241, 7)
Loading and preparing results...
Converting ndarray to lists...
(301241, 7)
Converting ndarray to lists...
0/301241
Converting ndarray to lists...
0/301241
Converting ndarray to lists...
(301241, 7)
(301241, 7)
(301241, 7)
(301241, 7)
0/301241
0/301241
0/301241
0/301241
0/301241
Converting ndarray to lists...
(301241, 7)
Loading and preparing results...
0/301241
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(301241, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(301241, 7)
0/301241
0/301241
(301241, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/301241
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(301241, 7)
(301241, 7)
(301241, 7)
0/301241
(301241, 7)
0/301241
(301241, 7)
0/301241
0/301241
0/301241
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(301241, 7)
Loading and preparing results...
(301241, 7)
Loading and preparing results...
(301241, 7)
Converting ndarray to lists...
0/301241
Converting ndarray to lists...
0/301241
Converting ndarray to lists...
0/301241
(301241, 7)
Loading and preparing results...
(301241, 7)
Loading and preparing results...
(301241, 7)
Converting ndarray to lists...
0/301241
Loading and preparing results...
Converting ndarray to lists...
0/301241
Loading and preparing results...
0/301241
(301241, 7)
Loading and preparing results...
(301241, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/301241
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/301241
Converting ndarray to lists...
(301241, 7)
Converting ndarray to lists...
(301241, 7)
0/301241
(301241, 7)
0/301241
(301241, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(301241, 7)
(301241, 7)
(301241, 7)
0/301241
0/301241
0/301241
0/301241
Converting ndarray to lists...
0/301241
(301241, 7)
0/301241
DONE (t=1.67s)
creating index...
DONE (t=1.67s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.81s)
creating index...
index created!
DONE (t=1.81s)
creating index...
index created!
index created!
DONE (t=1.82s)
creating index...
index created!
index created!
DONE (t=1.83s)
creating index...
index created!
DONE (t=1.84s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
DONE (t=1.85s)
creating index...
index created!
index created!
DONE (t=1.85s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=1.86s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=1.88s)
creating index...
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.88s)
creating index...
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.90s)
creating index...
index created!
index created!
DONE (t=1.91s)
creating index...
DONE (t=1.91s)
creating index...
DONE (t=1.91s)
creating index...
DONE (t=1.91s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
DONE (t=1.93s)
creating index...
index created!
index created!
index created!
DONE (t=1.97s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.01s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.09s)
creating index...
index created!
DONE (t=2.11s)
creating index...
DONE (t=2.12s)
creating index...
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=3.53s).
Accumulating evaluation results...
DONE (t=3.54s).
Accumulating evaluation results...
DONE (t=3.56s).
Accumulating evaluation results...
DONE (t=3.55s).
Accumulating evaluation results...
DONE (t=3.58s).
Accumulating evaluation results...
DONE (t=3.59s).
Accumulating evaluation results...
DONE (t=3.58s).
Accumulating evaluation results...
DONE (t=3.62s).
Accumulating evaluation results...
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
DONE (t=1.08s).
DONE (t=1.11s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.491
Current AP: 0.20949 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
DONE (t=1.11s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.491
Current AP: 0.20949 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.491
Current AP: 0.20949 AP goal: 0.21200
DONE (t=1.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.209
DONE (t=1.10s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
DONE (t=1.09s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.491
Current AP: 0.20949 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.491
Current AP: 0.20949 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.491
Current AP: 0.20949 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.491
Current AP: 0.20949 AP goal: 0.21200
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.315
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.491
Current AP: 0.20949 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711183.339686394 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711183.340616703 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.20949130580753605}

:::MLPv0.5.0 ssd 1541711183.341377258 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.20949130580753605}

:::MLPv0.5.0 ssd 1541711183.342138767 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711183.342896461 (train.py:338) eval_stop: 48
Iteration:   2820, Loss function: 3.074, Average Loss: 3.986, avg. samples / sec: 2715.34
Iteration:   2820, Loss function: 3.028, Average Loss: 3.986, avg. samples / sec: 2715.26
Iteration:   2820, Loss function: 2.673, Average Loss: 3.991, avg. samples / sec: 2715.27
Iteration:   2820, Loss function: 3.303, Average Loss: 3.989, avg. samples / sec: 2715.35
Iteration:   2820, Loss function: 3.246, Average Loss: 3.981, avg. samples / sec: 2715.25
Iteration:   2820, Loss function: 3.358, Average Loss: 3.989, avg. samples / sec: 2715.13
Iteration:   2820, Loss function: 3.194, Average Loss: 3.994, avg. samples / sec: 2715.15
Iteration:   2820, Loss function: 2.715, Average Loss: 3.983, avg. samples / sec: 2715.17

:::MLPv0.5.0 ssd 1541711185.497913361 (train.py:553) train_epoch: 49
Iteration:   2840, Loss function: 3.118, Average Loss: 3.980, avg. samples / sec: 21889.33
Iteration:   2840, Loss function: 2.788, Average Loss: 3.973, avg. samples / sec: 21878.41
Iteration:   2840, Loss function: 3.636, Average Loss: 3.976, avg. samples / sec: 21885.51
Iteration:   2840, Loss function: 3.088, Average Loss: 3.974, avg. samples / sec: 21883.73
Iteration:   2840, Loss function: 4.102, Average Loss: 3.974, avg. samples / sec: 21885.07
Iteration:   2840, Loss function: 2.842, Average Loss: 3.967, avg. samples / sec: 21887.94
Iteration:   2840, Loss function: 3.443, Average Loss: 3.965, avg. samples / sec: 21884.12
Iteration:   2840, Loss function: 3.625, Average Loss: 3.972, avg. samples / sec: 21880.25
Iteration:   2860, Loss function: 3.293, Average Loss: 3.956, avg. samples / sec: 21998.03
Iteration:   2860, Loss function: 3.007, Average Loss: 3.951, avg. samples / sec: 21997.80
Iteration:   2860, Loss function: 3.259, Average Loss: 3.958, avg. samples / sec: 21998.94
Iteration:   2860, Loss function: 3.266, Average Loss: 3.962, avg. samples / sec: 21993.08
Iteration:   2860, Loss function: 3.667, Average Loss: 3.962, avg. samples / sec: 21993.53
Iteration:   2860, Loss function: 3.400, Average Loss: 3.954, avg. samples / sec: 21992.34
Iteration:   2860, Loss function: 3.314, Average Loss: 3.965, avg. samples / sec: 21992.97
Iteration:   2860, Loss function: 3.027, Average Loss: 3.959, avg. samples / sec: 21988.19
Iteration:   2880, Loss function: 3.319, Average Loss: 3.945, avg. samples / sec: 21902.78
Iteration:   2880, Loss function: 3.328, Average Loss: 3.941, avg. samples / sec: 21887.97
Iteration:   2880, Loss function: 2.988, Average Loss: 3.940, avg. samples / sec: 21890.49
Iteration:   2880, Loss function: 3.191, Average Loss: 3.936, avg. samples / sec: 21890.59
Iteration:   2880, Loss function: 3.397, Average Loss: 3.947, avg. samples / sec: 21889.85
Iteration:   2880, Loss function: 3.119, Average Loss: 3.949, avg. samples / sec: 21890.69
Iteration:   2880, Loss function: 3.286, Average Loss: 3.941, avg. samples / sec: 21891.40
Iteration:   2880, Loss function: 3.133, Average Loss: 3.950, avg. samples / sec: 21886.20

:::MLPv0.5.0 ssd 1541711190.818254709 (train.py:553) train_epoch: 50
Iteration:   2900, Loss function: 3.148, Average Loss: 3.931, avg. samples / sec: 21938.10
Iteration:   2900, Loss function: 2.934, Average Loss: 3.927, avg. samples / sec: 21938.44
Iteration:   2900, Loss function: 3.122, Average Loss: 3.927, avg. samples / sec: 21942.35
Iteration:   2900, Loss function: 3.254, Average Loss: 3.927, avg. samples / sec: 21933.84
Iteration:   2900, Loss function: 2.954, Average Loss: 3.922, avg. samples / sec: 21935.40
Iteration:   2900, Loss function: 3.075, Average Loss: 3.933, avg. samples / sec: 21934.94
Iteration:   2900, Loss function: 3.496, Average Loss: 3.937, avg. samples / sec: 21934.13
Iteration:   2900, Loss function: 3.157, Average Loss: 3.936, avg. samples / sec: 21932.89
Iteration:   2920, Loss function: 3.358, Average Loss: 3.911, avg. samples / sec: 21976.52
Iteration:   2920, Loss function: 3.354, Average Loss: 3.917, avg. samples / sec: 21979.10
Iteration:   2920, Loss function: 3.551, Average Loss: 3.922, avg. samples / sec: 21983.40
Iteration:   2920, Loss function: 3.106, Average Loss: 3.911, avg. samples / sec: 21969.54
Iteration:   2920, Loss function: 3.127, Average Loss: 3.920, avg. samples / sec: 21964.52
Iteration:   2920, Loss function: 2.862, Average Loss: 3.921, avg. samples / sec: 21975.03
Iteration:   2920, Loss function: 3.517, Average Loss: 3.913, avg. samples / sec: 21967.39
Iteration:   2920, Loss function: 3.290, Average Loss: 3.907, avg. samples / sec: 21966.72
Iteration:   2940, Loss function: 3.791, Average Loss: 3.908, avg. samples / sec: 21958.66
Iteration:   2940, Loss function: 3.644, Average Loss: 3.906, avg. samples / sec: 21958.81
Iteration:   2940, Loss function: 3.644, Average Loss: 3.899, avg. samples / sec: 21957.90
Iteration:   2940, Loss function: 3.534, Average Loss: 3.909, avg. samples / sec: 21953.63
Iteration:   2940, Loss function: 3.248, Average Loss: 3.904, avg. samples / sec: 21949.29
Iteration:   2940, Loss function: 2.700, Average Loss: 3.894, avg. samples / sec: 21948.73
Iteration:   2940, Loss function: 3.224, Average Loss: 3.898, avg. samples / sec: 21949.02
Iteration:   2940, Loss function: 3.280, Average Loss: 3.893, avg. samples / sec: 21954.77

:::MLPv0.5.0 ssd 1541711196.229906797 (train.py:553) train_epoch: 51
Iteration:   2960, Loss function: 3.185, Average Loss: 3.879, avg. samples / sec: 21921.72
Iteration:   2960, Loss function: 2.785, Average Loss: 3.893, avg. samples / sec: 21913.41
Iteration:   2960, Loss function: 2.995, Average Loss: 3.893, avg. samples / sec: 21913.22
Iteration:   2960, Loss function: 2.829, Average Loss: 3.884, avg. samples / sec: 21912.69
Iteration:   2960, Loss function: 3.486, Average Loss: 3.894, avg. samples / sec: 21911.77
Iteration:   2960, Loss function: 3.325, Average Loss: 3.885, avg. samples / sec: 21914.73
Iteration:   2960, Loss function: 3.341, Average Loss: 3.892, avg. samples / sec: 21908.91
Iteration:   2960, Loss function: 3.529, Average Loss: 3.881, avg. samples / sec: 21906.17
Iteration:   2980, Loss function: 3.276, Average Loss: 3.880, avg. samples / sec: 21869.50
Iteration:   2980, Loss function: 2.694, Average Loss: 3.880, avg. samples / sec: 21867.93
Iteration:   2980, Loss function: 3.536, Average Loss: 3.879, avg. samples / sec: 21870.15
Iteration:   2980, Loss function: 3.453, Average Loss: 3.872, avg. samples / sec: 21867.46
Iteration:   2980, Loss function: 2.507, Average Loss: 3.864, avg. samples / sec: 21856.06
Iteration:   2980, Loss function: 3.213, Average Loss: 3.867, avg. samples / sec: 21874.36
Iteration:   2980, Loss function: 3.447, Average Loss: 3.880, avg. samples / sec: 21858.59
Iteration:   2980, Loss function: 3.575, Average Loss: 3.870, avg. samples / sec: 21858.10
Iteration:   3000, Loss function: 3.114, Average Loss: 3.866, avg. samples / sec: 21967.59
Iteration:   3000, Loss function: 3.211, Average Loss: 3.852, avg. samples / sec: 21967.53
Iteration:   3000, Loss function: 3.170, Average Loss: 3.867, avg. samples / sec: 21956.15
Iteration:   3000, Loss function: 3.236, Average Loss: 3.856, avg. samples / sec: 21966.15
Iteration:   3000, Loss function: 3.224, Average Loss: 3.852, avg. samples / sec: 21962.74
Iteration:   3000, Loss function: 3.073, Average Loss: 3.865, avg. samples / sec: 21956.91
Iteration:   3000, Loss function: 3.019, Average Loss: 3.864, avg. samples / sec: 21958.31
Iteration:   3000, Loss function: 3.137, Average Loss: 3.859, avg. samples / sec: 21956.26

:::MLPv0.5.0 ssd 1541711201.651060581 (train.py:553) train_epoch: 52
Iteration:   3020, Loss function: 3.050, Average Loss: 3.850, avg. samples / sec: 21915.47
Iteration:   3020, Loss function: 3.003, Average Loss: 3.844, avg. samples / sec: 21923.09
Iteration:   3020, Loss function: 3.547, Average Loss: 3.837, avg. samples / sec: 21911.71
Iteration:   3020, Loss function: 2.898, Average Loss: 3.842, avg. samples / sec: 21915.05
Iteration:   3020, Loss function: 3.051, Average Loss: 3.846, avg. samples / sec: 21917.88
Iteration:   3020, Loss function: 3.389, Average Loss: 3.854, avg. samples / sec: 21909.57
Iteration:   3020, Loss function: 3.277, Average Loss: 3.839, avg. samples / sec: 21910.42
Iteration:   3020, Loss function: 3.148, Average Loss: 3.853, avg. samples / sec: 21910.22
Iteration:   3040, Loss function: 3.409, Average Loss: 3.832, avg. samples / sec: 21852.40
Iteration:   3040, Loss function: 3.201, Average Loss: 3.823, avg. samples / sec: 21859.19
Iteration:   3040, Loss function: 3.540, Average Loss: 3.839, avg. samples / sec: 21848.39
Iteration:   3040, Loss function: 3.572, Average Loss: 3.834, avg. samples / sec: 21850.15
Iteration:   3040, Loss function: 2.979, Average Loss: 3.828, avg. samples / sec: 21849.64
Iteration:   3040, Loss function: 2.568, Average Loss: 3.838, avg. samples / sec: 21848.38
Iteration:   3040, Loss function: 3.391, Average Loss: 3.838, avg. samples / sec: 21852.22
Iteration:   3040, Loss function: 3.025, Average Loss: 3.824, avg. samples / sec: 21839.81
Iteration:   3060, Loss function: 2.962, Average Loss: 3.825, avg. samples / sec: 21879.68
Iteration:   3060, Loss function: 3.047, Average Loss: 3.827, avg. samples / sec: 21884.92
Iteration:   3060, Loss function: 2.809, Average Loss: 3.824, avg. samples / sec: 21882.86
Iteration:   3060, Loss function: 3.267, Average Loss: 3.818, avg. samples / sec: 21876.81
Iteration:   3060, Loss function: 2.931, Average Loss: 3.809, avg. samples / sec: 21868.38
Iteration:   3060, Loss function: 3.494, Average Loss: 3.816, avg. samples / sec: 21874.01
Iteration:   3060, Loss function: 3.327, Average Loss: 3.810, avg. samples / sec: 21880.70
Iteration:   3060, Loss function: 3.491, Average Loss: 3.822, avg. samples / sec: 21861.06

:::MLPv0.5.0 ssd 1541711207.079711437 (train.py:553) train_epoch: 53
Iteration:   3080, Loss function: 3.430, Average Loss: 3.815, avg. samples / sec: 21895.31
Iteration:   3080, Loss function: 2.833, Average Loss: 3.809, avg. samples / sec: 21896.13
Iteration:   3080, Loss function: 3.438, Average Loss: 3.797, avg. samples / sec: 21896.66
Iteration:   3080, Loss function: 3.672, Average Loss: 3.803, avg. samples / sec: 21893.66
Iteration:   3080, Loss function: 2.568, Average Loss: 3.808, avg. samples / sec: 21901.02
Iteration:   3080, Loss function: 3.189, Average Loss: 3.809, avg. samples / sec: 21881.88
Iteration:   3080, Loss function: 3.407, Average Loss: 3.803, avg. samples / sec: 21887.24
Iteration:   3080, Loss function: 3.408, Average Loss: 3.797, avg. samples / sec: 21888.65
Iteration:   3100, Loss function: 3.492, Average Loss: 3.801, avg. samples / sec: 21919.57
Iteration:   3100, Loss function: 3.070, Average Loss: 3.796, avg. samples / sec: 21927.34
Iteration:   3100, Loss function: 3.503, Average Loss: 3.785, avg. samples / sec: 21925.30
Iteration:   3100, Loss function: 3.096, Average Loss: 3.792, avg. samples / sec: 21924.48
Iteration:   3100, Loss function: 3.054, Average Loss: 3.783, avg. samples / sec: 21926.99
Iteration:   3100, Loss function: 3.171, Average Loss: 3.797, avg. samples / sec: 21924.36
Iteration:   3100, Loss function: 2.762, Average Loss: 3.796, avg. samples / sec: 21911.68
Iteration:   3100, Loss function: 3.050, Average Loss: 3.788, avg. samples / sec: 21911.16

:::MLPv0.5.0 ssd 1541711212.406300306 (train.py:553) train_epoch: 54
Iteration:   3120, Loss function: 2.931, Average Loss: 3.774, avg. samples / sec: 21952.66
Iteration:   3120, Loss function: 3.031, Average Loss: 3.788, avg. samples / sec: 21933.78
Iteration:   3120, Loss function: 3.680, Average Loss: 3.784, avg. samples / sec: 21933.64
Iteration:   3120, Loss function: 3.632, Average Loss: 3.773, avg. samples / sec: 21928.84
Iteration:   3120, Loss function: 3.214, Average Loss: 3.784, avg. samples / sec: 21933.24
Iteration:   3120, Loss function: 3.227, Average Loss: 3.783, avg. samples / sec: 21924.86
Iteration:   3120, Loss function: 3.357, Average Loss: 3.781, avg. samples / sec: 21927.07
Iteration:   3120, Loss function: 3.088, Average Loss: 3.771, avg. samples / sec: 21927.05
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2

:::MLPv0.5.0 ssd 1541711212.971272230 (train.py:586) opt_learning_rate: 0.0016

































































:::MLPv0.5.0 ssd 1541711213.063168526 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711213.063992262 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711213.064725399 (train.py:220) eval_start: 54
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.74 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.74 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.74 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.74 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.74 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.74 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.74 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.74 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(290078, 7)
(290078, 7)
(290078, 7)
0/290078
0/290078
0/290078
Loading and preparing results...
Converting ndarray to lists...
(290078, 7)
0/290078
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(290078, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/290078
Converting ndarray to lists...
(290078, 7)
(290078, 7)
(290078, 7)
0/290078
0/290078
0/290078
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(290078, 7)
(290078, 7)
Converting ndarray to lists...
0/290078
0/290078
(290078, 7)
Loading and preparing results...
0/290078
Converting ndarray to lists...
(290078, 7)
0/290078
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(290078, 7)
Converting ndarray to lists...
0/290078
(290078, 7)
0/290078
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(290078, 7)
(290078, 7)
0/290078
0/290078
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(290078, 7)
Loading and preparing results...
(290078, 7)
0/290078
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/290078
Loading and preparing results...
Converting ndarray to lists...
(290078, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(290078, 7)
0/290078
Loading and preparing results...
(290078, 7)
(290078, 7)
0/290078
0/290078
0/290078
Converting ndarray to lists...
Loading and preparing results...
(290078, 7)
Loading and preparing results...
0/290078
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(290078, 7)
(290078, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/290078
Loading and preparing results...
Loading and preparing results...
(290078, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/290078
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(290078, 7)
Converting ndarray to lists...
0/290078
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(290078, 7)
Converting ndarray to lists...
0/290078
(290078, 7)
(290078, 7)
(290078, 7)
(290078, 7)
Loading and preparing results...
(290078, 7)
Converting ndarray to lists...
(290078, 7)
0/290078
Converting ndarray to lists...
0/290078
0/290078
(290078, 7)
0/290078
Loading and preparing results...
0/290078
Loading and preparing results...
0/290078
(290078, 7)
0/290078
Converting ndarray to lists...
Converting ndarray to lists...
(290078, 7)
0/290078
Loading and preparing results...
0/290078
Loading and preparing results...
0/290078
(290078, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/290078
(290078, 7)
(290078, 7)
Loading and preparing results...
(290078, 7)
Loading and preparing results...
0/290078
Converting ndarray to lists...
0/290078
0/290078
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(290078, 7)
Loading and preparing results...
Converting ndarray to lists...
(290078, 7)
(290078, 7)
Converting ndarray to lists...
(290078, 7)
0/290078
Loading and preparing results...
0/290078
(290078, 7)
Converting ndarray to lists...
0/290078
Loading and preparing results...
(290078, 7)
0/290078
0/290078
0/290078
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(290078, 7)
Converting ndarray to lists...
(290078, 7)
Converting ndarray to lists...
(290078, 7)
(290078, 7)
(290078, 7)
0/290078
0/290078
Loading and preparing results...
(290078, 7)
0/290078
0/290078
Converting ndarray to lists...
0/290078
0/290078
Loading and preparing results...
(290078, 7)
Converting ndarray to lists...
0/290078
Converting ndarray to lists...
(290078, 7)
(290078, 7)
0/290078
0/290078
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(290078, 7)
(290078, 7)
Converting ndarray to lists...
(290078, 7)
0/290078
0/290078
(290078, 7)
0/290078
0/290078
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(290078, 7)
(290078, 7)
(290078, 7)
Converting ndarray to lists...
0/290078
0/290078
0/290078
(290078, 7)
0/290078
DONE (t=1.47s)
creating index...
DONE (t=1.47s)
creating index...
DONE (t=1.48s)
creating index...
DONE (t=1.49s)
creating index...
DONE (t=1.49s)
creating index...
DONE (t=1.50s)
creating index...
DONE (t=1.50s)
creating index...
DONE (t=1.50s)
creating index...
DONE (t=1.50s)
creating index...
DONE (t=1.51s)
creating index...
DONE (t=1.51s)
creating index...
DONE (t=1.51s)
creating index...
DONE (t=1.52s)
creating index...
DONE (t=1.54s)
creating index...
DONE (t=1.54s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=1.79s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.89s)
creating index...
index created!
DONE (t=1.93s)
creating index...
index created!
index created!
index created!
index created!
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
index created!
DONE (t=1.97s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.97s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.99s)
creating index...
index created!
DONE (t=1.99s)
creating index...
index created!
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
index created!
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
index created!
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.06s)
creating index...
index created!
DONE (t=2.06s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
DONE (t=3.84s).
Accumulating evaluation results...
DONE (t=3.53s).
Accumulating evaluation results...
DONE (t=3.48s).
Accumulating evaluation results...
DONE (t=3.50s).
Accumulating evaluation results...
DONE (t=3.47s).
Accumulating evaluation results...
DONE (t=3.53s).
Accumulating evaluation results...
DONE (t=3.50s).
Accumulating evaluation results...
DONE (t=3.52s).
Accumulating evaluation results...
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.333
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503
Current AP: 0.21261 AP goal: 0.21200
DONE (t=1.09s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.333
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503
Current AP: 0.21261 AP goal: 0.21200
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
DONE (t=1.09s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.333
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503
Current AP: 0.21261 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.333
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503
Current AP: 0.21261 AP goal: 0.21200
DONE (t=1.09s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.333
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.333
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503
Current AP: 0.21261 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503
Current AP: 0.21261 AP goal: 0.21200
DONE (t=1.10s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.333
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503
Current AP: 0.21261 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.333
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503
Current AP: 0.21261 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711225.725296497 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711225.726248503 (train.py:333) eval_accuracy: {"epoch": 54, "value": 0.21261247135836076}

:::MLPv0.5.0 ssd 1541711225.727015257 (train.py:336) eval_iteration_accuracy: {"epoch": 54, "value": 0.21261247135836076}

:::MLPv0.5.0 ssd 1541711225.727772236 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711225.728557110 (train.py:338) eval_stop: 54

:::MLPv0.5.0 ssd 1541711226.776159048 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1541711226.776901722 (train.py:707) run_final
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2018-11-08 09:07:11 PM
RESULT,OBJECT_DETECTION,,406,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:11 PM
RESULT,OBJECT_DETECTION,,406,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:11 PM
RESULT,OBJECT_DETECTION,,406,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:11 PM
RESULT,OBJECT_DETECTION,,406,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:11 PM
RESULT,OBJECT_DETECTION,,406,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:11 PM
RESULT,OBJECT_DETECTION,,406,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:11 PM
RESULT,OBJECT_DETECTION,,406,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:11 PM
RESULT,OBJECT_DETECTION,,406,nvidia,2018-11-08 09:00:25 PM
