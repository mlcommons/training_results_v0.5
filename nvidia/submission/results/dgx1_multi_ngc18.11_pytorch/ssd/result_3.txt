Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3

:::MLPv0.5.0 ssd 1541710824.674488306 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.678414583 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.718966246 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.726745844 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.775903463 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.778554678 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.862066031 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.906602144 (<string>:1) run_clear_caches
Launching on node sc-sdgx-369
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-372
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-425
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-427
+ pids+=($!)
++ eval echo srun -N 1 -n 1 -w '$hostn'
+ set +x
+++ echo srun -N 1 -n 1 -w sc-sdgx-372
Launching on node sc-sdgx-442
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-369
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-772
+ srun -N 1 -n 1 -w sc-sdgx-372 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=1 --master_addr=172.22.0.170 --master_port=4242' -e SLURM_JOB_ID=155377 -e SLURM_NTASKS_PER_NODE=8 cont_155377 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-425
+++ echo srun -N 1 -n 1 -w sc-sdgx-427
+ srun -N 1 -n 1 -w sc-sdgx-369 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=0 --master_addr=172.22.0.170 --master_port=4242' -e SLURM_JOB_ID=155377 -e SLURM_NTASKS_PER_NODE=8 cont_155377 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-774
+ srun -N 1 -n 1 -w sc-sdgx-425 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=2 --master_addr=172.22.0.170 --master_port=4242' -e SLURM_JOB_ID=155377 -e SLURM_NTASKS_PER_NODE=8 cont_155377 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-800
+ srun -N 1 -n 1 -w sc-sdgx-427 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=3 --master_addr=172.22.0.170 --master_port=4242' -e SLURM_JOB_ID=155377 -e SLURM_NTASKS_PER_NODE=8 cont_155377 ./run_and_time.sh
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-774
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-772
+ srun -N 1 -n 1 -w sc-sdgx-772 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=5 --master_addr=172.22.0.170 --master_port=4242' -e SLURM_JOB_ID=155377 -e SLURM_NTASKS_PER_NODE=8 cont_155377 ./run_and_time.sh
+ srun -N 1 -n 1 -w sc-sdgx-774 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=6 --master_addr=172.22.0.170 --master_port=4242' -e SLURM_JOB_ID=155377 -e SLURM_NTASKS_PER_NODE=8 cont_155377 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-800
+++ echo srun -N 1 -n 1 -w sc-sdgx-442
+ srun -N 1 -n 1 -w sc-sdgx-800 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=7 --master_addr=172.22.0.170 --master_port=4242' -e SLURM_JOB_ID=155377 -e SLURM_NTASKS_PER_NODE=8 cont_155377 ./run_and_time.sh
+ srun -N 1 -n 1 -w sc-sdgx-442 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=4 --master_addr=172.22.0.170 --master_port=4242' -e SLURM_JOB_ID=155377 -e SLURM_NTASKS_PER_NODE=8 cont_155377 ./run_and_time.sh
Run vars: id 155377 gpus 8 mparams  --nnodes=8 --node_rank=2 --master_addr=172.22.0.170 --master_port=4242
Run vars: id 155377 gpus 8 mparams  --nnodes=8 --node_rank=0 --master_addr=172.22.0.170 --master_port=4242
Run vars: id 155377 gpus 8 mparams  --nnodes=8 --node_rank=3 --master_addr=172.22.0.170 --master_port=4242
Run vars: id 155377 gpus 8 mparams  --nnodes=8 --node_rank=5 --master_addr=172.22.0.170 --master_port=4242
Run vars: id 155377 gpus 8 mparams  --nnodes=8 --node_rank=1 --master_addr=172.22.0.170 --master_port=4242
Run vars: id 155377 gpus 8 mparams  --nnodes=8 --node_rank=6 --master_addr=172.22.0.170 --master_port=4242
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=2 --master_addr=172.22.0.170 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
Run vars: id 155377 gpus 8 mparams  --nnodes=8 --node_rank=4 --master_addr=172.22.0.170 --master_port=4242
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=0 --master_addr=172.22.0.170 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
Run vars: id 155377 gpus 8 mparams  --nnodes=8 --node_rank=7 --master_addr=172.22.0.170 --master_port=4242
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=3 --master_addr=172.22.0.170 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=5 --master_addr=172.22.0.170 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=1 --master_addr=172.22.0.170 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=6 --master_addr=172.22.0.170 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=4 --master_addr=172.22.0.170 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=7 --master_addr=172.22.0.170 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
0 Using seed = 3021159531
1 Using seed = 3021159532
3 Using seed = 3021159534
5 Using seed = 3021159536
7 Using seed = 3021159538
4 Using seed = 3021159535
14 Using seed = 3021159545
15 Using seed = 3021159546
13 Using seed = 3021159544
12 Using seed = 3021159543
8 Using seed = 3021159539
10 Using seed = 3021159541
9 Using seed = 3021159540
11 Using seed = 3021159542
21 Using seed = 3021159552
23 Using seed = 3021159554
20 Using seed = 3021159551
22 Using seed = 3021159553
16 Using seed = 3021159547
18 Using seed = 3021159549
17 Using seed = 3021159548
19 Using seed = 3021159550
24 Using seed = 3021159555
25 Using seed = 3021159556
26 Using seed = 3021159557
27 Using seed = 3021159558
29 Using seed = 3021159560
31 Using seed = 3021159562
30 Using seed = 3021159561
28 Using seed = 3021159559
37 Using seed = 3021159568
38 Using seed = 3021159569
39 Using seed = 3021159570
36 Using seed = 3021159567
33 Using seed = 3021159564
35 Using seed = 3021159566
34 Using seed = 3021159565
32 Using seed = 3021159563
47 Using seed = 3021159578
46 Using seed = 3021159577
45 Using seed = 3021159576
44 Using seed = 3021159575
42 Using seed = 3021159573
40 Using seed = 3021159571
41 Using seed = 3021159572
43 Using seed = 3021159574
48 Using seed = 3021159579
49 Using seed = 3021159580
51 Using seed = 3021159582
50 Using seed = 3021159581
53 Using seed = 3021159584
54 Using seed = 3021159585
55 Using seed = 3021159586
52 Using seed = 3021159583
61 Using seed = 3021159592
62 Using seed = 3021159593
60 Using seed = 3021159591
63 Using seed = 3021159594
56 Using seed = 3021159587
57 Using seed = 3021159588
58 Using seed = 3021159589
59 Using seed = 3021159590
2 Using seed = 3021159533
6 Using seed = 3021159537

:::MLPv0.5.0 ssd 1541710837.241018295 (train.py:371) run_start

:::MLPv0.5.0 ssd 1541710837.243556261 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1541710837.256527662 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1541710837.269439936 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1541710837.270278931 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1541710837.323414803 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1541710837.337376356 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1541710837.350951195 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1541710837.370877266 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1541710837.390876770 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1541710837.397519112 (train.py:382) input_size: 300
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
index created!
Done (t=0.48s)
creating index...
index created!
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
index created!
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
index created!
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
index created!
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.50s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.50s)
creating index...
index created!
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Done (t=0.55s)
creating index...
index created!
time_check a: 1541710838.337909937
time_check a: 1541710838.370298386
time_check a: 1541710838.370457172
time_check a: 1541710838.370388985
time_check a: 1541710838.373023272
time_check a: 1541710838.374513149
time_check a: 1541710838.375166416
time_check a: 1541710838.383653641
time_check b: 1541710861.984411240
time_check b: 1541710862.025551319
time_check b: 1541710862.036010027
time_check b: 1541710862.073049068
time_check b: 1541710862.105268955
time_check b: 1541710862.140069008
time_check b: 1541710862.179899454
time_check b: 1541710863.328230858

:::MLPv0.5.0 ssd 1541710864.091413975 (train.py:413) input_order

:::MLPv0.5.0 ssd 1541710864.097819328 (train.py:414) input_batch_size: 32

:::MLPv0.5.0 ssd 1541710868.299565315 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710868.300464153 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710868.328811169 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1541710869.270017862 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1541710869.270847797 (train.py:477) opt_learning_rate: 0.16

:::MLPv0.5.0 ssd 1541710869.271528006 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1541710869.272241116 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1541710869.272932529 (train.py:483) opt_learning_rate_warmup_steps: 900

:::MLPv0.5.0 ssd 1541710873.440075874 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710873.440872192 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710873.469321012 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss

:::MLPv0.5.0 ssd 1541710876.304856777 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1541710876.305665731 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1541710876.309895754 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 22.568, Average Loss: 0.023, avg. samples / sec: 11945.20
Iteration:      0, Loss function: 21.870, Average Loss: 0.022, avg. samples / sec: 19588.07
Iteration:      0, Loss function: 22.611, Average Loss: 0.023, avg. samples / sec: 18188.73
Iteration:      0, Loss function: 22.691, Average Loss: 0.023, avg. samples / sec: 21665.06
Iteration:      0, Loss function: 22.588, Average Loss: 0.023, avg. samples / sec: 28433.32
Iteration:      0, Loss function: 22.993, Average Loss: 0.023, avg. samples / sec: 19392.03
Iteration:      0, Loss function: 22.817, Average Loss: 0.023, avg. samples / sec: 32996.20
Iteration:      0, Loss function: 22.786, Average Loss: 0.023, avg. samples / sec: 19442.64

:::MLPv0.5.0 ssd 1541710878.215794802 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.0001777777777777767}

:::MLPv0.5.0 ssd 1541710878.450126648 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0003555555555555534}

:::MLPv0.5.0 ssd 1541710878.558835506 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0005333333333333301}

:::MLPv0.5.0 ssd 1541710878.667038679 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0007111111111111068}

:::MLPv0.5.0 ssd 1541710878.777555704 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0008888888888888835}

:::MLPv0.5.0 ssd 1541710878.895601988 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0010666666666666602}

:::MLPv0.5.0 ssd 1541710878.999561548 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.001244444444444437}

:::MLPv0.5.0 ssd 1541710879.115688562 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0014222222222222136}

:::MLPv0.5.0 ssd 1541710879.216506481 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0015999999999999903}

:::MLPv0.5.0 ssd 1541710879.319452524 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.001777777777777767}

:::MLPv0.5.0 ssd 1541710879.424259424 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0019555555555555437}

:::MLPv0.5.0 ssd 1541710879.535025120 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.0021333333333333204}

:::MLPv0.5.0 ssd 1541710879.637114286 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.002311111111111097}

:::MLPv0.5.0 ssd 1541710879.738914013 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.002488888888888874}

:::MLPv0.5.0 ssd 1541710879.839467764 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.0026666666666666505}

:::MLPv0.5.0 ssd 1541710879.939782381 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.0028444444444444272}

:::MLPv0.5.0 ssd 1541710880.048495293 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.0030222222222222317}

:::MLPv0.5.0 ssd 1541710880.147833109 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.0032000000000000084}

:::MLPv0.5.0 ssd 1541710880.271467686 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.003377777777777785}

:::MLPv0.5.0 ssd 1541710880.375181913 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.003555555555555562}
Iteration:     20, Loss function: 20.864, Average Loss: 0.439, avg. samples / sec: 10109.01
Iteration:     20, Loss function: 20.398, Average Loss: 0.437, avg. samples / sec: 10103.12
Iteration:     20, Loss function: 21.045, Average Loss: 0.441, avg. samples / sec: 10109.96
Iteration:     20, Loss function: 21.024, Average Loss: 0.439, avg. samples / sec: 10110.54
Iteration:     20, Loss function: 20.537, Average Loss: 0.440, avg. samples / sec: 10104.16
Iteration:     20, Loss function: 20.869, Average Loss: 0.440, avg. samples / sec: 10107.73
Iteration:     20, Loss function: 20.478, Average Loss: 0.443, avg. samples / sec: 10105.17
Iteration:     20, Loss function: 20.714, Average Loss: 0.437, avg. samples / sec: 10101.89

:::MLPv0.5.0 ssd 1541710880.471677780 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.0037333333333333385}

:::MLPv0.5.0 ssd 1541710880.570040941 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.003911111111111115}

:::MLPv0.5.0 ssd 1541710880.671374559 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.004088888888888892}

:::MLPv0.5.0 ssd 1541710880.772931576 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.004266666666666669}

:::MLPv0.5.0 ssd 1541710880.881323099 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.004444444444444445}

:::MLPv0.5.0 ssd 1541710880.980797291 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.004622222222222222}

:::MLPv0.5.0 ssd 1541710881.085838079 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.004799999999999999}

:::MLPv0.5.0 ssd 1541710881.189980268 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.004977777777777775}

:::MLPv0.5.0 ssd 1541710881.292043924 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.005155555555555552}

:::MLPv0.5.0 ssd 1541710881.392810583 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.005333333333333329}

:::MLPv0.5.0 ssd 1541710881.500598669 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.0055111111111111055}

:::MLPv0.5.0 ssd 1541710881.599834919 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.005688888888888882}

:::MLPv0.5.0 ssd 1541710881.698680162 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.005866666666666659}

:::MLPv0.5.0 ssd 1541710881.803316116 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.006044444444444436}

:::MLPv0.5.0 ssd 1541710881.912193060 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.006222222222222212}

:::MLPv0.5.0 ssd 1541710882.013532400 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.006399999999999989}

:::MLPv0.5.0 ssd 1541710882.117048025 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.006577777777777766}

:::MLPv0.5.0 ssd 1541710882.215331316 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.0067555555555555424}

:::MLPv0.5.0 ssd 1541710882.314857483 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.006933333333333319}

:::MLPv0.5.0 ssd 1541710882.415438175 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.007111111111111096}
Iteration:     40, Loss function: 15.885, Average Loss: 0.808, avg. samples / sec: 20074.35
Iteration:     40, Loss function: 15.972, Average Loss: 0.808, avg. samples / sec: 20067.16
Iteration:     40, Loss function: 15.217, Average Loss: 0.802, avg. samples / sec: 20098.05
Iteration:     40, Loss function: 15.312, Average Loss: 0.811, avg. samples / sec: 20068.92
Iteration:     40, Loss function: 15.373, Average Loss: 0.807, avg. samples / sec: 20054.02
Iteration:     40, Loss function: 15.344, Average Loss: 0.810, avg. samples / sec: 20060.51
Iteration:     40, Loss function: 15.333, Average Loss: 0.809, avg. samples / sec: 20057.67
Iteration:     40, Loss function: 15.519, Average Loss: 0.810, avg. samples / sec: 19995.65

:::MLPv0.5.0 ssd 1541710882.524705172 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.0072888888888888725}

:::MLPv0.5.0 ssd 1541710882.622152567 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.007466666666666649}

:::MLPv0.5.0 ssd 1541710882.719043732 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.007644444444444454}

:::MLPv0.5.0 ssd 1541710882.817845821 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.00782222222222223}

:::MLPv0.5.0 ssd 1541710882.914494753 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.008000000000000007}

:::MLPv0.5.0 ssd 1541710883.015803576 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.008177777777777784}

:::MLPv0.5.0 ssd 1541710883.115175247 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.00835555555555556}

:::MLPv0.5.0 ssd 1541710883.213561535 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.008533333333333337}

:::MLPv0.5.0 ssd 1541710883.313483238 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.008711111111111114}

:::MLPv0.5.0 ssd 1541710883.413783073 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.00888888888888889}

:::MLPv0.5.0 ssd 1541710883.508970261 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.009066666666666667}

:::MLPv0.5.0 ssd 1541710883.608983278 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.009244444444444444}

:::MLPv0.5.0 ssd 1541710883.708563566 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.00942222222222222}

:::MLPv0.5.0 ssd 1541710883.809776545 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.009599999999999997}

:::MLPv0.5.0 ssd 1541710883.917629957 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.009777777777777774}

:::MLPv0.5.0 ssd 1541710884.015388250 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.00995555555555555}

:::MLPv0.5.0 ssd 1541710884.115635157 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.010133333333333328}

:::MLPv0.5.0 ssd 1541710884.208763838 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1541710884.214064360 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 58, "value": 0.010311111111111104}

:::MLPv0.5.0 ssd 1541710884.312307358 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 59, "value": 0.010488888888888881}

:::MLPv0.5.0 ssd 1541710884.412807465 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 60, "value": 0.010666666666666658}
Iteration:     60, Loss function: 10.585, Average Loss: 1.047, avg. samples / sec: 20540.84
Iteration:     60, Loss function: 10.457, Average Loss: 1.039, avg. samples / sec: 20523.18
Iteration:     60, Loss function: 10.511, Average Loss: 1.041, avg. samples / sec: 20533.31
Iteration:     60, Loss function: 10.657, Average Loss: 1.044, avg. samples / sec: 20593.07
Iteration:     60, Loss function: 10.639, Average Loss: 1.044, avg. samples / sec: 20511.21
Iteration:     60, Loss function: 10.482, Average Loss: 1.041, avg. samples / sec: 20549.78
Iteration:     60, Loss function: 9.910, Average Loss: 1.044, avg. samples / sec: 20472.76
Iteration:     60, Loss function: 10.565, Average Loss: 1.045, avg. samples / sec: 20488.22

:::MLPv0.5.0 ssd 1541710884.512306929 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 61, "value": 0.010844444444444434}

:::MLPv0.5.0 ssd 1541710884.609996557 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 62, "value": 0.011022222222222211}

:::MLPv0.5.0 ssd 1541710884.709011555 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 63, "value": 0.011199999999999988}

:::MLPv0.5.0 ssd 1541710884.806555510 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 64, "value": 0.011377777777777764}

:::MLPv0.5.0 ssd 1541710884.903849363 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 65, "value": 0.011555555555555541}

:::MLPv0.5.0 ssd 1541710885.003210545 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 66, "value": 0.011733333333333318}

:::MLPv0.5.0 ssd 1541710885.100487709 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 67, "value": 0.011911111111111095}

:::MLPv0.5.0 ssd 1541710885.196969509 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 68, "value": 0.012088888888888899}

:::MLPv0.5.0 ssd 1541710885.294311762 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 69, "value": 0.012266666666666676}

:::MLPv0.5.0 ssd 1541710885.391438484 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 70, "value": 0.012444444444444452}

:::MLPv0.5.0 ssd 1541710885.488157749 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 71, "value": 0.012622222222222229}

:::MLPv0.5.0 ssd 1541710885.585844755 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 72, "value": 0.012800000000000006}

:::MLPv0.5.0 ssd 1541710885.681945324 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 73, "value": 0.012977777777777783}

:::MLPv0.5.0 ssd 1541710885.780689716 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 74, "value": 0.01315555555555556}

:::MLPv0.5.0 ssd 1541710885.880992651 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 75, "value": 0.013333333333333336}

:::MLPv0.5.0 ssd 1541710885.985962868 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 76, "value": 0.013511111111111113}

:::MLPv0.5.0 ssd 1541710886.083109617 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 77, "value": 0.01368888888888889}

:::MLPv0.5.0 ssd 1541710886.178415060 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 78, "value": 0.013866666666666666}

:::MLPv0.5.0 ssd 1541710886.274379730 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 79, "value": 0.014044444444444443}

:::MLPv0.5.0 ssd 1541710886.371338606 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 80, "value": 0.01422222222222222}
Iteration:     80, Loss function: 9.942, Average Loss: 1.241, avg. samples / sec: 20960.84
Iteration:     80, Loss function: 9.929, Average Loss: 1.237, avg. samples / sec: 20927.21
Iteration:     80, Loss function: 10.711, Average Loss: 1.236, avg. samples / sec: 20912.08
Iteration:     80, Loss function: 10.673, Average Loss: 1.239, avg. samples / sec: 20909.37
Iteration:     80, Loss function: 10.362, Average Loss: 1.242, avg. samples / sec: 20940.76
Iteration:     80, Loss function: 10.163, Average Loss: 1.242, avg. samples / sec: 20892.78
Iteration:     80, Loss function: 10.182, Average Loss: 1.238, avg. samples / sec: 20900.25
Iteration:     80, Loss function: 10.255, Average Loss: 1.232, avg. samples / sec: 20886.94

:::MLPv0.5.0 ssd 1541710886.469321728 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 81, "value": 0.014399999999999996}

:::MLPv0.5.0 ssd 1541710886.569862843 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 82, "value": 0.014577777777777773}

:::MLPv0.5.0 ssd 1541710886.666114330 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 83, "value": 0.01475555555555555}

:::MLPv0.5.0 ssd 1541710886.763667822 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 84, "value": 0.014933333333333326}

:::MLPv0.5.0 ssd 1541710886.860200167 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 85, "value": 0.015111111111111103}

:::MLPv0.5.0 ssd 1541710886.959338188 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 86, "value": 0.01528888888888888}

:::MLPv0.5.0 ssd 1541710887.058312416 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 87, "value": 0.015466666666666656}

:::MLPv0.5.0 ssd 1541710887.153975487 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 88, "value": 0.015644444444444433}

:::MLPv0.5.0 ssd 1541710887.249558687 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 89, "value": 0.01582222222222221}

:::MLPv0.5.0 ssd 1541710887.347265959 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 90, "value": 0.015999999999999986}

:::MLPv0.5.0 ssd 1541710887.442111015 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 91, "value": 0.016177777777777763}

:::MLPv0.5.0 ssd 1541710887.539550304 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 92, "value": 0.01635555555555554}

:::MLPv0.5.0 ssd 1541710887.636803627 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 93, "value": 0.016533333333333317}

:::MLPv0.5.0 ssd 1541710887.733803272 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 94, "value": 0.01671111111111112}

:::MLPv0.5.0 ssd 1541710887.832774878 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 95, "value": 0.016888888888888898}

:::MLPv0.5.0 ssd 1541710887.929866552 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 96, "value": 0.017066666666666674}

:::MLPv0.5.0 ssd 1541710888.026021004 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 97, "value": 0.01724444444444445}

:::MLPv0.5.0 ssd 1541710888.121404409 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 98, "value": 0.017422222222222228}

:::MLPv0.5.0 ssd 1541710888.215540886 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 99, "value": 0.017600000000000005}

:::MLPv0.5.0 ssd 1541710888.315204859 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 100, "value": 0.01777777777777778}
Iteration:    100, Loss function: 9.307, Average Loss: 1.406, avg. samples / sec: 21082.86
Iteration:    100, Loss function: 9.203, Average Loss: 1.409, avg. samples / sec: 21066.73
Iteration:    100, Loss function: 9.110, Average Loss: 1.408, avg. samples / sec: 21084.19
Iteration:    100, Loss function: 9.217, Average Loss: 1.403, avg. samples / sec: 21072.39
Iteration:    100, Loss function: 9.501, Average Loss: 1.405, avg. samples / sec: 21065.61
Iteration:    100, Loss function: 9.151, Average Loss: 1.400, avg. samples / sec: 21092.13
Iteration:    100, Loss function: 9.517, Average Loss: 1.410, avg. samples / sec: 21072.88
Iteration:    100, Loss function: 9.558, Average Loss: 1.405, avg. samples / sec: 21054.72

:::MLPv0.5.0 ssd 1541710888.412533760 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 101, "value": 0.017955555555555558}

:::MLPv0.5.0 ssd 1541710888.511164427 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 102, "value": 0.018133333333333335}

:::MLPv0.5.0 ssd 1541710888.611219645 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 103, "value": 0.01831111111111111}

:::MLPv0.5.0 ssd 1541710888.706288338 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 104, "value": 0.018488888888888888}

:::MLPv0.5.0 ssd 1541710888.813326120 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 105, "value": 0.018666666666666665}

:::MLPv0.5.0 ssd 1541710888.909933090 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 106, "value": 0.01884444444444444}

:::MLPv0.5.0 ssd 1541710889.009655714 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 107, "value": 0.019022222222222218}

:::MLPv0.5.0 ssd 1541710889.105421543 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 108, "value": 0.019199999999999995}

:::MLPv0.5.0 ssd 1541710889.200362682 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 109, "value": 0.01937777777777777}

:::MLPv0.5.0 ssd 1541710889.301750183 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 110, "value": 0.019555555555555548}

:::MLPv0.5.0 ssd 1541710889.396723032 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 111, "value": 0.019733333333333325}

:::MLPv0.5.0 ssd 1541710889.494204998 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 112, "value": 0.0199111111111111}

:::MLPv0.5.0 ssd 1541710889.594353199 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 113, "value": 0.02008888888888888}

:::MLPv0.5.0 ssd 1541710889.691217422 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 114, "value": 0.020266666666666655}

:::MLPv0.5.0 ssd 1541710889.787283659 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 115, "value": 0.020444444444444432}

:::MLPv0.5.0 ssd 1541710889.889347076 (train.py:553) train_epoch: 2

:::MLPv0.5.0 ssd 1541710889.895155907 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 116, "value": 0.02062222222222221}

:::MLPv0.5.0 ssd 1541710889.993479967 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 117, "value": 0.020799999999999985}

:::MLPv0.5.0 ssd 1541710890.091839552 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 118, "value": 0.020977777777777762}

:::MLPv0.5.0 ssd 1541710890.186342716 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 119, "value": 0.02115555555555554}

:::MLPv0.5.0 ssd 1541710890.281358957 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 120, "value": 0.021333333333333343}
Iteration:    120, Loss function: 8.856, Average Loss: 1.554, avg. samples / sec: 20842.82
Iteration:    120, Loss function: 8.828, Average Loss: 1.551, avg. samples / sec: 20843.00
Iteration:    120, Loss function: 8.621, Average Loss: 1.558, avg. samples / sec: 20838.07
Iteration:    120, Loss function: 8.868, Average Loss: 1.555, avg. samples / sec: 20832.91
Iteration:    120, Loss function: 8.626, Average Loss: 1.553, avg. samples / sec: 20836.07
Iteration:    120, Loss function: 8.897, Average Loss: 1.554, avg. samples / sec: 20865.80
Iteration:    120, Loss function: 8.982, Average Loss: 1.559, avg. samples / sec: 20831.99
Iteration:    120, Loss function: 8.469, Average Loss: 1.560, avg. samples / sec: 20839.97

:::MLPv0.5.0 ssd 1541710890.375157833 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 121, "value": 0.02151111111111112}

:::MLPv0.5.0 ssd 1541710890.473575115 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 122, "value": 0.021688888888888896}

:::MLPv0.5.0 ssd 1541710890.567913532 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 123, "value": 0.021866666666666673}

:::MLPv0.5.0 ssd 1541710890.664330244 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 124, "value": 0.02204444444444445}

:::MLPv0.5.0 ssd 1541710890.759821415 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 125, "value": 0.022222222222222227}

:::MLPv0.5.0 ssd 1541710890.856612206 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 126, "value": 0.022400000000000003}

:::MLPv0.5.0 ssd 1541710890.951014996 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 127, "value": 0.02257777777777778}

:::MLPv0.5.0 ssd 1541710891.046839952 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 128, "value": 0.022755555555555557}

:::MLPv0.5.0 ssd 1541710891.147130728 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 129, "value": 0.022933333333333333}

:::MLPv0.5.0 ssd 1541710891.243691921 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 130, "value": 0.02311111111111111}

:::MLPv0.5.0 ssd 1541710891.345855474 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 131, "value": 0.023288888888888887}

:::MLPv0.5.0 ssd 1541710891.441187859 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 132, "value": 0.023466666666666663}

:::MLPv0.5.0 ssd 1541710891.537118196 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 133, "value": 0.02364444444444444}

:::MLPv0.5.0 ssd 1541710891.633197069 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 134, "value": 0.023822222222222217}

:::MLPv0.5.0 ssd 1541710891.726678848 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 135, "value": 0.023999999999999994}

:::MLPv0.5.0 ssd 1541710891.823667765 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 136, "value": 0.02417777777777777}

:::MLPv0.5.0 ssd 1541710891.921441555 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 137, "value": 0.024355555555555547}

:::MLPv0.5.0 ssd 1541710892.019696712 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 138, "value": 0.024533333333333324}

:::MLPv0.5.0 ssd 1541710892.117582560 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 139, "value": 0.0247111111111111}

:::MLPv0.5.0 ssd 1541710892.213911533 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 140, "value": 0.024888888888888877}
Iteration:    140, Loss function: 8.368, Average Loss: 1.699, avg. samples / sec: 21194.65
Iteration:    140, Loss function: 8.788, Average Loss: 1.704, avg. samples / sec: 21198.30
Iteration:    140, Loss function: 8.443, Average Loss: 1.696, avg. samples / sec: 21190.96
Iteration:    140, Loss function: 8.746, Average Loss: 1.698, avg. samples / sec: 21190.50
Iteration:    140, Loss function: 8.235, Average Loss: 1.695, avg. samples / sec: 21193.99
Iteration:    140, Loss function: 8.348, Average Loss: 1.702, avg. samples / sec: 21188.05
Iteration:    140, Loss function: 7.959, Average Loss: 1.698, avg. samples / sec: 21191.30
Iteration:    140, Loss function: 8.810, Average Loss: 1.704, avg. samples / sec: 21187.51

:::MLPv0.5.0 ssd 1541710892.309749365 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 141, "value": 0.025066666666666654}

:::MLPv0.5.0 ssd 1541710892.405452967 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 142, "value": 0.02524444444444443}

:::MLPv0.5.0 ssd 1541710892.503567457 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 143, "value": 0.025422222222222207}

:::MLPv0.5.0 ssd 1541710892.601517200 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 144, "value": 0.025599999999999984}

:::MLPv0.5.0 ssd 1541710892.696192265 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 145, "value": 0.02577777777777779}

:::MLPv0.5.0 ssd 1541710892.790524483 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 146, "value": 0.025955555555555565}

:::MLPv0.5.0 ssd 1541710892.887738705 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 147, "value": 0.026133333333333342}

:::MLPv0.5.0 ssd 1541710892.990006924 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 148, "value": 0.02631111111111112}

:::MLPv0.5.0 ssd 1541710893.083844423 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 149, "value": 0.026488888888888895}

:::MLPv0.5.0 ssd 1541710893.178765059 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 150, "value": 0.026666666666666672}

:::MLPv0.5.0 ssd 1541710893.273284197 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 151, "value": 0.02684444444444445}

:::MLPv0.5.0 ssd 1541710893.369452953 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 152, "value": 0.027022222222222225}

:::MLPv0.5.0 ssd 1541710893.468159437 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 153, "value": 0.027200000000000002}

:::MLPv0.5.0 ssd 1541710893.566550016 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 154, "value": 0.02737777777777778}

:::MLPv0.5.0 ssd 1541710893.665053844 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 155, "value": 0.027555555555555555}

:::MLPv0.5.0 ssd 1541710893.759541035 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 156, "value": 0.027733333333333332}

:::MLPv0.5.0 ssd 1541710893.854603767 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 157, "value": 0.02791111111111111}

:::MLPv0.5.0 ssd 1541710893.949428320 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 158, "value": 0.028088888888888885}

:::MLPv0.5.0 ssd 1541710894.043505669 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 159, "value": 0.028266666666666662}

:::MLPv0.5.0 ssd 1541710894.141818047 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 160, "value": 0.02844444444444444}
Iteration:    160, Loss function: 8.333, Average Loss: 1.835, avg. samples / sec: 21254.88
Iteration:    160, Loss function: 8.646, Average Loss: 1.834, avg. samples / sec: 21247.20
Iteration:    160, Loss function: 8.599, Average Loss: 1.829, avg. samples / sec: 21247.74
Iteration:    160, Loss function: 8.584, Average Loss: 1.830, avg. samples / sec: 21244.29
Iteration:    160, Loss function: 8.802, Average Loss: 1.839, avg. samples / sec: 21242.78
Iteration:    160, Loss function: 8.712, Average Loss: 1.834, avg. samples / sec: 21247.22
Iteration:    160, Loss function: 8.572, Average Loss: 1.838, avg. samples / sec: 21249.88
Iteration:    160, Loss function: 8.651, Average Loss: 1.832, avg. samples / sec: 21226.79

:::MLPv0.5.0 ssd 1541710894.238308668 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 161, "value": 0.028622222222222216}

:::MLPv0.5.0 ssd 1541710894.332693100 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 162, "value": 0.028799999999999992}

:::MLPv0.5.0 ssd 1541710894.428101063 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 163, "value": 0.02897777777777777}

:::MLPv0.5.0 ssd 1541710894.526798010 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 164, "value": 0.029155555555555546}

:::MLPv0.5.0 ssd 1541710894.623184919 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 165, "value": 0.029333333333333322}

:::MLPv0.5.0 ssd 1541710894.718068838 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 166, "value": 0.0295111111111111}

:::MLPv0.5.0 ssd 1541710894.816378355 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 167, "value": 0.029688888888888876}

:::MLPv0.5.0 ssd 1541710894.929867744 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 168, "value": 0.029866666666666652}

:::MLPv0.5.0 ssd 1541710895.030419350 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 169, "value": 0.03004444444444443}

:::MLPv0.5.0 ssd 1541710895.127455950 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 170, "value": 0.030222222222222206}

:::MLPv0.5.0 ssd 1541710895.220474720 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 171, "value": 0.03040000000000001}

:::MLPv0.5.0 ssd 1541710895.315991640 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 172, "value": 0.030577777777777787}

:::MLPv0.5.0 ssd 1541710895.411777020 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 173, "value": 0.030755555555555564}

:::MLPv0.5.0 ssd 1541710895.502860069 (train.py:553) train_epoch: 3

:::MLPv0.5.0 ssd 1541710895.507962942 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 174, "value": 0.03093333333333334}

:::MLPv0.5.0 ssd 1541710895.603746653 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 175, "value": 0.031111111111111117}

:::MLPv0.5.0 ssd 1541710895.699110985 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 176, "value": 0.031288888888888894}

:::MLPv0.5.0 ssd 1541710895.793915033 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 177, "value": 0.03146666666666667}

:::MLPv0.5.0 ssd 1541710895.890691757 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 178, "value": 0.03164444444444445}

:::MLPv0.5.0 ssd 1541710895.988169670 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 179, "value": 0.031822222222222224}

:::MLPv0.5.0 ssd 1541710896.086326122 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 180, "value": 0.032}
Iteration:    180, Loss function: 7.980, Average Loss: 1.960, avg. samples / sec: 21083.09
Iteration:    180, Loss function: 7.928, Average Loss: 1.960, avg. samples / sec: 21062.33
Iteration:    180, Loss function: 8.365, Average Loss: 1.958, avg. samples / sec: 21060.25
Iteration:    180, Loss function: 8.290, Average Loss: 1.963, avg. samples / sec: 21065.06
Iteration:    180, Loss function: 8.619, Average Loss: 1.968, avg. samples / sec: 21061.14
Iteration:    180, Loss function: 8.541, Average Loss: 1.968, avg. samples / sec: 21067.60
Iteration:    180, Loss function: 8.007, Average Loss: 1.965, avg. samples / sec: 21050.86
Iteration:    180, Loss function: 7.812, Average Loss: 1.960, avg. samples / sec: 21049.38

:::MLPv0.5.0 ssd 1541710896.181334019 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 181, "value": 0.03217777777777778}

:::MLPv0.5.0 ssd 1541710896.275496721 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 182, "value": 0.032355555555555554}

:::MLPv0.5.0 ssd 1541710896.371179581 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 183, "value": 0.03253333333333333}

:::MLPv0.5.0 ssd 1541710896.466768980 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 184, "value": 0.03271111111111111}

:::MLPv0.5.0 ssd 1541710896.563128471 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 185, "value": 0.032888888888888884}

:::MLPv0.5.0 ssd 1541710896.657305002 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 186, "value": 0.03306666666666666}

:::MLPv0.5.0 ssd 1541710896.755858898 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 187, "value": 0.03324444444444444}

:::MLPv0.5.0 ssd 1541710896.852486134 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 188, "value": 0.033422222222222214}

:::MLPv0.5.0 ssd 1541710896.947306395 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 189, "value": 0.03359999999999999}

:::MLPv0.5.0 ssd 1541710897.042391300 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 190, "value": 0.03377777777777777}

:::MLPv0.5.0 ssd 1541710897.136786938 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 191, "value": 0.033955555555555544}

:::MLPv0.5.0 ssd 1541710897.232488871 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 192, "value": 0.03413333333333332}

:::MLPv0.5.0 ssd 1541710897.326210499 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 193, "value": 0.0343111111111111}

:::MLPv0.5.0 ssd 1541710897.421534061 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 194, "value": 0.034488888888888874}

:::MLPv0.5.0 ssd 1541710897.519917250 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 195, "value": 0.03466666666666665}

:::MLPv0.5.0 ssd 1541710897.613849163 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 196, "value": 0.03484444444444443}

:::MLPv0.5.0 ssd 1541710897.708127022 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 197, "value": 0.03502222222222222}

:::MLPv0.5.0 ssd 1541710897.807640314 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 198, "value": 0.035199999999999995}

:::MLPv0.5.0 ssd 1541710897.902760983 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 199, "value": 0.03537777777777777}

:::MLPv0.5.0 ssd 1541710897.999110460 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 200, "value": 0.03555555555555555}
Iteration:    200, Loss function: 7.622, Average Loss: 2.081, avg. samples / sec: 21421.88
Iteration:    200, Loss function: 7.947, Average Loss: 2.079, avg. samples / sec: 21427.17
Iteration:    200, Loss function: 8.211, Average Loss: 2.089, avg. samples / sec: 21417.93
Iteration:    200, Loss function: 8.558, Average Loss: 2.090, avg. samples / sec: 21414.68
Iteration:    200, Loss function: 7.869, Average Loss: 2.081, avg. samples / sec: 21411.95
Iteration:    200, Loss function: 7.813, Average Loss: 2.078, avg. samples / sec: 21410.74
Iteration:    200, Loss function: 7.539, Average Loss: 2.080, avg. samples / sec: 21396.48
Iteration:    200, Loss function: 8.210, Average Loss: 2.084, avg. samples / sec: 21387.66

:::MLPv0.5.0 ssd 1541710898.093785763 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 201, "value": 0.035733333333333325}

:::MLPv0.5.0 ssd 1541710898.189277887 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 202, "value": 0.0359111111111111}

:::MLPv0.5.0 ssd 1541710898.284670353 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 203, "value": 0.03608888888888889}

:::MLPv0.5.0 ssd 1541710898.383464813 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 204, "value": 0.03626666666666667}

:::MLPv0.5.0 ssd 1541710898.482628345 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 205, "value": 0.036444444444444446}

:::MLPv0.5.0 ssd 1541710898.577769279 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 206, "value": 0.03662222222222222}

:::MLPv0.5.0 ssd 1541710898.674046516 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 207, "value": 0.0368}

:::MLPv0.5.0 ssd 1541710898.771124363 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 208, "value": 0.036977777777777776}

:::MLPv0.5.0 ssd 1541710898.867793560 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 209, "value": 0.03715555555555555}

:::MLPv0.5.0 ssd 1541710898.967539787 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 210, "value": 0.03733333333333333}

:::MLPv0.5.0 ssd 1541710899.066417456 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 211, "value": 0.037511111111111106}

:::MLPv0.5.0 ssd 1541710899.163733959 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 212, "value": 0.03768888888888888}

:::MLPv0.5.0 ssd 1541710899.257654428 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 213, "value": 0.03786666666666666}

:::MLPv0.5.0 ssd 1541710899.355237484 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 214, "value": 0.038044444444444436}

:::MLPv0.5.0 ssd 1541710899.449135780 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 215, "value": 0.03822222222222221}

:::MLPv0.5.0 ssd 1541710899.544630527 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 216, "value": 0.038400000000000004}

:::MLPv0.5.0 ssd 1541710899.638519764 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 217, "value": 0.03857777777777778}

:::MLPv0.5.0 ssd 1541710899.737127781 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 218, "value": 0.03875555555555556}

:::MLPv0.5.0 ssd 1541710899.832344770 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 219, "value": 0.038933333333333334}

:::MLPv0.5.0 ssd 1541710899.927648306 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 220, "value": 0.03911111111111111}
Iteration:    220, Loss function: 7.786, Average Loss: 2.201, avg. samples / sec: 21244.46
Iteration:    220, Loss function: 7.760, Average Loss: 2.194, avg. samples / sec: 21235.11
Iteration:    220, Loss function: 7.398, Average Loss: 2.191, avg. samples / sec: 21238.49
Iteration:    220, Loss function: 7.675, Average Loss: 2.194, avg. samples / sec: 21256.99
Iteration:    220, Loss function: 7.224, Average Loss: 2.194, avg. samples / sec: 21243.59
Iteration:    220, Loss function: 7.599, Average Loss: 2.190, avg. samples / sec: 21243.51
Iteration:    220, Loss function: 7.576, Average Loss: 2.198, avg. samples / sec: 21271.85
Iteration:    220, Loss function: 7.227, Average Loss: 2.201, avg. samples / sec: 21217.91

:::MLPv0.5.0 ssd 1541710900.026832819 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 221, "value": 0.03928888888888889}

:::MLPv0.5.0 ssd 1541710900.122620821 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 222, "value": 0.039466666666666664}

:::MLPv0.5.0 ssd 1541710900.219758749 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 223, "value": 0.03964444444444444}

:::MLPv0.5.0 ssd 1541710900.314643860 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 224, "value": 0.03982222222222222}

:::MLPv0.5.0 ssd 1541710900.409689188 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 225, "value": 0.039999999999999994}

:::MLPv0.5.0 ssd 1541710900.506746531 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 226, "value": 0.04017777777777777}

:::MLPv0.5.0 ssd 1541710900.601758003 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 227, "value": 0.04035555555555555}

:::MLPv0.5.0 ssd 1541710900.699397326 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 228, "value": 0.04053333333333334}

:::MLPv0.5.0 ssd 1541710900.797246456 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 229, "value": 0.040711111111111115}

:::MLPv0.5.0 ssd 1541710900.891077518 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 230, "value": 0.04088888888888889}

:::MLPv0.5.0 ssd 1541710900.986440897 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 231, "value": 0.04106666666666667}

:::MLPv0.5.0 ssd 1541710901.080283880 (train.py:553) train_epoch: 4

:::MLPv0.5.0 ssd 1541710901.085427284 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 232, "value": 0.041244444444444445}

:::MLPv0.5.0 ssd 1541710901.180873632 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 233, "value": 0.04142222222222222}

:::MLPv0.5.0 ssd 1541710901.274344206 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 234, "value": 0.0416}

:::MLPv0.5.0 ssd 1541710901.370198250 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 235, "value": 0.041777777777777775}

:::MLPv0.5.0 ssd 1541710901.464380503 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 236, "value": 0.04195555555555555}

:::MLPv0.5.0 ssd 1541710901.560052633 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 237, "value": 0.04213333333333333}

:::MLPv0.5.0 ssd 1541710901.657720566 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 238, "value": 0.042311111111111105}

:::MLPv0.5.0 ssd 1541710901.753147125 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 239, "value": 0.04248888888888888}

:::MLPv0.5.0 ssd 1541710901.850010872 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 240, "value": 0.04266666666666666}
Iteration:    240, Loss function: 7.521, Average Loss: 2.304, avg. samples / sec: 21317.57
Iteration:    240, Loss function: 7.676, Average Loss: 2.300, avg. samples / sec: 21307.74
Iteration:    240, Loss function: 7.857, Average Loss: 2.312, avg. samples / sec: 21333.69
Iteration:    240, Loss function: 7.620, Average Loss: 2.307, avg. samples / sec: 21309.29
Iteration:    240, Loss function: 7.438, Average Loss: 2.304, avg. samples / sec: 21304.90
Iteration:    240, Loss function: 7.631, Average Loss: 2.300, avg. samples / sec: 21303.94
Iteration:    240, Loss function: 7.144, Average Loss: 2.307, avg. samples / sec: 21298.78
Iteration:    240, Loss function: 7.444, Average Loss: 2.304, avg. samples / sec: 21301.84

:::MLPv0.5.0 ssd 1541710901.944164515 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 241, "value": 0.04284444444444445}

:::MLPv0.5.0 ssd 1541710902.038129568 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 242, "value": 0.043022222222222226}

:::MLPv0.5.0 ssd 1541710902.133923531 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 243, "value": 0.0432}

:::MLPv0.5.0 ssd 1541710902.229176283 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 244, "value": 0.04337777777777778}

:::MLPv0.5.0 ssd 1541710902.324527025 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 245, "value": 0.043555555555555556}

:::MLPv0.5.0 ssd 1541710902.421969175 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 246, "value": 0.04373333333333333}

:::MLPv0.5.0 ssd 1541710902.519295454 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 247, "value": 0.04391111111111111}

:::MLPv0.5.0 ssd 1541710902.614687204 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 248, "value": 0.044088888888888886}

:::MLPv0.5.0 ssd 1541710902.711098194 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 249, "value": 0.04426666666666666}

:::MLPv0.5.0 ssd 1541710902.809490919 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 250, "value": 0.04444444444444444}

:::MLPv0.5.0 ssd 1541710902.907153606 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 251, "value": 0.044622222222222216}

:::MLPv0.5.0 ssd 1541710903.002574682 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 252, "value": 0.04479999999999999}

:::MLPv0.5.0 ssd 1541710903.097446442 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 253, "value": 0.04497777777777777}

:::MLPv0.5.0 ssd 1541710903.191244602 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 254, "value": 0.04515555555555556}

:::MLPv0.5.0 ssd 1541710903.287468195 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 255, "value": 0.04533333333333334}

:::MLPv0.5.0 ssd 1541710903.381145000 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 256, "value": 0.04551111111111111}

:::MLPv0.5.0 ssd 1541710903.477626562 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 257, "value": 0.04568888888888889}

:::MLPv0.5.0 ssd 1541710903.572257042 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 258, "value": 0.04586666666666667}

:::MLPv0.5.0 ssd 1541710903.665900946 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 259, "value": 0.04604444444444444}

:::MLPv0.5.0 ssd 1541710903.761000156 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 260, "value": 0.04622222222222222}
Iteration:    260, Loss function: 7.369, Average Loss: 2.407, avg. samples / sec: 21443.86
Iteration:    260, Loss function: 7.340, Average Loss: 2.413, avg. samples / sec: 21438.44
Iteration:    260, Loss function: 7.410, Average Loss: 2.406, avg. samples / sec: 21439.58
Iteration:    260, Loss function: 7.326, Average Loss: 2.406, avg. samples / sec: 21436.28
Iteration:    260, Loss function: 7.460, Average Loss: 2.406, avg. samples / sec: 21421.40
Iteration:    260, Loss function: 7.737, Average Loss: 2.400, avg. samples / sec: 21430.34
Iteration:    260, Loss function: 7.022, Average Loss: 2.406, avg. samples / sec: 21435.88
Iteration:    260, Loss function: 6.992, Average Loss: 2.400, avg. samples / sec: 21426.43

:::MLPv0.5.0 ssd 1541710903.863036871 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 261, "value": 0.0464}

:::MLPv0.5.0 ssd 1541710903.960668087 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 262, "value": 0.046577777777777774}

:::MLPv0.5.0 ssd 1541710904.057647705 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 263, "value": 0.04675555555555555}

:::MLPv0.5.0 ssd 1541710904.152087927 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 264, "value": 0.04693333333333333}

:::MLPv0.5.0 ssd 1541710904.247006416 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 265, "value": 0.047111111111111104}

:::MLPv0.5.0 ssd 1541710904.340966225 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 266, "value": 0.04728888888888888}

:::MLPv0.5.0 ssd 1541710904.438706636 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 267, "value": 0.04746666666666667}

:::MLPv0.5.0 ssd 1541710904.532882452 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 268, "value": 0.04764444444444445}

:::MLPv0.5.0 ssd 1541710904.627722263 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 269, "value": 0.047822222222222224}

:::MLPv0.5.0 ssd 1541710904.722732544 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 270, "value": 0.048}

:::MLPv0.5.0 ssd 1541710904.818564177 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 271, "value": 0.04817777777777778}

:::MLPv0.5.0 ssd 1541710904.912899256 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 272, "value": 0.048355555555555554}

:::MLPv0.5.0 ssd 1541710905.010795832 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 273, "value": 0.04853333333333333}

:::MLPv0.5.0 ssd 1541710905.106157780 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 274, "value": 0.04871111111111111}

:::MLPv0.5.0 ssd 1541710905.201345682 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 275, "value": 0.048888888888888885}

:::MLPv0.5.0 ssd 1541710905.294847012 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 276, "value": 0.04906666666666666}

:::MLPv0.5.0 ssd 1541710905.390987873 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 277, "value": 0.04924444444444444}

:::MLPv0.5.0 ssd 1541710905.487539768 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 278, "value": 0.049422222222222215}

:::MLPv0.5.0 ssd 1541710905.584772587 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 279, "value": 0.04959999999999999}

:::MLPv0.5.0 ssd 1541710905.678636551 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 280, "value": 0.04977777777777778}
Iteration:    280, Loss function: 7.230, Average Loss: 2.516, avg. samples / sec: 21361.69
Iteration:    280, Loss function: 7.483, Average Loss: 2.503, avg. samples / sec: 21365.91
Iteration:    280, Loss function: 7.547, Average Loss: 2.508, avg. samples / sec: 21365.05
Iteration:    280, Loss function: 7.836, Average Loss: 2.506, avg. samples / sec: 21373.19
Iteration:    280, Loss function: 7.570, Average Loss: 2.510, avg. samples / sec: 21353.81
Iteration:    280, Loss function: 7.254, Average Loss: 2.507, avg. samples / sec: 21354.25
Iteration:    280, Loss function: 7.878, Average Loss: 2.510, avg. samples / sec: 21353.32
Iteration:    280, Loss function: 7.866, Average Loss: 2.510, avg. samples / sec: 21342.50

:::MLPv0.5.0 ssd 1541710905.776854515 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 281, "value": 0.04995555555555556}

:::MLPv0.5.0 ssd 1541710905.872350931 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 282, "value": 0.050133333333333335}

:::MLPv0.5.0 ssd 1541710905.969002008 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 283, "value": 0.05031111111111111}

:::MLPv0.5.0 ssd 1541710906.066590309 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 284, "value": 0.05048888888888889}

:::MLPv0.5.0 ssd 1541710906.160760880 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 285, "value": 0.050666666666666665}

:::MLPv0.5.0 ssd 1541710906.256274462 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 286, "value": 0.05084444444444444}

:::MLPv0.5.0 ssd 1541710906.351268053 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 287, "value": 0.05102222222222222}

:::MLPv0.5.0 ssd 1541710906.455078363 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 288, "value": 0.051199999999999996}

:::MLPv0.5.0 ssd 1541710906.547384024 (train.py:553) train_epoch: 5

:::MLPv0.5.0 ssd 1541710906.552973986 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 289, "value": 0.05137777777777777}

:::MLPv0.5.0 ssd 1541710906.646891832 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 290, "value": 0.05155555555555555}

:::MLPv0.5.0 ssd 1541710906.745372772 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 291, "value": 0.051733333333333326}

:::MLPv0.5.0 ssd 1541710906.840166569 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 292, "value": 0.0519111111111111}

:::MLPv0.5.0 ssd 1541710906.934658766 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 293, "value": 0.05208888888888889}

:::MLPv0.5.0 ssd 1541710907.029946089 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 294, "value": 0.05226666666666667}

:::MLPv0.5.0 ssd 1541710907.127367258 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 295, "value": 0.052444444444444446}

:::MLPv0.5.0 ssd 1541710907.222595930 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 296, "value": 0.05262222222222222}

:::MLPv0.5.0 ssd 1541710907.318559408 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 297, "value": 0.0528}

:::MLPv0.5.0 ssd 1541710907.413061142 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 298, "value": 0.052977777777777776}

:::MLPv0.5.0 ssd 1541710907.510722637 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 299, "value": 0.05315555555555555}

:::MLPv0.5.0 ssd 1541710907.608883142 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 300, "value": 0.05333333333333333}
Iteration:    300, Loss function: 6.979, Average Loss: 2.599, avg. samples / sec: 21229.39
Iteration:    300, Loss function: 7.406, Average Loss: 2.605, avg. samples / sec: 21229.55
Iteration:    300, Loss function: 6.973, Average Loss: 2.608, avg. samples / sec: 21219.03
Iteration:    300, Loss function: 6.717, Average Loss: 2.603, avg. samples / sec: 21219.34
Iteration:    300, Loss function: 6.928, Average Loss: 2.602, avg. samples / sec: 21221.57
Iteration:    300, Loss function: 6.958, Average Loss: 2.605, avg. samples / sec: 21237.08
Iteration:    300, Loss function: 6.857, Average Loss: 2.602, avg. samples / sec: 21217.18
Iteration:    300, Loss function: 6.472, Average Loss: 2.605, avg. samples / sec: 21217.52

:::MLPv0.5.0 ssd 1541710907.704123020 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 301, "value": 0.053511111111111107}

:::MLPv0.5.0 ssd 1541710907.800801277 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 302, "value": 0.05368888888888888}

:::MLPv0.5.0 ssd 1541710907.895596027 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 303, "value": 0.05386666666666666}

:::MLPv0.5.0 ssd 1541710907.994646549 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 304, "value": 0.05404444444444444}

:::MLPv0.5.0 ssd 1541710908.090159893 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 305, "value": 0.05422222222222223}

:::MLPv0.5.0 ssd 1541710908.183977365 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 306, "value": 0.054400000000000004}

:::MLPv0.5.0 ssd 1541710908.280631065 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 307, "value": 0.05457777777777778}

:::MLPv0.5.0 ssd 1541710908.376135349 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 308, "value": 0.05475555555555556}

:::MLPv0.5.0 ssd 1541710908.471699953 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 309, "value": 0.054933333333333334}

:::MLPv0.5.0 ssd 1541710908.565668583 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 310, "value": 0.05511111111111111}

:::MLPv0.5.0 ssd 1541710908.659954071 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 311, "value": 0.05528888888888889}

:::MLPv0.5.0 ssd 1541710908.756420851 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 312, "value": 0.055466666666666664}

:::MLPv0.5.0 ssd 1541710908.853261471 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 313, "value": 0.05564444444444444}

:::MLPv0.5.0 ssd 1541710908.947193384 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 314, "value": 0.05582222222222222}

:::MLPv0.5.0 ssd 1541710909.042901993 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 315, "value": 0.055999999999999994}

:::MLPv0.5.0 ssd 1541710909.139193058 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 316, "value": 0.05617777777777777}

:::MLPv0.5.0 ssd 1541710909.233822823 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 317, "value": 0.05635555555555555}

:::MLPv0.5.0 ssd 1541710909.330163002 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 318, "value": 0.05653333333333334}

:::MLPv0.5.0 ssd 1541710909.426349401 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 319, "value": 0.056711111111111115}

:::MLPv0.5.0 ssd 1541710909.521880865 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 320, "value": 0.05688888888888889}
Iteration:    320, Loss function: 6.583, Average Loss: 2.685, avg. samples / sec: 21409.68
Iteration:    320, Loss function: 7.342, Average Loss: 2.688, avg. samples / sec: 21428.70
Iteration:    320, Loss function: 7.180, Average Loss: 2.689, avg. samples / sec: 21417.62
Iteration:    320, Loss function: 7.206, Average Loss: 2.691, avg. samples / sec: 21421.29
Iteration:    320, Loss function: 7.411, Average Loss: 2.694, avg. samples / sec: 21411.94
Iteration:    320, Loss function: 6.804, Average Loss: 2.690, avg. samples / sec: 21408.04
Iteration:    320, Loss function: 7.383, Average Loss: 2.688, avg. samples / sec: 21411.63
Iteration:    320, Loss function: 7.717, Average Loss: 2.691, avg. samples / sec: 21419.30

:::MLPv0.5.0 ssd 1541710909.616299868 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 321, "value": 0.05706666666666667}

:::MLPv0.5.0 ssd 1541710909.722268820 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 322, "value": 0.057244444444444445}

:::MLPv0.5.0 ssd 1541710909.817529678 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 323, "value": 0.05742222222222222}

:::MLPv0.5.0 ssd 1541710909.912186146 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 324, "value": 0.0576}

:::MLPv0.5.0 ssd 1541710910.005571127 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 325, "value": 0.057777777777777775}

:::MLPv0.5.0 ssd 1541710910.100258589 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 326, "value": 0.05795555555555555}

:::MLPv0.5.0 ssd 1541710910.199265718 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 327, "value": 0.05813333333333333}

:::MLPv0.5.0 ssd 1541710910.293969393 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 328, "value": 0.058311111111111105}

:::MLPv0.5.0 ssd 1541710910.392107725 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 329, "value": 0.05848888888888888}

:::MLPv0.5.0 ssd 1541710910.486732721 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 330, "value": 0.05866666666666666}

:::MLPv0.5.0 ssd 1541710910.580536604 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 331, "value": 0.05884444444444445}

:::MLPv0.5.0 ssd 1541710910.675876856 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 332, "value": 0.059022222222222226}

:::MLPv0.5.0 ssd 1541710910.773341894 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 333, "value": 0.0592}

:::MLPv0.5.0 ssd 1541710910.869445086 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 334, "value": 0.05937777777777778}

:::MLPv0.5.0 ssd 1541710910.962995291 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 335, "value": 0.059555555555555556}

:::MLPv0.5.0 ssd 1541710911.057178020 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 336, "value": 0.05973333333333333}

:::MLPv0.5.0 ssd 1541710911.151892900 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 337, "value": 0.05991111111111111}

:::MLPv0.5.0 ssd 1541710911.249584436 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 338, "value": 0.060088888888888886}

:::MLPv0.5.0 ssd 1541710911.343620062 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 339, "value": 0.06026666666666666}

:::MLPv0.5.0 ssd 1541710911.439021111 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 340, "value": 0.06044444444444444}
Iteration:    340, Loss function: 7.062, Average Loss: 2.775, avg. samples / sec: 21366.04
Iteration:    340, Loss function: 6.816, Average Loss: 2.775, avg. samples / sec: 21362.91
Iteration:    340, Loss function: 7.073, Average Loss: 2.780, avg. samples / sec: 21363.02
Iteration:    340, Loss function: 7.069, Average Loss: 2.779, avg. samples / sec: 21367.07
Iteration:    340, Loss function: 7.467, Average Loss: 2.778, avg. samples / sec: 21363.04
Iteration:    340, Loss function: 7.197, Average Loss: 2.780, avg. samples / sec: 21364.50
Iteration:    340, Loss function: 6.655, Average Loss: 2.775, avg. samples / sec: 21361.69
Iteration:    340, Loss function: 7.149, Average Loss: 2.784, avg. samples / sec: 21358.14

:::MLPv0.5.0 ssd 1541710911.533149242 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 341, "value": 0.060622222222222216}

:::MLPv0.5.0 ssd 1541710911.627885580 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 342, "value": 0.06079999999999999}

:::MLPv0.5.0 ssd 1541710911.722063780 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 343, "value": 0.06097777777777777}

:::MLPv0.5.0 ssd 1541710911.816139221 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 344, "value": 0.06115555555555556}

:::MLPv0.5.0 ssd 1541710911.913936853 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 345, "value": 0.06133333333333334}

:::MLPv0.5.0 ssd 1541710912.009820223 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 346, "value": 0.061511111111111114}

:::MLPv0.5.0 ssd 1541710912.100684404 (train.py:553) train_epoch: 6

:::MLPv0.5.0 ssd 1541710912.105888605 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 347, "value": 0.06168888888888889}

:::MLPv0.5.0 ssd 1541710912.200470209 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 348, "value": 0.06186666666666667}

:::MLPv0.5.0 ssd 1541710912.294947624 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 349, "value": 0.062044444444444444}

:::MLPv0.5.0 ssd 1541710912.391548872 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 350, "value": 0.06222222222222222}

:::MLPv0.5.0 ssd 1541710912.486856461 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 351, "value": 0.0624}

:::MLPv0.5.0 ssd 1541710912.583189964 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 352, "value": 0.06257777777777777}

:::MLPv0.5.0 ssd 1541710912.677427530 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 353, "value": 0.06275555555555555}

:::MLPv0.5.0 ssd 1541710912.771106958 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 354, "value": 0.06293333333333333}

:::MLPv0.5.0 ssd 1541710912.871452093 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 355, "value": 0.0631111111111111}

:::MLPv0.5.0 ssd 1541710912.966259956 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 356, "value": 0.0632888888888889}

:::MLPv0.5.0 ssd 1541710913.063616514 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 357, "value": 0.06346666666666667}

:::MLPv0.5.0 ssd 1541710913.159375668 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 358, "value": 0.06364444444444445}

:::MLPv0.5.0 ssd 1541710913.253973961 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 359, "value": 0.06382222222222222}

:::MLPv0.5.0 ssd 1541710913.348280668 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 360, "value": 0.064}
Iteration:    360, Loss function: 6.715, Average Loss: 2.860, avg. samples / sec: 21458.38
Iteration:    360, Loss function: 5.939, Average Loss: 2.852, avg. samples / sec: 21467.21
Iteration:    360, Loss function: 7.176, Average Loss: 2.858, avg. samples / sec: 21459.53
Iteration:    360, Loss function: 6.293, Average Loss: 2.852, avg. samples / sec: 21450.05
Iteration:    360, Loss function: 7.064, Average Loss: 2.859, avg. samples / sec: 21452.76
Iteration:    360, Loss function: 6.659, Average Loss: 2.862, avg. samples / sec: 21459.49
Iteration:    360, Loss function: 7.100, Average Loss: 2.853, avg. samples / sec: 21441.92
Iteration:    360, Loss function: 6.576, Average Loss: 2.858, avg. samples / sec: 21433.16

:::MLPv0.5.0 ssd 1541710913.441564083 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 361, "value": 0.06417777777777778}

:::MLPv0.5.0 ssd 1541710913.536242723 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 362, "value": 0.06435555555555555}

:::MLPv0.5.0 ssd 1541710913.631265163 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 363, "value": 0.06453333333333333}

:::MLPv0.5.0 ssd 1541710913.725799322 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 364, "value": 0.06471111111111111}

:::MLPv0.5.0 ssd 1541710913.820441246 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 365, "value": 0.06488888888888888}

:::MLPv0.5.0 ssd 1541710913.914372921 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 366, "value": 0.06506666666666666}

:::MLPv0.5.0 ssd 1541710914.013648033 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 367, "value": 0.06524444444444444}

:::MLPv0.5.0 ssd 1541710914.107629061 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 368, "value": 0.06542222222222221}

:::MLPv0.5.0 ssd 1541710914.203360319 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 369, "value": 0.0656}

:::MLPv0.5.0 ssd 1541710914.297943592 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 370, "value": 0.06577777777777778}

:::MLPv0.5.0 ssd 1541710914.394040585 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 371, "value": 0.06595555555555556}

:::MLPv0.5.0 ssd 1541710914.488206148 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 372, "value": 0.06613333333333334}

:::MLPv0.5.0 ssd 1541710914.584027767 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 373, "value": 0.06631111111111111}

:::MLPv0.5.0 ssd 1541710914.680299044 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 374, "value": 0.06648888888888889}

:::MLPv0.5.0 ssd 1541710914.774807692 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 375, "value": 0.06666666666666667}

:::MLPv0.5.0 ssd 1541710914.869482279 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 376, "value": 0.06684444444444444}

:::MLPv0.5.0 ssd 1541710914.972748041 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 377, "value": 0.06702222222222222}

:::MLPv0.5.0 ssd 1541710915.068034649 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 378, "value": 0.0672}

:::MLPv0.5.0 ssd 1541710915.162358046 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 379, "value": 0.06737777777777777}

:::MLPv0.5.0 ssd 1541710915.258644819 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 380, "value": 0.06755555555555555}
Iteration:    380, Loss function: 6.359, Average Loss: 2.933, avg. samples / sec: 21444.81
Iteration:    380, Loss function: 6.814, Average Loss: 2.925, avg. samples / sec: 21440.99
Iteration:    380, Loss function: 6.704, Average Loss: 2.926, avg. samples / sec: 21447.39
Iteration:    380, Loss function: 6.718, Average Loss: 2.926, avg. samples / sec: 21447.67
Iteration:    380, Loss function: 6.277, Average Loss: 2.935, avg. samples / sec: 21442.09
Iteration:    380, Loss function: 6.721, Average Loss: 2.932, avg. samples / sec: 21440.24
Iteration:    380, Loss function: 6.452, Average Loss: 2.929, avg. samples / sec: 21460.88
Iteration:    380, Loss function: 6.760, Average Loss: 2.934, avg. samples / sec: 21436.03

:::MLPv0.5.0 ssd 1541710915.356873035 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 381, "value": 0.06773333333333333}

:::MLPv0.5.0 ssd 1541710915.450919390 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 382, "value": 0.06791111111111112}

:::MLPv0.5.0 ssd 1541710915.544649363 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 383, "value": 0.0680888888888889}

:::MLPv0.5.0 ssd 1541710915.639046907 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 384, "value": 0.06826666666666667}

:::MLPv0.5.0 ssd 1541710915.732882023 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 385, "value": 0.06844444444444445}

:::MLPv0.5.0 ssd 1541710915.826473236 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 386, "value": 0.06862222222222222}

:::MLPv0.5.0 ssd 1541710915.922297001 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 387, "value": 0.0688}

:::MLPv0.5.0 ssd 1541710916.018293858 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 388, "value": 0.06897777777777778}

:::MLPv0.5.0 ssd 1541710916.112260818 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 389, "value": 0.06915555555555555}

:::MLPv0.5.0 ssd 1541710916.206524611 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 390, "value": 0.06933333333333333}

:::MLPv0.5.0 ssd 1541710916.303081512 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 391, "value": 0.0695111111111111}

:::MLPv0.5.0 ssd 1541710916.397445917 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 392, "value": 0.06968888888888888}

:::MLPv0.5.0 ssd 1541710916.494524717 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 393, "value": 0.06986666666666666}

:::MLPv0.5.0 ssd 1541710916.591099501 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 394, "value": 0.07004444444444444}

:::MLPv0.5.0 ssd 1541710916.684736729 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 395, "value": 0.07022222222222223}

:::MLPv0.5.0 ssd 1541710916.782062054 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 396, "value": 0.0704}

:::MLPv0.5.0 ssd 1541710916.875977755 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 397, "value": 0.07057777777777778}

:::MLPv0.5.0 ssd 1541710916.974925756 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 398, "value": 0.07075555555555556}

:::MLPv0.5.0 ssd 1541710917.069436073 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 399, "value": 0.07093333333333333}

:::MLPv0.5.0 ssd 1541710917.165666819 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 400, "value": 0.07111111111111111}
Iteration:    400, Loss function: 6.735, Average Loss: 3.010, avg. samples / sec: 21494.63
Iteration:    400, Loss function: 6.467, Average Loss: 3.009, avg. samples / sec: 21477.63
Iteration:    400, Loss function: 6.230, Average Loss: 3.002, avg. samples / sec: 21482.32
Iteration:    400, Loss function: 6.848, Average Loss: 3.004, avg. samples / sec: 21477.81
Iteration:    400, Loss function: 6.589, Average Loss: 3.000, avg. samples / sec: 21474.16
Iteration:    400, Loss function: 5.997, Average Loss: 3.011, avg. samples / sec: 21478.33
Iteration:    400, Loss function: 6.751, Average Loss: 3.007, avg. samples / sec: 21478.27
Iteration:    400, Loss function: 6.605, Average Loss: 3.011, avg. samples / sec: 21476.35

:::MLPv0.5.0 ssd 1541710917.261942387 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 401, "value": 0.07128888888888889}

:::MLPv0.5.0 ssd 1541710917.355936766 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 402, "value": 0.07146666666666666}

:::MLPv0.5.0 ssd 1541710917.450031281 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 403, "value": 0.07164444444444444}

:::MLPv0.5.0 ssd 1541710917.544946909 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 404, "value": 0.07182222222222222}

:::MLPv0.5.0 ssd 1541710917.635794640 (train.py:553) train_epoch: 7

:::MLPv0.5.0 ssd 1541710917.641010523 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 405, "value": 0.072}

:::MLPv0.5.0 ssd 1541710917.736017942 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 406, "value": 0.07217777777777777}

:::MLPv0.5.0 ssd 1541710917.835051060 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 407, "value": 0.07235555555555555}

:::MLPv0.5.0 ssd 1541710917.928353310 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 408, "value": 0.07253333333333334}

:::MLPv0.5.0 ssd 1541710918.022561550 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 409, "value": 0.07271111111111112}

:::MLPv0.5.0 ssd 1541710918.118964911 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 410, "value": 0.07288888888888889}

:::MLPv0.5.0 ssd 1541710918.212857246 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 411, "value": 0.07306666666666667}

:::MLPv0.5.0 ssd 1541710918.308596134 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 412, "value": 0.07324444444444445}

:::MLPv0.5.0 ssd 1541710918.404077530 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 413, "value": 0.07342222222222222}

:::MLPv0.5.0 ssd 1541710918.498600721 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 414, "value": 0.0736}

:::MLPv0.5.0 ssd 1541710918.592321396 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 415, "value": 0.07377777777777778}

:::MLPv0.5.0 ssd 1541710918.686587811 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 416, "value": 0.07395555555555555}

:::MLPv0.5.0 ssd 1541710918.781458139 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 417, "value": 0.07413333333333333}

:::MLPv0.5.0 ssd 1541710918.875633001 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 418, "value": 0.0743111111111111}

:::MLPv0.5.0 ssd 1541710918.968393326 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 419, "value": 0.07448888888888888}

:::MLPv0.5.0 ssd 1541710919.063189507 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 420, "value": 0.07466666666666666}
Iteration:    420, Loss function: 6.395, Average Loss: 3.079, avg. samples / sec: 21589.00
Iteration:    420, Loss function: 6.152, Average Loss: 3.075, avg. samples / sec: 21586.54
Iteration:    420, Loss function: 6.211, Average Loss: 3.069, avg. samples / sec: 21583.05
Iteration:    420, Loss function: 6.189, Average Loss: 3.079, avg. samples / sec: 21589.14
Iteration:    420, Loss function: 5.791, Average Loss: 3.074, avg. samples / sec: 21589.68
Iteration:    420, Loss function: 6.112, Average Loss: 3.075, avg. samples / sec: 21582.33
Iteration:    420, Loss function: 6.258, Average Loss: 3.069, avg. samples / sec: 21580.77
Iteration:    420, Loss function: 5.906, Average Loss: 3.078, avg. samples / sec: 21585.19

:::MLPv0.5.0 ssd 1541710919.157081127 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 421, "value": 0.07484444444444445}

:::MLPv0.5.0 ssd 1541710919.250575542 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 422, "value": 0.07502222222222223}

:::MLPv0.5.0 ssd 1541710919.343938589 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 423, "value": 0.0752}

:::MLPv0.5.0 ssd 1541710919.440640926 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 424, "value": 0.07537777777777778}

:::MLPv0.5.0 ssd 1541710919.538259268 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 425, "value": 0.07555555555555556}

:::MLPv0.5.0 ssd 1541710919.638306618 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 426, "value": 0.07573333333333333}

:::MLPv0.5.0 ssd 1541710919.732511044 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 427, "value": 0.07591111111111111}

:::MLPv0.5.0 ssd 1541710919.826775551 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 428, "value": 0.07608888888888889}

:::MLPv0.5.0 ssd 1541710919.922028303 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 429, "value": 0.07626666666666666}

:::MLPv0.5.0 ssd 1541710920.018865108 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 430, "value": 0.07644444444444444}

:::MLPv0.5.0 ssd 1541710920.113237143 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 431, "value": 0.07662222222222222}

:::MLPv0.5.0 ssd 1541710920.207894802 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 432, "value": 0.0768}

:::MLPv0.5.0 ssd 1541710920.303086042 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 433, "value": 0.07697777777777778}

:::MLPv0.5.0 ssd 1541710920.397215605 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 434, "value": 0.07715555555555556}

:::MLPv0.5.0 ssd 1541710920.491241932 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 435, "value": 0.07733333333333334}

:::MLPv0.5.0 ssd 1541710920.585216522 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 436, "value": 0.07751111111111111}

:::MLPv0.5.0 ssd 1541710920.679120064 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 437, "value": 0.07768888888888889}

:::MLPv0.5.0 ssd 1541710920.773186207 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 438, "value": 0.07786666666666667}

:::MLPv0.5.0 ssd 1541710920.869159460 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 439, "value": 0.07804444444444444}

:::MLPv0.5.0 ssd 1541710920.966059685 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 440, "value": 0.07822222222222222}
Iteration:    440, Loss function: 8.136, Average Loss: 3.147, avg. samples / sec: 21528.34
Iteration:    440, Loss function: 7.784, Average Loss: 3.144, avg. samples / sec: 21534.95
Iteration:    440, Loss function: 8.191, Average Loss: 3.139, avg. samples / sec: 21537.79
Iteration:    440, Loss function: 8.336, Average Loss: 3.150, avg. samples / sec: 21521.61
Iteration:    440, Loss function: 7.848, Average Loss: 3.140, avg. samples / sec: 21528.64
Iteration:    440, Loss function: 7.837, Average Loss: 3.148, avg. samples / sec: 21526.67
Iteration:    440, Loss function: 7.914, Average Loss: 3.144, avg. samples / sec: 21525.12
Iteration:    440, Loss function: 7.945, Average Loss: 3.149, avg. samples / sec: 21526.75

:::MLPv0.5.0 ssd 1541710921.060150623 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 441, "value": 0.0784}

:::MLPv0.5.0 ssd 1541710921.154098034 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 442, "value": 0.07857777777777777}

:::MLPv0.5.0 ssd 1541710921.249177933 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 443, "value": 0.07875555555555555}

:::MLPv0.5.0 ssd 1541710921.343950033 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 444, "value": 0.07893333333333333}

:::MLPv0.5.0 ssd 1541710921.447522163 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 445, "value": 0.0791111111111111}

:::MLPv0.5.0 ssd 1541710921.549727678 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 446, "value": 0.0792888888888889}

:::MLPv0.5.0 ssd 1541710921.644676208 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 447, "value": 0.07946666666666667}

:::MLPv0.5.0 ssd 1541710921.738781929 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 448, "value": 0.07964444444444445}

:::MLPv0.5.0 ssd 1541710921.831761360 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 449, "value": 0.07982222222222222}

:::MLPv0.5.0 ssd 1541710921.926025629 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 450, "value": 0.08}

:::MLPv0.5.0 ssd 1541710922.019567966 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 451, "value": 0.08017777777777778}

:::MLPv0.5.0 ssd 1541710922.115580082 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 452, "value": 0.08035555555555556}

:::MLPv0.5.0 ssd 1541710922.209175825 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 453, "value": 0.08053333333333333}

:::MLPv0.5.0 ssd 1541710922.302687168 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 454, "value": 0.08071111111111111}

:::MLPv0.5.0 ssd 1541710922.395815134 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 455, "value": 0.08088888888888889}

:::MLPv0.5.0 ssd 1541710922.489603281 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 456, "value": 0.08106666666666666}

:::MLPv0.5.0 ssd 1541710922.583685398 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 457, "value": 0.08124444444444444}

:::MLPv0.5.0 ssd 1541710922.678167105 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 458, "value": 0.08142222222222222}

:::MLPv0.5.0 ssd 1541710922.772100210 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 459, "value": 0.0816}

:::MLPv0.5.0 ssd 1541710922.866894960 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 460, "value": 0.08177777777777778}
Iteration:    460, Loss function: 6.139, Average Loss: 3.222, avg. samples / sec: 21557.29
Iteration:    460, Loss function: 6.699, Average Loss: 3.217, avg. samples / sec: 21547.13
Iteration:    460, Loss function: 6.480, Average Loss: 3.221, avg. samples / sec: 21546.21
Iteration:    460, Loss function: 6.400, Average Loss: 3.225, avg. samples / sec: 21543.14
Iteration:    460, Loss function: 6.304, Average Loss: 3.229, avg. samples / sec: 21546.59
Iteration:    460, Loss function: 7.035, Average Loss: 3.219, avg. samples / sec: 21547.58
Iteration:    460, Loss function: 6.396, Average Loss: 3.225, avg. samples / sec: 21547.43
Iteration:    460, Loss function: 6.464, Average Loss: 3.228, avg. samples / sec: 21551.70

:::MLPv0.5.0 ssd 1541710922.960997581 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 461, "value": 0.08195555555555556}

:::MLPv0.5.0 ssd 1541710923.055613756 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 462, "value": 0.08213333333333334}

:::MLPv0.5.0 ssd 1541710923.146907091 (train.py:553) train_epoch: 8

:::MLPv0.5.0 ssd 1541710923.152364254 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 463, "value": 0.08231111111111111}

:::MLPv0.5.0 ssd 1541710923.246401072 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 464, "value": 0.08248888888888889}

:::MLPv0.5.0 ssd 1541710923.341231823 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 465, "value": 0.08266666666666667}

:::MLPv0.5.0 ssd 1541710923.434705257 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 466, "value": 0.08284444444444444}

:::MLPv0.5.0 ssd 1541710923.535493374 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 467, "value": 0.08302222222222222}

:::MLPv0.5.0 ssd 1541710923.630363703 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 468, "value": 0.0832}

:::MLPv0.5.0 ssd 1541710923.725122213 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 469, "value": 0.08337777777777777}

:::MLPv0.5.0 ssd 1541710923.820984364 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 470, "value": 0.08355555555555555}

:::MLPv0.5.0 ssd 1541710923.916193008 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 471, "value": 0.08373333333333333}

:::MLPv0.5.0 ssd 1541710924.010298967 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 472, "value": 0.08391111111111112}

:::MLPv0.5.0 ssd 1541710924.103684902 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 473, "value": 0.0840888888888889}

:::MLPv0.5.0 ssd 1541710924.197523355 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 474, "value": 0.08426666666666667}

:::MLPv0.5.0 ssd 1541710924.292842150 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 475, "value": 0.08444444444444445}

:::MLPv0.5.0 ssd 1541710924.387057781 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 476, "value": 0.08462222222222222}

:::MLPv0.5.0 ssd 1541710924.481316328 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 477, "value": 0.0848}

:::MLPv0.5.0 ssd 1541710924.580134153 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 478, "value": 0.08497777777777778}

:::MLPv0.5.0 ssd 1541710924.674377441 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 479, "value": 0.08515555555555555}

:::MLPv0.5.0 ssd 1541710924.771751404 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 480, "value": 0.08533333333333333}
Iteration:    480, Loss function: 6.416, Average Loss: 3.285, avg. samples / sec: 21506.90
Iteration:    480, Loss function: 5.926, Average Loss: 3.290, avg. samples / sec: 21504.08
Iteration:    480, Loss function: 6.345, Average Loss: 3.279, avg. samples / sec: 21503.86
Iteration:    480, Loss function: 6.343, Average Loss: 3.287, avg. samples / sec: 21506.41
Iteration:    480, Loss function: 5.734, Average Loss: 3.289, avg. samples / sec: 21507.58
Iteration:    480, Loss function: 5.883, Average Loss: 3.283, avg. samples / sec: 21497.07
Iteration:    480, Loss function: 6.369, Average Loss: 3.281, avg. samples / sec: 21500.09
Iteration:    480, Loss function: 6.476, Average Loss: 3.279, avg. samples / sec: 21498.42

:::MLPv0.5.0 ssd 1541710924.865611315 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 481, "value": 0.08551111111111111}

:::MLPv0.5.0 ssd 1541710924.959616184 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 482, "value": 0.08568888888888888}

:::MLPv0.5.0 ssd 1541710925.054077387 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 483, "value": 0.08586666666666666}

:::MLPv0.5.0 ssd 1541710925.148310184 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 484, "value": 0.08604444444444445}

:::MLPv0.5.0 ssd 1541710925.241711140 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 485, "value": 0.08622222222222223}

:::MLPv0.5.0 ssd 1541710925.336690426 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 486, "value": 0.0864}

:::MLPv0.5.0 ssd 1541710925.431854248 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 487, "value": 0.08657777777777778}

:::MLPv0.5.0 ssd 1541710925.525897980 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 488, "value": 0.08675555555555556}

:::MLPv0.5.0 ssd 1541710925.619695425 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 489, "value": 0.08693333333333333}

:::MLPv0.5.0 ssd 1541710925.713719606 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 490, "value": 0.08711111111111111}

:::MLPv0.5.0 ssd 1541710925.808912039 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 491, "value": 0.08728888888888889}

:::MLPv0.5.0 ssd 1541710925.903249502 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 492, "value": 0.08746666666666666}

:::MLPv0.5.0 ssd 1541710925.997017384 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 493, "value": 0.08764444444444444}

:::MLPv0.5.0 ssd 1541710926.091145277 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 494, "value": 0.08782222222222222}

:::MLPv0.5.0 ssd 1541710926.186620951 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 495, "value": 0.088}

:::MLPv0.5.0 ssd 1541710926.280845881 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 496, "value": 0.08817777777777777}

:::MLPv0.5.0 ssd 1541710926.374811411 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 497, "value": 0.08835555555555556}

:::MLPv0.5.0 ssd 1541710926.469064713 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 498, "value": 0.08853333333333334}

:::MLPv0.5.0 ssd 1541710926.562698603 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 499, "value": 0.08871111111111112}

:::MLPv0.5.0 ssd 1541710926.658285141 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 500, "value": 0.08888888888888889}
Iteration:    500, Loss function: 5.898, Average Loss: 3.339, avg. samples / sec: 21714.50
Iteration:    500, Loss function: 5.887, Average Loss: 3.339, avg. samples / sec: 21715.12
Iteration:    500, Loss function: 5.981, Average Loss: 3.344, avg. samples / sec: 21714.79
Iteration:    500, Loss function: 5.854, Average Loss: 3.334, avg. samples / sec: 21713.04
Iteration:    500, Loss function: 6.003, Average Loss: 3.344, avg. samples / sec: 21708.72
Iteration:    500, Loss function: 5.984, Average Loss: 3.338, avg. samples / sec: 21712.11
Iteration:    500, Loss function: 6.552, Average Loss: 3.334, avg. samples / sec: 21710.41
Iteration:    500, Loss function: 6.034, Average Loss: 3.342, avg. samples / sec: 21701.38

:::MLPv0.5.0 ssd 1541710926.753085613 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 501, "value": 0.08906666666666667}

:::MLPv0.5.0 ssd 1541710926.846617937 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 502, "value": 0.08924444444444445}

:::MLPv0.5.0 ssd 1541710926.940133572 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 503, "value": 0.08942222222222222}

:::MLPv0.5.0 ssd 1541710927.036621571 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 504, "value": 0.0896}

:::MLPv0.5.0 ssd 1541710927.131599903 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 505, "value": 0.08977777777777778}

:::MLPv0.5.0 ssd 1541710927.225679159 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 506, "value": 0.08995555555555555}

:::MLPv0.5.0 ssd 1541710927.319652796 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 507, "value": 0.09013333333333333}

:::MLPv0.5.0 ssd 1541710927.413992643 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 508, "value": 0.0903111111111111}

:::MLPv0.5.0 ssd 1541710927.507658958 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 509, "value": 0.09048888888888888}

:::MLPv0.5.0 ssd 1541710927.601216316 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 510, "value": 0.09066666666666667}

:::MLPv0.5.0 ssd 1541710927.696806669 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 511, "value": 0.09084444444444445}

:::MLPv0.5.0 ssd 1541710927.790321112 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 512, "value": 0.09102222222222223}

:::MLPv0.5.0 ssd 1541710927.884398937 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 513, "value": 0.0912}

:::MLPv0.5.0 ssd 1541710927.979423761 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 514, "value": 0.09137777777777778}

:::MLPv0.5.0 ssd 1541710928.074403763 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 515, "value": 0.09155555555555556}

:::MLPv0.5.0 ssd 1541710928.169754982 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 516, "value": 0.09173333333333333}

:::MLPv0.5.0 ssd 1541710928.263158798 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 517, "value": 0.09191111111111111}

:::MLPv0.5.0 ssd 1541710928.357576609 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 518, "value": 0.09208888888888889}

:::MLPv0.5.0 ssd 1541710928.452150583 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 519, "value": 0.09226666666666666}

:::MLPv0.5.0 ssd 1541710928.542652369 (train.py:553) train_epoch: 9

:::MLPv0.5.0 ssd 1541710928.547704458 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 520, "value": 0.09244444444444444}
Iteration:    520, Loss function: 6.259, Average Loss: 3.395, avg. samples / sec: 21676.90
Iteration:    520, Loss function: 5.630, Average Loss: 3.389, avg. samples / sec: 21688.85
Iteration:    520, Loss function: 6.280, Average Loss: 3.391, avg. samples / sec: 21682.71
Iteration:    520, Loss function: 6.381, Average Loss: 3.391, avg. samples / sec: 21680.18
Iteration:    520, Loss function: 6.160, Average Loss: 3.397, avg. samples / sec: 21687.18
Iteration:    520, Loss function: 6.120, Average Loss: 3.397, avg. samples / sec: 21675.83
Iteration:    520, Loss function: 5.903, Average Loss: 3.389, avg. samples / sec: 21676.34
Iteration:    520, Loss function: 6.710, Average Loss: 3.397, avg. samples / sec: 21669.65

:::MLPv0.5.0 ssd 1541710928.641678333 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 521, "value": 0.09262222222222222}

:::MLPv0.5.0 ssd 1541710928.735356092 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 522, "value": 0.0928}

:::MLPv0.5.0 ssd 1541710928.829225779 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 523, "value": 0.09297777777777778}

:::MLPv0.5.0 ssd 1541710928.923674345 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 524, "value": 0.09315555555555556}

:::MLPv0.5.0 ssd 1541710929.018591881 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 525, "value": 0.09333333333333334}

:::MLPv0.5.0 ssd 1541710929.113899469 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 526, "value": 0.09351111111111111}

:::MLPv0.5.0 ssd 1541710929.208471775 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 527, "value": 0.09368888888888889}

:::MLPv0.5.0 ssd 1541710929.302306652 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 528, "value": 0.09386666666666667}

:::MLPv0.5.0 ssd 1541710929.396106958 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 529, "value": 0.09404444444444444}

:::MLPv0.5.0 ssd 1541710929.490421295 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 530, "value": 0.09422222222222222}

:::MLPv0.5.0 ssd 1541710929.584290266 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 531, "value": 0.0944}

:::MLPv0.5.0 ssd 1541710929.678241253 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 532, "value": 0.09457777777777777}

:::MLPv0.5.0 ssd 1541710929.774476528 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 533, "value": 0.09475555555555555}

:::MLPv0.5.0 ssd 1541710929.869123220 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 534, "value": 0.09493333333333333}

:::MLPv0.5.0 ssd 1541710929.963082314 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 535, "value": 0.0951111111111111}

:::MLPv0.5.0 ssd 1541710930.057559013 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 536, "value": 0.0952888888888889}

:::MLPv0.5.0 ssd 1541710930.151485443 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 537, "value": 0.09546666666666667}

:::MLPv0.5.0 ssd 1541710930.245717287 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 538, "value": 0.09564444444444445}

:::MLPv0.5.0 ssd 1541710930.338899136 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 539, "value": 0.09582222222222223}

:::MLPv0.5.0 ssd 1541710930.432281494 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 540, "value": 0.096}
Iteration:    540, Loss function: 6.065, Average Loss: 3.438, avg. samples / sec: 21740.57
Iteration:    540, Loss function: 6.108, Average Loss: 3.445, avg. samples / sec: 21736.29
Iteration:    540, Loss function: 6.174, Average Loss: 3.446, avg. samples / sec: 21747.08
Iteration:    540, Loss function: 6.086, Average Loss: 3.437, avg. samples / sec: 21732.58
Iteration:    540, Loss function: 5.675, Average Loss: 3.438, avg. samples / sec: 21739.79
Iteration:    540, Loss function: 6.522, Average Loss: 3.445, avg. samples / sec: 21736.76
Iteration:    540, Loss function: 6.011, Average Loss: 3.447, avg. samples / sec: 21740.38
Iteration:    540, Loss function: 5.846, Average Loss: 3.441, avg. samples / sec: 21729.95

:::MLPv0.5.0 ssd 1541710930.526212931 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 541, "value": 0.09617777777777778}

:::MLPv0.5.0 ssd 1541710930.622917652 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 542, "value": 0.09635555555555556}

:::MLPv0.5.0 ssd 1541710930.716480732 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 543, "value": 0.09653333333333333}

:::MLPv0.5.0 ssd 1541710930.812898397 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 544, "value": 0.09671111111111111}

:::MLPv0.5.0 ssd 1541710930.908018827 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 545, "value": 0.09688888888888889}

:::MLPv0.5.0 ssd 1541710931.002079725 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 546, "value": 0.09706666666666666}

:::MLPv0.5.0 ssd 1541710931.095983028 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 547, "value": 0.09724444444444444}

:::MLPv0.5.0 ssd 1541710931.190329313 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 548, "value": 0.09742222222222222}

:::MLPv0.5.0 ssd 1541710931.286835194 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 549, "value": 0.09759999999999999}

:::MLPv0.5.0 ssd 1541710931.381294250 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 550, "value": 0.09777777777777777}

:::MLPv0.5.0 ssd 1541710931.477739334 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 551, "value": 0.09795555555555555}

:::MLPv0.5.0 ssd 1541710931.571535826 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 552, "value": 0.09813333333333334}

:::MLPv0.5.0 ssd 1541710931.665812492 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 553, "value": 0.09831111111111111}

:::MLPv0.5.0 ssd 1541710931.760013580 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 554, "value": 0.09848888888888889}

:::MLPv0.5.0 ssd 1541710931.855776548 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 555, "value": 0.09866666666666667}

:::MLPv0.5.0 ssd 1541710931.950264215 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 556, "value": 0.09884444444444444}

:::MLPv0.5.0 ssd 1541710932.044445515 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 557, "value": 0.09902222222222222}

:::MLPv0.5.0 ssd 1541710932.138651371 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 558, "value": 0.09920000000000001}

:::MLPv0.5.0 ssd 1541710932.235203266 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 559, "value": 0.09937777777777779}

:::MLPv0.5.0 ssd 1541710932.329843521 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 560, "value": 0.09955555555555556}
Iteration:    560, Loss function: 6.603, Average Loss: 3.496, avg. samples / sec: 21587.56
Iteration:    560, Loss function: 6.653, Average Loss: 3.497, avg. samples / sec: 21578.90
Iteration:    560, Loss function: 6.216, Average Loss: 3.504, avg. samples / sec: 21581.55
Iteration:    560, Loss function: 6.732, Average Loss: 3.501, avg. samples / sec: 21579.49
Iteration:    560, Loss function: 5.736, Average Loss: 3.498, avg. samples / sec: 21588.10
Iteration:    560, Loss function: 5.895, Average Loss: 3.497, avg. samples / sec: 21583.81
Iteration:    560, Loss function: 6.151, Average Loss: 3.503, avg. samples / sec: 21581.39
Iteration:    560, Loss function: 6.345, Average Loss: 3.505, avg. samples / sec: 21575.98

:::MLPv0.5.0 ssd 1541710932.425714016 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 561, "value": 0.09973333333333334}

:::MLPv0.5.0 ssd 1541710932.520169973 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 562, "value": 0.09991111111111112}

:::MLPv0.5.0 ssd 1541710932.613871813 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 563, "value": 0.1000888888888889}

:::MLPv0.5.0 ssd 1541710932.707572937 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 564, "value": 0.10026666666666667}

:::MLPv0.5.0 ssd 1541710932.801595926 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 565, "value": 0.10044444444444445}

:::MLPv0.5.0 ssd 1541710932.895072699 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 566, "value": 0.10062222222222222}

:::MLPv0.5.0 ssd 1541710932.990140200 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 567, "value": 0.1008}

:::MLPv0.5.0 ssd 1541710933.085176945 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 568, "value": 0.10097777777777778}

:::MLPv0.5.0 ssd 1541710933.180799961 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 569, "value": 0.10115555555555555}

:::MLPv0.5.0 ssd 1541710933.276011944 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 570, "value": 0.10133333333333333}

:::MLPv0.5.0 ssd 1541710933.370388508 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 571, "value": 0.10151111111111111}

:::MLPv0.5.0 ssd 1541710933.464147091 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 572, "value": 0.10168888888888888}

:::MLPv0.5.0 ssd 1541710933.559776783 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 573, "value": 0.10186666666666666}

:::MLPv0.5.0 ssd 1541710933.653088331 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 574, "value": 0.10204444444444444}

:::MLPv0.5.0 ssd 1541710933.747624397 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 575, "value": 0.10222222222222221}

:::MLPv0.5.0 ssd 1541710933.842848539 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 576, "value": 0.10239999999999999}

:::MLPv0.5.0 ssd 1541710933.939487696 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 577, "value": 0.10257777777777778}

:::MLPv0.5.0 ssd 1541710934.030668497 (train.py:553) train_epoch: 10

:::MLPv0.5.0 ssd 1541710934.035851955 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 578, "value": 0.10275555555555556}

:::MLPv0.5.0 ssd 1541710934.129480600 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 579, "value": 0.10293333333333334}

:::MLPv0.5.0 ssd 1541710934.222821712 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 580, "value": 0.10311111111111111}
Iteration:    580, Loss function: 5.679, Average Loss: 3.549, avg. samples / sec: 21645.07
Iteration:    580, Loss function: 6.024, Average Loss: 3.547, avg. samples / sec: 21644.78
Iteration:    580, Loss function: 6.296, Average Loss: 3.547, avg. samples / sec: 21637.88
Iteration:    580, Loss function: 5.701, Average Loss: 3.551, avg. samples / sec: 21636.80
Iteration:    580, Loss function: 6.042, Average Loss: 3.554, avg. samples / sec: 21650.71
Iteration:    580, Loss function: 5.332, Average Loss: 3.545, avg. samples / sec: 21630.58
Iteration:    580, Loss function: 5.492, Average Loss: 3.549, avg. samples / sec: 21639.22
Iteration:    580, Loss function: 5.673, Average Loss: 3.546, avg. samples / sec: 21627.64

:::MLPv0.5.0 ssd 1541710934.316734552 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 581, "value": 0.10328888888888889}

:::MLPv0.5.0 ssd 1541710934.410036564 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 582, "value": 0.10346666666666667}

:::MLPv0.5.0 ssd 1541710934.505109310 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 583, "value": 0.10364444444444444}

:::MLPv0.5.0 ssd 1541710934.598654985 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 584, "value": 0.10382222222222223}

:::MLPv0.5.0 ssd 1541710934.693073750 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 585, "value": 0.10400000000000001}

:::MLPv0.5.0 ssd 1541710934.788048983 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 586, "value": 0.10417777777777779}

:::MLPv0.5.0 ssd 1541710934.884763479 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 587, "value": 0.10435555555555556}

:::MLPv0.5.0 ssd 1541710934.980558395 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 588, "value": 0.10453333333333334}

:::MLPv0.5.0 ssd 1541710935.077102900 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 589, "value": 0.10471111111111112}

:::MLPv0.5.0 ssd 1541710935.171098709 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 590, "value": 0.10488888888888889}

:::MLPv0.5.0 ssd 1541710935.265756130 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 591, "value": 0.10506666666666667}

:::MLPv0.5.0 ssd 1541710935.359635830 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 592, "value": 0.10524444444444445}

:::MLPv0.5.0 ssd 1541710935.454019547 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 593, "value": 0.10542222222222222}

:::MLPv0.5.0 ssd 1541710935.549566269 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 594, "value": 0.1056}

:::MLPv0.5.0 ssd 1541710935.645990372 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 595, "value": 0.10577777777777778}

:::MLPv0.5.0 ssd 1541710935.741907120 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 596, "value": 0.10595555555555555}

:::MLPv0.5.0 ssd 1541710935.836096764 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 597, "value": 0.10613333333333333}

:::MLPv0.5.0 ssd 1541710935.930839300 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 598, "value": 0.1063111111111111}

:::MLPv0.5.0 ssd 1541710936.028033972 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 599, "value": 0.10648888888888888}

:::MLPv0.5.0 ssd 1541710936.123024464 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 600, "value": 0.10666666666666666}
Iteration:    600, Loss function: 5.968, Average Loss: 3.590, avg. samples / sec: 21567.30
Iteration:    600, Loss function: 6.265, Average Loss: 3.589, avg. samples / sec: 21564.27
Iteration:    600, Loss function: 6.169, Average Loss: 3.586, avg. samples / sec: 21569.53
Iteration:    600, Loss function: 5.320, Average Loss: 3.593, avg. samples / sec: 21554.69
Iteration:    600, Loss function: 5.929, Average Loss: 3.598, avg. samples / sec: 21561.24
Iteration:    600, Loss function: 5.653, Average Loss: 3.592, avg. samples / sec: 21558.08
Iteration:    600, Loss function: 5.870, Average Loss: 3.587, avg. samples / sec: 21562.16
Iteration:    600, Loss function: 5.820, Average Loss: 3.593, avg. samples / sec: 21547.46

:::MLPv0.5.0 ssd 1541710936.218426466 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 601, "value": 0.10684444444444444}

:::MLPv0.5.0 ssd 1541710936.313690424 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 602, "value": 0.10702222222222221}

:::MLPv0.5.0 ssd 1541710936.408034801 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 603, "value": 0.1072}

:::MLPv0.5.0 ssd 1541710936.503367424 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 604, "value": 0.10737777777777778}

:::MLPv0.5.0 ssd 1541710936.597915649 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 605, "value": 0.10755555555555556}

:::MLPv0.5.0 ssd 1541710936.691591263 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 606, "value": 0.10773333333333333}

:::MLPv0.5.0 ssd 1541710936.785751581 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 607, "value": 0.10791111111111111}

:::MLPv0.5.0 ssd 1541710936.880552292 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 608, "value": 0.10808888888888889}

:::MLPv0.5.0 ssd 1541710936.976723194 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 609, "value": 0.10826666666666668}

:::MLPv0.5.0 ssd 1541710937.071996927 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 610, "value": 0.10844444444444445}

:::MLPv0.5.0 ssd 1541710937.166540384 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 611, "value": 0.10862222222222223}

:::MLPv0.5.0 ssd 1541710937.261027575 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 612, "value": 0.10880000000000001}

:::MLPv0.5.0 ssd 1541710937.355256796 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 613, "value": 0.10897777777777778}

:::MLPv0.5.0 ssd 1541710937.449383020 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 614, "value": 0.10915555555555556}

:::MLPv0.5.0 ssd 1541710937.546604872 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 615, "value": 0.10933333333333334}

:::MLPv0.5.0 ssd 1541710937.640759468 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 616, "value": 0.10951111111111111}

:::MLPv0.5.0 ssd 1541710937.734520674 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 617, "value": 0.10968888888888889}

:::MLPv0.5.0 ssd 1541710937.829015493 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 618, "value": 0.10986666666666667}

:::MLPv0.5.0 ssd 1541710937.923742294 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 619, "value": 0.11004444444444444}

:::MLPv0.5.0 ssd 1541710938.018901110 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 620, "value": 0.11022222222222222}
Iteration:    620, Loss function: 5.550, Average Loss: 3.631, avg. samples / sec: 21605.79
Iteration:    620, Loss function: 5.703, Average Loss: 3.633, avg. samples / sec: 21608.10
Iteration:    620, Loss function: 5.040, Average Loss: 3.629, avg. samples / sec: 21608.47
Iteration:    620, Loss function: 5.499, Average Loss: 3.633, avg. samples / sec: 21624.90
Iteration:    620, Loss function: 5.900, Average Loss: 3.628, avg. samples / sec: 21601.67
Iteration:    620, Loss function: 5.657, Average Loss: 3.628, avg. samples / sec: 21606.10
Iteration:    620, Loss function: 5.374, Average Loss: 3.638, avg. samples / sec: 21591.23
Iteration:    620, Loss function: 4.803, Average Loss: 3.627, avg. samples / sec: 21577.59

:::MLPv0.5.0 ssd 1541710938.113648653 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 621, "value": 0.1104}

:::MLPv0.5.0 ssd 1541710938.208001137 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 622, "value": 0.11057777777777777}

:::MLPv0.5.0 ssd 1541710938.301740885 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 623, "value": 0.11075555555555555}

:::MLPv0.5.0 ssd 1541710938.395759821 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 624, "value": 0.11093333333333333}

:::MLPv0.5.0 ssd 1541710938.489490747 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 625, "value": 0.1111111111111111}

:::MLPv0.5.0 ssd 1541710938.583654642 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 626, "value": 0.11128888888888888}

:::MLPv0.5.0 ssd 1541710938.676956654 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 627, "value": 0.11146666666666666}

:::MLPv0.5.0 ssd 1541710938.771052599 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 628, "value": 0.11164444444444445}

:::MLPv0.5.0 ssd 1541710938.866881609 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 629, "value": 0.11182222222222223}

:::MLPv0.5.0 ssd 1541710938.960913181 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 630, "value": 0.112}

:::MLPv0.5.0 ssd 1541710939.055737019 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 631, "value": 0.11217777777777778}

:::MLPv0.5.0 ssd 1541710939.151220798 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 632, "value": 0.11235555555555556}

:::MLPv0.5.0 ssd 1541710939.244347572 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 633, "value": 0.11253333333333333}

:::MLPv0.5.0 ssd 1541710939.338593006 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 634, "value": 0.11271111111111111}

:::MLPv0.5.0 ssd 1541710939.432128906 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 635, "value": 0.1128888888888889}

:::MLPv0.5.0 ssd 1541710939.521712542 (train.py:553) train_epoch: 11

:::MLPv0.5.0 ssd 1541710939.527087450 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 636, "value": 0.11306666666666668}

:::MLPv0.5.0 ssd 1541710939.620781422 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 637, "value": 0.11324444444444445}

:::MLPv0.5.0 ssd 1541710939.714881182 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 638, "value": 0.11342222222222223}

:::MLPv0.5.0 ssd 1541710939.808207512 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 639, "value": 0.1136}

:::MLPv0.5.0 ssd 1541710939.902053833 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 640, "value": 0.11377777777777778}
Iteration:    640, Loss function: 5.946, Average Loss: 3.666, avg. samples / sec: 21758.53
Iteration:    640, Loss function: 5.325, Average Loss: 3.673, avg. samples / sec: 21766.16
Iteration:    640, Loss function: 5.897, Average Loss: 3.669, avg. samples / sec: 21741.86
Iteration:    640, Loss function: 5.686, Average Loss: 3.667, avg. samples / sec: 21751.13
Iteration:    640, Loss function: 5.226, Average Loss: 3.663, avg. samples / sec: 21772.62
Iteration:    640, Loss function: 4.992, Average Loss: 3.671, avg. samples / sec: 21747.56
Iteration:    640, Loss function: 5.318, Average Loss: 3.668, avg. samples / sec: 21749.33
Iteration:    640, Loss function: 5.196, Average Loss: 3.663, avg. samples / sec: 21750.13

:::MLPv0.5.0 ssd 1541710939.996711493 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 641, "value": 0.11395555555555556}

:::MLPv0.5.0 ssd 1541710940.091756105 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 642, "value": 0.11413333333333334}

:::MLPv0.5.0 ssd 1541710940.185316086 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 643, "value": 0.11431111111111111}

:::MLPv0.5.0 ssd 1541710940.281704426 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 644, "value": 0.11448888888888889}

:::MLPv0.5.0 ssd 1541710940.375964880 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 645, "value": 0.11466666666666667}

:::MLPv0.5.0 ssd 1541710940.471450806 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 646, "value": 0.11484444444444444}

:::MLPv0.5.0 ssd 1541710940.564922333 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 647, "value": 0.11502222222222222}

:::MLPv0.5.0 ssd 1541710940.662081480 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 648, "value": 0.1152}

:::MLPv0.5.0 ssd 1541710940.756164551 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 649, "value": 0.11537777777777777}

:::MLPv0.5.0 ssd 1541710940.852813482 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 650, "value": 0.11555555555555555}

:::MLPv0.5.0 ssd 1541710940.945539236 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 651, "value": 0.11573333333333333}

:::MLPv0.5.0 ssd 1541710941.039162874 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 652, "value": 0.1159111111111111}

:::MLPv0.5.0 ssd 1541710941.133108377 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 653, "value": 0.11608888888888888}

:::MLPv0.5.0 ssd 1541710941.226176977 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 654, "value": 0.11626666666666667}

:::MLPv0.5.0 ssd 1541710941.319205523 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 655, "value": 0.11644444444444445}

:::MLPv0.5.0 ssd 1541710941.419230938 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 656, "value": 0.11662222222222222}

:::MLPv0.5.0 ssd 1541710941.513614893 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 657, "value": 0.1168}

:::MLPv0.5.0 ssd 1541710941.607425451 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 658, "value": 0.11697777777777778}

:::MLPv0.5.0 ssd 1541710941.700978279 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 659, "value": 0.11715555555555555}

:::MLPv0.5.0 ssd 1541710941.795346022 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 660, "value": 0.11733333333333333}
Iteration:    660, Loss function: 5.835, Average Loss: 3.706, avg. samples / sec: 21645.51
Iteration:    660, Loss function: 6.596, Average Loss: 3.706, avg. samples / sec: 21643.03
Iteration:    660, Loss function: 5.802, Average Loss: 3.706, avg. samples / sec: 21630.71
Iteration:    660, Loss function: 6.423, Average Loss: 3.705, avg. samples / sec: 21635.86
Iteration:    660, Loss function: 5.401, Average Loss: 3.701, avg. samples / sec: 21636.57
Iteration:    660, Loss function: 6.126, Average Loss: 3.710, avg. samples / sec: 21632.83
Iteration:    660, Loss function: 5.797, Average Loss: 3.709, avg. samples / sec: 21631.92
Iteration:    660, Loss function: 6.318, Average Loss: 3.700, avg. samples / sec: 21635.74

:::MLPv0.5.0 ssd 1541710941.889360189 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 661, "value": 0.11751111111111112}

:::MLPv0.5.0 ssd 1541710941.983052492 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 662, "value": 0.1176888888888889}

:::MLPv0.5.0 ssd 1541710942.076798201 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 663, "value": 0.11786666666666668}

:::MLPv0.5.0 ssd 1541710942.170593739 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 664, "value": 0.11804444444444445}

:::MLPv0.5.0 ssd 1541710942.264202833 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 665, "value": 0.11822222222222223}

:::MLPv0.5.0 ssd 1541710942.359661579 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 666, "value": 0.1184}

:::MLPv0.5.0 ssd 1541710942.453161001 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 667, "value": 0.11857777777777778}

:::MLPv0.5.0 ssd 1541710942.551820755 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 668, "value": 0.11875555555555556}

:::MLPv0.5.0 ssd 1541710942.646769762 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 669, "value": 0.11893333333333334}

:::MLPv0.5.0 ssd 1541710942.743085623 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 670, "value": 0.11911111111111111}

:::MLPv0.5.0 ssd 1541710942.839035988 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 671, "value": 0.11928888888888889}

:::MLPv0.5.0 ssd 1541710942.933328390 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 672, "value": 0.11946666666666667}

:::MLPv0.5.0 ssd 1541710943.027136087 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 673, "value": 0.11964444444444444}

:::MLPv0.5.0 ssd 1541710943.121505260 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 674, "value": 0.11982222222222222}

:::MLPv0.5.0 ssd 1541710943.216003418 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 675, "value": 0.12}

:::MLPv0.5.0 ssd 1541710943.312788248 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 676, "value": 0.12017777777777777}

:::MLPv0.5.0 ssd 1541710943.407130003 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 677, "value": 0.12035555555555555}

:::MLPv0.5.0 ssd 1541710943.502824306 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 678, "value": 0.12053333333333333}

:::MLPv0.5.0 ssd 1541710943.596909285 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 679, "value": 0.1207111111111111}

:::MLPv0.5.0 ssd 1541710943.692691565 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 680, "value": 0.12088888888888889}
Iteration:    680, Loss function: 5.023, Average Loss: 3.743, avg. samples / sec: 21587.84
Iteration:    680, Loss function: 5.687, Average Loss: 3.743, avg. samples / sec: 21588.49
Iteration:    680, Loss function: 5.725, Average Loss: 3.741, avg. samples / sec: 21590.02
Iteration:    680, Loss function: 5.275, Average Loss: 3.739, avg. samples / sec: 21590.70
Iteration:    680, Loss function: 4.980, Average Loss: 3.736, avg. samples / sec: 21589.10
Iteration:    680, Loss function: 5.369, Average Loss: 3.744, avg. samples / sec: 21593.72
Iteration:    680, Loss function: 5.776, Average Loss: 3.746, avg. samples / sec: 21588.36
Iteration:    680, Loss function: 5.581, Average Loss: 3.738, avg. samples / sec: 21582.84

:::MLPv0.5.0 ssd 1541710943.787341118 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 681, "value": 0.12106666666666667}

:::MLPv0.5.0 ssd 1541710943.881223917 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 682, "value": 0.12124444444444445}

:::MLPv0.5.0 ssd 1541710943.979955196 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 683, "value": 0.12142222222222222}

:::MLPv0.5.0 ssd 1541710944.074839115 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 684, "value": 0.1216}

:::MLPv0.5.0 ssd 1541710944.168981791 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 685, "value": 0.12177777777777778}

:::MLPv0.5.0 ssd 1541710944.262595654 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 686, "value": 0.12195555555555557}

:::MLPv0.5.0 ssd 1541710944.356509686 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 687, "value": 0.12213333333333334}

:::MLPv0.5.0 ssd 1541710944.450504303 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 688, "value": 0.12231111111111112}

:::MLPv0.5.0 ssd 1541710944.544863224 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 689, "value": 0.1224888888888889}

:::MLPv0.5.0 ssd 1541710944.638065577 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 690, "value": 0.12266666666666667}

:::MLPv0.5.0 ssd 1541710944.735936403 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 691, "value": 0.12284444444444445}

:::MLPv0.5.0 ssd 1541710944.834788322 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 692, "value": 0.12302222222222223}

:::MLPv0.5.0 ssd 1541710944.928599358 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 693, "value": 0.1232}

:::MLPv0.5.0 ssd 1541710945.019263268 (train.py:553) train_epoch: 12

:::MLPv0.5.0 ssd 1541710945.024583578 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 694, "value": 0.12337777777777778}

:::MLPv0.5.0 ssd 1541710945.118497849 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 695, "value": 0.12355555555555556}

:::MLPv0.5.0 ssd 1541710945.213276148 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 696, "value": 0.12373333333333333}

:::MLPv0.5.0 ssd 1541710945.307434797 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 697, "value": 0.12391111111111111}

:::MLPv0.5.0 ssd 1541710945.400939941 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 698, "value": 0.12408888888888889}

:::MLPv0.5.0 ssd 1541710945.494902611 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 699, "value": 0.12426666666666666}

:::MLPv0.5.0 ssd 1541710945.588490963 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 700, "value": 0.12444444444444444}
Iteration:    700, Loss function: 5.477, Average Loss: 3.775, avg. samples / sec: 21624.02
Iteration:    700, Loss function: 5.377, Average Loss: 3.779, avg. samples / sec: 21605.95
Iteration:    700, Loss function: 4.961, Average Loss: 3.780, avg. samples / sec: 21606.44
Iteration:    700, Loss function: 5.003, Average Loss: 3.778, avg. samples / sec: 21598.92
Iteration:    700, Loss function: 4.978, Average Loss: 3.777, avg. samples / sec: 21600.42
Iteration:    700, Loss function: 5.283, Average Loss: 3.780, avg. samples / sec: 21608.96
Iteration:    700, Loss function: 5.451, Average Loss: 3.776, avg. samples / sec: 21601.30
Iteration:    700, Loss function: 5.255, Average Loss: 3.774, avg. samples / sec: 21598.71

:::MLPv0.5.0 ssd 1541710945.682194233 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 701, "value": 0.12462222222222222}

:::MLPv0.5.0 ssd 1541710945.776051760 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 702, "value": 0.1248}

:::MLPv0.5.0 ssd 1541710945.870522261 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 703, "value": 0.12497777777777777}

:::MLPv0.5.0 ssd 1541710945.964050770 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 704, "value": 0.12515555555555555}

:::MLPv0.5.0 ssd 1541710946.058686972 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 705, "value": 0.12533333333333335}

:::MLPv0.5.0 ssd 1541710946.151187181 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 706, "value": 0.12551111111111113}

:::MLPv0.5.0 ssd 1541710946.244637012 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 707, "value": 0.1256888888888889}

:::MLPv0.5.0 ssd 1541710946.338385820 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 708, "value": 0.12586666666666668}

:::MLPv0.5.0 ssd 1541710946.432366610 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 709, "value": 0.12604444444444446}

:::MLPv0.5.0 ssd 1541710946.526333809 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 710, "value": 0.12622222222222224}

:::MLPv0.5.0 ssd 1541710946.620610476 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 711, "value": 0.1264}

:::MLPv0.5.0 ssd 1541710946.713259935 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 712, "value": 0.1265777777777778}

:::MLPv0.5.0 ssd 1541710946.809022665 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 713, "value": 0.12675555555555557}

:::MLPv0.5.0 ssd 1541710946.903297424 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 714, "value": 0.12693333333333334}

:::MLPv0.5.0 ssd 1541710946.997690439 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 715, "value": 0.12711111111111112}

:::MLPv0.5.0 ssd 1541710947.091673374 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 716, "value": 0.1272888888888889}

:::MLPv0.5.0 ssd 1541710947.185902596 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 717, "value": 0.12746666666666667}

:::MLPv0.5.0 ssd 1541710947.282494783 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 718, "value": 0.12764444444444445}

:::MLPv0.5.0 ssd 1541710947.376643181 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 719, "value": 0.12782222222222223}

:::MLPv0.5.0 ssd 1541710947.473494053 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 720, "value": 0.128}
Iteration:    720, Loss function: 5.503, Average Loss: 3.811, avg. samples / sec: 21732.99
Iteration:    720, Loss function: 4.799, Average Loss: 3.806, avg. samples / sec: 21730.50
Iteration:    720, Loss function: 4.970, Average Loss: 3.808, avg. samples / sec: 21730.74
Iteration:    720, Loss function: 5.211, Average Loss: 3.805, avg. samples / sec: 21730.73
Iteration:    720, Loss function: 4.973, Average Loss: 3.803, avg. samples / sec: 21733.00
Iteration:    720, Loss function: 5.158, Average Loss: 3.807, avg. samples / sec: 21724.75
Iteration:    720, Loss function: 5.039, Average Loss: 3.803, avg. samples / sec: 21730.12
Iteration:    720, Loss function: 5.236, Average Loss: 3.808, avg. samples / sec: 21716.36

:::MLPv0.5.0 ssd 1541710947.568220854 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 721, "value": 0.12817777777777778}

:::MLPv0.5.0 ssd 1541710947.662265301 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 722, "value": 0.12835555555555556}

:::MLPv0.5.0 ssd 1541710947.761343479 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 723, "value": 0.12853333333333333}

:::MLPv0.5.0 ssd 1541710947.855388403 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 724, "value": 0.1287111111111111}

:::MLPv0.5.0 ssd 1541710947.949197531 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 725, "value": 0.1288888888888889}

:::MLPv0.5.0 ssd 1541710948.043736458 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 726, "value": 0.12906666666666666}

:::MLPv0.5.0 ssd 1541710948.138060093 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 727, "value": 0.12924444444444444}

:::MLPv0.5.0 ssd 1541710948.232553720 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 728, "value": 0.12942222222222222}

:::MLPv0.5.0 ssd 1541710948.326853275 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 729, "value": 0.1296}

:::MLPv0.5.0 ssd 1541710948.421806812 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 730, "value": 0.12977777777777777}

:::MLPv0.5.0 ssd 1541710948.517089128 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 731, "value": 0.12995555555555555}

:::MLPv0.5.0 ssd 1541710948.611751795 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 732, "value": 0.13013333333333332}

:::MLPv0.5.0 ssd 1541710948.706687212 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 733, "value": 0.1303111111111111}

:::MLPv0.5.0 ssd 1541710948.803546906 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 734, "value": 0.13048888888888888}

:::MLPv0.5.0 ssd 1541710948.904083729 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 735, "value": 0.13066666666666665}

:::MLPv0.5.0 ssd 1541710949.000417948 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 736, "value": 0.13084444444444446}

:::MLPv0.5.0 ssd 1541710949.093960285 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 737, "value": 0.13102222222222223}

:::MLPv0.5.0 ssd 1541710949.188593388 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 738, "value": 0.1312}

:::MLPv0.5.0 ssd 1541710949.283128023 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 739, "value": 0.1313777777777778}

:::MLPv0.5.0 ssd 1541710949.377969980 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 740, "value": 0.13155555555555556}
Iteration:    740, Loss function: 5.369, Average Loss: 3.837, avg. samples / sec: 21513.14
Iteration:    740, Loss function: 5.210, Average Loss: 3.843, avg. samples / sec: 21510.37
Iteration:    740, Loss function: 5.412, Average Loss: 3.831, avg. samples / sec: 21520.95
Iteration:    740, Loss function: 5.396, Average Loss: 3.834, avg. samples / sec: 21511.59
Iteration:    740, Loss function: 5.561, Average Loss: 3.834, avg. samples / sec: 21508.94
Iteration:    740, Loss function: 5.337, Average Loss: 3.839, avg. samples / sec: 21506.83
Iteration:    740, Loss function: 4.842, Average Loss: 3.839, avg. samples / sec: 21520.68
Iteration:    740, Loss function: 5.324, Average Loss: 3.838, avg. samples / sec: 21483.71

:::MLPv0.5.0 ssd 1541710949.473493814 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 741, "value": 0.13173333333333334}

:::MLPv0.5.0 ssd 1541710949.569734812 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 742, "value": 0.13191111111111112}

:::MLPv0.5.0 ssd 1541710949.663512468 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 743, "value": 0.1320888888888889}

:::MLPv0.5.0 ssd 1541710949.757776976 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 744, "value": 0.13226666666666667}

:::MLPv0.5.0 ssd 1541710949.850874901 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 745, "value": 0.13244444444444445}

:::MLPv0.5.0 ssd 1541710949.945595980 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 746, "value": 0.13262222222222222}

:::MLPv0.5.0 ssd 1541710950.039407969 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 747, "value": 0.1328}

:::MLPv0.5.0 ssd 1541710950.132181168 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 748, "value": 0.13297777777777778}

:::MLPv0.5.0 ssd 1541710950.227252245 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 749, "value": 0.13315555555555555}

:::MLPv0.5.0 ssd 1541710950.320186138 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 750, "value": 0.13333333333333333}

:::MLPv0.5.0 ssd 1541710950.410111666 (train.py:553) train_epoch: 13

:::MLPv0.5.0 ssd 1541710950.415514469 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 751, "value": 0.1335111111111111}

:::MLPv0.5.0 ssd 1541710950.509811878 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 752, "value": 0.13368888888888888}

:::MLPv0.5.0 ssd 1541710950.603523016 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 753, "value": 0.13386666666666666}

:::MLPv0.5.0 ssd 1541710950.696639538 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 754, "value": 0.13404444444444444}

:::MLPv0.5.0 ssd 1541710950.794630289 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 755, "value": 0.13422222222222221}

:::MLPv0.5.0 ssd 1541710950.888381481 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 756, "value": 0.1344}

:::MLPv0.5.0 ssd 1541710950.983128786 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 757, "value": 0.13457777777777777}

:::MLPv0.5.0 ssd 1541710951.076214075 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 758, "value": 0.13475555555555557}

:::MLPv0.5.0 ssd 1541710951.170419455 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 759, "value": 0.13493333333333335}

:::MLPv0.5.0 ssd 1541710951.264166355 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 760, "value": 0.13511111111111113}
Iteration:    760, Loss function: 5.498, Average Loss: 3.870, avg. samples / sec: 21729.09
Iteration:    760, Loss function: 5.103, Average Loss: 3.871, avg. samples / sec: 21720.99
Iteration:    760, Loss function: 5.358, Average Loss: 3.863, avg. samples / sec: 21721.87
Iteration:    760, Loss function: 5.729, Average Loss: 3.864, avg. samples / sec: 21715.84
Iteration:    760, Loss function: 4.997, Average Loss: 3.862, avg. samples / sec: 21719.99
Iteration:    760, Loss function: 5.900, Average Loss: 3.869, avg. samples / sec: 21714.18
Iteration:    760, Loss function: 5.008, Average Loss: 3.866, avg. samples / sec: 21733.91
Iteration:    760, Loss function: 6.291, Average Loss: 3.860, avg. samples / sec: 21700.05

:::MLPv0.5.0 ssd 1541710951.358441114 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 761, "value": 0.1352888888888889}

:::MLPv0.5.0 ssd 1541710951.452147007 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 762, "value": 0.13546666666666668}

:::MLPv0.5.0 ssd 1541710951.545404196 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 763, "value": 0.13564444444444446}

:::MLPv0.5.0 ssd 1541710951.642274618 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 764, "value": 0.13582222222222223}

:::MLPv0.5.0 ssd 1541710951.735863447 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 765, "value": 0.136}

:::MLPv0.5.0 ssd 1541710951.830385923 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 766, "value": 0.1361777777777778}

:::MLPv0.5.0 ssd 1541710951.924350262 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 767, "value": 0.13635555555555556}

:::MLPv0.5.0 ssd 1541710952.017516613 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 768, "value": 0.13653333333333334}

:::MLPv0.5.0 ssd 1541710952.111546278 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 769, "value": 0.13671111111111112}

:::MLPv0.5.0 ssd 1541710952.205759048 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 770, "value": 0.1368888888888889}

:::MLPv0.5.0 ssd 1541710952.299925566 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 771, "value": 0.13706666666666667}

:::MLPv0.5.0 ssd 1541710952.393305302 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 772, "value": 0.13724444444444445}

:::MLPv0.5.0 ssd 1541710952.486952305 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 773, "value": 0.13742222222222222}

:::MLPv0.5.0 ssd 1541710952.580789089 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 774, "value": 0.1376}

:::MLPv0.5.0 ssd 1541710952.674822092 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 775, "value": 0.13777777777777778}

:::MLPv0.5.0 ssd 1541710952.769023418 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 776, "value": 0.13795555555555555}

:::MLPv0.5.0 ssd 1541710952.862966537 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 777, "value": 0.13813333333333333}

:::MLPv0.5.0 ssd 1541710952.956729889 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 778, "value": 0.1383111111111111}

:::MLPv0.5.0 ssd 1541710953.050387144 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 779, "value": 0.13848888888888888}

:::MLPv0.5.0 ssd 1541710953.146723270 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 780, "value": 0.13866666666666666}
Iteration:    780, Loss function: 5.049, Average Loss: 3.893, avg. samples / sec: 21763.31
Iteration:    780, Loss function: 5.550, Average Loss: 3.901, avg. samples / sec: 21755.98
Iteration:    780, Loss function: 5.564, Average Loss: 3.896, avg. samples / sec: 21755.82
Iteration:    780, Loss function: 5.400, Average Loss: 3.903, avg. samples / sec: 21766.23
Iteration:    780, Loss function: 5.268, Average Loss: 3.895, avg. samples / sec: 21758.31
Iteration:    780, Loss function: 5.451, Average Loss: 3.901, avg. samples / sec: 21752.33
Iteration:    780, Loss function: 6.126, Average Loss: 3.892, avg. samples / sec: 21768.98
Iteration:    780, Loss function: 5.336, Average Loss: 3.898, avg. samples / sec: 21748.02

:::MLPv0.5.0 ssd 1541710953.240817547 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 781, "value": 0.13884444444444444}

:::MLPv0.5.0 ssd 1541710953.334498167 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 782, "value": 0.1390222222222222}

:::MLPv0.5.0 ssd 1541710953.429250479 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 783, "value": 0.1392}

:::MLPv0.5.0 ssd 1541710953.523020506 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 784, "value": 0.13937777777777777}

:::MLPv0.5.0 ssd 1541710953.616877556 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 785, "value": 0.13955555555555554}

:::MLPv0.5.0 ssd 1541710953.711975574 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 786, "value": 0.13973333333333332}

:::MLPv0.5.0 ssd 1541710953.806287527 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 787, "value": 0.13991111111111112}

:::MLPv0.5.0 ssd 1541710953.901363850 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 788, "value": 0.1400888888888889}

:::MLPv0.5.0 ssd 1541710953.995202303 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 789, "value": 0.14026666666666668}

:::MLPv0.5.0 ssd 1541710954.090886593 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 790, "value": 0.14044444444444446}

:::MLPv0.5.0 ssd 1541710954.184964418 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 791, "value": 0.14062222222222223}

:::MLPv0.5.0 ssd 1541710954.279127836 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 792, "value": 0.1408}

:::MLPv0.5.0 ssd 1541710954.374773026 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 793, "value": 0.14097777777777779}

:::MLPv0.5.0 ssd 1541710954.469068527 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 794, "value": 0.14115555555555556}

:::MLPv0.5.0 ssd 1541710954.562530279 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 795, "value": 0.14133333333333334}

:::MLPv0.5.0 ssd 1541710954.658114195 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 796, "value": 0.14151111111111112}

:::MLPv0.5.0 ssd 1541710954.755597591 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 797, "value": 0.1416888888888889}

:::MLPv0.5.0 ssd 1541710954.849433422 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 798, "value": 0.14186666666666667}

:::MLPv0.5.0 ssd 1541710954.953169584 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 799, "value": 0.14204444444444445}

:::MLPv0.5.0 ssd 1541710955.047939301 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 800, "value": 0.14222222222222222}
Iteration:    800, Loss function: 5.069, Average Loss: 3.929, avg. samples / sec: 21545.93
Iteration:    800, Loss function: 5.725, Average Loss: 3.920, avg. samples / sec: 21554.27
Iteration:    800, Loss function: 5.283, Average Loss: 3.920, avg. samples / sec: 21548.21
Iteration:    800, Loss function: 5.063, Average Loss: 3.925, avg. samples / sec: 21543.21
Iteration:    800, Loss function: 5.167, Average Loss: 3.932, avg. samples / sec: 21540.55
Iteration:    800, Loss function: 5.547, Average Loss: 3.919, avg. samples / sec: 21537.79
Iteration:    800, Loss function: 4.715, Average Loss: 3.930, avg. samples / sec: 21537.71
Iteration:    800, Loss function: 5.577, Average Loss: 3.927, avg. samples / sec: 21538.67

:::MLPv0.5.0 ssd 1541710955.142388582 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 801, "value": 0.1424}

:::MLPv0.5.0 ssd 1541710955.236617565 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 802, "value": 0.14257777777777778}

:::MLPv0.5.0 ssd 1541710955.330976009 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 803, "value": 0.14275555555555555}

:::MLPv0.5.0 ssd 1541710955.424605131 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 804, "value": 0.14293333333333333}

:::MLPv0.5.0 ssd 1541710955.520738602 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 805, "value": 0.1431111111111111}

:::MLPv0.5.0 ssd 1541710955.614603996 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 806, "value": 0.14328888888888888}

:::MLPv0.5.0 ssd 1541710955.709826708 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 807, "value": 0.14346666666666666}

:::MLPv0.5.0 ssd 1541710955.803780079 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 808, "value": 0.14364444444444444}

:::MLPv0.5.0 ssd 1541710955.894303322 (train.py:553) train_epoch: 14

:::MLPv0.5.0 ssd 1541710955.900460005 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 809, "value": 0.14382222222222224}

:::MLPv0.5.0 ssd 1541710955.994544029 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 810, "value": 0.14400000000000002}

:::MLPv0.5.0 ssd 1541710956.088355064 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 811, "value": 0.1441777777777778}

:::MLPv0.5.0 ssd 1541710956.182499170 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 812, "value": 0.14435555555555557}

:::MLPv0.5.0 ssd 1541710956.275963306 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 813, "value": 0.14453333333333335}

:::MLPv0.5.0 ssd 1541710956.370098829 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 814, "value": 0.14471111111111112}

:::MLPv0.5.0 ssd 1541710956.463897943 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 815, "value": 0.1448888888888889}

:::MLPv0.5.0 ssd 1541710956.558842421 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 816, "value": 0.14506666666666668}

:::MLPv0.5.0 ssd 1541710956.654867411 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 817, "value": 0.14524444444444445}

:::MLPv0.5.0 ssd 1541710956.749080658 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 818, "value": 0.14542222222222223}

:::MLPv0.5.0 ssd 1541710956.843181133 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 819, "value": 0.1456}

:::MLPv0.5.0 ssd 1541710956.938128948 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 820, "value": 0.14577777777777778}
Iteration:    820, Loss function: 5.480, Average Loss: 3.943, avg. samples / sec: 21668.52
Iteration:    820, Loss function: 4.516, Average Loss: 3.944, avg. samples / sec: 21666.53
Iteration:    820, Loss function: 4.619, Average Loss: 3.953, avg. samples / sec: 21666.93
Iteration:    820, Loss function: 5.360, Average Loss: 3.953, avg. samples / sec: 21696.53
Iteration:    820, Loss function: 5.849, Average Loss: 3.950, avg. samples / sec: 21664.00
Iteration:    820, Loss function: 4.901, Average Loss: 3.959, avg. samples / sec: 21667.58
Iteration:    820, Loss function: 4.634, Average Loss: 3.953, avg. samples / sec: 21672.33
Iteration:    820, Loss function: 5.008, Average Loss: 3.945, avg. samples / sec: 21663.98

:::MLPv0.5.0 ssd 1541710957.032854319 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 821, "value": 0.14595555555555556}

:::MLPv0.5.0 ssd 1541710957.126817465 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 822, "value": 0.14613333333333334}

:::MLPv0.5.0 ssd 1541710957.221814156 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 823, "value": 0.14631111111111111}

:::MLPv0.5.0 ssd 1541710957.316427946 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 824, "value": 0.1464888888888889}

:::MLPv0.5.0 ssd 1541710957.410815001 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 825, "value": 0.14666666666666667}

:::MLPv0.5.0 ssd 1541710957.505121946 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 826, "value": 0.14684444444444444}

:::MLPv0.5.0 ssd 1541710957.600931406 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 827, "value": 0.14702222222222222}

:::MLPv0.5.0 ssd 1541710957.695324898 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 828, "value": 0.1472}

:::MLPv0.5.0 ssd 1541710957.789725542 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 829, "value": 0.14737777777777777}

:::MLPv0.5.0 ssd 1541710957.885657549 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 830, "value": 0.14755555555555555}

:::MLPv0.5.0 ssd 1541710957.980698824 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 831, "value": 0.14773333333333333}

:::MLPv0.5.0 ssd 1541710958.075248957 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 832, "value": 0.1479111111111111}

:::MLPv0.5.0 ssd 1541710958.168933153 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 833, "value": 0.14808888888888888}

:::MLPv0.5.0 ssd 1541710958.264486074 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 834, "value": 0.14826666666666666}

:::MLPv0.5.0 ssd 1541710958.359050751 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 835, "value": 0.14844444444444443}

:::MLPv0.5.0 ssd 1541710958.453638315 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 836, "value": 0.1486222222222222}

:::MLPv0.5.0 ssd 1541710958.548902035 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 837, "value": 0.14880000000000002}

:::MLPv0.5.0 ssd 1541710958.644542217 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 838, "value": 0.1489777777777778}

:::MLPv0.5.0 ssd 1541710958.755751371 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 839, "value": 0.14915555555555557}

:::MLPv0.5.0 ssd 1541710958.849395752 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 840, "value": 0.14933333333333335}
Iteration:    840, Loss function: 4.850, Average Loss: 3.965, avg. samples / sec: 21430.69
Iteration:    840, Loss function: 5.700, Average Loss: 3.966, avg. samples / sec: 21431.46
Iteration:    840, Loss function: 4.931, Average Loss: 3.975, avg. samples / sec: 21433.91
Iteration:    840, Loss function: 5.236, Average Loss: 3.975, avg. samples / sec: 21424.59
Iteration:    840, Loss function: 6.384, Average Loss: 3.975, avg. samples / sec: 21427.86
Iteration:    840, Loss function: 5.177, Average Loss: 3.975, avg. samples / sec: 21428.64
Iteration:    840, Loss function: 5.104, Average Loss: 3.966, avg. samples / sec: 21428.49
Iteration:    840, Loss function: 5.536, Average Loss: 3.981, avg. samples / sec: 21424.92

:::MLPv0.5.0 ssd 1541710958.944203377 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 841, "value": 0.14951111111111112}

:::MLPv0.5.0 ssd 1541710959.037989140 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 842, "value": 0.1496888888888889}

:::MLPv0.5.0 ssd 1541710959.131611824 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 843, "value": 0.14986666666666668}

:::MLPv0.5.0 ssd 1541710959.226708412 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 844, "value": 0.15004444444444445}

:::MLPv0.5.0 ssd 1541710959.320232391 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 845, "value": 0.15022222222222223}

:::MLPv0.5.0 ssd 1541710959.414366722 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 846, "value": 0.1504}

:::MLPv0.5.0 ssd 1541710959.508318901 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 847, "value": 0.15057777777777778}

:::MLPv0.5.0 ssd 1541710959.602250338 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 848, "value": 0.15075555555555556}

:::MLPv0.5.0 ssd 1541710959.696008444 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 849, "value": 0.15093333333333334}

:::MLPv0.5.0 ssd 1541710959.790704489 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 850, "value": 0.1511111111111111}

:::MLPv0.5.0 ssd 1541710959.885511160 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 851, "value": 0.1512888888888889}

:::MLPv0.5.0 ssd 1541710959.980416775 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 852, "value": 0.15146666666666667}

:::MLPv0.5.0 ssd 1541710960.075715303 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 853, "value": 0.15164444444444444}

:::MLPv0.5.0 ssd 1541710960.170400858 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 854, "value": 0.15182222222222222}

:::MLPv0.5.0 ssd 1541710960.265084267 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 855, "value": 0.152}

:::MLPv0.5.0 ssd 1541710960.359847784 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 856, "value": 0.15217777777777777}

:::MLPv0.5.0 ssd 1541710960.454738855 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 857, "value": 0.15235555555555555}

:::MLPv0.5.0 ssd 1541710960.549604654 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 858, "value": 0.15253333333333333}

:::MLPv0.5.0 ssd 1541710960.643937826 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 859, "value": 0.1527111111111111}

:::MLPv0.5.0 ssd 1541710960.737689257 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 860, "value": 0.15288888888888888}
Iteration:    860, Loss function: 5.241, Average Loss: 4.000, avg. samples / sec: 21694.26
Iteration:    860, Loss function: 5.748, Average Loss: 4.000, avg. samples / sec: 21692.40
Iteration:    860, Loss function: 5.972, Average Loss: 4.007, avg. samples / sec: 21691.04
Iteration:    860, Loss function: 5.325, Average Loss: 4.013, avg. samples / sec: 21700.46
Iteration:    860, Loss function: 5.301, Average Loss: 4.007, avg. samples / sec: 21689.65
Iteration:    860, Loss function: 5.533, Average Loss: 4.006, avg. samples / sec: 21692.73
Iteration:    860, Loss function: 5.212, Average Loss: 4.006, avg. samples / sec: 21689.74
Iteration:    860, Loss function: 5.726, Average Loss: 3.997, avg. samples / sec: 21689.36

:::MLPv0.5.0 ssd 1541710960.833588362 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 861, "value": 0.15306666666666666}

:::MLPv0.5.0 ssd 1541710960.928087711 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 862, "value": 0.15324444444444446}

:::MLPv0.5.0 ssd 1541710961.021826506 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 863, "value": 0.15342222222222224}

:::MLPv0.5.0 ssd 1541710961.116988182 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 864, "value": 0.15360000000000001}

:::MLPv0.5.0 ssd 1541710961.211360216 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 865, "value": 0.1537777777777778}

:::MLPv0.5.0 ssd 1541710961.306839228 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 866, "value": 0.15395555555555557}

:::MLPv0.5.0 ssd 1541710961.397313595 (train.py:553) train_epoch: 15

:::MLPv0.5.0 ssd 1541710961.402441502 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 867, "value": 0.15413333333333334}

:::MLPv0.5.0 ssd 1541710961.496608973 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 868, "value": 0.15431111111111112}

:::MLPv0.5.0 ssd 1541710961.590478182 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 869, "value": 0.1544888888888889}

:::MLPv0.5.0 ssd 1541710961.684135437 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 870, "value": 0.15466666666666667}

:::MLPv0.5.0 ssd 1541710961.778222799 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 871, "value": 0.15484444444444445}

:::MLPv0.5.0 ssd 1541710961.872744560 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 872, "value": 0.15502222222222223}

:::MLPv0.5.0 ssd 1541710961.966293097 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 873, "value": 0.1552}

:::MLPv0.5.0 ssd 1541710962.060444832 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 874, "value": 0.15537777777777778}

:::MLPv0.5.0 ssd 1541710962.154135466 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 875, "value": 0.15555555555555556}

:::MLPv0.5.0 ssd 1541710962.248426199 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 876, "value": 0.15573333333333333}

:::MLPv0.5.0 ssd 1541710962.343042850 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 877, "value": 0.1559111111111111}

:::MLPv0.5.0 ssd 1541710962.437559128 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 878, "value": 0.1560888888888889}

:::MLPv0.5.0 ssd 1541710962.531752348 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 879, "value": 0.15626666666666666}

:::MLPv0.5.0 ssd 1541710962.625513077 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 880, "value": 0.15644444444444444}
Iteration:    880, Loss function: 4.347, Average Loss: 4.022, avg. samples / sec: 21697.37
Iteration:    880, Loss function: 4.826, Average Loss: 4.030, avg. samples / sec: 21705.13
Iteration:    880, Loss function: 5.318, Average Loss: 4.026, avg. samples / sec: 21708.03
Iteration:    880, Loss function: 4.896, Average Loss: 4.023, avg. samples / sec: 21695.24
Iteration:    880, Loss function: 4.618, Average Loss: 4.033, avg. samples / sec: 21698.18
Iteration:    880, Loss function: 4.448, Average Loss: 4.019, avg. samples / sec: 21706.17
Iteration:    880, Loss function: 4.962, Average Loss: 4.029, avg. samples / sec: 21696.24
Iteration:    880, Loss function: 4.728, Average Loss: 4.031, avg. samples / sec: 21670.99

:::MLPv0.5.0 ssd 1541710962.720756531 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 881, "value": 0.15662222222222222}

:::MLPv0.5.0 ssd 1541710962.816232443 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 882, "value": 0.1568}

:::MLPv0.5.0 ssd 1541710962.910889149 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 883, "value": 0.15697777777777777}

:::MLPv0.5.0 ssd 1541710963.008932352 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 884, "value": 0.15715555555555555}

:::MLPv0.5.0 ssd 1541710963.103847980 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 885, "value": 0.15733333333333333}

:::MLPv0.5.0 ssd 1541710963.197806835 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 886, "value": 0.1575111111111111}

:::MLPv0.5.0 ssd 1541710963.293068171 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 887, "value": 0.15768888888888888}

:::MLPv0.5.0 ssd 1541710963.389195681 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 888, "value": 0.15786666666666668}

:::MLPv0.5.0 ssd 1541710963.482795000 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 889, "value": 0.15804444444444446}

:::MLPv0.5.0 ssd 1541710963.576508760 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 890, "value": 0.15822222222222224}

:::MLPv0.5.0 ssd 1541710963.670333862 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 891, "value": 0.1584}

:::MLPv0.5.0 ssd 1541710963.764155149 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 892, "value": 0.1585777777777778}

:::MLPv0.5.0 ssd 1541710963.858077288 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 893, "value": 0.15875555555555557}

:::MLPv0.5.0 ssd 1541710963.951999426 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 894, "value": 0.15893333333333334}

:::MLPv0.5.0 ssd 1541710964.046337128 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 895, "value": 0.15911111111111112}

:::MLPv0.5.0 ssd 1541710964.139987230 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 896, "value": 0.1592888888888889}

:::MLPv0.5.0 ssd 1541710964.234520435 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 897, "value": 0.15946666666666667}

:::MLPv0.5.0 ssd 1541710964.328110218 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 898, "value": 0.15964444444444445}

:::MLPv0.5.0 ssd 1541710964.422939301 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 899, "value": 0.15982222222222223}
Iteration:    900, Loss function: 5.347, Average Loss: 4.046, avg. samples / sec: 21680.39
Iteration:    900, Loss function: 5.146, Average Loss: 4.047, avg. samples / sec: 21680.15
Iteration:    900, Loss function: 5.375, Average Loss: 4.049, avg. samples / sec: 21677.70
Iteration:    900, Loss function: 5.153, Average Loss: 4.051, avg. samples / sec: 21708.25
Iteration:    900, Loss function: 4.966, Average Loss: 4.050, avg. samples / sec: 21679.81
Iteration:    900, Loss function: 4.861, Average Loss: 4.048, avg. samples / sec: 21681.72
Iteration:    900, Loss function: 5.234, Average Loss: 4.036, avg. samples / sec: 21673.69
Iteration:    900, Loss function: 4.800, Average Loss: 4.042, avg. samples / sec: 21663.53
Iteration:    920, Loss function: 4.866, Average Loss: 4.066, avg. samples / sec: 21962.88
Iteration:    920, Loss function: 4.231, Average Loss: 4.065, avg. samples / sec: 21965.59
Iteration:    920, Loss function: 4.606, Average Loss: 4.070, avg. samples / sec: 21954.43
Iteration:    920, Loss function: 5.093, Average Loss: 4.059, avg. samples / sec: 21966.43
Iteration:    920, Loss function: 4.577, Average Loss: 4.063, avg. samples / sec: 21952.20
Iteration:    920, Loss function: 5.355, Average Loss: 4.068, avg. samples / sec: 21953.24
Iteration:    920, Loss function: 5.427, Average Loss: 4.065, avg. samples / sec: 21949.75
Iteration:    920, Loss function: 5.061, Average Loss: 4.055, avg. samples / sec: 21953.46

:::MLPv0.5.0 ssd 1541710966.840950966 (train.py:553) train_epoch: 16
Iteration:    940, Loss function: 4.828, Average Loss: 4.082, avg. samples / sec: 21985.61
Iteration:    940, Loss function: 5.336, Average Loss: 4.084, avg. samples / sec: 21992.85
Iteration:    940, Loss function: 4.407, Average Loss: 4.077, avg. samples / sec: 21989.85
Iteration:    940, Loss function: 5.397, Average Loss: 4.087, avg. samples / sec: 21991.95
Iteration:    940, Loss function: 5.572, Average Loss: 4.088, avg. samples / sec: 21987.38
Iteration:    940, Loss function: 5.564, Average Loss: 4.084, avg. samples / sec: 21983.20
Iteration:    940, Loss function: 5.071, Average Loss: 4.073, avg. samples / sec: 21995.01
Iteration:    940, Loss function: 5.432, Average Loss: 4.083, avg. samples / sec: 21985.20
Iteration:    960, Loss function: 5.404, Average Loss: 4.103, avg. samples / sec: 21859.56
Iteration:    960, Loss function: 5.487, Average Loss: 4.108, avg. samples / sec: 21855.97
Iteration:    960, Loss function: 4.441, Average Loss: 4.102, avg. samples / sec: 21849.88
Iteration:    960, Loss function: 4.682, Average Loss: 4.105, avg. samples / sec: 21854.77
Iteration:    960, Loss function: 5.312, Average Loss: 4.104, avg. samples / sec: 21857.79
Iteration:    960, Loss function: 5.279, Average Loss: 4.104, avg. samples / sec: 21851.57
Iteration:    960, Loss function: 5.461, Average Loss: 4.093, avg. samples / sec: 21855.18
Iteration:    960, Loss function: 4.842, Average Loss: 4.095, avg. samples / sec: 21846.14
Iteration:    980, Loss function: 4.801, Average Loss: 4.120, avg. samples / sec: 21867.19
Iteration:    980, Loss function: 4.444, Average Loss: 4.125, avg. samples / sec: 21864.67
Iteration:    980, Loss function: 5.000, Average Loss: 4.120, avg. samples / sec: 21868.44
Iteration:    980, Loss function: 4.489, Average Loss: 4.119, avg. samples / sec: 21865.92
Iteration:    980, Loss function: 4.688, Average Loss: 4.123, avg. samples / sec: 21862.01
Iteration:    980, Loss function: 4.618, Average Loss: 4.112, avg. samples / sec: 21866.95
Iteration:    980, Loss function: 4.686, Average Loss: 4.119, avg. samples / sec: 21852.76
Iteration:    980, Loss function: 4.812, Average Loss: 4.110, avg. samples / sec: 21859.72

:::MLPv0.5.0 ssd 1541710972.176116228 (train.py:553) train_epoch: 17
Iteration:   1000, Loss function: 5.446, Average Loss: 4.131, avg. samples / sec: 21855.12
Iteration:   1000, Loss function: 4.569, Average Loss: 4.132, avg. samples / sec: 21845.17
Iteration:   1000, Loss function: 4.644, Average Loss: 4.132, avg. samples / sec: 21846.99
Iteration:   1000, Loss function: 4.745, Average Loss: 4.132, avg. samples / sec: 21844.12
Iteration:   1000, Loss function: 5.030, Average Loss: 4.125, avg. samples / sec: 21847.37
Iteration:   1000, Loss function: 4.731, Average Loss: 4.124, avg. samples / sec: 21848.00
Iteration:   1000, Loss function: 4.505, Average Loss: 4.136, avg. samples / sec: 21840.96
Iteration:   1000, Loss function: 5.927, Average Loss: 4.138, avg. samples / sec: 21836.00
Iteration:   1020, Loss function: 4.593, Average Loss: 4.148, avg. samples / sec: 21922.74
Iteration:   1020, Loss function: 4.508, Average Loss: 4.148, avg. samples / sec: 21917.97
Iteration:   1020, Loss function: 5.224, Average Loss: 4.146, avg. samples / sec: 21923.58
Iteration:   1020, Loss function: 4.895, Average Loss: 4.155, avg. samples / sec: 21930.67
Iteration:   1020, Loss function: 5.136, Average Loss: 4.154, avg. samples / sec: 21927.38
Iteration:   1020, Loss function: 4.328, Average Loss: 4.138, avg. samples / sec: 21926.51
Iteration:   1020, Loss function: 5.205, Average Loss: 4.152, avg. samples / sec: 21916.32
Iteration:   1020, Loss function: 4.721, Average Loss: 4.146, avg. samples / sec: 21920.95

:::MLPv0.5.0 ssd 1541710977.605203867 (train.py:553) train_epoch: 18
Iteration:   1040, Loss function: 4.662, Average Loss: 4.157, avg. samples / sec: 21857.53
Iteration:   1040, Loss function: 4.585, Average Loss: 4.160, avg. samples / sec: 21852.58
Iteration:   1040, Loss function: 5.075, Average Loss: 4.158, avg. samples / sec: 21853.86
Iteration:   1040, Loss function: 4.564, Average Loss: 4.166, avg. samples / sec: 21852.29
Iteration:   1040, Loss function: 4.842, Average Loss: 4.161, avg. samples / sec: 21848.74
Iteration:   1040, Loss function: 4.779, Average Loss: 4.151, avg. samples / sec: 21851.07
Iteration:   1040, Loss function: 5.187, Average Loss: 4.166, avg. samples / sec: 21851.75
Iteration:   1040, Loss function: 4.676, Average Loss: 4.169, avg. samples / sec: 21848.08
Iteration:   1060, Loss function: 4.665, Average Loss: 4.179, avg. samples / sec: 21871.79
Iteration:   1060, Loss function: 4.917, Average Loss: 4.169, avg. samples / sec: 21860.25
Iteration:   1060, Loss function: 4.756, Average Loss: 4.174, avg. samples / sec: 21865.20
Iteration:   1060, Loss function: 4.957, Average Loss: 4.179, avg. samples / sec: 21862.02
Iteration:   1060, Loss function: 5.085, Average Loss: 4.179, avg. samples / sec: 21862.01
Iteration:   1060, Loss function: 4.875, Average Loss: 4.165, avg. samples / sec: 21860.10
Iteration:   1060, Loss function: 5.328, Average Loss: 4.172, avg. samples / sec: 21858.24
Iteration:   1060, Loss function: 4.737, Average Loss: 4.172, avg. samples / sec: 21854.12
Iteration:   1080, Loss function: 4.860, Average Loss: 4.185, avg. samples / sec: 21944.45
Iteration:   1080, Loss function: 4.904, Average Loss: 4.181, avg. samples / sec: 21950.94
Iteration:   1080, Loss function: 4.550, Average Loss: 4.188, avg. samples / sec: 21937.17
Iteration:   1080, Loss function: 4.356, Average Loss: 4.190, avg. samples / sec: 21941.40
Iteration:   1080, Loss function: 4.145, Average Loss: 4.191, avg. samples / sec: 21941.72
Iteration:   1080, Loss function: 4.518, Average Loss: 4.184, avg. samples / sec: 21932.53
Iteration:   1080, Loss function: 5.373, Average Loss: 4.178, avg. samples / sec: 21939.87
Iteration:   1080, Loss function: 4.669, Average Loss: 4.184, avg. samples / sec: 21938.71

:::MLPv0.5.0 ssd 1541710983.034954548 (train.py:553) train_epoch: 19
Iteration:   1100, Loss function: 4.414, Average Loss: 4.189, avg. samples / sec: 21848.11
Iteration:   1100, Loss function: 4.349, Average Loss: 4.199, avg. samples / sec: 21849.50
Iteration:   1100, Loss function: 4.935, Average Loss: 4.198, avg. samples / sec: 21845.76
Iteration:   1100, Loss function: 4.237, Average Loss: 4.194, avg. samples / sec: 21853.67
Iteration:   1100, Loss function: 4.678, Average Loss: 4.192, avg. samples / sec: 21850.48
Iteration:   1100, Loss function: 4.521, Average Loss: 4.192, avg. samples / sec: 21838.68
Iteration:   1100, Loss function: 4.536, Average Loss: 4.199, avg. samples / sec: 21845.45
Iteration:   1100, Loss function: 4.955, Average Loss: 4.187, avg. samples / sec: 21848.79
Iteration:   1120, Loss function: 4.725, Average Loss: 4.212, avg. samples / sec: 21846.01
Iteration:   1120, Loss function: 4.452, Average Loss: 4.204, avg. samples / sec: 21839.80
Iteration:   1120, Loss function: 4.553, Average Loss: 4.200, avg. samples / sec: 21831.75
Iteration:   1120, Loss function: 4.828, Average Loss: 4.211, avg. samples / sec: 21832.34
Iteration:   1120, Loss function: 5.140, Average Loss: 4.205, avg. samples / sec: 21836.59
Iteration:   1120, Loss function: 5.139, Average Loss: 4.211, avg. samples / sec: 21831.89
Iteration:   1120, Loss function: 4.586, Average Loss: 4.205, avg. samples / sec: 21831.33
Iteration:   1120, Loss function: 5.106, Average Loss: 4.198, avg. samples / sec: 21824.04
Iteration:   1140, Loss function: 4.997, Average Loss: 4.219, avg. samples / sec: 21949.52
Iteration:   1140, Loss function: 4.496, Average Loss: 4.210, avg. samples / sec: 21942.53
Iteration:   1140, Loss function: 4.733, Average Loss: 4.209, avg. samples / sec: 21939.63
Iteration:   1140, Loss function: 4.066, Average Loss: 4.214, avg. samples / sec: 21938.87
Iteration:   1140, Loss function: 4.917, Average Loss: 4.213, avg. samples / sec: 21944.76
Iteration:   1140, Loss function: 4.632, Average Loss: 4.220, avg. samples / sec: 21939.60
Iteration:   1140, Loss function: 4.778, Average Loss: 4.222, avg. samples / sec: 21924.18
Iteration:   1140, Loss function: 4.511, Average Loss: 4.206, avg. samples / sec: 21946.65

:::MLPv0.5.0 ssd 1541710988.471152782 (train.py:553) train_epoch: 20
Iteration:   1160, Loss function: 4.938, Average Loss: 4.219, avg. samples / sec: 21793.30
Iteration:   1160, Loss function: 5.358, Average Loss: 4.230, avg. samples / sec: 21800.88
Iteration:   1160, Loss function: 4.794, Average Loss: 4.218, avg. samples / sec: 21790.55
Iteration:   1160, Loss function: 4.930, Average Loss: 4.224, avg. samples / sec: 21788.08
Iteration:   1160, Loss function: 4.100, Average Loss: 4.219, avg. samples / sec: 21785.61
Iteration:   1160, Loss function: 4.917, Average Loss: 4.229, avg. samples / sec: 21787.78
Iteration:   1160, Loss function: 4.301, Average Loss: 4.227, avg. samples / sec: 21775.79
Iteration:   1160, Loss function: 5.291, Average Loss: 4.214, avg. samples / sec: 21780.98
Iteration:   1180, Loss function: 4.141, Average Loss: 4.229, avg. samples / sec: 21890.52
Iteration:   1180, Loss function: 4.637, Average Loss: 4.239, avg. samples / sec: 21889.76
Iteration:   1180, Loss function: 3.952, Average Loss: 4.233, avg. samples / sec: 21898.27
Iteration:   1180, Loss function: 4.974, Average Loss: 4.226, avg. samples / sec: 21892.60
Iteration:   1180, Loss function: 4.359, Average Loss: 4.232, avg. samples / sec: 21887.43
Iteration:   1180, Loss function: 4.436, Average Loss: 4.219, avg. samples / sec: 21901.47
Iteration:   1180, Loss function: 4.497, Average Loss: 4.236, avg. samples / sec: 21887.52
Iteration:   1180, Loss function: 4.833, Average Loss: 4.224, avg. samples / sec: 21880.87
Iteration:   1200, Loss function: 4.258, Average Loss: 4.236, avg. samples / sec: 21936.54
Iteration:   1200, Loss function: 4.476, Average Loss: 4.242, avg. samples / sec: 21939.50
Iteration:   1200, Loss function: 4.706, Average Loss: 4.247, avg. samples / sec: 21935.34
Iteration:   1200, Loss function: 3.980, Average Loss: 4.241, avg. samples / sec: 21939.06
Iteration:   1200, Loss function: 3.787, Average Loss: 4.244, avg. samples / sec: 21935.93
Iteration:   1200, Loss function: 4.854, Average Loss: 4.232, avg. samples / sec: 21930.09
Iteration:   1200, Loss function: 4.525, Average Loss: 4.232, avg. samples / sec: 21937.01
Iteration:   1200, Loss function: 4.902, Average Loss: 4.225, avg. samples / sec: 21933.75

:::MLPv0.5.0 ssd 1541710993.805863619 (train.py:553) train_epoch: 21
Iteration:   1220, Loss function: 4.298, Average Loss: 4.242, avg. samples / sec: 21835.32
Iteration:   1220, Loss function: 4.577, Average Loss: 4.249, avg. samples / sec: 21830.10
Iteration:   1220, Loss function: 3.761, Average Loss: 4.238, avg. samples / sec: 21838.32
Iteration:   1220, Loss function: 4.875, Average Loss: 4.233, avg. samples / sec: 21839.68
Iteration:   1220, Loss function: 4.007, Average Loss: 4.252, avg. samples / sec: 21828.17
Iteration:   1220, Loss function: 4.573, Average Loss: 4.250, avg. samples / sec: 21834.72
Iteration:   1220, Loss function: 4.378, Average Loss: 4.237, avg. samples / sec: 21832.60
Iteration:   1220, Loss function: 5.071, Average Loss: 4.249, avg. samples / sec: 21827.96
Iteration:   1240, Loss function: 5.374, Average Loss: 4.255, avg. samples / sec: 21923.21
Iteration:   1240, Loss function: 3.925, Average Loss: 4.242, avg. samples / sec: 21925.59
Iteration:   1240, Loss function: 4.538, Average Loss: 4.256, avg. samples / sec: 21922.60
Iteration:   1240, Loss function: 4.126, Average Loss: 4.245, avg. samples / sec: 21917.99
Iteration:   1240, Loss function: 4.228, Average Loss: 4.248, avg. samples / sec: 21906.13
Iteration:   1240, Loss function: 4.379, Average Loss: 4.239, avg. samples / sec: 21917.00
Iteration:   1240, Loss function: 4.194, Average Loss: 4.252, avg. samples / sec: 21921.42
Iteration:   1240, Loss function: 5.012, Average Loss: 4.255, avg. samples / sec: 21910.70
Iteration:   1260, Loss function: 3.938, Average Loss: 4.252, avg. samples / sec: 21832.01
Iteration:   1260, Loss function: 4.375, Average Loss: 4.250, avg. samples / sec: 21827.75
Iteration:   1260, Loss function: 4.590, Average Loss: 4.261, avg. samples / sec: 21819.34
Iteration:   1260, Loss function: 4.804, Average Loss: 4.263, avg. samples / sec: 21828.24
Iteration:   1260, Loss function: 4.145, Average Loss: 4.247, avg. samples / sec: 21818.01
Iteration:   1260, Loss function: 4.546, Average Loss: 4.259, avg. samples / sec: 21819.07
Iteration:   1260, Loss function: 4.446, Average Loss: 4.264, avg. samples / sec: 21814.64
Iteration:   1260, Loss function: 4.441, Average Loss: 4.245, avg. samples / sec: 21817.23

:::MLPv0.5.0 ssd 1541710999.229469538 (train.py:553) train_epoch: 22
Iteration:   1280, Loss function: 4.280, Average Loss: 4.269, avg. samples / sec: 21942.16
Iteration:   1280, Loss function: 4.203, Average Loss: 4.249, avg. samples / sec: 21943.86
Iteration:   1280, Loss function: 4.688, Average Loss: 4.263, avg. samples / sec: 21943.93
Iteration:   1280, Loss function: 4.646, Average Loss: 4.264, avg. samples / sec: 21935.97
Iteration:   1280, Loss function: 3.866, Average Loss: 4.253, avg. samples / sec: 21938.97
Iteration:   1280, Loss function: 4.474, Average Loss: 4.256, avg. samples / sec: 21930.33
Iteration:   1280, Loss function: 4.735, Average Loss: 4.268, avg. samples / sec: 21940.00
Iteration:   1280, Loss function: 4.140, Average Loss: 4.257, avg. samples / sec: 21923.20
Iteration:   1300, Loss function: 4.484, Average Loss: 4.278, avg. samples / sec: 21829.66
Iteration:   1300, Loss function: 4.334, Average Loss: 4.265, avg. samples / sec: 21837.86
Iteration:   1300, Loss function: 4.808, Average Loss: 4.265, avg. samples / sec: 21830.11
Iteration:   1300, Loss function: 4.997, Average Loss: 4.275, avg. samples / sec: 21831.96
Iteration:   1300, Loss function: 4.506, Average Loss: 4.267, avg. samples / sec: 21826.77
Iteration:   1300, Loss function: 4.325, Average Loss: 4.271, avg. samples / sec: 21827.58
Iteration:   1300, Loss function: 4.164, Average Loss: 4.259, avg. samples / sec: 21826.84
Iteration:   1300, Loss function: 4.578, Average Loss: 4.257, avg. samples / sec: 21825.92
Iteration:   1320, Loss function: 4.442, Average Loss: 4.263, avg. samples / sec: 21969.65
Iteration:   1320, Loss function: 4.181, Average Loss: 4.280, avg. samples / sec: 21958.67
Iteration:   1320, Loss function: 4.179, Average Loss: 4.279, avg. samples / sec: 21966.15
Iteration:   1320, Loss function: 5.155, Average Loss: 4.273, avg. samples / sec: 21965.04
Iteration:   1320, Loss function: 4.491, Average Loss: 4.278, avg. samples / sec: 21962.89
Iteration:   1320, Loss function: 4.628, Average Loss: 4.263, avg. samples / sec: 21964.01
Iteration:   1320, Loss function: 4.580, Average Loss: 4.271, avg. samples / sec: 21957.00
Iteration:   1320, Loss function: 5.416, Average Loss: 4.270, avg. samples / sec: 21960.49

:::MLPv0.5.0 ssd 1541711004.656549215 (train.py:553) train_epoch: 23
Iteration:   1340, Loss function: 5.023, Average Loss: 4.287, avg. samples / sec: 21834.85
Iteration:   1340, Loss function: 4.317, Average Loss: 4.283, avg. samples / sec: 21834.86
Iteration:   1340, Loss function: 4.610, Average Loss: 4.277, avg. samples / sec: 21834.05
Iteration:   1340, Loss function: 4.804, Average Loss: 4.273, avg. samples / sec: 21831.72
Iteration:   1340, Loss function: 4.753, Average Loss: 4.278, avg. samples / sec: 21830.02
Iteration:   1340, Loss function: 4.225, Average Loss: 4.285, avg. samples / sec: 21830.05
Iteration:   1340, Loss function: 5.200, Average Loss: 4.271, avg. samples / sec: 21815.11
Iteration:   1340, Loss function: 4.604, Average Loss: 4.277, avg. samples / sec: 21816.75
Iteration:   1360, Loss function: 4.242, Average Loss: 4.285, avg. samples / sec: 21977.38
Iteration:   1360, Loss function: 4.014, Average Loss: 4.284, avg. samples / sec: 21976.49
Iteration:   1360, Loss function: 4.139, Average Loss: 4.292, avg. samples / sec: 21965.03
Iteration:   1360, Loss function: 4.202, Average Loss: 4.284, avg. samples / sec: 21987.14
Iteration:   1360, Loss function: 4.670, Average Loss: 4.281, avg. samples / sec: 21970.04
Iteration:   1360, Loss function: 4.709, Average Loss: 4.278, avg. samples / sec: 21979.56
Iteration:   1360, Loss function: 4.247, Average Loss: 4.290, avg. samples / sec: 21968.85
Iteration:   1360, Loss function: 5.169, Average Loss: 4.291, avg. samples / sec: 21955.37
Iteration:   1380, Loss function: 4.266, Average Loss: 4.287, avg. samples / sec: 21934.61
Iteration:   1380, Loss function: 4.008, Average Loss: 4.287, avg. samples / sec: 21937.99
Iteration:   1380, Loss function: 4.087, Average Loss: 4.293, avg. samples / sec: 21943.44
Iteration:   1380, Loss function: 4.712, Average Loss: 4.296, avg. samples / sec: 21931.98
Iteration:   1380, Loss function: 4.382, Average Loss: 4.284, avg. samples / sec: 21932.79
Iteration:   1380, Loss function: 4.123, Average Loss: 4.290, avg. samples / sec: 21935.77
Iteration:   1380, Loss function: 4.089, Average Loss: 4.286, avg. samples / sec: 21915.87
Iteration:   1380, Loss function: 4.678, Average Loss: 4.280, avg. samples / sec: 21922.04

:::MLPv0.5.0 ssd 1541711010.078333378 (train.py:553) train_epoch: 24
Iteration:   1400, Loss function: 4.438, Average Loss: 4.286, avg. samples / sec: 21874.88
Iteration:   1400, Loss function: 4.089, Average Loss: 4.291, avg. samples / sec: 21854.86
Iteration:   1400, Loss function: 4.576, Average Loss: 4.282, avg. samples / sec: 21874.24
Iteration:   1400, Loss function: 4.888, Average Loss: 4.294, avg. samples / sec: 21861.18
Iteration:   1400, Loss function: 4.261, Average Loss: 4.293, avg. samples / sec: 21856.07
Iteration:   1400, Loss function: 4.365, Average Loss: 4.288, avg. samples / sec: 21853.89
Iteration:   1400, Loss function: 4.788, Average Loss: 4.289, avg. samples / sec: 21852.34
Iteration:   1400, Loss function: 4.355, Average Loss: 4.298, avg. samples / sec: 21845.75
Iteration:   1420, Loss function: 4.790, Average Loss: 4.300, avg. samples / sec: 21807.45
Iteration:   1420, Loss function: 4.539, Average Loss: 4.292, avg. samples / sec: 21791.72
Iteration:   1420, Loss function: 4.120, Average Loss: 4.288, avg. samples / sec: 21784.71
Iteration:   1420, Loss function: 4.934, Average Loss: 4.293, avg. samples / sec: 21783.81
Iteration:   1420, Loss function: 4.544, Average Loss: 4.297, avg. samples / sec: 21787.54
Iteration:   1420, Loss function: 4.082, Average Loss: 4.295, avg. samples / sec: 21787.94
Iteration:   1420, Loss function: 4.573, Average Loss: 4.285, avg. samples / sec: 21787.51
Iteration:   1420, Loss function: 4.237, Average Loss: 4.292, avg. samples / sec: 21792.80
Iteration:   1440, Loss function: 4.735, Average Loss: 4.304, avg. samples / sec: 21808.31
Iteration:   1440, Loss function: 3.779, Average Loss: 4.292, avg. samples / sec: 21813.59
Iteration:   1440, Loss function: 4.374, Average Loss: 4.296, avg. samples / sec: 21815.03
Iteration:   1440, Loss function: 4.599, Average Loss: 4.292, avg. samples / sec: 21811.78
Iteration:   1440, Loss function: 4.512, Average Loss: 4.298, avg. samples / sec: 21811.86
Iteration:   1440, Loss function: 4.352, Average Loss: 4.300, avg. samples / sec: 21809.24
Iteration:   1440, Loss function: 4.802, Average Loss: 4.297, avg. samples / sec: 21805.23
Iteration:   1440, Loss function: 4.442, Average Loss: 4.287, avg. samples / sec: 21797.20

:::MLPv0.5.0 ssd 1541711015.433769703 (train.py:553) train_epoch: 25
Iteration:   1460, Loss function: 3.608, Average Loss: 4.304, avg. samples / sec: 21872.12
Iteration:   1460, Loss function: 4.159, Average Loss: 4.298, avg. samples / sec: 21867.22
Iteration:   1460, Loss function: 4.073, Average Loss: 4.295, avg. samples / sec: 21862.57
Iteration:   1460, Loss function: 4.051, Average Loss: 4.301, avg. samples / sec: 21863.06
Iteration:   1460, Loss function: 4.258, Average Loss: 4.297, avg. samples / sec: 21870.91
Iteration:   1460, Loss function: 4.626, Average Loss: 4.296, avg. samples / sec: 21856.24
Iteration:   1460, Loss function: 3.865, Average Loss: 4.288, avg. samples / sec: 21866.63
Iteration:   1460, Loss function: 4.077, Average Loss: 4.302, avg. samples / sec: 21853.19
Iteration:   1480, Loss function: 4.595, Average Loss: 4.299, avg. samples / sec: 21809.28
Iteration:   1480, Loss function: 4.786, Average Loss: 4.296, avg. samples / sec: 21820.76
Iteration:   1480, Loss function: 4.659, Average Loss: 4.306, avg. samples / sec: 21798.91
Iteration:   1480, Loss function: 4.529, Average Loss: 4.297, avg. samples / sec: 21810.86
Iteration:   1480, Loss function: 4.233, Average Loss: 4.296, avg. samples / sec: 21804.54
Iteration:   1480, Loss function: 4.658, Average Loss: 4.303, avg. samples / sec: 21819.84
Iteration:   1480, Loss function: 4.641, Average Loss: 4.304, avg. samples / sec: 21806.73
Iteration:   1480, Loss function: 4.179, Average Loss: 4.289, avg. samples / sec: 21810.55
Iteration:   1500, Loss function: 4.731, Average Loss: 4.301, avg. samples / sec: 21958.61
Iteration:   1500, Loss function: 4.073, Average Loss: 4.308, avg. samples / sec: 21962.03
Iteration:   1500, Loss function: 4.128, Average Loss: 4.300, avg. samples / sec: 21957.08
Iteration:   1500, Loss function: 3.773, Average Loss: 4.303, avg. samples / sec: 21958.66
Iteration:   1500, Loss function: 4.525, Average Loss: 4.307, avg. samples / sec: 21956.59
Iteration:   1500, Loss function: 4.461, Average Loss: 4.297, avg. samples / sec: 21945.73
Iteration:   1500, Loss function: 4.404, Average Loss: 4.291, avg. samples / sec: 21961.42
Iteration:   1500, Loss function: 4.405, Average Loss: 4.298, avg. samples / sec: 21952.05

:::MLPv0.5.0 ssd 1541711020.860393286 (train.py:553) train_epoch: 26
Iteration:   1520, Loss function: 4.204, Average Loss: 4.312, avg. samples / sec: 21906.97
Iteration:   1520, Loss function: 4.321, Average Loss: 4.309, avg. samples / sec: 21915.90
Iteration:   1520, Loss function: 4.524, Average Loss: 4.306, avg. samples / sec: 21913.04
Iteration:   1520, Loss function: 4.844, Average Loss: 4.304, avg. samples / sec: 21901.73
Iteration:   1520, Loss function: 4.567, Average Loss: 4.303, avg. samples / sec: 21912.36
Iteration:   1520, Loss function: 4.769, Average Loss: 4.303, avg. samples / sec: 21906.97
Iteration:   1520, Loss function: 4.800, Average Loss: 4.300, avg. samples / sec: 21910.58
Iteration:   1520, Loss function: 4.521, Average Loss: 4.295, avg. samples / sec: 21906.71
Iteration:   1540, Loss function: 4.129, Average Loss: 4.304, avg. samples / sec: 21899.61
Iteration:   1540, Loss function: 4.451, Average Loss: 4.313, avg. samples / sec: 21888.83
Iteration:   1540, Loss function: 4.534, Average Loss: 4.309, avg. samples / sec: 21892.33
Iteration:   1540, Loss function: 4.602, Average Loss: 4.304, avg. samples / sec: 21897.56
Iteration:   1540, Loss function: 4.591, Average Loss: 4.297, avg. samples / sec: 21902.19
Iteration:   1540, Loss function: 4.464, Average Loss: 4.302, avg. samples / sec: 21895.94
Iteration:   1540, Loss function: 4.129, Average Loss: 4.299, avg. samples / sec: 21894.43
Iteration:   1540, Loss function: 4.049, Average Loss: 4.306, avg. samples / sec: 21886.02

:::MLPv0.5.0 ssd 1541711026.286328316 (train.py:553) train_epoch: 27
Iteration:   1560, Loss function: 4.138, Average Loss: 4.305, avg. samples / sec: 21842.83
Iteration:   1560, Loss function: 4.294, Average Loss: 4.304, avg. samples / sec: 21846.18
Iteration:   1560, Loss function: 4.324, Average Loss: 4.296, avg. samples / sec: 21844.98
Iteration:   1560, Loss function: 4.685, Average Loss: 4.301, avg. samples / sec: 21846.48
Iteration:   1560, Loss function: 4.890, Average Loss: 4.311, avg. samples / sec: 21838.89
Iteration:   1560, Loss function: 4.538, Average Loss: 4.308, avg. samples / sec: 21846.30
Iteration:   1560, Loss function: 4.521, Average Loss: 4.307, avg. samples / sec: 21832.27
Iteration:   1560, Loss function: 4.562, Average Loss: 4.316, avg. samples / sec: 21829.73
Iteration:   1580, Loss function: 4.028, Average Loss: 4.304, avg. samples / sec: 21916.77
Iteration:   1580, Loss function: 4.212, Average Loss: 4.308, avg. samples / sec: 21935.05
Iteration:   1580, Loss function: 4.178, Average Loss: 4.302, avg. samples / sec: 21921.53
Iteration:   1580, Loss function: 4.497, Average Loss: 4.313, avg. samples / sec: 21926.70
Iteration:   1580, Loss function: 3.843, Average Loss: 4.300, avg. samples / sec: 21917.78
Iteration:   1580, Loss function: 4.414, Average Loss: 4.295, avg. samples / sec: 21914.21
Iteration:   1580, Loss function: 4.578, Average Loss: 4.309, avg. samples / sec: 21915.03
Iteration:   1580, Loss function: 3.882, Average Loss: 4.314, avg. samples / sec: 21921.30
Iteration:   1600, Loss function: 4.602, Average Loss: 4.303, avg. samples / sec: 21984.40
Iteration:   1600, Loss function: 4.787, Average Loss: 4.315, avg. samples / sec: 22000.55
Iteration:   1600, Loss function: 4.052, Average Loss: 4.308, avg. samples / sec: 21979.40
Iteration:   1600, Loss function: 4.058, Average Loss: 4.311, avg. samples / sec: 21980.33
Iteration:   1600, Loss function: 4.478, Average Loss: 4.301, avg. samples / sec: 21979.38
Iteration:   1600, Loss function: 4.255, Average Loss: 4.310, avg. samples / sec: 21990.32
Iteration:   1600, Loss function: 3.967, Average Loss: 4.295, avg. samples / sec: 21986.46
Iteration:   1600, Loss function: 4.006, Average Loss: 4.301, avg. samples / sec: 21982.06

:::MLPv0.5.0 ssd 1541711031.702732801 (train.py:553) train_epoch: 28
Iteration:   1620, Loss function: 3.793, Average Loss: 4.304, avg. samples / sec: 21895.19
Iteration:   1620, Loss function: 4.106, Average Loss: 4.310, avg. samples / sec: 21894.81
Iteration:   1620, Loss function: 4.348, Average Loss: 4.303, avg. samples / sec: 21892.26
Iteration:   1620, Loss function: 4.339, Average Loss: 4.296, avg. samples / sec: 21891.37
Iteration:   1620, Loss function: 4.714, Average Loss: 4.308, avg. samples / sec: 21891.11
Iteration:   1620, Loss function: 4.887, Average Loss: 4.315, avg. samples / sec: 21888.06
Iteration:   1620, Loss function: 4.352, Average Loss: 4.301, avg. samples / sec: 21892.44
Iteration:   1620, Loss function: 4.229, Average Loss: 4.316, avg. samples / sec: 21879.62
Iteration:   1640, Loss function: 3.920, Average Loss: 4.310, avg. samples / sec: 21907.67
Iteration:   1640, Loss function: 3.872, Average Loss: 4.297, avg. samples / sec: 21907.63
Iteration:   1640, Loss function: 4.022, Average Loss: 4.304, avg. samples / sec: 21897.40
Iteration:   1640, Loss function: 4.888, Average Loss: 4.302, avg. samples / sec: 21903.33
Iteration:   1640, Loss function: 3.799, Average Loss: 4.314, avg. samples / sec: 21904.60
Iteration:   1640, Loss function: 3.820, Average Loss: 4.315, avg. samples / sec: 21904.91
Iteration:   1640, Loss function: 3.756, Average Loss: 4.307, avg. samples / sec: 21902.04
Iteration:   1640, Loss function: 4.176, Average Loss: 4.301, avg. samples / sec: 21904.48
Iteration:   1660, Loss function: 4.177, Average Loss: 4.299, avg. samples / sec: 21877.44
Iteration:   1660, Loss function: 4.076, Average Loss: 4.309, avg. samples / sec: 21863.52
Iteration:   1660, Loss function: 4.547, Average Loss: 4.314, avg. samples / sec: 21875.54
Iteration:   1660, Loss function: 4.336, Average Loss: 4.313, avg. samples / sec: 21870.50
Iteration:   1660, Loss function: 4.284, Average Loss: 4.294, avg. samples / sec: 21865.48
Iteration:   1660, Loss function: 4.053, Average Loss: 4.302, avg. samples / sec: 21864.76
Iteration:   1660, Loss function: 4.411, Average Loss: 4.302, avg. samples / sec: 21866.54
Iteration:   1660, Loss function: 3.900, Average Loss: 4.303, avg. samples / sec: 21869.56

:::MLPv0.5.0 ssd 1541711037.037966490 (train.py:553) train_epoch: 29
Iteration:   1680, Loss function: 4.075, Average Loss: 4.311, avg. samples / sec: 21898.73
Iteration:   1680, Loss function: 4.997, Average Loss: 4.304, avg. samples / sec: 21898.26
Iteration:   1680, Loss function: 4.426, Average Loss: 4.315, avg. samples / sec: 21892.36
Iteration:   1680, Loss function: 4.301, Average Loss: 4.300, avg. samples / sec: 21895.99
Iteration:   1680, Loss function: 4.596, Average Loss: 4.299, avg. samples / sec: 21887.47
Iteration:   1680, Loss function: 4.398, Average Loss: 4.295, avg. samples / sec: 21891.91
Iteration:   1680, Loss function: 3.857, Average Loss: 4.302, avg. samples / sec: 21888.73
Iteration:   1680, Loss function: 4.500, Average Loss: 4.308, avg. samples / sec: 21879.85
Iteration:   1700, Loss function: 4.630, Average Loss: 4.314, avg. samples / sec: 21992.84
Iteration:   1700, Loss function: 4.319, Average Loss: 4.305, avg. samples / sec: 22006.54
Iteration:   1700, Loss function: 4.239, Average Loss: 4.306, avg. samples / sec: 21994.67
Iteration:   1700, Loss function: 4.387, Average Loss: 4.312, avg. samples / sec: 22006.84
Iteration:   1700, Loss function: 4.506, Average Loss: 4.304, avg. samples / sec: 21993.72
Iteration:   1700, Loss function: 4.515, Average Loss: 4.308, avg. samples / sec: 21986.75
Iteration:   1700, Loss function: 4.083, Average Loss: 4.299, avg. samples / sec: 21990.91
Iteration:   1700, Loss function: 4.085, Average Loss: 4.316, avg. samples / sec: 21983.48
Iteration:   1720, Loss function: 4.251, Average Loss: 4.307, avg. samples / sec: 21946.69
Iteration:   1720, Loss function: 4.411, Average Loss: 4.314, avg. samples / sec: 21937.70
Iteration:   1720, Loss function: 4.277, Average Loss: 4.300, avg. samples / sec: 21944.81
Iteration:   1720, Loss function: 3.707, Average Loss: 4.302, avg. samples / sec: 21934.86
Iteration:   1720, Loss function: 4.411, Average Loss: 4.312, avg. samples / sec: 21936.51
Iteration:   1720, Loss function: 4.199, Average Loss: 4.307, avg. samples / sec: 21936.31
Iteration:   1720, Loss function: 3.920, Average Loss: 4.317, avg. samples / sec: 21946.53
Iteration:   1720, Loss function: 4.229, Average Loss: 4.302, avg. samples / sec: 21929.71

:::MLPv0.5.0 ssd 1541711042.448013306 (train.py:553) train_epoch: 30
Iteration:   1740, Loss function: 4.217, Average Loss: 4.316, avg. samples / sec: 21912.55
Iteration:   1740, Loss function: 3.576, Average Loss: 4.303, avg. samples / sec: 21911.01
Iteration:   1740, Loss function: 4.019, Average Loss: 4.311, avg. samples / sec: 21910.38
Iteration:   1740, Loss function: 4.292, Average Loss: 4.297, avg. samples / sec: 21908.59
Iteration:   1740, Loss function: 4.430, Average Loss: 4.302, avg. samples / sec: 21906.48
Iteration:   1740, Loss function: 4.193, Average Loss: 4.307, avg. samples / sec: 21903.12
Iteration:   1740, Loss function: 4.470, Average Loss: 4.308, avg. samples / sec: 21899.89
Iteration:   1740, Loss function: 4.045, Average Loss: 4.302, avg. samples / sec: 21904.98
Iteration:   1760, Loss function: 4.399, Average Loss: 4.316, avg. samples / sec: 21894.94
Iteration:   1760, Loss function: 4.301, Average Loss: 4.311, avg. samples / sec: 21895.86
Iteration:   1760, Loss function: 4.334, Average Loss: 4.306, avg. samples / sec: 21894.23
Iteration:   1760, Loss function: 4.534, Average Loss: 4.304, avg. samples / sec: 21888.72
Iteration:   1760, Loss function: 4.290, Average Loss: 4.302, avg. samples / sec: 21892.04
Iteration:   1760, Loss function: 3.802, Average Loss: 4.309, avg. samples / sec: 21892.54
Iteration:   1760, Loss function: 4.302, Average Loss: 4.298, avg. samples / sec: 21884.97
Iteration:   1760, Loss function: 4.439, Average Loss: 4.303, avg. samples / sec: 21894.93
Iteration:   1780, Loss function: 4.072, Average Loss: 4.302, avg. samples / sec: 21970.17
Iteration:   1780, Loss function: 3.932, Average Loss: 4.308, avg. samples / sec: 21964.15
Iteration:   1780, Loss function: 3.522, Average Loss: 4.304, avg. samples / sec: 21973.40
Iteration:   1780, Loss function: 4.231, Average Loss: 4.314, avg. samples / sec: 21959.46
Iteration:   1780, Loss function: 3.984, Average Loss: 4.297, avg. samples / sec: 21968.16
Iteration:   1780, Loss function: 4.237, Average Loss: 4.301, avg. samples / sec: 21966.67
Iteration:   1780, Loss function: 4.014, Average Loss: 4.299, avg. samples / sec: 21953.81
Iteration:   1780, Loss function: 3.895, Average Loss: 4.304, avg. samples / sec: 21936.66

:::MLPv0.5.0 ssd 1541711047.870051146 (train.py:553) train_epoch: 31
Iteration:   1800, Loss function: 4.402, Average Loss: 4.298, avg. samples / sec: 21847.38
Iteration:   1800, Loss function: 5.323, Average Loss: 4.303, avg. samples / sec: 21829.65
Iteration:   1800, Loss function: 4.263, Average Loss: 4.299, avg. samples / sec: 21861.18
Iteration:   1800, Loss function: 4.424, Average Loss: 4.309, avg. samples / sec: 21826.22
Iteration:   1800, Loss function: 4.459, Average Loss: 4.312, avg. samples / sec: 21828.50
Iteration:   1800, Loss function: 4.716, Average Loss: 4.296, avg. samples / sec: 21830.81
Iteration:   1800, Loss function: 4.458, Average Loss: 4.301, avg. samples / sec: 21835.55
Iteration:   1800, Loss function: 4.394, Average Loss: 4.305, avg. samples / sec: 21815.09
Iteration:   1820, Loss function: 4.517, Average Loss: 4.303, avg. samples / sec: 21776.78
Iteration:   1820, Loss function: 4.300, Average Loss: 4.311, avg. samples / sec: 21774.86
Iteration:   1820, Loss function: 4.353, Average Loss: 4.306, avg. samples / sec: 21787.01
Iteration:   1820, Loss function: 4.778, Average Loss: 4.301, avg. samples / sec: 21777.53
Iteration:   1820, Loss function: 4.599, Average Loss: 4.316, avg. samples / sec: 21774.09
Iteration:   1820, Loss function: 4.077, Average Loss: 4.300, avg. samples / sec: 21768.03
Iteration:   1820, Loss function: 4.370, Average Loss: 4.301, avg. samples / sec: 21765.38
Iteration:   1820, Loss function: 4.526, Average Loss: 4.305, avg. samples / sec: 21768.67
Iteration:   1840, Loss function: 3.626, Average Loss: 4.314, avg. samples / sec: 21956.71
Iteration:   1840, Loss function: 3.791, Average Loss: 4.301, avg. samples / sec: 21956.71
Iteration:   1840, Loss function: 4.069, Average Loss: 4.310, avg. samples / sec: 21951.58
Iteration:   1840, Loss function: 4.322, Average Loss: 4.300, avg. samples / sec: 21947.26
Iteration:   1840, Loss function: 4.100, Average Loss: 4.299, avg. samples / sec: 21960.37
Iteration:   1840, Loss function: 4.136, Average Loss: 4.304, avg. samples / sec: 21949.73
Iteration:   1840, Loss function: 4.078, Average Loss: 4.299, avg. samples / sec: 21950.59
Iteration:   1840, Loss function: 4.099, Average Loss: 4.302, avg. samples / sec: 21956.31

:::MLPv0.5.0 ssd 1541711053.317561865 (train.py:553) train_epoch: 32
Iteration:   1860, Loss function: 3.899, Average Loss: 4.309, avg. samples / sec: 21713.59
Iteration:   1860, Loss function: 4.239, Average Loss: 4.299, avg. samples / sec: 21710.35
Iteration:   1860, Loss function: 4.200, Average Loss: 4.302, avg. samples / sec: 21703.72
Iteration:   1860, Loss function: 4.135, Average Loss: 4.296, avg. samples / sec: 21711.35
Iteration:   1860, Loss function: 4.123, Average Loss: 4.301, avg. samples / sec: 21707.87
Iteration:   1860, Loss function: 3.773, Average Loss: 4.308, avg. samples / sec: 21703.51
Iteration:   1860, Loss function: 3.615, Average Loss: 4.294, avg. samples / sec: 21702.43
Iteration:   1860, Loss function: 4.328, Average Loss: 4.299, avg. samples / sec: 21705.91

































































:::MLPv0.5.0 ssd 1541711055.846295357 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711055.847086191 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711055.847871542 (train.py:220) eval_start: 32
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.19 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.19 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.19 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.19 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.19 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.19 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.19 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.19 s
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(389103, 7)
Converting ndarray to lists...
0/389103
(389103, 7)
0/389103
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(389103, 7)
Loading and preparing results...
Converting ndarray to lists...
0/389103
Converting ndarray to lists...
(389103, 7)
(389103, 7)
0/389103
0/389103
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(389103, 7)
Converting ndarray to lists...
0/389103
(389103, 7)
0/389103
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(389103, 7)
0/389103
Loading and preparing results...
Converting ndarray to lists...
(389103, 7)
Loading and preparing results...
0/389103
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(389103, 7)
Converting ndarray to lists...
0/389103
Loading and preparing results...
Converting ndarray to lists...
(389103, 7)
(389103, 7)
Converting ndarray to lists...
Loading and preparing results...
0/389103
Converting ndarray to lists...
0/389103
(389103, 7)
(389103, 7)
0/389103
Converting ndarray to lists...
(389103, 7)
0/389103
Loading and preparing results...
0/389103
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(389103, 7)
Converting ndarray to lists...
0/389103
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(389103, 7)
(389103, 7)
(389103, 7)
(389103, 7)
0/389103
0/389103
0/389103
Loading and preparing results...
0/389103
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(389103, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(389103, 7)
(389103, 7)
(389103, 7)
Converting ndarray to lists...
0/389103
0/389103
Converting ndarray to lists...
(389103, 7)
(389103, 7)
(389103, 7)
(389103, 7)
0/389103
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/389103
0/389103
Loading and preparing results...
0/389103
Loading and preparing results...
Loading and preparing results...
0/389103
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/389103
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(389103, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(389103, 7)
Loading and preparing results...
(389103, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(389103, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/389103
(389103, 7)
Converting ndarray to lists...
(389103, 7)
(389103, 7)
0/389103
Converting ndarray to lists...
(389103, 7)
Loading and preparing results...
(389103, 7)
0/389103
Loading and preparing results...
0/389103
(389103, 7)
(389103, 7)
0/389103
(389103, 7)
0/389103
Converting ndarray to lists...
(389103, 7)
(389103, 7)
Loading and preparing results...
Loading and preparing results...
0/389103
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/389103
Loading and preparing results...
0/389103
0/389103
(389103, 7)
0/389103
(389103, 7)
0/389103
Loading and preparing results...
0/389103
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/389103
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/389103
Converting ndarray to lists...
(389103, 7)
Converting ndarray to lists...
(389103, 7)
0/389103
Loading and preparing results...
Converting ndarray to lists...
0/389103
Loading and preparing results...
(389103, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(389103, 7)
Converting ndarray to lists...
(389103, 7)
Loading and preparing results...
(389103, 7)
(389103, 7)
(389103, 7)
(389103, 7)
0/389103
0/389103
0/389103
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/389103
0/389103
0/389103
Converting ndarray to lists...
0/389103
0/389103
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(389103, 7)
(389103, 7)
(389103, 7)
Loading and preparing results...
Converting ndarray to lists...
(389103, 7)
0/389103
Loading and preparing results...
0/389103
(389103, 7)
Loading and preparing results...
(389103, 7)
0/389103
Converting ndarray to lists...
0/389103
0/389103
Converting ndarray to lists...
0/389103
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(389103, 7)
(389103, 7)
0/389103
(389103, 7)
(389103, 7)
(389103, 7)
0/389103
0/389103
0/389103
0/389103
DONE (t=2.34s)
creating index...
DONE (t=2.35s)
creating index...
DONE (t=2.36s)
creating index...
DONE (t=2.38s)
creating index...
DONE (t=2.38s)
creating index...
DONE (t=2.38s)
creating index...
DONE (t=2.38s)
creating index...
DONE (t=2.38s)
creating index...
DONE (t=2.39s)
creating index...
DONE (t=2.39s)
creating index...
DONE (t=2.39s)
creating index...
DONE (t=2.39s)
creating index...
DONE (t=2.39s)
creating index...
DONE (t=2.39s)
creating index...
DONE (t=2.40s)
creating index...
DONE (t=2.40s)
creating index...
DONE (t=2.40s)
creating index...
DONE (t=2.40s)
creating index...
DONE (t=2.40s)
creating index...
DONE (t=2.40s)
creating index...
DONE (t=2.40s)
creating index...
DONE (t=2.40s)
creating index...
DONE (t=2.40s)
creating index...
DONE (t=2.41s)
creating index...
DONE (t=2.41s)
creating index...
DONE (t=2.41s)
creating index...
DONE (t=2.41s)
creating index...
DONE (t=2.41s)
creating index...
DONE (t=2.41s)
creating index...
DONE (t=2.41s)
creating index...
DONE (t=2.41s)
creating index...
DONE (t=2.41s)
creating index...
DONE (t=2.41s)
creating index...
DONE (t=2.41s)
creating index...
DONE (t=2.41s)
creating index...
DONE (t=2.41s)
creating index...
DONE (t=2.42s)
creating index...
DONE (t=2.42s)
creating index...
DONE (t=2.42s)
creating index...
DONE (t=2.42s)
creating index...
DONE (t=2.42s)
creating index...
DONE (t=2.42s)
creating index...
DONE (t=2.42s)
creating index...
DONE (t=2.42s)
creating index...
DONE (t=2.43s)
creating index...
DONE (t=2.43s)
creating index...
DONE (t=2.43s)
creating index...
DONE (t=2.43s)
creating index...
DONE (t=2.43s)
creating index...
DONE (t=2.43s)
creating index...
DONE (t=2.44s)
creating index...
DONE (t=2.44s)
creating index...
DONE (t=2.44s)
creating index...
DONE (t=2.45s)
creating index...
DONE (t=2.45s)
creating index...
DONE (t=2.45s)
creating index...
DONE (t=2.45s)
creating index...
DONE (t=2.46s)
creating index...
DONE (t=2.46s)
creating index...
DONE (t=2.46s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.56s)
creating index...
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.58s)
creating index...
DONE (t=2.58s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
DONE (t=2.63s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
DONE (t=4.04s).
Accumulating evaluation results...
DONE (t=4.07s).
Accumulating evaluation results...
DONE (t=4.08s).
Accumulating evaluation results...
DONE (t=4.10s).
Accumulating evaluation results...
DONE (t=4.09s).
Accumulating evaluation results...
DONE (t=4.13s).
Accumulating evaluation results...
DONE (t=4.11s).
Accumulating evaluation results...
DONE (t=4.10s).
Accumulating evaluation results...
DONE (t=1.28s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.132
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.123
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.153
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.236
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.347
Current AP: 0.13182 AP goal: 0.21200
DONE (t=1.37s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.132
DONE (t=1.36s).
DONE (t=1.36s).
DONE (t=1.37s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.132
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.132
DONE (t=1.34s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.132
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.123
DONE (t=1.37s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.132
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.132
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.123
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.123
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.123
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.153
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.123
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.123
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.225
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.236
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.347
Current AP: 0.13182 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.153
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.153
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.153
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.236
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.347
Current AP: 0.13182 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.236
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.347
Current AP: 0.13182 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.236
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.347
Current AP: 0.13182 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.153
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.153
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.236
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.347
Current AP: 0.13182 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.236
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.347
Current AP: 0.13182 AP goal: 0.21200
DONE (t=1.39s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.132
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.257
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.123
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.153
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.236
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.347
Current AP: 0.13182 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711071.320000887 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711071.320958138 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.13181618481596016}

:::MLPv0.5.0 ssd 1541711071.321726322 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.13181618481596016}

:::MLPv0.5.0 ssd 1541711071.322547913 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711071.323347569 (train.py:338) eval_stop: 32
Iteration:   1880, Loss function: 3.821, Average Loss: 4.291, avg. samples / sec: 2289.67
Iteration:   1880, Loss function: 3.898, Average Loss: 4.296, avg. samples / sec: 2289.56
Iteration:   1880, Loss function: 4.493, Average Loss: 4.299, avg. samples / sec: 2289.69
Iteration:   1880, Loss function: 4.328, Average Loss: 4.300, avg. samples / sec: 2289.55
Iteration:   1880, Loss function: 4.306, Average Loss: 4.298, avg. samples / sec: 2289.56
Iteration:   1880, Loss function: 3.845, Average Loss: 4.303, avg. samples / sec: 2289.53
Iteration:   1880, Loss function: 4.351, Average Loss: 4.293, avg. samples / sec: 2289.52
Iteration:   1880, Loss function: 4.281, Average Loss: 4.306, avg. samples / sec: 2289.40
Iteration:   1900, Loss function: 4.618, Average Loss: 4.306, avg. samples / sec: 22017.17
Iteration:   1900, Loss function: 4.388, Average Loss: 4.297, avg. samples / sec: 22002.95
Iteration:   1900, Loss function: 4.053, Average Loss: 4.298, avg. samples / sec: 22007.81
Iteration:   1900, Loss function: 4.757, Average Loss: 4.299, avg. samples / sec: 22005.72
Iteration:   1900, Loss function: 4.264, Average Loss: 4.294, avg. samples / sec: 22009.52
Iteration:   1900, Loss function: 4.479, Average Loss: 4.304, avg. samples / sec: 22009.43
Iteration:   1900, Loss function: 4.399, Average Loss: 4.290, avg. samples / sec: 21988.65
Iteration:   1900, Loss function: 4.622, Average Loss: 4.300, avg. samples / sec: 21989.34

:::MLPv0.5.0 ssd 1541711074.658960104 (train.py:553) train_epoch: 33
Iteration:   1920, Loss function: 4.528, Average Loss: 4.306, avg. samples / sec: 21936.94
Iteration:   1920, Loss function: 4.295, Average Loss: 4.290, avg. samples / sec: 21951.34
Iteration:   1920, Loss function: 4.428, Average Loss: 4.292, avg. samples / sec: 21934.90
Iteration:   1920, Loss function: 4.141, Average Loss: 4.292, avg. samples / sec: 21933.97
Iteration:   1920, Loss function: 3.724, Average Loss: 4.297, avg. samples / sec: 21943.85
Iteration:   1920, Loss function: 4.352, Average Loss: 4.302, avg. samples / sec: 21928.29
Iteration:   1920, Loss function: 4.383, Average Loss: 4.298, avg. samples / sec: 21927.70
Iteration:   1920, Loss function: 3.947, Average Loss: 4.296, avg. samples / sec: 21918.24
Iteration:   1940, Loss function: 4.152, Average Loss: 4.288, avg. samples / sec: 21922.09
Iteration:   1940, Loss function: 4.523, Average Loss: 4.303, avg. samples / sec: 21920.63
Iteration:   1940, Loss function: 4.196, Average Loss: 4.287, avg. samples / sec: 21921.76
Iteration:   1940, Loss function: 3.878, Average Loss: 4.293, avg. samples / sec: 21938.65
Iteration:   1940, Loss function: 4.165, Average Loss: 4.290, avg. samples / sec: 21925.74
Iteration:   1940, Loss function: 3.983, Average Loss: 4.289, avg. samples / sec: 21922.51
Iteration:   1940, Loss function: 3.975, Average Loss: 4.296, avg. samples / sec: 21925.87
Iteration:   1940, Loss function: 4.491, Average Loss: 4.294, avg. samples / sec: 21925.14
Iteration:   1960, Loss function: 4.116, Average Loss: 4.299, avg. samples / sec: 21982.63
Iteration:   1960, Loss function: 3.671, Average Loss: 4.284, avg. samples / sec: 21984.44
Iteration:   1960, Loss function: 4.426, Average Loss: 4.288, avg. samples / sec: 21985.79
Iteration:   1960, Loss function: 4.032, Average Loss: 4.285, avg. samples / sec: 21977.91
Iteration:   1960, Loss function: 4.222, Average Loss: 4.293, avg. samples / sec: 21983.96
Iteration:   1960, Loss function: 3.944, Average Loss: 4.290, avg. samples / sec: 21978.69
Iteration:   1960, Loss function: 3.913, Average Loss: 4.285, avg. samples / sec: 21972.35
Iteration:   1960, Loss function: 4.178, Average Loss: 4.286, avg. samples / sec: 21970.59

:::MLPv0.5.0 ssd 1541711080.076702833 (train.py:553) train_epoch: 34
Iteration:   1980, Loss function: 4.238, Average Loss: 4.286, avg. samples / sec: 21785.50
Iteration:   1980, Loss function: 4.153, Average Loss: 4.301, avg. samples / sec: 21775.93
Iteration:   1980, Loss function: 3.561, Average Loss: 4.288, avg. samples / sec: 21780.49
Iteration:   1980, Loss function: 4.137, Average Loss: 4.285, avg. samples / sec: 21781.60
Iteration:   1980, Loss function: 4.438, Average Loss: 4.293, avg. samples / sec: 21779.99
Iteration:   1980, Loss function: 4.114, Average Loss: 4.286, avg. samples / sec: 21787.47
Iteration:   1980, Loss function: 3.775, Average Loss: 4.284, avg. samples / sec: 21771.99
Iteration:   1980, Loss function: 4.297, Average Loss: 4.290, avg. samples / sec: 21772.75
Iteration:   2000, Loss function: 3.728, Average Loss: 4.292, avg. samples / sec: 21918.63
Iteration:   2000, Loss function: 4.106, Average Loss: 4.283, avg. samples / sec: 21912.15
Iteration:   2000, Loss function: 3.888, Average Loss: 4.281, avg. samples / sec: 21917.04
Iteration:   2000, Loss function: 3.878, Average Loss: 4.295, avg. samples / sec: 21903.49
Iteration:   2000, Loss function: 4.152, Average Loss: 4.286, avg. samples / sec: 21916.08
Iteration:   2000, Loss function: 4.363, Average Loss: 4.284, avg. samples / sec: 21907.60
Iteration:   2000, Loss function: 4.321, Average Loss: 4.282, avg. samples / sec: 21904.56
Iteration:   2000, Loss function: 4.231, Average Loss: 4.280, avg. samples / sec: 21910.74
Iteration:   2020, Loss function: 4.427, Average Loss: 4.295, avg. samples / sec: 21983.24
Iteration:   2020, Loss function: 4.282, Average Loss: 4.282, avg. samples / sec: 21971.73
Iteration:   2020, Loss function: 4.684, Average Loss: 4.290, avg. samples / sec: 21970.19
Iteration:   2020, Loss function: 4.945, Average Loss: 4.282, avg. samples / sec: 21968.80
Iteration:   2020, Loss function: 4.595, Average Loss: 4.280, avg. samples / sec: 21977.16
Iteration:   2020, Loss function: 4.229, Average Loss: 4.286, avg. samples / sec: 21976.56
Iteration:   2020, Loss function: 5.397, Average Loss: 4.281, avg. samples / sec: 21979.16
Iteration:   2020, Loss function: 4.327, Average Loss: 4.280, avg. samples / sec: 21976.42

:::MLPv0.5.0 ssd 1541711085.502025127 (train.py:553) train_epoch: 35
Iteration:   2040, Loss function: 3.877, Average Loss: 4.280, avg. samples / sec: 21891.92
Iteration:   2040, Loss function: 4.051, Average Loss: 4.292, avg. samples / sec: 21888.71
Iteration:   2040, Loss function: 3.521, Average Loss: 4.289, avg. samples / sec: 21890.69
Iteration:   2040, Loss function: 3.950, Average Loss: 4.287, avg. samples / sec: 21892.07
Iteration:   2040, Loss function: 3.679, Average Loss: 4.281, avg. samples / sec: 21887.77
Iteration:   2040, Loss function: 3.669, Average Loss: 4.278, avg. samples / sec: 21888.80
Iteration:   2040, Loss function: 3.715, Average Loss: 4.280, avg. samples / sec: 21886.57
Iteration:   2040, Loss function: 4.974, Average Loss: 4.281, avg. samples / sec: 21871.14
Iteration:   2060, Loss function: 4.614, Average Loss: 4.291, avg. samples / sec: 21939.28
Iteration:   2060, Loss function: 4.483, Average Loss: 4.274, avg. samples / sec: 21938.53
Iteration:   2060, Loss function: 4.916, Average Loss: 4.280, avg. samples / sec: 21942.01
Iteration:   2060, Loss function: 4.113, Average Loss: 4.283, avg. samples / sec: 21939.18
Iteration:   2060, Loss function: 4.330, Average Loss: 4.278, avg. samples / sec: 21956.02
Iteration:   2060, Loss function: 3.548, Average Loss: 4.272, avg. samples / sec: 21938.47
Iteration:   2060, Loss function: 4.848, Average Loss: 4.276, avg. samples / sec: 21940.42
Iteration:   2060, Loss function: 3.947, Average Loss: 4.283, avg. samples / sec: 21921.12

:::MLPv0.5.0 ssd 1541711090.926635027 (train.py:553) train_epoch: 36
Iteration:   2080, Loss function: 3.955, Average Loss: 4.273, avg. samples / sec: 21859.28
Iteration:   2080, Loss function: 4.129, Average Loss: 4.282, avg. samples / sec: 21862.78
Iteration:   2080, Loss function: 3.729, Average Loss: 4.275, avg. samples / sec: 21867.50
Iteration:   2080, Loss function: 4.035, Average Loss: 4.284, avg. samples / sec: 21875.42
Iteration:   2080, Loss function: 3.966, Average Loss: 4.288, avg. samples / sec: 21855.32
Iteration:   2080, Loss function: 3.938, Average Loss: 4.277, avg. samples / sec: 21857.80
Iteration:   2080, Loss function: 4.453, Average Loss: 4.269, avg. samples / sec: 21863.40
Iteration:   2080, Loss function: 4.271, Average Loss: 4.272, avg. samples / sec: 21858.92
Iteration:   2100, Loss function: 3.875, Average Loss: 4.269, avg. samples / sec: 21976.88
Iteration:   2100, Loss function: 4.288, Average Loss: 4.279, avg. samples / sec: 21977.20
Iteration:   2100, Loss function: 4.236, Average Loss: 4.268, avg. samples / sec: 21980.10
Iteration:   2100, Loss function: 3.500, Average Loss: 4.272, avg. samples / sec: 21976.24
Iteration:   2100, Loss function: 4.112, Average Loss: 4.283, avg. samples / sec: 21977.22
Iteration:   2100, Loss function: 4.098, Average Loss: 4.280, avg. samples / sec: 21974.89
Iteration:   2100, Loss function: 4.146, Average Loss: 4.274, avg. samples / sec: 21974.15
Iteration:   2100, Loss function: 4.756, Average Loss: 4.270, avg. samples / sec: 21979.86
Iteration:   2120, Loss function: 3.918, Average Loss: 4.282, avg. samples / sec: 21952.09
Iteration:   2120, Loss function: 3.790, Average Loss: 4.267, avg. samples / sec: 21944.27
Iteration:   2120, Loss function: 4.203, Average Loss: 4.264, avg. samples / sec: 21944.31
Iteration:   2120, Loss function: 4.024, Average Loss: 4.276, avg. samples / sec: 21945.93
Iteration:   2120, Loss function: 4.250, Average Loss: 4.269, avg. samples / sec: 21947.74
Iteration:   2120, Loss function: 3.686, Average Loss: 4.266, avg. samples / sec: 21940.15
Iteration:   2120, Loss function: 4.101, Average Loss: 4.276, avg. samples / sec: 21940.84
Iteration:   2120, Loss function: 3.952, Average Loss: 4.274, avg. samples / sec: 21942.66

:::MLPv0.5.0 ssd 1541711096.340223074 (train.py:553) train_epoch: 37
Iteration:   2140, Loss function: 4.339, Average Loss: 4.263, avg. samples / sec: 21917.29
Iteration:   2140, Loss function: 4.344, Average Loss: 4.271, avg. samples / sec: 21923.80
Iteration:   2140, Loss function: 3.991, Average Loss: 4.267, avg. samples / sec: 21916.97
Iteration:   2140, Loss function: 4.497, Average Loss: 4.260, avg. samples / sec: 21914.43
Iteration:   2140, Loss function: 4.848, Average Loss: 4.279, avg. samples / sec: 21907.29
Iteration:   2140, Loss function: 4.466, Average Loss: 4.273, avg. samples / sec: 21911.35
Iteration:   2140, Loss function: 4.469, Average Loss: 4.273, avg. samples / sec: 21912.06
Iteration:   2140, Loss function: 3.904, Average Loss: 4.261, avg. samples / sec: 21902.43
Iteration:   2160, Loss function: 4.487, Average Loss: 4.257, avg. samples / sec: 21950.01
Iteration:   2160, Loss function: 4.423, Average Loss: 4.271, avg. samples / sec: 21955.44
Iteration:   2160, Loss function: 3.683, Average Loss: 4.273, avg. samples / sec: 21951.58
Iteration:   2160, Loss function: 4.268, Average Loss: 4.258, avg. samples / sec: 21962.88
Iteration:   2160, Loss function: 3.978, Average Loss: 4.268, avg. samples / sec: 21949.37
Iteration:   2160, Loss function: 4.330, Average Loss: 4.257, avg. samples / sec: 21947.63
Iteration:   2160, Loss function: 4.197, Average Loss: 4.264, avg. samples / sec: 21942.27
Iteration:   2160, Loss function: 4.281, Average Loss: 4.270, avg. samples / sec: 21939.18
Iteration:   2180, Loss function: 4.216, Average Loss: 4.267, avg. samples / sec: 21974.14
Iteration:   2180, Loss function: 4.184, Average Loss: 4.252, avg. samples / sec: 21973.31
Iteration:   2180, Loss function: 4.154, Average Loss: 4.253, avg. samples / sec: 21961.87
Iteration:   2180, Loss function: 4.624, Average Loss: 4.270, avg. samples / sec: 21981.50
Iteration:   2180, Loss function: 4.491, Average Loss: 4.254, avg. samples / sec: 21966.30
Iteration:   2180, Loss function: 3.986, Average Loss: 4.265, avg. samples / sec: 21962.95
Iteration:   2180, Loss function: 3.677, Average Loss: 4.262, avg. samples / sec: 21969.53
Iteration:   2180, Loss function: 4.330, Average Loss: 4.267, avg. samples / sec: 21954.14

:::MLPv0.5.0 ssd 1541711101.656499863 (train.py:553) train_epoch: 38
Iteration:   2200, Loss function: 4.203, Average Loss: 4.251, avg. samples / sec: 21970.19
Iteration:   2200, Loss function: 4.356, Average Loss: 4.263, avg. samples / sec: 21956.49
Iteration:   2200, Loss function: 3.232, Average Loss: 4.246, avg. samples / sec: 21957.89
Iteration:   2200, Loss function: 3.750, Average Loss: 4.264, avg. samples / sec: 21964.28
Iteration:   2200, Loss function: 3.737, Average Loss: 4.251, avg. samples / sec: 21963.15
Iteration:   2200, Loss function: 3.749, Average Loss: 4.260, avg. samples / sec: 21964.98
Iteration:   2200, Loss function: 3.594, Average Loss: 4.259, avg. samples / sec: 21965.87
Iteration:   2200, Loss function: 3.653, Average Loss: 4.261, avg. samples / sec: 21970.00
Iteration:   2220, Loss function: 4.098, Average Loss: 4.259, avg. samples / sec: 21902.15
Iteration:   2220, Loss function: 4.056, Average Loss: 4.247, avg. samples / sec: 21886.95
Iteration:   2220, Loss function: 3.703, Average Loss: 4.260, avg. samples / sec: 21891.49
Iteration:   2220, Loss function: 4.558, Average Loss: 4.256, avg. samples / sec: 21891.46
Iteration:   2220, Loss function: 4.658, Average Loss: 4.246, avg. samples / sec: 21891.34
Iteration:   2220, Loss function: 4.437, Average Loss: 4.258, avg. samples / sec: 21891.87
Iteration:   2220, Loss function: 3.776, Average Loss: 4.254, avg. samples / sec: 21889.64
Iteration:   2220, Loss function: 3.965, Average Loss: 4.243, avg. samples / sec: 21882.72
Iteration:   2240, Loss function: 4.122, Average Loss: 4.258, avg. samples / sec: 21948.03
Iteration:   2240, Loss function: 4.031, Average Loss: 4.246, avg. samples / sec: 21950.37
Iteration:   2240, Loss function: 4.636, Average Loss: 4.241, avg. samples / sec: 21954.47
Iteration:   2240, Loss function: 4.372, Average Loss: 4.256, avg. samples / sec: 21933.26
Iteration:   2240, Loss function: 4.226, Average Loss: 4.253, avg. samples / sec: 21951.16
Iteration:   2240, Loss function: 3.853, Average Loss: 4.257, avg. samples / sec: 21947.43
Iteration:   2240, Loss function: 3.885, Average Loss: 4.252, avg. samples / sec: 21944.98
Iteration:   2240, Loss function: 4.450, Average Loss: 4.247, avg. samples / sec: 21935.11

:::MLPv0.5.0 ssd 1541711107.071461201 (train.py:553) train_epoch: 39
Iteration:   2260, Loss function: 3.866, Average Loss: 4.250, avg. samples / sec: 21994.73
Iteration:   2260, Loss function: 3.735, Average Loss: 4.255, avg. samples / sec: 21991.11
Iteration:   2260, Loss function: 3.842, Average Loss: 4.244, avg. samples / sec: 21991.32
Iteration:   2260, Loss function: 4.585, Average Loss: 4.241, avg. samples / sec: 21990.20
Iteration:   2260, Loss function: 3.975, Average Loss: 4.247, avg. samples / sec: 22003.13
Iteration:   2260, Loss function: 4.018, Average Loss: 4.250, avg. samples / sec: 21987.15
Iteration:   2260, Loss function: 3.920, Average Loss: 4.253, avg. samples / sec: 21986.67
Iteration:   2260, Loss function: 3.739, Average Loss: 4.250, avg. samples / sec: 21983.31
Iteration:   2280, Loss function: 4.395, Average Loss: 4.246, avg. samples / sec: 21966.49
Iteration:   2280, Loss function: 3.892, Average Loss: 4.250, avg. samples / sec: 21966.81
Iteration:   2280, Loss function: 3.632, Average Loss: 4.247, avg. samples / sec: 21970.79
Iteration:   2280, Loss function: 4.189, Average Loss: 4.244, avg. samples / sec: 21974.90
Iteration:   2280, Loss function: 3.852, Average Loss: 4.238, avg. samples / sec: 21963.84
Iteration:   2280, Loss function: 4.144, Average Loss: 4.241, avg. samples / sec: 21959.89
Iteration:   2280, Loss function: 4.000, Average Loss: 4.244, avg. samples / sec: 21960.80
Iteration:   2280, Loss function: 4.667, Average Loss: 4.249, avg. samples / sec: 21965.93
Iteration:   2300, Loss function: 3.639, Average Loss: 4.240, avg. samples / sec: 21960.79
Iteration:   2300, Loss function: 3.899, Average Loss: 4.240, avg. samples / sec: 21952.30
Iteration:   2300, Loss function: 3.685, Average Loss: 4.244, avg. samples / sec: 21947.18
Iteration:   2300, Loss function: 3.864, Average Loss: 4.235, avg. samples / sec: 21953.38
Iteration:   2300, Loss function: 3.434, Average Loss: 4.247, avg. samples / sec: 21948.20
Iteration:   2300, Loss function: 3.428, Average Loss: 4.240, avg. samples / sec: 21953.03
Iteration:   2300, Loss function: 4.055, Average Loss: 4.239, avg. samples / sec: 21949.98
Iteration:   2300, Loss function: 3.903, Average Loss: 4.245, avg. samples / sec: 21954.00

:::MLPv0.5.0 ssd 1541711112.482395172 (train.py:553) train_epoch: 40
Iteration:   2320, Loss function: 4.271, Average Loss: 4.241, avg. samples / sec: 21898.07
Iteration:   2320, Loss function: 4.266, Average Loss: 4.242, avg. samples / sec: 21902.99
Iteration:   2320, Loss function: 4.876, Average Loss: 4.237, avg. samples / sec: 21891.24
Iteration:   2320, Loss function: 3.953, Average Loss: 4.228, avg. samples / sec: 21896.74
Iteration:   2320, Loss function: 4.124, Average Loss: 4.237, avg. samples / sec: 21894.44
Iteration:   2320, Loss function: 4.539, Average Loss: 4.240, avg. samples / sec: 21897.78
Iteration:   2320, Loss function: 4.454, Average Loss: 4.242, avg. samples / sec: 21895.06
Iteration:   2320, Loss function: 4.339, Average Loss: 4.232, avg. samples / sec: 21893.49
Iteration:   2340, Loss function: 3.780, Average Loss: 4.236, avg. samples / sec: 21961.01
Iteration:   2340, Loss function: 3.936, Average Loss: 4.227, avg. samples / sec: 21959.61
Iteration:   2340, Loss function: 3.786, Average Loss: 4.238, avg. samples / sec: 21958.20
Iteration:   2340, Loss function: 4.055, Average Loss: 4.236, avg. samples / sec: 21957.91
Iteration:   2340, Loss function: 3.520, Average Loss: 4.238, avg. samples / sec: 21957.04
Iteration:   2340, Loss function: 4.232, Average Loss: 4.241, avg. samples / sec: 21955.37
Iteration:   2340, Loss function: 4.024, Average Loss: 4.236, avg. samples / sec: 21951.74
Iteration:   2340, Loss function: 3.786, Average Loss: 4.230, avg. samples / sec: 21958.47
Iteration:   2360, Loss function: 4.219, Average Loss: 4.232, avg. samples / sec: 21849.81
Iteration:   2360, Loss function: 3.537, Average Loss: 4.233, avg. samples / sec: 21855.47
Iteration:   2360, Loss function: 4.066, Average Loss: 4.222, avg. samples / sec: 21847.77
Iteration:   2360, Loss function: 3.992, Average Loss: 4.233, avg. samples / sec: 21851.03
Iteration:   2360, Loss function: 4.080, Average Loss: 4.231, avg. samples / sec: 21845.57
Iteration:   2360, Loss function: 4.454, Average Loss: 4.236, avg. samples / sec: 21848.33
Iteration:   2360, Loss function: 3.749, Average Loss: 4.224, avg. samples / sec: 21849.85
Iteration:   2360, Loss function: 4.482, Average Loss: 4.235, avg. samples / sec: 21842.72

:::MLPv0.5.0 ssd 1541711117.908698559 (train.py:553) train_epoch: 41
Iteration:   2380, Loss function: 4.117, Average Loss: 4.218, avg. samples / sec: 21863.12
Iteration:   2380, Loss function: 4.384, Average Loss: 4.229, avg. samples / sec: 21856.08
Iteration:   2380, Loss function: 3.977, Average Loss: 4.229, avg. samples / sec: 21859.37
Iteration:   2380, Loss function: 3.967, Average Loss: 4.226, avg. samples / sec: 21862.13
Iteration:   2380, Loss function: 4.409, Average Loss: 4.232, avg. samples / sec: 21857.30
Iteration:   2380, Loss function: 4.333, Average Loss: 4.232, avg. samples / sec: 21860.71
Iteration:   2380, Loss function: 3.609, Average Loss: 4.229, avg. samples / sec: 21861.24
Iteration:   2380, Loss function: 4.441, Average Loss: 4.223, avg. samples / sec: 21859.21
Iteration:   2400, Loss function: 3.648, Average Loss: 4.228, avg. samples / sec: 21841.49
Iteration:   2400, Loss function: 4.393, Average Loss: 4.229, avg. samples / sec: 21837.53
Iteration:   2400, Loss function: 3.826, Average Loss: 4.216, avg. samples / sec: 21831.27
Iteration:   2400, Loss function: 3.907, Average Loss: 4.230, avg. samples / sec: 21839.12
Iteration:   2400, Loss function: 3.981, Average Loss: 4.227, avg. samples / sec: 21838.00
Iteration:   2400, Loss function: 4.065, Average Loss: 4.222, avg. samples / sec: 21839.17
Iteration:   2400, Loss function: 4.381, Average Loss: 4.227, avg. samples / sec: 21835.13
Iteration:   2400, Loss function: 4.551, Average Loss: 4.230, avg. samples / sec: 21836.40
Iteration:   2420, Loss function: 3.962, Average Loss: 4.214, avg. samples / sec: 21930.52
Iteration:   2420, Loss function: 4.473, Average Loss: 4.223, avg. samples / sec: 21919.38
Iteration:   2420, Loss function: 4.289, Average Loss: 4.226, avg. samples / sec: 21926.37
Iteration:   2420, Loss function: 4.281, Average Loss: 4.223, avg. samples / sec: 21923.46
Iteration:   2420, Loss function: 4.195, Average Loss: 4.226, avg. samples / sec: 21916.79
Iteration:   2420, Loss function: 4.143, Average Loss: 4.219, avg. samples / sec: 21920.14
Iteration:   2420, Loss function: 4.047, Average Loss: 4.223, avg. samples / sec: 21915.97
Iteration:   2420, Loss function: 4.028, Average Loss: 4.223, avg. samples / sec: 21922.49

:::MLPv0.5.0 ssd 1541711123.246311665 (train.py:553) train_epoch: 42
Iteration:   2440, Loss function: 4.093, Average Loss: 4.211, avg. samples / sec: 21898.72
Iteration:   2440, Loss function: 4.125, Average Loss: 4.222, avg. samples / sec: 21891.70
Iteration:   2440, Loss function: 4.047, Average Loss: 4.224, avg. samples / sec: 21895.65
Iteration:   2440, Loss function: 4.701, Average Loss: 4.221, avg. samples / sec: 21898.23
Iteration:   2440, Loss function: 4.163, Average Loss: 4.225, avg. samples / sec: 21898.99
Iteration:   2440, Loss function: 4.577, Average Loss: 4.220, avg. samples / sec: 21896.15
Iteration:   2440, Loss function: 3.900, Average Loss: 4.219, avg. samples / sec: 21900.63
Iteration:   2440, Loss function: 3.892, Average Loss: 4.217, avg. samples / sec: 21888.74
Iteration:   2460, Loss function: 4.236, Average Loss: 4.209, avg. samples / sec: 21798.88
Iteration:   2460, Loss function: 4.312, Average Loss: 4.222, avg. samples / sec: 21807.45
Iteration:   2460, Loss function: 4.379, Average Loss: 4.220, avg. samples / sec: 21804.64
Iteration:   2460, Loss function: 4.034, Average Loss: 4.215, avg. samples / sec: 21809.18
Iteration:   2460, Loss function: 4.433, Average Loss: 4.224, avg. samples / sec: 21805.91
Iteration:   2460, Loss function: 4.126, Average Loss: 4.221, avg. samples / sec: 21801.36
Iteration:   2460, Loss function: 4.041, Average Loss: 4.218, avg. samples / sec: 21801.62
Iteration:   2460, Loss function: 3.622, Average Loss: 4.216, avg. samples / sec: 21811.51
Iteration:   2480, Loss function: 4.101, Average Loss: 4.211, avg. samples / sec: 21840.52
Iteration:   2480, Loss function: 4.470, Average Loss: 4.218, avg. samples / sec: 21834.84
Iteration:   2480, Loss function: 4.408, Average Loss: 4.221, avg. samples / sec: 21835.67
Iteration:   2480, Loss function: 4.458, Average Loss: 4.217, avg. samples / sec: 21839.69
Iteration:   2480, Loss function: 4.511, Average Loss: 4.206, avg. samples / sec: 21828.38
Iteration:   2480, Loss function: 4.034, Average Loss: 4.209, avg. samples / sec: 21832.39
Iteration:   2480, Loss function: 3.959, Average Loss: 4.212, avg. samples / sec: 21837.31
Iteration:   2480, Loss function: 4.714, Average Loss: 4.216, avg. samples / sec: 21828.76

:::MLPv0.5.0 ssd 1541711128.684129238 (train.py:553) train_epoch: 43
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1

:::MLPv0.5.0 ssd 1541711130.189249039 (train.py:578) opt_learning_rate: 0.016
Iteration:   2500, Loss function: 4.050, Average Loss: 4.213, avg. samples / sec: 21827.89
Iteration:   2500, Loss function: 4.470, Average Loss: 4.206, avg. samples / sec: 21825.12
Iteration:   2500, Loss function: 4.483, Average Loss: 4.213, avg. samples / sec: 21820.47
Iteration:   2500, Loss function: 4.099, Average Loss: 4.218, avg. samples / sec: 21825.85
Iteration:   2500, Loss function: 3.976, Average Loss: 4.219, avg. samples / sec: 21819.67
Iteration:   2500, Loss function: 3.570, Average Loss: 4.211, avg. samples / sec: 21820.38
Iteration:   2500, Loss function: 4.666, Average Loss: 4.218, avg. samples / sec: 21811.75
Iteration:   2500, Loss function: 3.222, Average Loss: 4.205, avg. samples / sec: 21813.54

































































:::MLPv0.5.0 ssd 1541711130.280772209 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711130.281599998 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711130.282433748 (train.py:220) eval_start: 43
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 22
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 34
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 46
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 43
No object detected in idx: 59
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 68
Predicting Ended, total time: 4.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.64 s
Predicting Ended, total time: 4.64 s
Predicting Ended, total time: 4.64 s
Predicting Ended, total time: 4.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.64 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.64 s
Predicting Ended, total time: 4.64 s
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(176811, 7)
Converting ndarray to lists...
0/176811
(176811, 7)
0/176811
Loading and preparing results...
Converting ndarray to lists...
(176811, 7)
Loading and preparing results...
0/176811
Converting ndarray to lists...
Loading and preparing results...
(176811, 7)
Converting ndarray to lists...
0/176811
(176811, 7)
Loading and preparing results...
0/176811
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(176811, 7)
(176811, 7)
0/176811
0/176811
Loading and preparing results...
Converting ndarray to lists...
(176811, 7)
0/176811
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(176811, 7)
Loading and preparing results...
(176811, 7)
0/176811
0/176811
Converting ndarray to lists...
(176811, 7)
0/176811
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(176811, 7)
(176811, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/176811
0/176811
(176811, 7)
(176811, 7)
0/176811
Converting ndarray to lists...
0/176811
(176811, 7)
0/176811
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(176811, 7)
Loading and preparing results...
Converting ndarray to lists...
(176811, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/176811
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(176811, 7)
(176811, 7)
(176811, 7)
Converting ndarray to lists...
0/176811
Converting ndarray to lists...
Converting ndarray to lists...
(176811, 7)
(176811, 7)
Converting ndarray to lists...
0/176811
Converting ndarray to lists...
Loading and preparing results...
(176811, 7)
(176811, 7)
(176811, 7)
(176811, 7)
(176811, 7)
0/176811
0/176811
Loading and preparing results...
0/176811
(176811, 7)
Converting ndarray to lists...
(176811, 7)
0/176811
Loading and preparing results...
0/176811
0/176811
(176811, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/176811
(176811, 7)
0/176811
Loading and preparing results...
0/176811
Loading and preparing results...
0/176811
(176811, 7)
0/176811
Loading and preparing results...
0/176811
Loading and preparing results...
(176811, 7)
(176811, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/176811
Converting ndarray to lists...
0/176811
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/176811
Loading and preparing results...
Loading and preparing results...
(176811, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/176811
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(176811, 7)
Converting ndarray to lists...
(176811, 7)
(176811, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(176811, 7)
Loading and preparing results...
0/176811
Converting ndarray to lists...
(176811, 7)
Converting ndarray to lists...
0/176811
(176811, 7)
Loading and preparing results...
(176811, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/176811
0/176811
(176811, 7)
0/176811
(176811, 7)
(176811, 7)
Converting ndarray to lists...
0/176811
Converting ndarray to lists...
Converting ndarray to lists...
0/176811
(176811, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/176811
(176811, 7)
(176811, 7)
0/176811
(176811, 7)
0/176811
Loading and preparing results...
(176811, 7)
(176811, 7)
0/176811
Converting ndarray to lists...
0/176811
Converting ndarray to lists...
0/176811
0/176811
Converting ndarray to lists...
Loading and preparing results...
0/176811
(176811, 7)
(176811, 7)
(176811, 7)
0/176811
0/176811
Converting ndarray to lists...
0/176811
(176811, 7)
0/176811
(176811, 7)
(176811, 7)
Loading and preparing results...
0/176811
Loading and preparing results...
0/176811
0/176811
0/176811
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(176811, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/176811
(176811, 7)
(176811, 7)
0/176811
(176811, 7)
(176811, 7)
Converting ndarray to lists...
0/176811
0/176811
0/176811
(176811, 7)
0/176811
DONE (t=1.17s)
creating index...
DONE (t=1.18s)
creating index...
DONE (t=1.18s)
creating index...
DONE (t=1.18s)
creating index...
DONE (t=1.18s)
creating index...
DONE (t=1.18s)
creating index...
DONE (t=1.19s)
creating index...
DONE (t=1.19s)
creating index...
DONE (t=1.19s)
creating index...
DONE (t=1.19s)
creating index...
DONE (t=1.19s)
creating index...
DONE (t=1.19s)
creating index...
DONE (t=1.19s)
creating index...
DONE (t=1.19s)
creating index...
DONE (t=1.19s)
creating index...
DONE (t=1.19s)
creating index...
DONE (t=1.19s)
creating index...
DONE (t=1.20s)
creating index...
DONE (t=1.20s)
creating index...
DONE (t=1.20s)
creating index...
DONE (t=1.20s)
creating index...
DONE (t=1.20s)
creating index...
DONE (t=1.20s)
creating index...
DONE (t=1.20s)
creating index...
DONE (t=1.20s)
creating index...
DONE (t=1.21s)
creating index...
DONE (t=1.21s)
creating index...
DONE (t=1.21s)
creating index...
DONE (t=1.21s)
creating index...
DONE (t=1.21s)
creating index...
DONE (t=1.21s)
creating index...
DONE (t=1.21s)
creating index...
DONE (t=1.21s)
creating index...
DONE (t=1.22s)
creating index...
DONE (t=1.22s)
creating index...
DONE (t=1.22s)
creating index...
DONE (t=1.22s)
creating index...
DONE (t=1.22s)
creating index...
DONE (t=1.22s)
creating index...
DONE (t=1.22s)
creating index...
DONE (t=1.22s)
creating index...
DONE (t=1.22s)
creating index...
DONE (t=1.22s)
creating index...
DONE (t=1.23s)
creating index...
DONE (t=1.23s)
creating index...
DONE (t=1.23s)
creating index...
DONE (t=1.23s)
creating index...
DONE (t=1.24s)
creating index...
DONE (t=1.24s)
creating index...
DONE (t=1.24s)
creating index...
DONE (t=1.24s)
creating index...
DONE (t=1.24s)
creating index...
DONE (t=1.24s)
creating index...
DONE (t=1.25s)
creating index...
DONE (t=1.25s)
creating index...
DONE (t=1.25s)
creating index...
DONE (t=1.25s)
creating index...
index created!
index created!
DONE (t=1.26s)
creating index...
DONE (t=1.27s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
DONE (t=1.30s)
creating index...
index created!
DONE (t=1.30s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.31s)
index created!
creating index...
index created!
index created!
index created!
index created!
index created!
DONE (t=1.32s)
creating index...
index created!
index created!
index created!
index created!
index created!
DONE (t=1.32s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.85s).
Accumulating evaluation results...
DONE (t=2.89s).
Accumulating evaluation results...
DONE (t=2.84s).
Accumulating evaluation results...
DONE (t=2.89s).
Accumulating evaluation results...
DONE (t=3.06s).
Accumulating evaluation results...
DONE (t=3.06s).
Accumulating evaluation results...
DONE (t=3.07s).
Accumulating evaluation results...
DONE (t=3.11s).
Accumulating evaluation results...
DONE (t=0.82s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.272
DONE (t=0.83s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
DONE (t=0.83s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.272
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.272
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.230
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.240
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14476 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.152
DONE (t=0.84s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.152
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.230
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.240
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14476 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.230
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.240
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14476 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.272
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.230
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.240
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14476 AP goal: 0.21200
DONE (t=0.84s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
DONE (t=0.85s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.272
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
DONE (t=0.86s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.272
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.152
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.230
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.240
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14476 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
DONE (t=0.84s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.230
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.240
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14476 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.230
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.240
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14476 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.230
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.240
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14476 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711140.424201012 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711140.425139904 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.14475607215619213}

:::MLPv0.5.0 ssd 1541711140.425896645 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.14475607215619213}

:::MLPv0.5.0 ssd 1541711140.426694155 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711140.427450418 (train.py:338) eval_stop: 43
Iteration:   2520, Loss function: 3.574, Average Loss: 4.210, avg. samples / sec: 3315.18
Iteration:   2520, Loss function: 3.570, Average Loss: 4.210, avg. samples / sec: 3315.36
Iteration:   2520, Loss function: 3.948, Average Loss: 4.208, avg. samples / sec: 3315.13
Iteration:   2520, Loss function: 3.342, Average Loss: 4.197, avg. samples / sec: 3315.12
Iteration:   2520, Loss function: 3.763, Average Loss: 4.206, avg. samples / sec: 3315.06
Iteration:   2520, Loss function: 3.322, Average Loss: 4.201, avg. samples / sec: 3314.86
Iteration:   2520, Loss function: 3.550, Average Loss: 4.197, avg. samples / sec: 3315.21
Iteration:   2520, Loss function: 3.455, Average Loss: 4.202, avg. samples / sec: 3315.06
Iteration:   2540, Loss function: 2.696, Average Loss: 4.197, avg. samples / sec: 21868.44
Iteration:   2540, Loss function: 3.398, Average Loss: 4.199, avg. samples / sec: 21864.96
Iteration:   2540, Loss function: 3.717, Average Loss: 4.184, avg. samples / sec: 21869.76
Iteration:   2540, Loss function: 3.331, Average Loss: 4.194, avg. samples / sec: 21865.16
Iteration:   2540, Loss function: 3.142, Average Loss: 4.193, avg. samples / sec: 21861.06
Iteration:   2540, Loss function: 3.501, Average Loss: 4.189, avg. samples / sec: 21864.41
Iteration:   2540, Loss function: 3.250, Average Loss: 4.183, avg. samples / sec: 21858.55
Iteration:   2540, Loss function: 4.271, Average Loss: 4.189, avg. samples / sec: 21866.35

:::MLPv0.5.0 ssd 1541711144.603521109 (train.py:553) train_epoch: 44
Iteration:   2560, Loss function: 3.132, Average Loss: 4.182, avg. samples / sec: 21838.87
Iteration:   2560, Loss function: 3.722, Average Loss: 4.185, avg. samples / sec: 21833.85
Iteration:   2560, Loss function: 3.129, Average Loss: 4.178, avg. samples / sec: 21839.07
Iteration:   2560, Loss function: 3.783, Average Loss: 4.180, avg. samples / sec: 21837.72
Iteration:   2560, Loss function: 3.167, Average Loss: 4.174, avg. samples / sec: 21839.41
Iteration:   2560, Loss function: 3.403, Average Loss: 4.169, avg. samples / sec: 21838.92
Iteration:   2560, Loss function: 3.174, Average Loss: 4.169, avg. samples / sec: 21829.12
Iteration:   2560, Loss function: 2.971, Average Loss: 4.176, avg. samples / sec: 21828.74
Iteration:   2580, Loss function: 3.075, Average Loss: 4.168, avg. samples / sec: 21930.42
Iteration:   2580, Loss function: 3.584, Average Loss: 4.162, avg. samples / sec: 21928.81
Iteration:   2580, Loss function: 3.589, Average Loss: 4.154, avg. samples / sec: 21929.00
Iteration:   2580, Loss function: 3.253, Average Loss: 4.165, avg. samples / sec: 21927.69
Iteration:   2580, Loss function: 3.817, Average Loss: 4.161, avg. samples / sec: 21928.04
Iteration:   2580, Loss function: 3.873, Average Loss: 4.165, avg. samples / sec: 21919.99
Iteration:   2580, Loss function: 3.618, Average Loss: 4.154, avg. samples / sec: 21932.11
Iteration:   2580, Loss function: 3.164, Average Loss: 4.161, avg. samples / sec: 21930.33

:::MLPv0.5.0 ssd 1541711150.031814575 (train.py:553) train_epoch: 45
Iteration:   2600, Loss function: 3.889, Average Loss: 4.151, avg. samples / sec: 21869.76
Iteration:   2600, Loss function: 3.260, Average Loss: 4.138, avg. samples / sec: 21865.35
Iteration:   2600, Loss function: 3.323, Average Loss: 4.148, avg. samples / sec: 21862.30
Iteration:   2600, Loss function: 3.341, Average Loss: 4.138, avg. samples / sec: 21861.19
Iteration:   2600, Loss function: 3.570, Average Loss: 4.145, avg. samples / sec: 21855.47
Iteration:   2600, Loss function: 3.424, Average Loss: 4.149, avg. samples / sec: 21855.84
Iteration:   2600, Loss function: 3.234, Average Loss: 4.151, avg. samples / sec: 21848.70
Iteration:   2600, Loss function: 3.594, Average Loss: 4.148, avg. samples / sec: 21851.72
Iteration:   2620, Loss function: 3.120, Average Loss: 4.124, avg. samples / sec: 21913.34
Iteration:   2620, Loss function: 2.931, Average Loss: 4.134, avg. samples / sec: 21909.07
Iteration:   2620, Loss function: 3.940, Average Loss: 4.129, avg. samples / sec: 21917.40
Iteration:   2620, Loss function: 3.105, Average Loss: 4.135, avg. samples / sec: 21914.18
Iteration:   2620, Loss function: 3.140, Average Loss: 4.122, avg. samples / sec: 21910.09
Iteration:   2620, Loss function: 3.144, Average Loss: 4.133, avg. samples / sec: 21924.58
Iteration:   2620, Loss function: 3.094, Average Loss: 4.136, avg. samples / sec: 21908.95
Iteration:   2620, Loss function: 3.625, Average Loss: 4.134, avg. samples / sec: 21898.94
Iteration:   2640, Loss function: 3.096, Average Loss: 4.121, avg. samples / sec: 21920.67
Iteration:   2640, Loss function: 3.704, Average Loss: 4.109, avg. samples / sec: 21910.17
Iteration:   2640, Loss function: 3.014, Average Loss: 4.120, avg. samples / sec: 21926.23
Iteration:   2640, Loss function: 3.582, Average Loss: 4.116, avg. samples / sec: 21916.34
Iteration:   2640, Loss function: 3.166, Average Loss: 4.108, avg. samples / sec: 21912.85
Iteration:   2640, Loss function: 2.807, Average Loss: 4.114, avg. samples / sec: 21906.68
Iteration:   2640, Loss function: 3.504, Average Loss: 4.122, avg. samples / sec: 21916.15
Iteration:   2640, Loss function: 3.930, Average Loss: 4.120, avg. samples / sec: 21903.77

:::MLPv0.5.0 ssd 1541711155.359100103 (train.py:553) train_epoch: 46
Iteration:   2660, Loss function: 3.269, Average Loss: 4.094, avg. samples / sec: 21930.24
Iteration:   2660, Loss function: 3.119, Average Loss: 4.094, avg. samples / sec: 21936.42
Iteration:   2660, Loss function: 3.445, Average Loss: 4.104, avg. samples / sec: 21934.81
Iteration:   2660, Loss function: 3.292, Average Loss: 4.101, avg. samples / sec: 21928.83
Iteration:   2660, Loss function: 3.605, Average Loss: 4.106, avg. samples / sec: 21930.48
Iteration:   2660, Loss function: 4.083, Average Loss: 4.105, avg. samples / sec: 21922.38
Iteration:   2660, Loss function: 3.405, Average Loss: 4.105, avg. samples / sec: 21916.23
Iteration:   2660, Loss function: 3.461, Average Loss: 4.098, avg. samples / sec: 21923.62
Iteration:   2680, Loss function: 3.474, Average Loss: 4.091, avg. samples / sec: 21901.93
Iteration:   2680, Loss function: 3.929, Average Loss: 4.081, avg. samples / sec: 21888.67
Iteration:   2680, Loss function: 3.643, Average Loss: 4.090, avg. samples / sec: 21897.79
Iteration:   2680, Loss function: 3.089, Average Loss: 4.077, avg. samples / sec: 21886.30
Iteration:   2680, Loss function: 3.185, Average Loss: 4.088, avg. samples / sec: 21890.27
Iteration:   2680, Loss function: 3.386, Average Loss: 4.081, avg. samples / sec: 21898.28
Iteration:   2680, Loss function: 3.292, Average Loss: 4.086, avg. samples / sec: 21892.76
Iteration:   2680, Loss function: 3.396, Average Loss: 4.091, avg. samples / sec: 21886.91
Iteration:   2700, Loss function: 3.443, Average Loss: 4.073, avg. samples / sec: 21924.03
Iteration:   2700, Loss function: 3.423, Average Loss: 4.062, avg. samples / sec: 21924.88
Iteration:   2700, Loss function: 3.513, Average Loss: 4.066, avg. samples / sec: 21919.95
Iteration:   2700, Loss function: 3.173, Average Loss: 4.075, avg. samples / sec: 21918.42
Iteration:   2700, Loss function: 3.554, Average Loss: 4.074, avg. samples / sec: 21927.69
Iteration:   2700, Loss function: 3.565, Average Loss: 4.073, avg. samples / sec: 21920.91
Iteration:   2700, Loss function: 3.022, Average Loss: 4.065, avg. samples / sec: 21917.51
Iteration:   2700, Loss function: 3.589, Average Loss: 4.075, avg. samples / sec: 21915.87

:::MLPv0.5.0 ssd 1541711160.778272629 (train.py:553) train_epoch: 47
Iteration:   2720, Loss function: 3.456, Average Loss: 4.058, avg. samples / sec: 21922.44
Iteration:   2720, Loss function: 3.671, Average Loss: 4.051, avg. samples / sec: 21929.97
Iteration:   2720, Loss function: 3.269, Average Loss: 4.049, avg. samples / sec: 21924.28
Iteration:   2720, Loss function: 2.728, Average Loss: 4.058, avg. samples / sec: 21927.02
Iteration:   2720, Loss function: 3.214, Average Loss: 4.052, avg. samples / sec: 21928.95
Iteration:   2720, Loss function: 3.221, Average Loss: 4.058, avg. samples / sec: 21923.88
Iteration:   2720, Loss function: 3.462, Average Loss: 4.059, avg. samples / sec: 21926.55
Iteration:   2720, Loss function: 3.786, Average Loss: 4.061, avg. samples / sec: 21929.06
Iteration:   2740, Loss function: 3.292, Average Loss: 4.044, avg. samples / sec: 21876.12
Iteration:   2740, Loss function: 2.816, Average Loss: 4.037, avg. samples / sec: 21876.78
Iteration:   2740, Loss function: 3.498, Average Loss: 4.044, avg. samples / sec: 21877.97
Iteration:   2740, Loss function: 2.700, Average Loss: 4.037, avg. samples / sec: 21870.92
Iteration:   2740, Loss function: 3.205, Average Loss: 4.042, avg. samples / sec: 21876.19
Iteration:   2740, Loss function: 3.581, Average Loss: 4.045, avg. samples / sec: 21869.71
Iteration:   2740, Loss function: 3.798, Average Loss: 4.042, avg. samples / sec: 21872.34
Iteration:   2740, Loss function: 3.534, Average Loss: 4.037, avg. samples / sec: 21870.85
Iteration:   2760, Loss function: 3.155, Average Loss: 4.024, avg. samples / sec: 22028.20
Iteration:   2760, Loss function: 3.296, Average Loss: 4.022, avg. samples / sec: 22023.42
Iteration:   2760, Loss function: 3.495, Average Loss: 4.024, avg. samples / sec: 22022.92
Iteration:   2760, Loss function: 3.250, Average Loss: 4.029, avg. samples / sec: 22023.25
Iteration:   2760, Loss function: 3.065, Average Loss: 4.027, avg. samples / sec: 22018.64
Iteration:   2760, Loss function: 3.101, Average Loss: 4.031, avg. samples / sec: 22018.48
Iteration:   2760, Loss function: 2.747, Average Loss: 4.030, avg. samples / sec: 22013.32
Iteration:   2760, Loss function: 4.171, Average Loss: 4.028, avg. samples / sec: 22013.79

:::MLPv0.5.0 ssd 1541711166.201819181 (train.py:553) train_epoch: 48
Iteration:   2780, Loss function: 3.092, Average Loss: 4.011, avg. samples / sec: 21864.73
Iteration:   2780, Loss function: 3.044, Average Loss: 4.015, avg. samples / sec: 21868.07
Iteration:   2780, Loss function: 3.194, Average Loss: 4.014, avg. samples / sec: 21872.43
Iteration:   2780, Loss function: 3.332, Average Loss: 4.008, avg. samples / sec: 21864.14
Iteration:   2780, Loss function: 3.158, Average Loss: 4.014, avg. samples / sec: 21870.13
Iteration:   2780, Loss function: 4.019, Average Loss: 4.013, avg. samples / sec: 21865.90
Iteration:   2780, Loss function: 3.003, Average Loss: 4.007, avg. samples / sec: 21858.56
Iteration:   2780, Loss function: 3.284, Average Loss: 4.016, avg. samples / sec: 21861.22
Iteration:   2800, Loss function: 2.996, Average Loss: 3.992, avg. samples / sec: 21882.29
Iteration:   2800, Loss function: 3.174, Average Loss: 3.997, avg. samples / sec: 21870.65
Iteration:   2800, Loss function: 3.212, Average Loss: 4.001, avg. samples / sec: 21883.34
Iteration:   2800, Loss function: 3.267, Average Loss: 3.996, avg. samples / sec: 21873.71
Iteration:   2800, Loss function: 3.602, Average Loss: 3.998, avg. samples / sec: 21871.58
Iteration:   2800, Loss function: 2.905, Average Loss: 3.998, avg. samples / sec: 21874.56
Iteration:   2800, Loss function: 3.406, Average Loss: 4.000, avg. samples / sec: 21872.57
Iteration:   2800, Loss function: 3.314, Average Loss: 3.998, avg. samples / sec: 21862.25

































































:::MLPv0.5.0 ssd 1541711169.944502831 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711169.945336580 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711169.946076393 (train.py:220) eval_start: 48
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.92 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.92 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.92 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.92 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.92 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.92 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.92 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.92 s
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(293047, 7)
Converting ndarray to lists...
0/293047
Loading and preparing results...
(293047, 7)
Loading and preparing results...
0/293047
Converting ndarray to lists...
Converting ndarray to lists...
(293047, 7)
(293047, 7)
0/293047
0/293047
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(293047, 7)
(293047, 7)
0/293047
Loading and preparing results...
0/293047
Converting ndarray to lists...
(293047, 7)
Loading and preparing results...
0/293047
Converting ndarray to lists...
(293047, 7)
0/293047
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(293047, 7)
(293047, 7)
0/293047
0/293047
Loading and preparing results...
Converting ndarray to lists...
(293047, 7)
0/293047
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(293047, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/293047
(293047, 7)
(293047, 7)
0/293047
Converting ndarray to lists...
0/293047
(293047, 7)
Loading and preparing results...
0/293047
Converting ndarray to lists...
(293047, 7)
0/293047
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(293047, 7)
(293047, 7)
(293047, 7)
Loading and preparing results...
Converting ndarray to lists...
(293047, 7)
Converting ndarray to lists...
0/293047
0/293047
0/293047
Converting ndarray to lists...
(293047, 7)
(293047, 7)
0/293047
0/293047
(293047, 7)
Loading and preparing results...
0/293047
Converting ndarray to lists...
0/293047
Loading and preparing results...
Converting ndarray to lists...
(293047, 7)
Loading and preparing results...
0/293047
Converting ndarray to lists...
Loading and preparing results...
(293047, 7)
Loading and preparing results...
Loading and preparing results...
0/293047
(293047, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/293047
(293047, 7)
(293047, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(293047, 7)
Loading and preparing results...
0/293047
0/293047
0/293047
Converting ndarray to lists...
Converting ndarray to lists...
(293047, 7)
(293047, 7)
Loading and preparing results...
0/293047
Loading and preparing results...
0/293047
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(293047, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/293047
Loading and preparing results...
Loading and preparing results...
(293047, 7)
Converting ndarray to lists...
Loading and preparing results...
(293047, 7)
Loading and preparing results...
Converting ndarray to lists...
(293047, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(293047, 7)
0/293047
(293047, 7)
Converting ndarray to lists...
0/293047
(293047, 7)
Converting ndarray to lists...
0/293047
Converting ndarray to lists...
Converting ndarray to lists...
(293047, 7)
(293047, 7)
Converting ndarray to lists...
Loading and preparing results...
0/293047
0/293047
Loading and preparing results...
0/293047
0/293047
(293047, 7)
Loading and preparing results...
(293047, 7)
Loading and preparing results...
0/293047
(293047, 7)
0/293047
Converting ndarray to lists...
(293047, 7)
0/293047
Loading and preparing results...
0/293047
(293047, 7)
0/293047
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/293047
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(293047, 7)
Converting ndarray to lists...
(293047, 7)
(293047, 7)
(293047, 7)
(293047, 7)
0/293047
(293047, 7)
0/293047
0/293047
Loading and preparing results...
0/293047
Converting ndarray to lists...
0/293047
(293047, 7)
0/293047
0/293047
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(293047, 7)
Loading and preparing results...
(293047, 7)
0/293047
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/293047
Loading and preparing results...
(293047, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(293047, 7)
0/293047
(293047, 7)
(293047, 7)
(293047, 7)
0/293047
Loading and preparing results...
0/293047
0/293047
0/293047
Loading and preparing results...
Converting ndarray to lists...
(293047, 7)
Converting ndarray to lists...
Loading and preparing results...
0/293047
Converting ndarray to lists...
(293047, 7)
Loading and preparing results...
(293047, 7)
0/293047
0/293047
Converting ndarray to lists...
Loading and preparing results...
(293047, 7)
Converting ndarray to lists...
0/293047
(293047, 7)
0/293047
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.79s)
creating index...
index created!
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
index created!
index created!
index created!
index created!
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
index created!
index created!
DONE (t=1.87s)
creating index...
DONE (t=1.87s)
creating index...
index created!
index created!
index created!
DONE (t=1.87s)
creating index...
index created!
DONE (t=1.87s)
creating index...
index created!
index created!
index created!
DONE (t=1.88s)
creating index...
index created!
DONE (t=1.88s)
creating index...
index created!
DONE (t=1.88s)
creating index...
index created!
DONE (t=1.88s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
DONE (t=1.89s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=1.90s)
creating index...
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.93s)
creating index...
index created!
index created!
DONE (t=1.97s)
creating index...
index created!
DONE (t=1.99s)
creating index...
index created!
index created!
index created!
DONE (t=2.00s)
creating index...
index created!
index created!
index created!
index created!
DONE (t=2.01s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.05s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=2.11s)
creating index...
DONE (t=2.10s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=2.17s)
creating index...
index created!
index created!
index created!
index created!
DONE (t=3.50s).
Accumulating evaluation results...
DONE (t=3.54s).
Accumulating evaluation results...
DONE (t=3.52s).
Accumulating evaluation results...
DONE (t=3.53s).
Accumulating evaluation results...
DONE (t=3.52s).
Accumulating evaluation results...
DONE (t=3.53s).
Accumulating evaluation results...
DONE (t=3.52s).
Accumulating evaluation results...
DONE (t=3.56s).
Accumulating evaluation results...
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
DONE (t=1.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.307
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.348
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21186 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.307
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.348
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21186 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
DONE (t=1.12s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.307
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.348
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21186 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
DONE (t=1.11s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.307
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.348
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21186 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.307
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.348
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21186 AP goal: 0.21200
DONE (t=1.11s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
DONE (t=1.08s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.307
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.348
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21186 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.307
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.348
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21186 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.307
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.348
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21186 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711182.848971367 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711182.849863768 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.21185994045507936}

:::MLPv0.5.0 ssd 1541711182.850624800 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.21185994045507936}

:::MLPv0.5.0 ssd 1541711182.851396084 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711182.852180958 (train.py:338) eval_stop: 48
Iteration:   2820, Loss function: 3.471, Average Loss: 3.986, avg. samples / sec: 2685.23
Iteration:   2820, Loss function: 3.074, Average Loss: 3.980, avg. samples / sec: 2685.26
Iteration:   2820, Loss function: 3.119, Average Loss: 3.984, avg. samples / sec: 2685.13
Iteration:   2820, Loss function: 3.260, Average Loss: 3.983, avg. samples / sec: 2685.06
Iteration:   2820, Loss function: 3.415, Average Loss: 3.985, avg. samples / sec: 2685.29
Iteration:   2820, Loss function: 2.837, Average Loss: 3.977, avg. samples / sec: 2685.02
Iteration:   2820, Loss function: 2.859, Average Loss: 3.983, avg. samples / sec: 2685.07
Iteration:   2820, Loss function: 3.087, Average Loss: 3.985, avg. samples / sec: 2685.02

:::MLPv0.5.0 ssd 1541711185.006700993 (train.py:553) train_epoch: 49
Iteration:   2840, Loss function: 3.631, Average Loss: 3.971, avg. samples / sec: 21918.52
Iteration:   2840, Loss function: 3.650, Average Loss: 3.965, avg. samples / sec: 21924.54
Iteration:   2840, Loss function: 3.020, Average Loss: 3.970, avg. samples / sec: 21926.33
Iteration:   2840, Loss function: 2.933, Average Loss: 3.968, avg. samples / sec: 21924.09
Iteration:   2840, Loss function: 3.516, Average Loss: 3.970, avg. samples / sec: 21922.06
Iteration:   2840, Loss function: 3.148, Average Loss: 3.969, avg. samples / sec: 21909.39
Iteration:   2840, Loss function: 3.465, Average Loss: 3.971, avg. samples / sec: 21916.81
Iteration:   2840, Loss function: 3.345, Average Loss: 3.970, avg. samples / sec: 21911.21
Iteration:   2860, Loss function: 2.928, Average Loss: 3.958, avg. samples / sec: 21970.22
Iteration:   2860, Loss function: 3.057, Average Loss: 3.956, avg. samples / sec: 21981.30
Iteration:   2860, Loss function: 3.164, Average Loss: 3.949, avg. samples / sec: 21968.40
Iteration:   2860, Loss function: 3.300, Average Loss: 3.956, avg. samples / sec: 21966.78
Iteration:   2860, Loss function: 3.507, Average Loss: 3.955, avg. samples / sec: 21971.52
Iteration:   2860, Loss function: 3.029, Average Loss: 3.957, avg. samples / sec: 21966.38
Iteration:   2860, Loss function: 3.034, Average Loss: 3.956, avg. samples / sec: 21962.27
Iteration:   2860, Loss function: 3.260, Average Loss: 3.959, avg. samples / sec: 21965.18
Iteration:   2880, Loss function: 2.728, Average Loss: 3.943, avg. samples / sec: 22015.85
Iteration:   2880, Loss function: 3.545, Average Loss: 3.941, avg. samples / sec: 22017.74
Iteration:   2880, Loss function: 3.010, Average Loss: 3.941, avg. samples / sec: 22012.02
Iteration:   2880, Loss function: 3.054, Average Loss: 3.943, avg. samples / sec: 22019.03
Iteration:   2880, Loss function: 2.682, Average Loss: 3.940, avg. samples / sec: 22013.93
Iteration:   2880, Loss function: 2.577, Average Loss: 3.940, avg. samples / sec: 22014.69
Iteration:   2880, Loss function: 3.146, Average Loss: 3.946, avg. samples / sec: 22022.66
Iteration:   2880, Loss function: 3.242, Average Loss: 3.936, avg. samples / sec: 22002.49

:::MLPv0.5.0 ssd 1541711190.318718433 (train.py:553) train_epoch: 50
Iteration:   2900, Loss function: 3.729, Average Loss: 3.921, avg. samples / sec: 21940.55
Iteration:   2900, Loss function: 2.894, Average Loss: 3.930, avg. samples / sec: 21924.99
Iteration:   2900, Loss function: 2.669, Average Loss: 3.930, avg. samples / sec: 21929.23
Iteration:   2900, Loss function: 2.787, Average Loss: 3.923, avg. samples / sec: 21923.44
Iteration:   2900, Loss function: 3.710, Average Loss: 3.924, avg. samples / sec: 21926.32
Iteration:   2900, Loss function: 3.411, Average Loss: 3.928, avg. samples / sec: 21921.39
Iteration:   2900, Loss function: 3.449, Average Loss: 3.926, avg. samples / sec: 21922.99
Iteration:   2900, Loss function: 3.727, Average Loss: 3.933, avg. samples / sec: 21921.00
Iteration:   2920, Loss function: 2.920, Average Loss: 3.919, avg. samples / sec: 21974.68
Iteration:   2920, Loss function: 3.086, Average Loss: 3.907, avg. samples / sec: 21976.60
Iteration:   2920, Loss function: 2.848, Average Loss: 3.908, avg. samples / sec: 21983.32
Iteration:   2920, Loss function: 3.111, Average Loss: 3.910, avg. samples / sec: 21979.04
Iteration:   2920, Loss function: 2.874, Average Loss: 3.913, avg. samples / sec: 21980.93
Iteration:   2920, Loss function: 3.853, Average Loss: 3.920, avg. samples / sec: 21980.81
Iteration:   2920, Loss function: 3.261, Average Loss: 3.915, avg. samples / sec: 21975.26
Iteration:   2920, Loss function: 3.273, Average Loss: 3.917, avg. samples / sec: 21968.31
Iteration:   2940, Loss function: 2.934, Average Loss: 3.896, avg. samples / sec: 21990.90
Iteration:   2940, Loss function: 3.611, Average Loss: 3.895, avg. samples / sec: 21982.08
Iteration:   2940, Loss function: 3.442, Average Loss: 3.900, avg. samples / sec: 21989.37
Iteration:   2940, Loss function: 3.406, Average Loss: 3.891, avg. samples / sec: 21983.80
Iteration:   2940, Loss function: 3.006, Average Loss: 3.901, avg. samples / sec: 21990.29
Iteration:   2940, Loss function: 2.921, Average Loss: 3.902, avg. samples / sec: 21988.23
Iteration:   2940, Loss function: 3.151, Average Loss: 3.904, avg. samples / sec: 21986.26
Iteration:   2940, Loss function: 3.387, Average Loss: 3.907, avg. samples / sec: 21973.55

:::MLPv0.5.0 ssd 1541711195.727829218 (train.py:553) train_epoch: 51
Iteration:   2960, Loss function: 3.258, Average Loss: 3.889, avg. samples / sec: 21981.41
Iteration:   2960, Loss function: 3.205, Average Loss: 3.893, avg. samples / sec: 21984.91
Iteration:   2960, Loss function: 2.990, Average Loss: 3.877, avg. samples / sec: 21974.07
Iteration:   2960, Loss function: 3.481, Average Loss: 3.881, avg. samples / sec: 21974.03
Iteration:   2960, Loss function: 3.115, Average Loss: 3.882, avg. samples / sec: 21964.43
Iteration:   2960, Loss function: 2.946, Average Loss: 3.885, avg. samples / sec: 21962.83
Iteration:   2960, Loss function: 3.395, Average Loss: 3.888, avg. samples / sec: 21965.25
Iteration:   2960, Loss function: 3.444, Average Loss: 3.891, avg. samples / sec: 21965.78
Iteration:   2980, Loss function: 3.314, Average Loss: 3.879, avg. samples / sec: 21942.81
Iteration:   2980, Loss function: 2.780, Average Loss: 3.877, avg. samples / sec: 21955.96
Iteration:   2980, Loss function: 3.236, Average Loss: 3.868, avg. samples / sec: 21948.06
Iteration:   2980, Loss function: 3.104, Average Loss: 3.867, avg. samples / sec: 21939.73
Iteration:   2980, Loss function: 3.549, Average Loss: 3.873, avg. samples / sec: 21948.39
Iteration:   2980, Loss function: 3.279, Average Loss: 3.865, avg. samples / sec: 21936.18
Iteration:   2980, Loss function: 3.377, Average Loss: 3.875, avg. samples / sec: 21932.37
Iteration:   2980, Loss function: 2.990, Average Loss: 3.873, avg. samples / sec: 21944.99
Iteration:   3000, Loss function: 3.474, Average Loss: 3.864, avg. samples / sec: 21937.11
Iteration:   3000, Loss function: 3.075, Average Loss: 3.851, avg. samples / sec: 21942.58
Iteration:   3000, Loss function: 2.969, Average Loss: 3.859, avg. samples / sec: 21946.12
Iteration:   3000, Loss function: 2.734, Average Loss: 3.851, avg. samples / sec: 21938.00
Iteration:   3000, Loss function: 3.671, Average Loss: 3.856, avg. samples / sec: 21935.47
Iteration:   3000, Loss function: 2.737, Average Loss: 3.860, avg. samples / sec: 21939.00
Iteration:   3000, Loss function: 3.135, Average Loss: 3.858, avg. samples / sec: 21944.48
Iteration:   3000, Loss function: 2.990, Average Loss: 3.864, avg. samples / sec: 21930.72

:::MLPv0.5.0 ssd 1541711201.142298460 (train.py:553) train_epoch: 52
Iteration:   3020, Loss function: 3.266, Average Loss: 3.838, avg. samples / sec: 21967.63
Iteration:   3020, Loss function: 3.075, Average Loss: 3.842, avg. samples / sec: 21967.47
Iteration:   3020, Loss function: 2.893, Average Loss: 3.837, avg. samples / sec: 21966.42
Iteration:   3020, Loss function: 3.492, Average Loss: 3.851, avg. samples / sec: 21963.99
Iteration:   3020, Loss function: 3.273, Average Loss: 3.845, avg. samples / sec: 21957.71
Iteration:   3020, Loss function: 3.294, Average Loss: 3.849, avg. samples / sec: 21960.07
Iteration:   3020, Loss function: 3.190, Average Loss: 3.842, avg. samples / sec: 21959.97
Iteration:   3020, Loss function: 2.755, Average Loss: 3.851, avg. samples / sec: 21956.84
Iteration:   3040, Loss function: 2.976, Average Loss: 3.836, avg. samples / sec: 21890.77
Iteration:   3040, Loss function: 3.184, Average Loss: 3.824, avg. samples / sec: 21885.42
Iteration:   3040, Loss function: 3.198, Average Loss: 3.832, avg. samples / sec: 21895.21
Iteration:   3040, Loss function: 2.666, Average Loss: 3.821, avg. samples / sec: 21885.95
Iteration:   3040, Loss function: 2.895, Average Loss: 3.829, avg. samples / sec: 21892.13
Iteration:   3040, Loss function: 3.565, Average Loss: 3.836, avg. samples / sec: 21896.22
Iteration:   3040, Loss function: 3.326, Average Loss: 3.829, avg. samples / sec: 21879.01
Iteration:   3040, Loss function: 3.266, Average Loss: 3.835, avg. samples / sec: 21884.53
Iteration:   3060, Loss function: 2.646, Average Loss: 3.822, avg. samples / sec: 21965.98
Iteration:   3060, Loss function: 2.784, Average Loss: 3.816, avg. samples / sec: 21963.95
Iteration:   3060, Loss function: 2.717, Average Loss: 3.815, avg. samples / sec: 21972.19
Iteration:   3060, Loss function: 3.439, Average Loss: 3.811, avg. samples / sec: 21963.32
Iteration:   3060, Loss function: 2.858, Average Loss: 3.824, avg. samples / sec: 21974.44
Iteration:   3060, Loss function: 3.183, Average Loss: 3.807, avg. samples / sec: 21963.99
Iteration:   3060, Loss function: 2.886, Average Loss: 3.816, avg. samples / sec: 21965.21
Iteration:   3060, Loss function: 3.035, Average Loss: 3.825, avg. samples / sec: 21966.08

:::MLPv0.5.0 ssd 1541711206.556792974 (train.py:553) train_epoch: 53
Iteration:   3080, Loss function: 3.482, Average Loss: 3.806, avg. samples / sec: 21872.25
Iteration:   3080, Loss function: 3.596, Average Loss: 3.799, avg. samples / sec: 21869.95
Iteration:   3080, Loss function: 2.838, Average Loss: 3.796, avg. samples / sec: 21870.11
Iteration:   3080, Loss function: 2.526, Average Loss: 3.801, avg. samples / sec: 21870.04
Iteration:   3080, Loss function: 3.345, Average Loss: 3.811, avg. samples / sec: 21864.55
Iteration:   3080, Loss function: 2.858, Average Loss: 3.811, avg. samples / sec: 21867.45
Iteration:   3080, Loss function: 3.105, Average Loss: 3.804, avg. samples / sec: 21861.39
Iteration:   3080, Loss function: 2.845, Average Loss: 3.810, avg. samples / sec: 21827.78
Iteration:   3100, Loss function: 3.298, Average Loss: 3.786, avg. samples / sec: 21971.09
Iteration:   3100, Loss function: 3.034, Average Loss: 3.795, avg. samples / sec: 21964.23
Iteration:   3100, Loss function: 2.881, Average Loss: 3.785, avg. samples / sec: 21967.26
Iteration:   3100, Loss function: 3.021, Average Loss: 3.791, avg. samples / sec: 21976.46
Iteration:   3100, Loss function: 3.171, Average Loss: 3.785, avg. samples / sec: 21967.87
Iteration:   3100, Loss function: 2.892, Average Loss: 3.798, avg. samples / sec: 21971.39
Iteration:   3100, Loss function: 3.455, Average Loss: 3.799, avg. samples / sec: 21970.54
Iteration:   3100, Loss function: 3.340, Average Loss: 3.797, avg. samples / sec: 21993.43

:::MLPv0.5.0 ssd 1541711211.891613722 (train.py:553) train_epoch: 54
Iteration:   3120, Loss function: 3.129, Average Loss: 3.772, avg. samples / sec: 21801.82
Iteration:   3120, Loss function: 3.470, Average Loss: 3.782, avg. samples / sec: 21798.74
Iteration:   3120, Loss function: 2.591, Average Loss: 3.772, avg. samples / sec: 21793.51
Iteration:   3120, Loss function: 3.520, Average Loss: 3.786, avg. samples / sec: 21794.31
Iteration:   3120, Loss function: 3.975, Average Loss: 3.780, avg. samples / sec: 21791.69
Iteration:   3120, Loss function: 3.080, Average Loss: 3.785, avg. samples / sec: 21802.56
Iteration:   3120, Loss function: 2.614, Average Loss: 3.784, avg. samples / sec: 21793.84
Iteration:   3120, Loss function: 3.394, Average Loss: 3.772, avg. samples / sec: 21789.93
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2

:::MLPv0.5.0 ssd 1541711212.458508730 (train.py:586) opt_learning_rate: 0.0016

































































:::MLPv0.5.0 ssd 1541711212.550062180 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711212.550940990 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711212.551692486 (train.py:220) eval_start: 54
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.93 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.93 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.93 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.93 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.93 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.93 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.93 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.93 s
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(297127, 7)
Converting ndarray to lists...
0/297127
Loading and preparing results...
Loading and preparing results...
(297127, 7)
Loading and preparing results...
0/297127
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(297127, 7)
0/297127
(297127, 7)
(297127, 7)
Loading and preparing results...
0/297127
0/297127
Converting ndarray to lists...
Loading and preparing results...
(297127, 7)
0/297127
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(297127, 7)
(297127, 7)
Converting ndarray to lists...
(297127, 7)
0/297127
Converting ndarray to lists...
(297127, 7)
0/297127
0/297127
0/297127
Converting ndarray to lists...
(297127, 7)
(297127, 7)
0/297127
0/297127
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(297127, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/297127
Converting ndarray to lists...
(297127, 7)
(297127, 7)
0/297127
(297127, 7)
0/297127
0/297127
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(297127, 7)
Loading and preparing results...
(297127, 7)
Converting ndarray to lists...
0/297127
Loading and preparing results...
0/297127
(297127, 7)
Converting ndarray to lists...
Loading and preparing results...
0/297127
(297127, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/297127
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(297127, 7)
(297127, 7)
(297127, 7)
Converting ndarray to lists...
0/297127
0/297127
0/297127
(297127, 7)
0/297127
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(297127, 7)
Converting ndarray to lists...
Loading and preparing results...
(297127, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(297127, 7)
(297127, 7)
(297127, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(297127, 7)
(297127, 7)
Converting ndarray to lists...
(297127, 7)
(297127, 7)
Converting ndarray to lists...
0/297127
0/297127
(297127, 7)
0/297127
0/297127
Converting ndarray to lists...
0/297127
0/297127
Loading and preparing results...
0/297127
Converting ndarray to lists...
(297127, 7)
Converting ndarray to lists...
0/297127
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/297127
(297127, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/297127
0/297127
(297127, 7)
(297127, 7)
Converting ndarray to lists...
(297127, 7)
Converting ndarray to lists...
(297127, 7)
(297127, 7)
0/297127
(297127, 7)
0/297127
0/297127
0/297127
0/297127
Loading and preparing results...
Loading and preparing results...
0/297127
0/297127
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(297127, 7)
Loading and preparing results...
Loading and preparing results...
(297127, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(297127, 7)
Converting ndarray to lists...
(297127, 7)
Converting ndarray to lists...
(297127, 7)
0/297127
Converting ndarray to lists...
Loading and preparing results...
0/297127
0/297127
(297127, 7)
(297127, 7)
(297127, 7)
0/297127
(297127, 7)
0/297127
0/297127
Loading and preparing results...
0/297127
Converting ndarray to lists...
0/297127
Loading and preparing results...
Loading and preparing results...
0/297127
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(297127, 7)
(297127, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/297127
Converting ndarray to lists...
Loading and preparing results...
0/297127
Loading and preparing results...
(297127, 7)
Converting ndarray to lists...
(297127, 7)
Converting ndarray to lists...
0/297127
0/297127
(297127, 7)
(297127, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/297127
(297127, 7)
0/297127
(297127, 7)
Loading and preparing results...
(297127, 7)
0/297127
Converting ndarray to lists...
(297127, 7)
Loading and preparing results...
0/297127
Converting ndarray to lists...
0/297127
(297127, 7)
0/297127
(297127, 7)
Converting ndarray to lists...
0/297127
0/297127
(297127, 7)
0/297127
DONE (t=1.66s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.73s)
creating index...
index created!
DONE (t=1.80s)
creating index...
index created!
index created!
DONE (t=1.86s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=1.92s)
creating index...
DONE (t=1.92s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.94s)
creating index...
index created!
DONE (t=1.95s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
index created!
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
index created!
index created!
index created!
DONE (t=2.06s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
DONE (t=3.49s).
Accumulating evaluation results...
DONE (t=3.46s).
Accumulating evaluation results...
DONE (t=3.49s).
Accumulating evaluation results...
DONE (t=3.49s).
Accumulating evaluation results...
DONE (t=3.51s).
Accumulating evaluation results...
DONE (t=3.51s).
Accumulating evaluation results...
DONE (t=3.57s).
Accumulating evaluation results...
DONE (t=3.53s).
Accumulating evaluation results...
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.347
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.496
Current AP: 0.21150 AP goal: 0.21200
DONE (t=1.11s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
DONE (t=1.10s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.347
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.496
Current AP: 0.21150 AP goal: 0.21200
DONE (t=1.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
DONE (t=1.11s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
DONE (t=1.12s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
DONE (t=1.12s).
DONE (t=1.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.347
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.496
Current AP: 0.21150 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.347
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.496
Current AP: 0.21150 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.347
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.496
Current AP: 0.21150 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.347
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.496
Current AP: 0.21150 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.347
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.496
Current AP: 0.21150 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.347
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.496
Current AP: 0.21150 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711225.378442764 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711225.379378796 (train.py:333) eval_accuracy: {"epoch": 54, "value": 0.21150477926957267}

:::MLPv0.5.0 ssd 1541711225.380116701 (train.py:336) eval_iteration_accuracy: {"epoch": 54, "value": 0.21150477926957267}

:::MLPv0.5.0 ssd 1541711225.380856514 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711225.381608725 (train.py:338) eval_stop: 54
Iteration:   3140, Loss function: 3.621, Average Loss: 3.760, avg. samples / sec: 2699.20
Iteration:   3140, Loss function: 3.314, Average Loss: 3.761, avg. samples / sec: 2699.05
Iteration:   3140, Loss function: 2.757, Average Loss: 3.771, avg. samples / sec: 2699.22
Iteration:   3140, Loss function: 2.926, Average Loss: 3.769, avg. samples / sec: 2699.08
Iteration:   3140, Loss function: 3.763, Average Loss: 3.769, avg. samples / sec: 2699.15
Iteration:   3140, Loss function: 2.839, Average Loss: 3.769, avg. samples / sec: 2699.16
Iteration:   3140, Loss function: 2.893, Average Loss: 3.761, avg. samples / sec: 2699.11
Iteration:   3140, Loss function: 3.127, Average Loss: 3.773, avg. samples / sec: 2699.04
Iteration:   3160, Loss function: 3.180, Average Loss: 3.748, avg. samples / sec: 21978.18
Iteration:   3160, Loss function: 3.081, Average Loss: 3.757, avg. samples / sec: 21980.59
Iteration:   3160, Loss function: 3.605, Average Loss: 3.749, avg. samples / sec: 21988.69
Iteration:   3160, Loss function: 3.173, Average Loss: 3.748, avg. samples / sec: 21980.59
Iteration:   3160, Loss function: 3.313, Average Loss: 3.760, avg. samples / sec: 21989.66
Iteration:   3160, Loss function: 2.974, Average Loss: 3.758, avg. samples / sec: 21977.29
Iteration:   3160, Loss function: 3.180, Average Loss: 3.757, avg. samples / sec: 21980.86
Iteration:   3160, Loss function: 3.382, Average Loss: 3.757, avg. samples / sec: 21978.92

:::MLPv0.5.0 ssd 1541711230.607164860 (train.py:553) train_epoch: 55
Iteration:   3180, Loss function: 2.861, Average Loss: 3.744, avg. samples / sec: 22007.22
Iteration:   3180, Loss function: 2.772, Average Loss: 3.737, avg. samples / sec: 22008.38
Iteration:   3180, Loss function: 3.167, Average Loss: 3.736, avg. samples / sec: 22006.29
Iteration:   3180, Loss function: 2.893, Average Loss: 3.744, avg. samples / sec: 22010.17
Iteration:   3180, Loss function: 2.993, Average Loss: 3.744, avg. samples / sec: 22007.14
Iteration:   3180, Loss function: 2.917, Average Loss: 3.736, avg. samples / sec: 22000.83
Iteration:   3180, Loss function: 3.054, Average Loss: 3.741, avg. samples / sec: 22005.32
Iteration:   3180, Loss function: 3.228, Average Loss: 3.747, avg. samples / sec: 21999.54
Iteration:   3200, Loss function: 3.326, Average Loss: 3.732, avg. samples / sec: 21981.82
Iteration:   3200, Loss function: 3.131, Average Loss: 3.730, avg. samples / sec: 21982.98
Iteration:   3200, Loss function: 2.910, Average Loss: 3.725, avg. samples / sec: 21981.03
Iteration:   3200, Loss function: 3.366, Average Loss: 3.725, avg. samples / sec: 21979.31
Iteration:   3200, Loss function: 2.959, Average Loss: 3.722, avg. samples / sec: 21984.56
Iteration:   3200, Loss function: 2.964, Average Loss: 3.737, avg. samples / sec: 21985.88
Iteration:   3200, Loss function: 2.954, Average Loss: 3.729, avg. samples / sec: 21976.06
Iteration:   3200, Loss function: 3.090, Average Loss: 3.732, avg. samples / sec: 21972.37
Iteration:   3220, Loss function: 2.919, Average Loss: 3.715, avg. samples / sec: 22020.54
Iteration:   3220, Loss function: 2.917, Average Loss: 3.720, avg. samples / sec: 22016.76
Iteration:   3220, Loss function: 3.086, Average Loss: 3.712, avg. samples / sec: 22016.43
Iteration:   3220, Loss function: 3.321, Average Loss: 3.718, avg. samples / sec: 22012.23
Iteration:   3220, Loss function: 3.006, Average Loss: 3.714, avg. samples / sec: 22011.67
Iteration:   3220, Loss function: 3.344, Average Loss: 3.718, avg. samples / sec: 22021.46
Iteration:   3220, Loss function: 3.281, Average Loss: 3.718, avg. samples / sec: 22021.70
Iteration:   3220, Loss function: 2.852, Average Loss: 3.723, avg. samples / sec: 22010.00

:::MLPv0.5.0 ssd 1541711236.015568972 (train.py:553) train_epoch: 56
Iteration:   3240, Loss function: 3.276, Average Loss: 3.707, avg. samples / sec: 21891.90
Iteration:   3240, Loss function: 2.819, Average Loss: 3.706, avg. samples / sec: 21887.02
Iteration:   3240, Loss function: 3.174, Average Loss: 3.710, avg. samples / sec: 21902.06
Iteration:   3240, Loss function: 3.163, Average Loss: 3.705, avg. samples / sec: 21892.07
Iteration:   3240, Loss function: 3.088, Average Loss: 3.708, avg. samples / sec: 21897.00
Iteration:   3240, Loss function: 3.152, Average Loss: 3.704, avg. samples / sec: 21892.69
Iteration:   3240, Loss function: 3.643, Average Loss: 3.707, avg. samples / sec: 21893.34
Iteration:   3240, Loss function: 2.819, Average Loss: 3.700, avg. samples / sec: 21884.24
Iteration:   3260, Loss function: 3.221, Average Loss: 3.694, avg. samples / sec: 22010.91
Iteration:   3260, Loss function: 3.235, Average Loss: 3.689, avg. samples / sec: 22017.39
Iteration:   3260, Loss function: 2.785, Average Loss: 3.691, avg. samples / sec: 22011.76
Iteration:   3260, Loss function: 2.751, Average Loss: 3.696, avg. samples / sec: 22004.46
Iteration:   3260, Loss function: 3.740, Average Loss: 3.695, avg. samples / sec: 22003.33
Iteration:   3260, Loss function: 3.237, Average Loss: 3.694, avg. samples / sec: 22003.50
Iteration:   3260, Loss function: 2.979, Average Loss: 3.695, avg. samples / sec: 21998.19
Iteration:   3260, Loss function: 3.127, Average Loss: 3.692, avg. samples / sec: 22000.65
Iteration:   3280, Loss function: 3.170, Average Loss: 3.679, avg. samples / sec: 21932.78
Iteration:   3280, Loss function: 2.934, Average Loss: 3.682, avg. samples / sec: 21933.11
Iteration:   3280, Loss function: 3.386, Average Loss: 3.683, avg. samples / sec: 21925.33
Iteration:   3280, Loss function: 2.980, Average Loss: 3.685, avg. samples / sec: 21929.40
Iteration:   3280, Loss function: 3.280, Average Loss: 3.682, avg. samples / sec: 21933.26
Iteration:   3280, Loss function: 3.099, Average Loss: 3.679, avg. samples / sec: 21938.97
Iteration:   3280, Loss function: 2.914, Average Loss: 3.684, avg. samples / sec: 21935.64
Iteration:   3280, Loss function: 2.714, Average Loss: 3.677, avg. samples / sec: 21925.14

:::MLPv0.5.0 ssd 1541711241.420958996 (train.py:553) train_epoch: 57
Iteration:   3300, Loss function: 2.949, Average Loss: 3.672, avg. samples / sec: 21951.35
Iteration:   3300, Loss function: 3.058, Average Loss: 3.664, avg. samples / sec: 21952.59
Iteration:   3300, Loss function: 3.214, Average Loss: 3.668, avg. samples / sec: 21945.08
Iteration:   3300, Loss function: 3.241, Average Loss: 3.665, avg. samples / sec: 21950.25
Iteration:   3300, Loss function: 2.582, Average Loss: 3.670, avg. samples / sec: 21947.34
Iteration:   3300, Loss function: 2.709, Average Loss: 3.672, avg. samples / sec: 21948.83
Iteration:   3300, Loss function: 3.240, Average Loss: 3.671, avg. samples / sec: 21941.20
Iteration:   3300, Loss function: 3.028, Average Loss: 3.670, avg. samples / sec: 21941.67
Iteration:   3320, Loss function: 3.620, Average Loss: 3.656, avg. samples / sec: 21981.89
Iteration:   3320, Loss function: 3.129, Average Loss: 3.657, avg. samples / sec: 21978.38
Iteration:   3320, Loss function: 3.341, Average Loss: 3.660, avg. samples / sec: 21979.92
Iteration:   3320, Loss function: 2.992, Average Loss: 3.662, avg. samples / sec: 21973.22
Iteration:   3320, Loss function: 3.678, Average Loss: 3.660, avg. samples / sec: 21980.03
Iteration:   3320, Loss function: 2.795, Average Loss: 3.653, avg. samples / sec: 21966.56
Iteration:   3320, Loss function: 2.797, Average Loss: 3.652, avg. samples / sec: 21956.15
Iteration:   3320, Loss function: 2.924, Average Loss: 3.659, avg. samples / sec: 21946.56
Iteration:   3340, Loss function: 3.032, Average Loss: 3.642, avg. samples / sec: 22016.11
Iteration:   3340, Loss function: 3.106, Average Loss: 3.645, avg. samples / sec: 22004.93
Iteration:   3340, Loss function: 2.448, Average Loss: 3.649, avg. samples / sec: 22006.88
Iteration:   3340, Loss function: 2.815, Average Loss: 3.646, avg. samples / sec: 21998.04
Iteration:   3340, Loss function: 2.577, Average Loss: 3.646, avg. samples / sec: 22031.76
Iteration:   3340, Loss function: 3.270, Average Loss: 3.642, avg. samples / sec: 22013.73
Iteration:   3340, Loss function: 2.964, Average Loss: 3.652, avg. samples / sec: 21998.70
Iteration:   3340, Loss function: 3.149, Average Loss: 3.648, avg. samples / sec: 21999.34

:::MLPv0.5.0 ssd 1541711246.735696793 (train.py:553) train_epoch: 58
Iteration:   3360, Loss function: 3.527, Average Loss: 3.633, avg. samples / sec: 21911.71
Iteration:   3360, Loss function: 2.671, Average Loss: 3.634, avg. samples / sec: 21907.59
Iteration:   3360, Loss function: 3.257, Average Loss: 3.634, avg. samples / sec: 21905.55
Iteration:   3360, Loss function: 2.991, Average Loss: 3.642, avg. samples / sec: 21916.43
Iteration:   3360, Loss function: 2.956, Average Loss: 3.637, avg. samples / sec: 21912.32
Iteration:   3360, Loss function: 3.085, Average Loss: 3.630, avg. samples / sec: 21911.53
Iteration:   3360, Loss function: 3.398, Average Loss: 3.636, avg. samples / sec: 21901.81
Iteration:   3360, Loss function: 3.115, Average Loss: 3.632, avg. samples / sec: 21895.75
Iteration:   3380, Loss function: 3.542, Average Loss: 3.623, avg. samples / sec: 22020.10
Iteration:   3380, Loss function: 2.737, Average Loss: 3.620, avg. samples / sec: 22010.79
Iteration:   3380, Loss function: 2.915, Average Loss: 3.622, avg. samples / sec: 22023.21
Iteration:   3380, Loss function: 2.800, Average Loss: 3.623, avg. samples / sec: 22013.27
Iteration:   3380, Loss function: 3.106, Average Loss: 3.633, avg. samples / sec: 22013.97
Iteration:   3380, Loss function: 3.342, Average Loss: 3.626, avg. samples / sec: 22016.13
Iteration:   3380, Loss function: 2.993, Average Loss: 3.628, avg. samples / sec: 22014.17
Iteration:   3380, Loss function: 2.591, Average Loss: 3.618, avg. samples / sec: 22012.29
Iteration:   3400, Loss function: 2.720, Average Loss: 3.613, avg. samples / sec: 21965.04
Iteration:   3400, Loss function: 2.796, Average Loss: 3.610, avg. samples / sec: 21950.48
Iteration:   3400, Loss function: 2.851, Average Loss: 3.611, avg. samples / sec: 21955.10
Iteration:   3400, Loss function: 2.995, Average Loss: 3.619, avg. samples / sec: 21960.83
Iteration:   3400, Loss function: 3.127, Average Loss: 3.613, avg. samples / sec: 21958.31
Iteration:   3400, Loss function: 2.687, Average Loss: 3.621, avg. samples / sec: 21954.35
Iteration:   3400, Loss function: 3.186, Average Loss: 3.608, avg. samples / sec: 21958.05
Iteration:   3400, Loss function: 3.350, Average Loss: 3.619, avg. samples / sec: 21952.25

:::MLPv0.5.0 ssd 1541711252.143885612 (train.py:553) train_epoch: 59
Iteration:   3420, Loss function: 2.665, Average Loss: 3.604, avg. samples / sec: 21964.86
Iteration:   3420, Loss function: 3.204, Average Loss: 3.601, avg. samples / sec: 21969.43
Iteration:   3420, Loss function: 2.750, Average Loss: 3.605, avg. samples / sec: 21970.79
Iteration:   3420, Loss function: 3.043, Average Loss: 3.599, avg. samples / sec: 21966.38
Iteration:   3420, Loss function: 2.714, Average Loss: 3.610, avg. samples / sec: 21969.74
Iteration:   3420, Loss function: 3.122, Average Loss: 3.596, avg. samples / sec: 21969.02
Iteration:   3420, Loss function: 3.349, Average Loss: 3.602, avg. samples / sec: 21960.92
Iteration:   3420, Loss function: 3.507, Average Loss: 3.607, avg. samples / sec: 21965.68

































































:::MLPv0.5.0 ssd 1541711254.947443247 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711254.948260546 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711254.949005604 (train.py:220) eval_start: 59
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.25 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.25 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.25 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.25 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.25 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.25 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.25 s
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(285147, 7)
0/285147
(285147, 7)
0/285147
Loading and preparing results...
Converting ndarray to lists...
(285147, 7)
0/285147
Loading and preparing results...
Converting ndarray to lists...
(285147, 7)
0/285147
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(285147, 7)
Loading and preparing results...
Converting ndarray to lists...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.25 s
0/285147
Loading and preparing results...
(285147, 7)
Converting ndarray to lists...
0/285147
(285147, 7)
Converting ndarray to lists...
(285147, 7)
0/285147
0/285147
Loading and preparing results...
Converting ndarray to lists...
(285147, 7)
(285147, 7)
0/285147
Loading and preparing results...
Converting ndarray to lists...
0/285147
(285147, 7)
0/285147
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(285147, 7)
(285147, 7)
0/285147
Converting ndarray to lists...
Converting ndarray to lists...
0/285147
(285147, 7)
(285147, 7)
0/285147
Loading and preparing results...
Converting ndarray to lists...
(285147, 7)
0/285147
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(285147, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/285147
Loading and preparing results...
(285147, 7)
Converting ndarray to lists...
(285147, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
0/285147
Loading and preparing results...
0/285147
(285147, 7)
Converting ndarray to lists...
(285147, 7)
(285147, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/285147
0/285147
Loading and preparing results...
0/285147
Converting ndarray to lists...
Loading and preparing results...
(285147, 7)
(285147, 7)
Converting ndarray to lists...
(285147, 7)
Converting ndarray to lists...
0/285147
Loading and preparing results...
0/285147
Loading and preparing results...
Loading and preparing results...
(285147, 7)
Converting ndarray to lists...
(285147, 7)
(285147, 7)
Converting ndarray to lists...
0/285147
0/285147
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
0/285147
Converting ndarray to lists...
Converting ndarray to lists...
0/285147
Loading and preparing results...
Converting ndarray to lists...
(285147, 7)
(285147, 7)
(285147, 7)
(285147, 7)
Converting ndarray to lists...
(285147, 7)
0/285147
0/285147
0/285147
(285147, 7)
0/285147
0/285147
0/285147
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(285147, 7)
(285147, 7)
(285147, 7)
0/285147
Loading and preparing results...
0/285147
0/285147
Loading and preparing results...
Converting ndarray to lists...
(285147, 7)
Converting ndarray to lists...
Loading and preparing results...
0/285147
(285147, 7)
Converting ndarray to lists...
0/285147
(285147, 7)
0/285147
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(285147, 7)
(285147, 7)
Loading and preparing results...
(285147, 7)
Loading and preparing results...
0/285147
0/285147
0/285147
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(285147, 7)
(285147, 7)
Loading and preparing results...
(285147, 7)
0/285147
0/285147
Converting ndarray to lists...
0/285147
(285147, 7)
Converting ndarray to lists...
Loading and preparing results...
(285147, 7)
0/285147
Loading and preparing results...
Converting ndarray to lists...
0/285147
Converting ndarray to lists...
(285147, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/285147
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(285147, 7)
(285147, 7)
(285147, 7)
0/285147
0/285147
0/285147
Loading and preparing results...
(285147, 7)
0/285147
Converting ndarray to lists...
(285147, 7)
Loading and preparing results...
0/285147
Converting ndarray to lists...
Loading and preparing results...
(285147, 7)
Converting ndarray to lists...
0/285147
(285147, 7)
0/285147
0/285147
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(285147, 7)
(285147, 7)
(285147, 7)
0/285147
0/285147
0/285147
Loading and preparing results...
Converting ndarray to lists...
(285147, 7)
0/285147
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(285147, 7)
(285147, 7)
(285147, 7)
(285147, 7)
0/285147
0/285147
0/285147
0/285147
DONE (t=1.43s)
creating index...
DONE (t=1.45s)
creating index...
DONE (t=1.46s)
creating index...
index created!
index created!
DONE (t=1.59s)
creating index...
index created!
DONE (t=1.62s)
creating index...
DONE (t=1.62s)
creating index...
DONE (t=1.63s)
creating index...
DONE (t=1.63s)
creating index...
DONE (t=1.64s)
creating index...
DONE (t=1.66s)
creating index...
DONE (t=1.66s)
creating index...
DONE (t=1.67s)
creating index...
DONE (t=1.67s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
index created!
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.75s)
creating index...
index created!
DONE (t=1.75s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=1.80s)
creating index...
index created!
index created!
DONE (t=1.81s)
creating index...
index created!
index created!
index created!
DONE (t=1.82s)
creating index...
index created!
DONE (t=1.83s)
creating index...
index created!
index created!
index created!
index created!
index created!
DONE (t=1.83s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.85s)
creating index...
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
DONE (t=1.89s)
creating index...
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
DONE (t=2.00s)
creating index...
index created!
DONE (t=2.04s)
creating index...
index created!
index created!
DONE (t=3.50s).
Accumulating evaluation results...
DONE (t=3.45s).
Accumulating evaluation results...
DONE (t=3.46s).
Accumulating evaluation results...
DONE (t=3.45s).
Accumulating evaluation results...
DONE (t=3.49s).
Accumulating evaluation results...
DONE (t=3.49s).
Accumulating evaluation results...
DONE (t=3.48s).
Accumulating evaluation results...
DONE (t=3.44s).
Accumulating evaluation results...
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.377
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
DONE (t=1.06s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.231
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.311
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.355
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21813 AP goal: 0.21200
DONE (t=1.05s).
DONE (t=1.06s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.377
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.377
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.377
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.377
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.231
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.231
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.346
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.377
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.311
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.231
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.355
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21813 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.311
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.355
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21813 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.231
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.346
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.377
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.311
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.231
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.355
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21813 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.311
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.355
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21813 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.346
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.311
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.231
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.355
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21813 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.215
DONE (t=1.09s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.311
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.355
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21813 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.377
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.231
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.311
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.355
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505
Current AP: 0.21813 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711267.036717892 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711267.037607431 (train.py:333) eval_accuracy: {"epoch": 59, "value": 0.21813246339912024}

:::MLPv0.5.0 ssd 1541711267.038318872 (train.py:336) eval_iteration_accuracy: {"epoch": 59, "value": 0.21813246339912024}

:::MLPv0.5.0 ssd 1541711267.039060116 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711267.039837122 (train.py:338) eval_stop: 59

:::MLPv0.5.0 ssd 1541711268.091415644 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1541711268.092134476 (train.py:707) run_final
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2018-11-08 09:07:53 PM
RESULT,OBJECT_DETECTION,,448,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:53 PM
RESULT,OBJECT_DETECTION,,448,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:53 PM
RESULT,OBJECT_DETECTION,,448,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:53 PM
RESULT,OBJECT_DETECTION,,448,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:53 PM
RESULT,OBJECT_DETECTION,,448,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:53 PM
RESULT,OBJECT_DETECTION,,448,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:53 PM
RESULT,OBJECT_DETECTION,,448,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:53 PM
RESULT,OBJECT_DETECTION,,448,nvidia,2018-11-08 09:00:25 PM
