Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3

:::MLPv0.5.0 ssd 1541710833.905434132 (<string>:1) run_clear_caches
Launching on node sc-sdgx-364
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-364
+ srun -N 1 -n 1 -w sc-sdgx-364 docker exec -e DGXSYSTEM=DGX1 -e MULTI_NODE= -e SLURM_JOB_ID=155385 -e SLURM_NTASKS_PER_NODE=8 cont_155385 ./run_and_time.sh
Run vars: id 155385 gpus 8 mparams 
STARTING TIMING RUN AT 2018-11-08 09:00:34 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 152 --warmup 300 --nhwc --pad-input
1 Using seed = 1933774775
3 Using seed = 1933774777
2 Using seed = 1933774776
4 Using seed = 1933774778
6 Using seed = 1933774780
5 Using seed = 1933774779
7 Using seed = 1933774781
0 Using seed = 1933774774

:::MLPv0.5.0 ssd 1541710845.318763018 (train.py:371) run_start

:::MLPv0.5.0 ssd 1541710845.319637299 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1541710845.320369005 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1541710845.321114540 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1541710845.321868658 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1541710845.354291439 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1541710845.355390310 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1541710845.356469154 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1541710845.357458830 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1541710845.358435631 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1541710845.359373808 (train.py:382) input_size: 300
loading annotations into memory...
Done (t=0.47s)
creating index...
Done (t=0.48s)
creating index...
index created!
Done (t=0.50s)
creating index...
index created!
Done (t=0.53s)
creating index...
index created!
Done (t=0.53s)
creating index...
Done (t=0.54s)
creating index...
Done (t=0.54s)
creating index...
Done (t=0.54s)
creating index...
index created!
index created!
index created!
index created!
index created!
time_check a: 1541710846.378671646
time_check b: 1541710870.051386356

:::MLPv0.5.0 ssd 1541710871.890295982 (train.py:413) input_order

:::MLPv0.5.0 ssd 1541710871.895931959 (train.py:414) input_batch_size: 152

:::MLPv0.5.0 ssd 1541710875.977643490 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710875.978626966 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710876.035173655 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1541710876.479383469 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1541710876.481186152 (train.py:477) opt_learning_rate: 0.095

:::MLPv0.5.0 ssd 1541710876.482863426 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1541710876.484399319 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1541710876.485796928 (train.py:483) opt_learning_rate_warmup_steps: 300

:::MLPv0.5.0 ssd 1541710880.507466078 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710880.509486437 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710880.602793932 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss

:::MLPv0.5.0 ssd 1541710885.071820021 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1541710885.072626352 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1541710885.076186419 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 22.575, Average Loss: 0.023, avg. samples / sec: 6318.67

:::MLPv0.5.0 ssd 1541710888.236243248 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.0003166666666666734}

:::MLPv0.5.0 ssd 1541710889.030339956 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0006333333333333468}

:::MLPv0.5.0 ssd 1541710889.519288063 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0009500000000000064}

:::MLPv0.5.0 ssd 1541710889.978992701 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0012666666666666798}

:::MLPv0.5.0 ssd 1541710890.444128275 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0015833333333333394}

:::MLPv0.5.0 ssd 1541710890.918979406 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0019000000000000128}

:::MLPv0.5.0 ssd 1541710891.418496609 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.0022166666666666723}

:::MLPv0.5.0 ssd 1541710891.861589909 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0025333333333333458}

:::MLPv0.5.0 ssd 1541710892.300602436 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0028500000000000053}

:::MLPv0.5.0 ssd 1541710892.758032322 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.0031666666666666787}

:::MLPv0.5.0 ssd 1541710893.254115105 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0034833333333333383}

:::MLPv0.5.0 ssd 1541710893.693260431 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.0038000000000000117}

:::MLPv0.5.0 ssd 1541710894.124065399 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.004116666666666671}

:::MLPv0.5.0 ssd 1541710894.546038389 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.004433333333333345}

:::MLPv0.5.0 ssd 1541710895.059492350 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.004750000000000004}

:::MLPv0.5.0 ssd 1541710895.454242706 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.005066666666666678}

:::MLPv0.5.0 ssd 1541710895.902132988 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.005383333333333337}

:::MLPv0.5.0 ssd 1541710896.355783701 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.005700000000000011}

:::MLPv0.5.0 ssd 1541710896.809170246 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.00601666666666667}

:::MLPv0.5.0 ssd 1541710897.273531675 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.006333333333333344}
Iteration:     20, Loss function: 20.185, Average Loss: 0.439, avg. samples / sec: 1995.29

:::MLPv0.5.0 ssd 1541710897.681370258 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.006650000000000003}

:::MLPv0.5.0 ssd 1541710898.188572884 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.0069666666666666766}

:::MLPv0.5.0 ssd 1541710898.624382496 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.007283333333333336}

:::MLPv0.5.0 ssd 1541710899.053570271 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.0076000000000000095}

:::MLPv0.5.0 ssd 1541710899.472441435 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.007916666666666669}

:::MLPv0.5.0 ssd 1541710899.882672787 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.008233333333333342}

:::MLPv0.5.0 ssd 1541710900.294841766 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.008550000000000002}

:::MLPv0.5.0 ssd 1541710900.713976145 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.008866666666666675}

:::MLPv0.5.0 ssd 1541710901.162936687 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.009183333333333335}

:::MLPv0.5.0 ssd 1541710901.600492477 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.009500000000000008}

:::MLPv0.5.0 ssd 1541710902.020960331 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.009816666666666668}

:::MLPv0.5.0 ssd 1541710902.436084986 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.010133333333333341}

:::MLPv0.5.0 ssd 1541710902.892046452 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.010450000000000001}

:::MLPv0.5.0 ssd 1541710903.317475557 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.010766666666666674}

:::MLPv0.5.0 ssd 1541710903.741873503 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.011083333333333334}

:::MLPv0.5.0 ssd 1541710904.161772490 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.011400000000000007}

:::MLPv0.5.0 ssd 1541710904.549975395 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.011716666666666667}

:::MLPv0.5.0 ssd 1541710904.990095854 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.01203333333333334}

:::MLPv0.5.0 ssd 1541710905.382866383 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.01235}

:::MLPv0.5.0 ssd 1541710905.805073261 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.012666666666666673}
Iteration:     40, Loss function: 13.004, Average Loss: 0.794, avg. samples / sec: 2847.03

:::MLPv0.5.0 ssd 1541710906.220801353 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.012983333333333333}

:::MLPv0.5.0 ssd 1541710906.621804237 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.013300000000000006}

:::MLPv0.5.0 ssd 1541710907.013409138 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.013616666666666666}

:::MLPv0.5.0 ssd 1541710907.442995787 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.01393333333333334}

:::MLPv0.5.0 ssd 1541710907.818027258 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.014250000000000013}

:::MLPv0.5.0 ssd 1541710908.212660789 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.014566666666666672}

:::MLPv0.5.0 ssd 1541710908.608223677 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.014883333333333346}

:::MLPv0.5.0 ssd 1541710909.027601004 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.015200000000000005}

:::MLPv0.5.0 ssd 1541710909.429993868 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.015516666666666679}

:::MLPv0.5.0 ssd 1541710909.851495266 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.015833333333333338}

:::MLPv0.5.0 ssd 1541710910.192096233 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.01615000000000001}

:::MLPv0.5.0 ssd 1541710910.592259645 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.01646666666666667}

:::MLPv0.5.0 ssd 1541710910.961711645 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.016783333333333345}

:::MLPv0.5.0 ssd 1541710911.342213392 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.017100000000000004}

:::MLPv0.5.0 ssd 1541710911.764480829 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.017416666666666678}

:::MLPv0.5.0 ssd 1541710912.105058193 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.017733333333333337}

:::MLPv0.5.0 ssd 1541710912.530692339 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.01805000000000001}

:::MLPv0.5.0 ssd 1541710912.909608364 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 58, "value": 0.01836666666666667}

:::MLPv0.5.0 ssd 1541710913.336595535 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 59, "value": 0.018683333333333343}

:::MLPv0.5.0 ssd 1541710913.676131010 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 60, "value": 0.019000000000000003}
Iteration:     60, Loss function: 12.415, Average Loss: 1.058, avg. samples / sec: 3091.84

:::MLPv0.5.0 ssd 1541710914.039430857 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 61, "value": 0.019316666666666676}

:::MLPv0.5.0 ssd 1541710914.470131874 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 62, "value": 0.019633333333333336}

:::MLPv0.5.0 ssd 1541710914.817562103 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 63, "value": 0.01995000000000001}

:::MLPv0.5.0 ssd 1541710915.175315380 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 64, "value": 0.02026666666666667}

:::MLPv0.5.0 ssd 1541710915.586863518 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 65, "value": 0.020583333333333342}

:::MLPv0.5.0 ssd 1541710915.977867842 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 66, "value": 0.020900000000000002}

:::MLPv0.5.0 ssd 1541710916.380037308 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 67, "value": 0.021216666666666675}

:::MLPv0.5.0 ssd 1541710916.744960785 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 68, "value": 0.021533333333333335}

:::MLPv0.5.0 ssd 1541710917.108223438 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 69, "value": 0.02185000000000001}

:::MLPv0.5.0 ssd 1541710917.463455439 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 70, "value": 0.022166666666666668}

:::MLPv0.5.0 ssd 1541710917.843064547 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 71, "value": 0.02248333333333334}

:::MLPv0.5.0 ssd 1541710918.229668617 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 72, "value": 0.0228}

:::MLPv0.5.0 ssd 1541710918.588902712 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 73, "value": 0.023116666666666674}

:::MLPv0.5.0 ssd 1541710919.072351933 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 74, "value": 0.023433333333333334}

:::MLPv0.5.0 ssd 1541710919.470396042 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 75, "value": 0.023750000000000007}

:::MLPv0.5.0 ssd 1541710919.860827923 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 76, "value": 0.024066666666666667}

:::MLPv0.5.0 ssd 1541710920.213278055 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 77, "value": 0.02438333333333334}

:::MLPv0.5.0 ssd 1541710920.567642450 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 78, "value": 0.0247}

:::MLPv0.5.0 ssd 1541710920.996910572 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 79, "value": 0.025016666666666673}

:::MLPv0.5.0 ssd 1541710921.323924303 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 80, "value": 0.025333333333333333}
Iteration:     80, Loss function: 9.709, Average Loss: 1.247, avg. samples / sec: 3179.54

:::MLPv0.5.0 ssd 1541710921.680112123 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 81, "value": 0.025650000000000006}

:::MLPv0.5.0 ssd 1541710922.020751953 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 82, "value": 0.025966666666666666}

:::MLPv0.5.0 ssd 1541710922.418714285 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 83, "value": 0.02628333333333334}

:::MLPv0.5.0 ssd 1541710922.766144276 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 84, "value": 0.0266}

:::MLPv0.5.0 ssd 1541710923.144946575 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 85, "value": 0.026916666666666672}

:::MLPv0.5.0 ssd 1541710923.527719021 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 86, "value": 0.02723333333333333}

:::MLPv0.5.0 ssd 1541710923.902659655 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 87, "value": 0.027550000000000005}

:::MLPv0.5.0 ssd 1541710924.307645082 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 88, "value": 0.02786666666666668}

:::MLPv0.5.0 ssd 1541710924.640247822 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 89, "value": 0.028183333333333338}

:::MLPv0.5.0 ssd 1541710924.972893476 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 90, "value": 0.02850000000000001}

:::MLPv0.5.0 ssd 1541710925.381067276 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 91, "value": 0.02881666666666667}

:::MLPv0.5.0 ssd 1541710925.722577333 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 92, "value": 0.029133333333333344}

:::MLPv0.5.0 ssd 1541710926.066470146 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 93, "value": 0.029450000000000004}

:::MLPv0.5.0 ssd 1541710926.415083408 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 94, "value": 0.029766666666666677}

:::MLPv0.5.0 ssd 1541710926.762392759 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 95, "value": 0.030083333333333337}

:::MLPv0.5.0 ssd 1541710927.095350504 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 96, "value": 0.03040000000000001}

:::MLPv0.5.0 ssd 1541710927.431600571 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 97, "value": 0.03071666666666667}

:::MLPv0.5.0 ssd 1541710927.712526798 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1541710927.749984741 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 98, "value": 0.031033333333333343}

:::MLPv0.5.0 ssd 1541710928.080875635 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 99, "value": 0.03135}

:::MLPv0.5.0 ssd 1541710928.402719975 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 100, "value": 0.031666666666666676}
Iteration:    100, Loss function: 9.349, Average Loss: 1.413, avg. samples / sec: 3441.38

:::MLPv0.5.0 ssd 1541710928.738665104 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 101, "value": 0.031983333333333336}

:::MLPv0.5.0 ssd 1541710929.063260078 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 102, "value": 0.03230000000000001}

:::MLPv0.5.0 ssd 1541710929.388702154 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 103, "value": 0.03261666666666667}

:::MLPv0.5.0 ssd 1541710929.715517521 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 104, "value": 0.032933333333333335}

:::MLPv0.5.0 ssd 1541710930.046332121 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 105, "value": 0.03325}

:::MLPv0.5.0 ssd 1541710930.386341095 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 106, "value": 0.03356666666666667}

:::MLPv0.5.0 ssd 1541710930.698858500 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 107, "value": 0.033883333333333335}

:::MLPv0.5.0 ssd 1541710931.050119162 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 108, "value": 0.03420000000000001}

:::MLPv0.5.0 ssd 1541710931.386142492 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 109, "value": 0.034516666666666675}

:::MLPv0.5.0 ssd 1541710931.738981962 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 110, "value": 0.03483333333333334}

:::MLPv0.5.0 ssd 1541710932.058486700 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 111, "value": 0.03515000000000001}

:::MLPv0.5.0 ssd 1541710932.385363340 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 112, "value": 0.035466666666666674}

:::MLPv0.5.0 ssd 1541710932.703085423 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 113, "value": 0.03578333333333334}

:::MLPv0.5.0 ssd 1541710933.056586504 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 114, "value": 0.03610000000000001}

:::MLPv0.5.0 ssd 1541710933.385818958 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 115, "value": 0.036416666666666674}

:::MLPv0.5.0 ssd 1541710933.728024006 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 116, "value": 0.03673333333333334}

:::MLPv0.5.0 ssd 1541710934.095627785 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 117, "value": 0.03705000000000001}

:::MLPv0.5.0 ssd 1541710934.436192751 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 118, "value": 0.03736666666666667}

:::MLPv0.5.0 ssd 1541710934.774233341 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 119, "value": 0.03768333333333334}

:::MLPv0.5.0 ssd 1541710935.110203981 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 120, "value": 0.038000000000000006}
Iteration:    120, Loss function: 9.542, Average Loss: 1.570, avg. samples / sec: 3623.53

:::MLPv0.5.0 ssd 1541710935.446263313 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 121, "value": 0.03831666666666667}

:::MLPv0.5.0 ssd 1541710935.794752836 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 122, "value": 0.03863333333333334}

:::MLPv0.5.0 ssd 1541710936.131308794 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 123, "value": 0.038950000000000005}

:::MLPv0.5.0 ssd 1541710936.441938162 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 124, "value": 0.03926666666666667}

:::MLPv0.5.0 ssd 1541710936.764829159 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 125, "value": 0.03958333333333334}

:::MLPv0.5.0 ssd 1541710937.100064754 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 126, "value": 0.039900000000000005}

:::MLPv0.5.0 ssd 1541710937.438260794 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 127, "value": 0.04021666666666667}

:::MLPv0.5.0 ssd 1541710937.764687300 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 128, "value": 0.04053333333333334}

:::MLPv0.5.0 ssd 1541710938.088146925 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 129, "value": 0.040850000000000004}

:::MLPv0.5.0 ssd 1541710938.399969101 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 130, "value": 0.04116666666666667}

:::MLPv0.5.0 ssd 1541710938.733087063 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 131, "value": 0.04148333333333334}

:::MLPv0.5.0 ssd 1541710939.066637754 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 132, "value": 0.041800000000000004}

:::MLPv0.5.0 ssd 1541710939.398681641 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 133, "value": 0.04211666666666667}

:::MLPv0.5.0 ssd 1541710939.712128878 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 134, "value": 0.04243333333333334}

:::MLPv0.5.0 ssd 1541710940.049582481 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 135, "value": 0.04275}

:::MLPv0.5.0 ssd 1541710940.352782249 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 136, "value": 0.04306666666666667}

:::MLPv0.5.0 ssd 1541710940.673014164 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 137, "value": 0.043383333333333336}

:::MLPv0.5.0 ssd 1541710940.992437363 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 138, "value": 0.0437}

:::MLPv0.5.0 ssd 1541710941.316790581 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 139, "value": 0.04401666666666667}

:::MLPv0.5.0 ssd 1541710941.639400959 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 140, "value": 0.044333333333333336}
Iteration:    140, Loss function: 9.182, Average Loss: 1.721, avg. samples / sec: 3727.82

:::MLPv0.5.0 ssd 1541710941.962118149 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 141, "value": 0.04465}

:::MLPv0.5.0 ssd 1541710942.279341459 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 142, "value": 0.04496666666666667}

:::MLPv0.5.0 ssd 1541710942.600569010 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 143, "value": 0.045283333333333335}

:::MLPv0.5.0 ssd 1541710942.920221806 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 144, "value": 0.0456}

:::MLPv0.5.0 ssd 1541710943.252921104 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 145, "value": 0.04591666666666667}

:::MLPv0.5.0 ssd 1541710943.593247414 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 146, "value": 0.046233333333333335}

:::MLPv0.5.0 ssd 1541710943.923968554 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 147, "value": 0.04655}

:::MLPv0.5.0 ssd 1541710944.262029171 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 148, "value": 0.04686666666666667}

:::MLPv0.5.0 ssd 1541710944.593436480 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 149, "value": 0.047183333333333334}

:::MLPv0.5.0 ssd 1541710944.918940067 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 150, "value": 0.0475}

:::MLPv0.5.0 ssd 1541710945.244360924 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 151, "value": 0.047816666666666674}

:::MLPv0.5.0 ssd 1541710945.549717665 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 152, "value": 0.04813333333333334}

:::MLPv0.5.0 ssd 1541710945.884639740 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 153, "value": 0.04845000000000001}

:::MLPv0.5.0 ssd 1541710946.208115816 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 154, "value": 0.04876666666666667}

:::MLPv0.5.0 ssd 1541710946.526293516 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 155, "value": 0.04908333333333334}

:::MLPv0.5.0 ssd 1541710946.883659124 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 156, "value": 0.049400000000000006}

:::MLPv0.5.0 ssd 1541710947.218562126 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 157, "value": 0.04971666666666667}

:::MLPv0.5.0 ssd 1541710947.558960915 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 158, "value": 0.05003333333333334}

:::MLPv0.5.0 ssd 1541710947.907416821 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 159, "value": 0.050350000000000006}

:::MLPv0.5.0 ssd 1541710948.218409061 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 160, "value": 0.05066666666666667}
Iteration:    160, Loss function: 8.579, Average Loss: 1.864, avg. samples / sec: 3692.61

:::MLPv0.5.0 ssd 1541710948.567049503 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 161, "value": 0.05098333333333334}

:::MLPv0.5.0 ssd 1541710948.895539284 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 162, "value": 0.051300000000000005}

:::MLPv0.5.0 ssd 1541710949.226423740 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 163, "value": 0.05161666666666667}

:::MLPv0.5.0 ssd 1541710949.558552980 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 164, "value": 0.05193333333333334}

:::MLPv0.5.0 ssd 1541710949.894081354 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 165, "value": 0.052250000000000005}

:::MLPv0.5.0 ssd 1541710950.225035667 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 166, "value": 0.05256666666666667}

:::MLPv0.5.0 ssd 1541710950.555455208 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 167, "value": 0.05288333333333334}

:::MLPv0.5.0 ssd 1541710950.915062428 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 168, "value": 0.053200000000000004}

:::MLPv0.5.0 ssd 1541710951.224228621 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 169, "value": 0.05351666666666667}

:::MLPv0.5.0 ssd 1541710951.576728344 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 170, "value": 0.05383333333333334}

:::MLPv0.5.0 ssd 1541710951.892738819 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 171, "value": 0.054150000000000004}

:::MLPv0.5.0 ssd 1541710952.220028400 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 172, "value": 0.05446666666666667}

:::MLPv0.5.0 ssd 1541710952.546520233 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 173, "value": 0.05478333333333334}

:::MLPv0.5.0 ssd 1541710952.885006189 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 174, "value": 0.0551}

:::MLPv0.5.0 ssd 1541710953.206913710 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 175, "value": 0.05541666666666667}

:::MLPv0.5.0 ssd 1541710953.518014193 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 176, "value": 0.055733333333333336}

:::MLPv0.5.0 ssd 1541710953.841935158 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 177, "value": 0.05605}

:::MLPv0.5.0 ssd 1541710954.153925180 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 178, "value": 0.05636666666666667}

:::MLPv0.5.0 ssd 1541710954.479337215 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 179, "value": 0.056683333333333336}

:::MLPv0.5.0 ssd 1541710954.795202494 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 180, "value": 0.057}
Iteration:    180, Loss function: 8.473, Average Loss: 1.997, avg. samples / sec: 3701.81

:::MLPv0.5.0 ssd 1541710955.114658117 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 181, "value": 0.05731666666666667}

:::MLPv0.5.0 ssd 1541710955.433822155 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 182, "value": 0.057633333333333335}

:::MLPv0.5.0 ssd 1541710955.770375729 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 183, "value": 0.05795}

:::MLPv0.5.0 ssd 1541710956.083981991 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 184, "value": 0.05826666666666667}

:::MLPv0.5.0 ssd 1541710956.403548241 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 185, "value": 0.058583333333333334}

:::MLPv0.5.0 ssd 1541710956.732227802 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 186, "value": 0.0589}

:::MLPv0.5.0 ssd 1541710957.076440811 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 187, "value": 0.05921666666666667}

:::MLPv0.5.0 ssd 1541710957.412859678 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 188, "value": 0.059533333333333334}

:::MLPv0.5.0 ssd 1541710957.729538679 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 189, "value": 0.05985}

:::MLPv0.5.0 ssd 1541710958.053359270 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 190, "value": 0.06016666666666667}

:::MLPv0.5.0 ssd 1541710958.369179249 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 191, "value": 0.06048333333333333}

:::MLPv0.5.0 ssd 1541710958.694186926 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 192, "value": 0.0608}

:::MLPv0.5.0 ssd 1541710959.009564400 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 193, "value": 0.061116666666666666}

:::MLPv0.5.0 ssd 1541710959.333405733 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 194, "value": 0.06143333333333334}

:::MLPv0.5.0 ssd 1541710959.622635126 (train.py:553) train_epoch: 2

:::MLPv0.5.0 ssd 1541710959.651470661 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 195, "value": 0.061750000000000006}

:::MLPv0.5.0 ssd 1541710959.972487450 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 196, "value": 0.06206666666666667}

:::MLPv0.5.0 ssd 1541710960.300945520 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 197, "value": 0.06238333333333334}

:::MLPv0.5.0 ssd 1541710960.625671864 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 198, "value": 0.0627}

:::MLPv0.5.0 ssd 1541710960.964875937 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 199, "value": 0.06301666666666667}

:::MLPv0.5.0 ssd 1541710961.291280746 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 200, "value": 0.06333333333333334}
Iteration:    200, Loss function: 8.108, Average Loss: 2.124, avg. samples / sec: 3744.10

:::MLPv0.5.0 ssd 1541710961.612644672 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 201, "value": 0.06365000000000001}

:::MLPv0.5.0 ssd 1541710961.932606220 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 202, "value": 0.06396666666666667}

:::MLPv0.5.0 ssd 1541710962.249100208 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 203, "value": 0.06428333333333333}

:::MLPv0.5.0 ssd 1541710962.554959059 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 204, "value": 0.0646}

:::MLPv0.5.0 ssd 1541710962.884872913 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 205, "value": 0.06491666666666668}

:::MLPv0.5.0 ssd 1541710963.202213049 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 206, "value": 0.06523333333333334}

:::MLPv0.5.0 ssd 1541710963.511037350 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 207, "value": 0.06555}

:::MLPv0.5.0 ssd 1541710963.825943947 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 208, "value": 0.06586666666666667}

:::MLPv0.5.0 ssd 1541710964.145263195 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 209, "value": 0.06618333333333334}

:::MLPv0.5.0 ssd 1541710964.459848881 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 210, "value": 0.0665}

:::MLPv0.5.0 ssd 1541710964.787983418 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 211, "value": 0.06681666666666666}

:::MLPv0.5.0 ssd 1541710965.104620695 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 212, "value": 0.06713333333333334}

:::MLPv0.5.0 ssd 1541710965.421818972 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 213, "value": 0.06745000000000001}

:::MLPv0.5.0 ssd 1541710965.734316111 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 214, "value": 0.06776666666666667}

:::MLPv0.5.0 ssd 1541710966.045379877 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 215, "value": 0.06808333333333333}

:::MLPv0.5.0 ssd 1541710966.358022690 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 216, "value": 0.0684}

:::MLPv0.5.0 ssd 1541710966.683471680 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 217, "value": 0.06871666666666668}

:::MLPv0.5.0 ssd 1541710966.996614933 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 218, "value": 0.06903333333333334}

:::MLPv0.5.0 ssd 1541710967.319561958 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 219, "value": 0.06935}

:::MLPv0.5.0 ssd 1541710967.627054691 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 220, "value": 0.06966666666666667}
Iteration:    220, Loss function: 8.057, Average Loss: 2.243, avg. samples / sec: 3837.81

:::MLPv0.5.0 ssd 1541710967.935820818 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 221, "value": 0.06998333333333334}

:::MLPv0.5.0 ssd 1541710968.246296406 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 222, "value": 0.0703}

:::MLPv0.5.0 ssd 1541710968.561193228 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 223, "value": 0.07061666666666666}

:::MLPv0.5.0 ssd 1541710968.880536795 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 224, "value": 0.07093333333333333}

:::MLPv0.5.0 ssd 1541710969.200132608 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 225, "value": 0.07125000000000001}

:::MLPv0.5.0 ssd 1541710969.517607450 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 226, "value": 0.07156666666666667}

:::MLPv0.5.0 ssd 1541710969.834531307 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 227, "value": 0.07188333333333334}

:::MLPv0.5.0 ssd 1541710970.154532194 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 228, "value": 0.0722}

:::MLPv0.5.0 ssd 1541710970.477860689 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 229, "value": 0.07251666666666667}

:::MLPv0.5.0 ssd 1541710970.797107458 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 230, "value": 0.07283333333333333}

:::MLPv0.5.0 ssd 1541710971.117013454 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 231, "value": 0.07315}

:::MLPv0.5.0 ssd 1541710971.435602427 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 232, "value": 0.07346666666666667}

:::MLPv0.5.0 ssd 1541710971.760844231 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 233, "value": 0.07378333333333334}

:::MLPv0.5.0 ssd 1541710972.082017899 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 234, "value": 0.0741}

:::MLPv0.5.0 ssd 1541710972.406358480 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 235, "value": 0.07441666666666667}

:::MLPv0.5.0 ssd 1541710972.721261978 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 236, "value": 0.07473333333333333}

:::MLPv0.5.0 ssd 1541710973.037871361 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 237, "value": 0.07505}

:::MLPv0.5.0 ssd 1541710973.349648237 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 238, "value": 0.07536666666666667}

:::MLPv0.5.0 ssd 1541710973.662782192 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 239, "value": 0.07568333333333334}

:::MLPv0.5.0 ssd 1541710973.977728844 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 240, "value": 0.076}
Iteration:    240, Loss function: 8.408, Average Loss: 2.359, avg. samples / sec: 3830.04

:::MLPv0.5.0 ssd 1541710974.336988688 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 241, "value": 0.07631666666666667}

:::MLPv0.5.0 ssd 1541710974.662918806 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 242, "value": 0.07663333333333333}

:::MLPv0.5.0 ssd 1541710974.998502970 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 243, "value": 0.07695}

:::MLPv0.5.0 ssd 1541710975.315293789 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 244, "value": 0.07726666666666666}

:::MLPv0.5.0 ssd 1541710975.630187273 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 245, "value": 0.07758333333333334}

:::MLPv0.5.0 ssd 1541710975.946795225 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 246, "value": 0.0779}

:::MLPv0.5.0 ssd 1541710976.244846106 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 247, "value": 0.07821666666666667}

:::MLPv0.5.0 ssd 1541710976.583300352 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 248, "value": 0.07853333333333334}

:::MLPv0.5.0 ssd 1541710976.905314922 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 249, "value": 0.07885}

:::MLPv0.5.0 ssd 1541710977.214384317 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 250, "value": 0.07916666666666666}

:::MLPv0.5.0 ssd 1541710977.530198097 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 251, "value": 0.07948333333333334}

:::MLPv0.5.0 ssd 1541710977.848041534 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 252, "value": 0.07980000000000001}

:::MLPv0.5.0 ssd 1541710978.159574509 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 253, "value": 0.08011666666666667}

:::MLPv0.5.0 ssd 1541710978.477651358 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 254, "value": 0.08043333333333333}

:::MLPv0.5.0 ssd 1541710978.795021296 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 255, "value": 0.08075}

:::MLPv0.5.0 ssd 1541710979.113069296 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 256, "value": 0.08106666666666668}

:::MLPv0.5.0 ssd 1541710979.426993370 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 257, "value": 0.08138333333333334}

:::MLPv0.5.0 ssd 1541710979.740914106 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 258, "value": 0.0817}

:::MLPv0.5.0 ssd 1541710980.046896458 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 259, "value": 0.08201666666666667}

:::MLPv0.5.0 ssd 1541710980.363143444 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 260, "value": 0.08233333333333334}
Iteration:    260, Loss function: 7.917, Average Loss: 2.470, avg. samples / sec: 3808.63

:::MLPv0.5.0 ssd 1541710980.682699919 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 261, "value": 0.08265}

:::MLPv0.5.0 ssd 1541710980.999043703 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 262, "value": 0.08296666666666666}

:::MLPv0.5.0 ssd 1541710981.327227592 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 263, "value": 0.08328333333333333}

:::MLPv0.5.0 ssd 1541710981.636763573 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 264, "value": 0.08360000000000001}

:::MLPv0.5.0 ssd 1541710981.969425678 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 265, "value": 0.08391666666666667}

:::MLPv0.5.0 ssd 1541710982.276522636 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 266, "value": 0.08423333333333334}

:::MLPv0.5.0 ssd 1541710982.607038498 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 267, "value": 0.08455}

:::MLPv0.5.0 ssd 1541710982.924752474 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 268, "value": 0.08486666666666667}

:::MLPv0.5.0 ssd 1541710983.234094381 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 269, "value": 0.08518333333333333}

:::MLPv0.5.0 ssd 1541710983.560809374 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 270, "value": 0.0855}

:::MLPv0.5.0 ssd 1541710983.890745640 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 271, "value": 0.08581666666666667}

:::MLPv0.5.0 ssd 1541710984.209126949 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 272, "value": 0.08613333333333334}

:::MLPv0.5.0 ssd 1541710984.519756079 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 273, "value": 0.08645}

:::MLPv0.5.0 ssd 1541710984.812277079 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 274, "value": 0.08676666666666667}

:::MLPv0.5.0 ssd 1541710985.121441603 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 275, "value": 0.08708333333333333}

:::MLPv0.5.0 ssd 1541710985.427522898 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 276, "value": 0.0874}

:::MLPv0.5.0 ssd 1541710985.723087311 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 277, "value": 0.08771666666666667}

:::MLPv0.5.0 ssd 1541710986.043018818 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 278, "value": 0.08803333333333334}

:::MLPv0.5.0 ssd 1541710986.360572338 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 279, "value": 0.08835}

:::MLPv0.5.0 ssd 1541710986.677666903 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 280, "value": 0.08866666666666667}
Iteration:    280, Loss function: 7.664, Average Loss: 2.574, avg. samples / sec: 3851.19

:::MLPv0.5.0 ssd 1541710986.970816374 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 281, "value": 0.08898333333333333}

:::MLPv0.5.0 ssd 1541710987.263996840 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 282, "value": 0.0893}

:::MLPv0.5.0 ssd 1541710987.576222420 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 283, "value": 0.08961666666666666}

:::MLPv0.5.0 ssd 1541710987.907843590 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 284, "value": 0.08993333333333334}

:::MLPv0.5.0 ssd 1541710988.215442419 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 285, "value": 0.09025}

:::MLPv0.5.0 ssd 1541710988.526755571 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 286, "value": 0.09056666666666667}

:::MLPv0.5.0 ssd 1541710988.846747160 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 287, "value": 0.09088333333333333}

:::MLPv0.5.0 ssd 1541710989.167701244 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 288, "value": 0.0912}

:::MLPv0.5.0 ssd 1541710989.461690187 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 289, "value": 0.09151666666666666}

:::MLPv0.5.0 ssd 1541710989.772813559 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 290, "value": 0.09183333333333334}

:::MLPv0.5.0 ssd 1541710990.084777355 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 291, "value": 0.09215}

:::MLPv0.5.0 ssd 1541710990.383599043 (train.py:553) train_epoch: 3

:::MLPv0.5.0 ssd 1541710990.414768457 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 292, "value": 0.09246666666666667}

:::MLPv0.5.0 ssd 1541710990.731183052 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 293, "value": 0.09278333333333333}

:::MLPv0.5.0 ssd 1541710991.044036865 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 294, "value": 0.0931}

:::MLPv0.5.0 ssd 1541710991.352915764 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 295, "value": 0.09341666666666666}

:::MLPv0.5.0 ssd 1541710991.662491798 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 296, "value": 0.09373333333333334}

:::MLPv0.5.0 ssd 1541710991.979686737 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 297, "value": 0.09405}

:::MLPv0.5.0 ssd 1541710992.278095007 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 298, "value": 0.09436666666666667}

:::MLPv0.5.0 ssd 1541710992.599840164 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 299, "value": 0.09468333333333333}
Iteration:    300, Loss function: 7.419, Average Loss: 2.672, avg. samples / sec: 3906.81
Iteration:    320, Loss function: 7.166, Average Loss: 2.766, avg. samples / sec: 3918.72
Iteration:    340, Loss function: 7.077, Average Loss: 2.855, avg. samples / sec: 3951.07
Iteration:    360, Loss function: 6.858, Average Loss: 2.934, avg. samples / sec: 3932.88
Iteration:    380, Loss function: 6.909, Average Loss: 3.016, avg. samples / sec: 3944.33

:::MLPv0.5.0 ssd 1541711020.652228832 (train.py:553) train_epoch: 4
Iteration:    400, Loss function: 6.514, Average Loss: 3.089, avg. samples / sec: 3965.98
Iteration:    420, Loss function: 6.359, Average Loss: 3.159, avg. samples / sec: 3961.35
Iteration:    440, Loss function: 6.243, Average Loss: 3.223, avg. samples / sec: 3970.15
Iteration:    460, Loss function: 6.335, Average Loss: 3.287, avg. samples / sec: 3965.91
Iteration:    480, Loss function: 6.431, Average Loss: 3.346, avg. samples / sec: 3990.03

:::MLPv0.5.0 ssd 1541711050.359290123 (train.py:553) train_epoch: 5
Iteration:    500, Loss function: 5.964, Average Loss: 3.401, avg. samples / sec: 3973.82
Iteration:    520, Loss function: 6.115, Average Loss: 3.457, avg. samples / sec: 4019.02
Iteration:    540, Loss function: 5.732, Average Loss: 3.508, avg. samples / sec: 3992.16
Iteration:    560, Loss function: 5.720, Average Loss: 3.554, avg. samples / sec: 4018.27
Iteration:    580, Loss function: 5.914, Average Loss: 3.602, avg. samples / sec: 3970.14

:::MLPv0.5.0 ssd 1541711079.878772259 (train.py:553) train_epoch: 6
Iteration:    600, Loss function: 5.826, Average Loss: 3.643, avg. samples / sec: 4020.38
Iteration:    620, Loss function: 5.486, Average Loss: 3.685, avg. samples / sec: 4003.69
Iteration:    640, Loss function: 5.895, Average Loss: 3.722, avg. samples / sec: 3986.69
Iteration:    660, Loss function: 5.417, Average Loss: 3.757, avg. samples / sec: 4050.58
Iteration:    680, Loss function: 5.571, Average Loss: 3.791, avg. samples / sec: 4021.87

:::MLPv0.5.0 ssd 1541711109.253328562 (train.py:553) train_epoch: 7
Iteration:    700, Loss function: 5.426, Average Loss: 3.822, avg. samples / sec: 4017.49
Iteration:    720, Loss function: 5.294, Average Loss: 3.852, avg. samples / sec: 3983.25
Iteration:    740, Loss function: 5.457, Average Loss: 3.880, avg. samples / sec: 4006.19
Iteration:    760, Loss function: 5.118, Average Loss: 3.907, avg. samples / sec: 4024.76

:::MLPv0.5.0 ssd 1541711139.007851601 (train.py:553) train_epoch: 8
Iteration:    780, Loss function: 5.314, Average Loss: 3.933, avg. samples / sec: 3989.78
Iteration:    800, Loss function: 4.875, Average Loss: 3.956, avg. samples / sec: 4033.12
Iteration:    820, Loss function: 5.132, Average Loss: 3.980, avg. samples / sec: 4065.80
Iteration:    840, Loss function: 4.886, Average Loss: 4.003, avg. samples / sec: 4010.57
Iteration:    860, Loss function: 4.899, Average Loss: 4.023, avg. samples / sec: 4038.16

:::MLPv0.5.0 ssd 1541711168.290088892 (train.py:553) train_epoch: 9
Iteration:    880, Loss function: 5.379, Average Loss: 4.043, avg. samples / sec: 4013.37
Iteration:    900, Loss function: 4.594, Average Loss: 4.064, avg. samples / sec: 4024.38
Iteration:    920, Loss function: 4.912, Average Loss: 4.081, avg. samples / sec: 4069.07
Iteration:    940, Loss function: 4.916, Average Loss: 4.097, avg. samples / sec: 3980.74
Iteration:    960, Loss function: 4.922, Average Loss: 4.115, avg. samples / sec: 4014.07

:::MLPv0.5.0 ssd 1541711197.604706049 (train.py:553) train_epoch: 10
Iteration:    980, Loss function: 4.942, Average Loss: 4.131, avg. samples / sec: 4008.55
Iteration:   1000, Loss function: 4.912, Average Loss: 4.144, avg. samples / sec: 4028.77
Iteration:   1020, Loss function: 4.830, Average Loss: 4.158, avg. samples / sec: 4002.75
Iteration:   1040, Loss function: 4.952, Average Loss: 4.172, avg. samples / sec: 4069.09
Iteration:   1060, Loss function: 4.733, Average Loss: 4.183, avg. samples / sec: 4077.27

:::MLPv0.5.0 ssd 1541711227.119943142 (train.py:553) train_epoch: 11
Iteration:   1080, Loss function: 4.507, Average Loss: 4.195, avg. samples / sec: 4028.50
Iteration:   1100, Loss function: 4.880, Average Loss: 4.206, avg. samples / sec: 4059.92
Iteration:   1120, Loss function: 4.648, Average Loss: 4.216, avg. samples / sec: 4036.97
Iteration:   1140, Loss function: 4.750, Average Loss: 4.225, avg. samples / sec: 4048.17
Iteration:   1160, Loss function: 4.626, Average Loss: 4.236, avg. samples / sec: 4067.03

:::MLPv0.5.0 ssd 1541711256.196695328 (train.py:553) train_epoch: 12
Iteration:   1180, Loss function: 4.579, Average Loss: 4.244, avg. samples / sec: 4066.60
Iteration:   1200, Loss function: 4.676, Average Loss: 4.254, avg. samples / sec: 4072.50
Iteration:   1220, Loss function: 4.753, Average Loss: 4.262, avg. samples / sec: 4049.96
Iteration:   1240, Loss function: 4.821, Average Loss: 4.269, avg. samples / sec: 4046.93
Iteration:   1260, Loss function: 4.456, Average Loss: 4.275, avg. samples / sec: 4052.30

:::MLPv0.5.0 ssd 1541711285.273940325 (train.py:553) train_epoch: 13
Iteration:   1280, Loss function: 4.683, Average Loss: 4.282, avg. samples / sec: 4105.38
Iteration:   1300, Loss function: 4.543, Average Loss: 4.291, avg. samples / sec: 4067.21
Iteration:   1320, Loss function: 4.777, Average Loss: 4.295, avg. samples / sec: 4056.06
Iteration:   1340, Loss function: 4.612, Average Loss: 4.299, avg. samples / sec: 4067.71
Iteration:   1360, Loss function: 4.690, Average Loss: 4.306, avg. samples / sec: 4072.49

:::MLPv0.5.0 ssd 1541711314.242723465 (train.py:553) train_epoch: 14
Iteration:   1380, Loss function: 4.533, Average Loss: 4.311, avg. samples / sec: 4048.14
Iteration:   1400, Loss function: 4.639, Average Loss: 4.314, avg. samples / sec: 4067.10
Iteration:   1420, Loss function: 4.491, Average Loss: 4.317, avg. samples / sec: 4078.22
Iteration:   1440, Loss function: 4.431, Average Loss: 4.321, avg. samples / sec: 4083.54

:::MLPv0.5.0 ssd 1541711343.504801989 (train.py:553) train_epoch: 15
Iteration:   1460, Loss function: 4.614, Average Loss: 4.325, avg. samples / sec: 4078.17
Iteration:   1480, Loss function: 4.163, Average Loss: 4.327, avg. samples / sec: 4065.41
Iteration:   1500, Loss function: 4.441, Average Loss: 4.329, avg. samples / sec: 4040.02
Iteration:   1520, Loss function: 4.324, Average Loss: 4.329, avg. samples / sec: 4038.56
Iteration:   1540, Loss function: 4.463, Average Loss: 4.331, avg. samples / sec: 4071.46

:::MLPv0.5.0 ssd 1541711372.584112883 (train.py:553) train_epoch: 16
Iteration:   1560, Loss function: 4.328, Average Loss: 4.332, avg. samples / sec: 4065.01
Iteration:   1580, Loss function: 4.638, Average Loss: 4.334, avg. samples / sec: 4086.18
Iteration:   1600, Loss function: 4.085, Average Loss: 4.334, avg. samples / sec: 4096.76
Iteration:   1620, Loss function: 4.555, Average Loss: 4.335, avg. samples / sec: 4041.36
Iteration:   1640, Loss function: 4.272, Average Loss: 4.336, avg. samples / sec: 4047.16

:::MLPv0.5.0 ssd 1541711401.594260693 (train.py:553) train_epoch: 17
Iteration:   1660, Loss function: 4.636, Average Loss: 4.338, avg. samples / sec: 4076.19
Iteration:   1680, Loss function: 4.066, Average Loss: 4.337, avg. samples / sec: 4068.31
Iteration:   1700, Loss function: 4.174, Average Loss: 4.336, avg. samples / sec: 4071.89
Iteration:   1720, Loss function: 4.328, Average Loss: 4.336, avg. samples / sec: 4069.98
Iteration:   1740, Loss function: 4.339, Average Loss: 4.335, avg. samples / sec: 4043.79

:::MLPv0.5.0 ssd 1541711430.589874506 (train.py:553) train_epoch: 18
Iteration:   1760, Loss function: 4.120, Average Loss: 4.334, avg. samples / sec: 4062.68
Iteration:   1780, Loss function: 4.262, Average Loss: 4.332, avg. samples / sec: 4060.10
Iteration:   1800, Loss function: 4.347, Average Loss: 4.332, avg. samples / sec: 4067.44
Iteration:   1820, Loss function: 4.144, Average Loss: 4.333, avg. samples / sec: 4102.38
Iteration:   1840, Loss function: 4.279, Average Loss: 4.330, avg. samples / sec: 4061.61

:::MLPv0.5.0 ssd 1541711459.864724874 (train.py:553) train_epoch: 19
Iteration:   1860, Loss function: 4.506, Average Loss: 4.330, avg. samples / sec: 4068.40
Iteration:   1880, Loss function: 4.166, Average Loss: 4.330, avg. samples / sec: 4075.59
Iteration:   1900, Loss function: 4.504, Average Loss: 4.328, avg. samples / sec: 4077.54
Iteration:   1920, Loss function: 4.121, Average Loss: 4.329, avg. samples / sec: 4077.92
Iteration:   1940, Loss function: 4.231, Average Loss: 4.327, avg. samples / sec: 4054.29

:::MLPv0.5.0 ssd 1541711488.831506729 (train.py:553) train_epoch: 20
Iteration:   1960, Loss function: 4.477, Average Loss: 4.326, avg. samples / sec: 4091.18
Iteration:   1980, Loss function: 4.304, Average Loss: 4.325, avg. samples / sec: 4094.98
Iteration:   2000, Loss function: 4.267, Average Loss: 4.323, avg. samples / sec: 4073.15
Iteration:   2020, Loss function: 4.124, Average Loss: 4.320, avg. samples / sec: 4051.58
Iteration:   2040, Loss function: 3.988, Average Loss: 4.317, avg. samples / sec: 4069.11

:::MLPv0.5.0 ssd 1541711517.791677952 (train.py:553) train_epoch: 21
Iteration:   2060, Loss function: 4.445, Average Loss: 4.315, avg. samples / sec: 4071.71
Iteration:   2080, Loss function: 4.378, Average Loss: 4.312, avg. samples / sec: 4127.23
Iteration:   2100, Loss function: 4.141, Average Loss: 4.310, avg. samples / sec: 4082.72
Iteration:   2120, Loss function: 4.122, Average Loss: 4.308, avg. samples / sec: 4030.51
Iteration:   2140, Loss function: 4.367, Average Loss: 4.306, avg. samples / sec: 4087.10

:::MLPv0.5.0 ssd 1541711546.990356445 (train.py:553) train_epoch: 22
Iteration:   2160, Loss function: 4.265, Average Loss: 4.304, avg. samples / sec: 4083.20
Iteration:   2180, Loss function: 4.106, Average Loss: 4.303, avg. samples / sec: 4078.80
Iteration:   2200, Loss function: 3.895, Average Loss: 4.299, avg. samples / sec: 4099.93
Iteration:   2220, Loss function: 4.100, Average Loss: 4.295, avg. samples / sec: 4101.68

:::MLPv0.5.0 ssd 1541711575.846669197 (train.py:553) train_epoch: 23
Iteration:   2240, Loss function: 4.223, Average Loss: 4.294, avg. samples / sec: 4079.78
Iteration:   2260, Loss function: 3.852, Average Loss: 4.289, avg. samples / sec: 4098.37
Iteration:   2280, Loss function: 4.135, Average Loss: 4.285, avg. samples / sec: 4084.56
Iteration:   2300, Loss function: 4.308, Average Loss: 4.281, avg. samples / sec: 4086.88
Iteration:   2320, Loss function: 4.020, Average Loss: 4.276, avg. samples / sec: 4104.80

:::MLPv0.5.0 ssd 1541711604.668352604 (train.py:553) train_epoch: 24
Iteration:   2340, Loss function: 3.808, Average Loss: 4.272, avg. samples / sec: 4077.34
Iteration:   2360, Loss function: 4.341, Average Loss: 4.268, avg. samples / sec: 4075.71
Iteration:   2380, Loss function: 3.970, Average Loss: 4.265, avg. samples / sec: 4101.57
Iteration:   2400, Loss function: 4.141, Average Loss: 4.261, avg. samples / sec: 4098.54
Iteration:   2420, Loss function: 3.895, Average Loss: 4.257, avg. samples / sec: 4081.74

:::MLPv0.5.0 ssd 1541711633.542415857 (train.py:553) train_epoch: 25
Iteration:   2440, Loss function: 4.156, Average Loss: 4.255, avg. samples / sec: 4083.43
Iteration:   2460, Loss function: 4.051, Average Loss: 4.251, avg. samples / sec: 4089.02
Iteration:   2480, Loss function: 4.012, Average Loss: 4.248, avg. samples / sec: 4080.77
Iteration:   2500, Loss function: 4.105, Average Loss: 4.246, avg. samples / sec: 4103.21
Iteration:   2520, Loss function: 4.161, Average Loss: 4.244, avg. samples / sec: 4079.72

:::MLPv0.5.0 ssd 1541711662.694600105 (train.py:553) train_epoch: 26
Iteration:   2540, Loss function: 4.090, Average Loss: 4.241, avg. samples / sec: 4088.24
Iteration:   2560, Loss function: 4.130, Average Loss: 4.238, avg. samples / sec: 4103.86
Iteration:   2580, Loss function: 4.013, Average Loss: 4.234, avg. samples / sec: 4103.13
Iteration:   2600, Loss function: 3.908, Average Loss: 4.231, avg. samples / sec: 4068.21
Iteration:   2620, Loss function: 4.125, Average Loss: 4.227, avg. samples / sec: 4077.61

:::MLPv0.5.0 ssd 1541711691.559447527 (train.py:553) train_epoch: 27
Iteration:   2640, Loss function: 4.028, Average Loss: 4.224, avg. samples / sec: 4071.63
Iteration:   2660, Loss function: 4.167, Average Loss: 4.222, avg. samples / sec: 4105.41
Iteration:   2680, Loss function: 4.019, Average Loss: 4.217, avg. samples / sec: 4092.82
Iteration:   2700, Loss function: 3.806, Average Loss: 4.213, avg. samples / sec: 4048.58
Iteration:   2720, Loss function: 3.943, Average Loss: 4.208, avg. samples / sec: 4079.99

:::MLPv0.5.0 ssd 1541711720.447163820 (train.py:553) train_epoch: 28
Iteration:   2740, Loss function: 3.938, Average Loss: 4.205, avg. samples / sec: 4109.07
Iteration:   2760, Loss function: 4.148, Average Loss: 4.202, avg. samples / sec: 4096.06
Iteration:   2780, Loss function: 4.087, Average Loss: 4.198, avg. samples / sec: 4085.72
Iteration:   2800, Loss function: 3.974, Average Loss: 4.194, avg. samples / sec: 4090.90
Iteration:   2820, Loss function: 3.983, Average Loss: 4.191, avg. samples / sec: 4099.10

:::MLPv0.5.0 ssd 1541711749.254705191 (train.py:553) train_epoch: 29
Iteration:   2840, Loss function: 4.157, Average Loss: 4.186, avg. samples / sec: 4102.42
Iteration:   2860, Loss function: 4.130, Average Loss: 4.184, avg. samples / sec: 4097.16
Iteration:   2880, Loss function: 4.078, Average Loss: 4.182, avg. samples / sec: 4094.06
Iteration:   2900, Loss function: 3.826, Average Loss: 4.178, avg. samples / sec: 4066.30

:::MLPv0.5.0 ssd 1541711778.347838879 (train.py:553) train_epoch: 30
Iteration:   2920, Loss function: 3.850, Average Loss: 4.174, avg. samples / sec: 4112.71
Iteration:   2940, Loss function: 3.817, Average Loss: 4.169, avg. samples / sec: 4090.08
Iteration:   2960, Loss function: 4.048, Average Loss: 4.165, avg. samples / sec: 4071.92
Iteration:   2980, Loss function: 4.173, Average Loss: 4.161, avg. samples / sec: 4077.01
Iteration:   3000, Loss function: 4.166, Average Loss: 4.158, avg. samples / sec: 4081.32

:::MLPv0.5.0 ssd 1541711807.252314091 (train.py:553) train_epoch: 31
Iteration:   3020, Loss function: 3.824, Average Loss: 4.154, avg. samples / sec: 4091.56
Iteration:   3040, Loss function: 3.920, Average Loss: 4.151, avg. samples / sec: 4101.99
Iteration:   3060, Loss function: 4.128, Average Loss: 4.147, avg. samples / sec: 4097.51
Iteration:   3080, Loss function: 3.953, Average Loss: 4.144, avg. samples / sec: 4093.16
Iteration:   3100, Loss function: 3.942, Average Loss: 4.139, avg. samples / sec: 4112.87

:::MLPv0.5.0 ssd 1541711836.007381678 (train.py:553) train_epoch: 32
Iteration:   3120, Loss function: 3.715, Average Loss: 4.136, avg. samples / sec: 4087.97
Iteration:   3140, Loss function: 3.834, Average Loss: 4.132, avg. samples / sec: 4090.34









:::MLPv0.5.0 ssd 1541711849.417543650 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711849.418384075 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711849.419078112 (train.py:220) eval_start: 32
Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3No object detected in idx: 59
Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Predicting Ended, total time: 35.79 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(275924, 7)
(275924, 7)
Loading and preparing results...
(275924, 7)
Converting ndarray to lists...
0/275924
0/275924
Loading and preparing results...
0/275924
(275924, 7)
Converting ndarray to lists...
0/275924
Converting ndarray to lists...
(275924, 7)
(275924, 7)
0/275924
0/275924
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(275924, 7)
0/275924
(275924, 7)
0/275924
DONE (t=1.82s)
creating index...
DONE (t=2.09s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.11s)
creating index...
DONE (t=2.11s)
creating index...
DONE (t=2.12s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.45s).
Accumulating evaluation results...
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.136
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.266
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.159
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.231
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.242
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.263
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.371
Current AP: 0.13643 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711892.074707985 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711892.075558901 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.13642682667197048}

:::MLPv0.5.0 ssd 1541711892.076351881 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.13642682667197048}

:::MLPv0.5.0 ssd 1541711892.077145576 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711892.077939034 (train.py:338) eval_stop: 32
Iteration:   3160, Loss function: 3.647, Average Loss: 4.128, avg. samples / sec: 492.42
Iteration:   3180, Loss function: 4.040, Average Loss: 4.123, avg. samples / sec: 4120.32
Iteration:   3200, Loss function: 4.162, Average Loss: 4.119, avg. samples / sec: 4128.83

:::MLPv0.5.0 ssd 1541711908.467868090 (train.py:553) train_epoch: 33
Iteration:   3220, Loss function: 3.852, Average Loss: 4.117, avg. samples / sec: 4131.51
Iteration:   3240, Loss function: 4.136, Average Loss: 4.112, avg. samples / sec: 4088.74
Iteration:   3260, Loss function: 4.080, Average Loss: 4.109, avg. samples / sec: 4072.71
Iteration:   3280, Loss function: 3.956, Average Loss: 4.106, avg. samples / sec: 4105.74
Iteration:   3300, Loss function: 3.943, Average Loss: 4.102, avg. samples / sec: 4092.53

:::MLPv0.5.0 ssd 1541711937.264622450 (train.py:553) train_epoch: 34
Iteration:   3320, Loss function: 3.725, Average Loss: 4.099, avg. samples / sec: 4115.93
Iteration:   3340, Loss function: 3.943, Average Loss: 4.095, avg. samples / sec: 4113.25
Iteration:   3360, Loss function: 3.921, Average Loss: 4.093, avg. samples / sec: 4102.30
Iteration:   3380, Loss function: 3.952, Average Loss: 4.090, avg. samples / sec: 4111.53
Iteration:   3400, Loss function: 3.853, Average Loss: 4.086, avg. samples / sec: 4106.92

:::MLPv0.5.0 ssd 1541711965.978266478 (train.py:553) train_epoch: 35
Iteration:   3420, Loss function: 3.908, Average Loss: 4.081, avg. samples / sec: 4104.33
Iteration:   3440, Loss function: 3.863, Average Loss: 4.078, avg. samples / sec: 4060.08
Iteration:   3460, Loss function: 3.853, Average Loss: 4.074, avg. samples / sec: 4097.51
Iteration:   3480, Loss function: 4.050, Average Loss: 4.071, avg. samples / sec: 4094.96
Iteration:   3500, Loss function: 3.995, Average Loss: 4.067, avg. samples / sec: 4092.03

:::MLPv0.5.0 ssd 1541711994.808007002 (train.py:553) train_epoch: 36
Iteration:   3520, Loss function: 4.013, Average Loss: 4.065, avg. samples / sec: 4099.28
Iteration:   3540, Loss function: 4.003, Average Loss: 4.063, avg. samples / sec: 4084.70
Iteration:   3560, Loss function: 3.902, Average Loss: 4.059, avg. samples / sec: 4091.37
Iteration:   3580, Loss function: 3.793, Average Loss: 4.055, avg. samples / sec: 4089.72

:::MLPv0.5.0 ssd 1541712023.942779541 (train.py:553) train_epoch: 37
Iteration:   3600, Loss function: 3.982, Average Loss: 4.052, avg. samples / sec: 4089.85
Iteration:   3620, Loss function: 3.789, Average Loss: 4.048, avg. samples / sec: 4122.45
Iteration:   3640, Loss function: 3.935, Average Loss: 4.045, avg. samples / sec: 4096.53
Iteration:   3660, Loss function: 3.916, Average Loss: 4.040, avg. samples / sec: 4083.56
Iteration:   3680, Loss function: 4.006, Average Loss: 4.035, avg. samples / sec: 4103.36

:::MLPv0.5.0 ssd 1541712052.691201210 (train.py:553) train_epoch: 38
Iteration:   3700, Loss function: 3.859, Average Loss: 4.032, avg. samples / sec: 4113.05
Iteration:   3720, Loss function: 4.038, Average Loss: 4.031, avg. samples / sec: 4110.84
Iteration:   3740, Loss function: 3.797, Average Loss: 4.028, avg. samples / sec: 4109.84
Iteration:   3760, Loss function: 3.974, Average Loss: 4.024, avg. samples / sec: 4088.02
Iteration:   3780, Loss function: 3.554, Average Loss: 4.021, avg. samples / sec: 4093.50

:::MLPv0.5.0 ssd 1541712081.465231180 (train.py:553) train_epoch: 39
Iteration:   3800, Loss function: 3.621, Average Loss: 4.016, avg. samples / sec: 4099.30
Iteration:   3820, Loss function: 3.569, Average Loss: 4.013, avg. samples / sec: 4128.98
Iteration:   3840, Loss function: 3.595, Average Loss: 4.008, avg. samples / sec: 4112.77
Iteration:   3860, Loss function: 3.662, Average Loss: 4.004, avg. samples / sec: 4121.86
Iteration:   3880, Loss function: 4.207, Average Loss: 4.000, avg. samples / sec: 4075.83

:::MLPv0.5.0 ssd 1541712110.471930265 (train.py:553) train_epoch: 40
Iteration:   3900, Loss function: 3.758, Average Loss: 3.998, avg. samples / sec: 4091.91
Iteration:   3920, Loss function: 3.828, Average Loss: 3.996, avg. samples / sec: 4117.37
Iteration:   3940, Loss function: 3.848, Average Loss: 3.994, avg. samples / sec: 4110.36
Iteration:   3960, Loss function: 3.570, Average Loss: 3.991, avg. samples / sec: 4121.44
Iteration:   3980, Loss function: 3.870, Average Loss: 3.988, avg. samples / sec: 4111.28

:::MLPv0.5.0 ssd 1541712139.146041870 (train.py:553) train_epoch: 41
Iteration:   4000, Loss function: 4.279, Average Loss: 3.985, avg. samples / sec: 4119.66
Iteration:   4020, Loss function: 3.683, Average Loss: 3.980, avg. samples / sec: 4133.74
Iteration:   4040, Loss function: 3.905, Average Loss: 3.977, avg. samples / sec: 4112.76
Iteration:   4060, Loss function: 3.825, Average Loss: 3.974, avg. samples / sec: 4105.68
Iteration:   4080, Loss function: 3.880, Average Loss: 3.971, avg. samples / sec: 4126.07

:::MLPv0.5.0 ssd 1541712167.786662102 (train.py:553) train_epoch: 42
Iteration:   4100, Loss function: 3.778, Average Loss: 3.967, avg. samples / sec: 4100.53
Iteration:   4120, Loss function: 3.650, Average Loss: 3.964, avg. samples / sec: 4085.78
Iteration:   4140, Loss function: 3.669, Average Loss: 3.962, avg. samples / sec: 4095.53
Iteration:   4160, Loss function: 3.939, Average Loss: 3.962, avg. samples / sec: 4103.77
Iteration:   4180, Loss function: 3.831, Average Loss: 3.959, avg. samples / sec: 4108.89

:::MLPv0.5.0 ssd 1541712196.561756849 (train.py:553) train_epoch: 43
Iteration:   4200, Loss function: 3.980, Average Loss: 3.956, avg. samples / sec: 4104.57
lr decay step #1

:::MLPv0.5.0 ssd 1541712204.582238913 (train.py:578) opt_learning_rate: 0.009500000000000001









:::MLPv0.5.0 ssd 1541712204.865063667 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541712204.865921736 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541712204.866681337 (train.py:220) eval_start: 43
Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Predicting Ended, total time: 36.86 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(378198, 7)
(378198, 7)
Loading and preparing results...
(378198, 7)
Loading and preparing results...
Loading and preparing results...
0/378198
0/378198
0/378198
Converting ndarray to lists...
(378198, 7)
Converting ndarray to lists...
(378198, 7)
Converting ndarray to lists...
0/378198
(378198, 7)
0/378198
0/378198
Converting ndarray to lists...
(378198, 7)
0/378198
Loading and preparing results...
Converting ndarray to lists...
(378198, 7)
0/378198
DONE (t=2.19s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.31s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=4.24s).
Accumulating evaluation results...
DONE (t=1.30s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.275
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.139
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.155
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.161
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.250
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.371
Current AP: 0.14483 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541712249.969499588 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541712249.970439196 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.14483448720498832}

:::MLPv0.5.0 ssd 1541712249.971199751 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.14483448720498832}

:::MLPv0.5.0 ssd 1541712249.971920729 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541712249.972745657 (train.py:338) eval_stop: 43
Iteration:   4220, Loss function: 3.547, Average Loss: 3.953, avg. samples / sec: 472.56
Iteration:   4240, Loss function: 3.143, Average Loss: 3.943, avg. samples / sec: 4136.09
Iteration:   4260, Loss function: 3.266, Average Loss: 3.932, avg. samples / sec: 4155.99
Iteration:   4280, Loss function: 3.265, Average Loss: 3.918, avg. samples / sec: 4128.16

:::MLPv0.5.0 ssd 1541712270.980015278 (train.py:553) train_epoch: 44
Iteration:   4300, Loss function: 3.214, Average Loss: 3.905, avg. samples / sec: 4119.09
Iteration:   4320, Loss function: 3.287, Average Loss: 3.893, avg. samples / sec: 4131.57
Iteration:   4340, Loss function: 3.102, Average Loss: 3.881, avg. samples / sec: 4110.82
Iteration:   4360, Loss function: 3.289, Average Loss: 3.868, avg. samples / sec: 4116.87

:::MLPv0.5.0 ssd 1541712299.612388372 (train.py:553) train_epoch: 45
Iteration:   4380, Loss function: 3.333, Average Loss: 3.854, avg. samples / sec: 4122.44
Iteration:   4400, Loss function: 3.157, Average Loss: 3.840, avg. samples / sec: 4114.03
Iteration:   4420, Loss function: 3.258, Average Loss: 3.828, avg. samples / sec: 4119.10
Iteration:   4440, Loss function: 2.885, Average Loss: 3.815, avg. samples / sec: 4115.69
Iteration:   4460, Loss function: 3.060, Average Loss: 3.803, avg. samples / sec: 4112.58

:::MLPv0.5.0 ssd 1541712328.294100523 (train.py:553) train_epoch: 46
Iteration:   4480, Loss function: 3.159, Average Loss: 3.790, avg. samples / sec: 4097.80
Iteration:   4500, Loss function: 3.226, Average Loss: 3.777, avg. samples / sec: 4115.95
Iteration:   4520, Loss function: 3.138, Average Loss: 3.765, avg. samples / sec: 4096.00
Iteration:   4540, Loss function: 3.228, Average Loss: 3.753, avg. samples / sec: 4108.26
Iteration:   4560, Loss function: 3.200, Average Loss: 3.740, avg. samples / sec: 4112.39

:::MLPv0.5.0 ssd 1541712357.022835970 (train.py:553) train_epoch: 47
Iteration:   4580, Loss function: 3.271, Average Loss: 3.728, avg. samples / sec: 4082.34
Iteration:   4600, Loss function: 3.255, Average Loss: 3.716, avg. samples / sec: 4117.15
Iteration:   4620, Loss function: 3.123, Average Loss: 3.704, avg. samples / sec: 4110.32
Iteration:   4640, Loss function: 2.984, Average Loss: 3.691, avg. samples / sec: 4101.26
Iteration:   4660, Loss function: 3.176, Average Loss: 3.679, avg. samples / sec: 4105.47

:::MLPv0.5.0 ssd 1541712386.067854881 (train.py:553) train_epoch: 48
Iteration:   4680, Loss function: 3.103, Average Loss: 3.667, avg. samples / sec: 4105.02
Iteration:   4700, Loss function: 3.128, Average Loss: 3.655, avg. samples / sec: 4090.49
Iteration:   4720, Loss function: 2.985, Average Loss: 3.644, avg. samples / sec: 4129.26









:::MLPv0.5.0 ssd 1541712405.869934320 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541712405.870798826 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541712405.871883631 (train.py:220) eval_start: 48
Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Predicting Ended, total time: 33.31 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(297088, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(297088, 7)
Converting ndarray to lists...
Loading and preparing results...
0/297088
Converting ndarray to lists...
(297088, 7)
(297088, 7)
0/297088
(297088, 7)
0/297088
0/297088
0/297088
Converting ndarray to lists...
(297088, 7)
Loading and preparing results...
0/297088
Converting ndarray to lists...
Converting ndarray to lists...
(297088, 7)
(297088, 7)
0/297088
0/297088
DONE (t=1.64s)
creating index...
index created!
DONE (t=1.92s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=2.02s)
creating index...
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.20s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.24s)
creating index...
index created!
index created!
index created!
DONE (t=3.49s).
Accumulating evaluation results...
DONE (t=1.11s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.374
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.058
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.307
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.096
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.342
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.498
Current AP: 0.21690 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541712446.194880724 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541712446.195794344 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.21690499125557547}

:::MLPv0.5.0 ssd 1541712446.196566343 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.21690499125557547}

:::MLPv0.5.0 ssd 1541712446.197300673 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541712446.198019743 (train.py:338) eval_stop: 48

:::MLPv0.5.0 ssd 1541712447.837957144 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1541712447.838686466 (train.py:707) run_final
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2018-11-08 09:27:33 PM
RESULT,OBJECT_DETECTION,,1619,nvidia,2018-11-08 09:00:34 PM
