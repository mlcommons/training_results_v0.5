Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3

:::MLPv0.5.0 ssd 1541710836.494753838 (<string>:1) run_clear_caches
Launching on node sc-sdgx-370
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-370
+ srun -N 1 -n 1 -w sc-sdgx-370 docker exec -e DGXSYSTEM=DGX1 -e MULTI_NODE= -e SLURM_JOB_ID=155386 -e SLURM_NTASKS_PER_NODE=8 cont_155386 ./run_and_time.sh
Run vars: id 155386 gpus 8 mparams 
STARTING TIMING RUN AT 2018-11-08 09:00:36 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 152 --warmup 300 --nhwc --pad-input
2 Using seed = 3529956342
1 Using seed = 3529956341
3 Using seed = 3529956343
4 Using seed = 3529956344
5 Using seed = 3529956345
6 Using seed = 3529956346
7 Using seed = 3529956347
0 Using seed = 3529956340

:::MLPv0.5.0 ssd 1541710847.742904902 (train.py:371) run_start

:::MLPv0.5.0 ssd 1541710847.743773460 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1541710847.744503498 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1541710847.745245934 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1541710847.746002674 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1541710847.822362900 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1541710847.823846817 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1541710847.825268269 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1541710847.826640368 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1541710847.828035355 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1541710847.829400301 (train.py:382) input_size: 300
loading annotations into memory...
Done (t=0.52s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.54s)
creating index...
index created!
index created!
index created!
index created!
index created!
Done (t=0.55s)
creating index...
index created!
index created!
index created!
time_check a: 1541710848.851332903
time_check b: 1541710872.468387365

:::MLPv0.5.0 ssd 1541710874.454423904 (train.py:413) input_order

:::MLPv0.5.0 ssd 1541710874.456895590 (train.py:414) input_batch_size: 152

:::MLPv0.5.0 ssd 1541710878.749341488 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710878.750393867 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710878.806821585 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1541710879.239284754 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1541710879.241205931 (train.py:477) opt_learning_rate: 0.095

:::MLPv0.5.0 ssd 1541710879.243005991 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1541710879.244579554 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1541710879.245971680 (train.py:483) opt_learning_rate_warmup_steps: 300

:::MLPv0.5.0 ssd 1541710883.308551311 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710883.311109543 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710883.368677616 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss

:::MLPv0.5.0 ssd 1541710888.029689550 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1541710888.030424118 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1541710888.034084320 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 22.368, Average Loss: 0.022, avg. samples / sec: 10138.04

:::MLPv0.5.0 ssd 1541710891.115722418 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.0003166666666666734}

:::MLPv0.5.0 ssd 1541710891.918804169 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0006333333333333468}

:::MLPv0.5.0 ssd 1541710892.449117422 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0009500000000000064}

:::MLPv0.5.0 ssd 1541710892.970966101 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0012666666666666798}

:::MLPv0.5.0 ssd 1541710893.425634384 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0015833333333333394}

:::MLPv0.5.0 ssd 1541710893.910657883 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0019000000000000128}

:::MLPv0.5.0 ssd 1541710894.383590221 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.0022166666666666723}

:::MLPv0.5.0 ssd 1541710894.831693649 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0025333333333333458}

:::MLPv0.5.0 ssd 1541710895.317072630 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0028500000000000053}

:::MLPv0.5.0 ssd 1541710895.733661413 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.0031666666666666787}

:::MLPv0.5.0 ssd 1541710896.231288910 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0034833333333333383}

:::MLPv0.5.0 ssd 1541710896.693733931 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.0038000000000000117}

:::MLPv0.5.0 ssd 1541710897.175833702 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.004116666666666671}

:::MLPv0.5.0 ssd 1541710897.623317957 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.004433333333333345}

:::MLPv0.5.0 ssd 1541710898.033660889 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.004750000000000004}

:::MLPv0.5.0 ssd 1541710898.484280348 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.005066666666666678}

:::MLPv0.5.0 ssd 1541710898.929013014 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.005383333333333337}

:::MLPv0.5.0 ssd 1541710899.393873215 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.005700000000000011}

:::MLPv0.5.0 ssd 1541710899.837143660 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.00601666666666667}

:::MLPv0.5.0 ssd 1541710900.263834476 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.006333333333333344}
Iteration:     20, Loss function: 20.484, Average Loss: 0.438, avg. samples / sec: 1990.60

:::MLPv0.5.0 ssd 1541710900.731987238 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.006650000000000003}

:::MLPv0.5.0 ssd 1541710901.158649206 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.0069666666666666766}

:::MLPv0.5.0 ssd 1541710901.650083780 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.007283333333333336}

:::MLPv0.5.0 ssd 1541710902.105396986 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.0076000000000000095}

:::MLPv0.5.0 ssd 1541710902.598942757 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.007916666666666669}

:::MLPv0.5.0 ssd 1541710903.041705370 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.008233333333333342}

:::MLPv0.5.0 ssd 1541710903.453099966 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.008550000000000002}

:::MLPv0.5.0 ssd 1541710903.907629728 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.008866666666666675}

:::MLPv0.5.0 ssd 1541710904.321784496 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.009183333333333335}

:::MLPv0.5.0 ssd 1541710904.775336266 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.009500000000000008}

:::MLPv0.5.0 ssd 1541710905.162975073 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.009816666666666668}

:::MLPv0.5.0 ssd 1541710905.555312157 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.010133333333333341}

:::MLPv0.5.0 ssd 1541710905.984756708 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.010450000000000001}

:::MLPv0.5.0 ssd 1541710906.447632074 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.010766666666666674}

:::MLPv0.5.0 ssd 1541710906.868782997 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.011083333333333334}

:::MLPv0.5.0 ssd 1541710907.289341927 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.011400000000000007}

:::MLPv0.5.0 ssd 1541710907.717404366 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.011716666666666667}

:::MLPv0.5.0 ssd 1541710908.146530151 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.01203333333333334}

:::MLPv0.5.0 ssd 1541710908.623251200 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.01235}

:::MLPv0.5.0 ssd 1541710908.992069483 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.012666666666666673}
Iteration:     40, Loss function: 12.730, Average Loss: 0.792, avg. samples / sec: 2781.78

:::MLPv0.5.0 ssd 1541710909.409318924 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.012983333333333333}

:::MLPv0.5.0 ssd 1541710909.861720562 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.013300000000000006}

:::MLPv0.5.0 ssd 1541710910.249789238 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.013616666666666666}

:::MLPv0.5.0 ssd 1541710910.665471077 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.01393333333333334}

:::MLPv0.5.0 ssd 1541710911.039110422 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.014250000000000013}

:::MLPv0.5.0 ssd 1541710911.447891235 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.014566666666666672}

:::MLPv0.5.0 ssd 1541710911.813581228 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.014883333333333346}

:::MLPv0.5.0 ssd 1541710912.267834187 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.015200000000000005}

:::MLPv0.5.0 ssd 1541710912.680294752 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.015516666666666679}

:::MLPv0.5.0 ssd 1541710913.100908279 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.015833333333333338}

:::MLPv0.5.0 ssd 1541710913.478507757 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.01615000000000001}

:::MLPv0.5.0 ssd 1541710913.926746845 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.01646666666666667}

:::MLPv0.5.0 ssd 1541710914.331272602 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.016783333333333345}

:::MLPv0.5.0 ssd 1541710914.704147339 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.017100000000000004}

:::MLPv0.5.0 ssd 1541710915.069271088 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.017416666666666678}

:::MLPv0.5.0 ssd 1541710915.516465902 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.017733333333333337}

:::MLPv0.5.0 ssd 1541710915.921690702 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.01805000000000001}

:::MLPv0.5.0 ssd 1541710916.357290506 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 58, "value": 0.01836666666666667}

:::MLPv0.5.0 ssd 1541710916.729509354 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 59, "value": 0.018683333333333343}

:::MLPv0.5.0 ssd 1541710917.129068375 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 60, "value": 0.019000000000000003}
Iteration:     60, Loss function: 11.352, Average Loss: 1.053, avg. samples / sec: 2994.50

:::MLPv0.5.0 ssd 1541710917.543141842 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 61, "value": 0.019316666666666676}

:::MLPv0.5.0 ssd 1541710917.932422161 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 62, "value": 0.019633333333333336}

:::MLPv0.5.0 ssd 1541710918.308285236 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 63, "value": 0.01995000000000001}

:::MLPv0.5.0 ssd 1541710918.722778797 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 64, "value": 0.02026666666666667}

:::MLPv0.5.0 ssd 1541710919.080487967 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 65, "value": 0.020583333333333342}

:::MLPv0.5.0 ssd 1541710919.491284132 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 66, "value": 0.020900000000000002}

:::MLPv0.5.0 ssd 1541710919.863451242 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 67, "value": 0.021216666666666675}

:::MLPv0.5.0 ssd 1541710920.257845879 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 68, "value": 0.021533333333333335}

:::MLPv0.5.0 ssd 1541710920.611469507 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 69, "value": 0.02185000000000001}

:::MLPv0.5.0 ssd 1541710920.962212801 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 70, "value": 0.022166666666666668}

:::MLPv0.5.0 ssd 1541710921.377171993 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 71, "value": 0.02248333333333334}

:::MLPv0.5.0 ssd 1541710921.778861761 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 72, "value": 0.0228}

:::MLPv0.5.0 ssd 1541710922.177141666 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 73, "value": 0.023116666666666674}

:::MLPv0.5.0 ssd 1541710922.552633524 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 74, "value": 0.023433333333333334}

:::MLPv0.5.0 ssd 1541710922.938733578 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 75, "value": 0.023750000000000007}

:::MLPv0.5.0 ssd 1541710923.325882435 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 76, "value": 0.024066666666666667}

:::MLPv0.5.0 ssd 1541710923.696932793 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 77, "value": 0.02438333333333334}

:::MLPv0.5.0 ssd 1541710924.029490232 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 78, "value": 0.0247}

:::MLPv0.5.0 ssd 1541710924.437290668 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 79, "value": 0.025016666666666673}

:::MLPv0.5.0 ssd 1541710924.817617655 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 80, "value": 0.025333333333333333}
Iteration:     80, Loss function: 9.811, Average Loss: 1.258, avg. samples / sec: 3158.15

:::MLPv0.5.0 ssd 1541710925.168527126 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 81, "value": 0.025650000000000006}

:::MLPv0.5.0 ssd 1541710925.525832891 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 82, "value": 0.025966666666666666}

:::MLPv0.5.0 ssd 1541710925.880033493 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 83, "value": 0.02628333333333334}

:::MLPv0.5.0 ssd 1541710926.275404215 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 84, "value": 0.0266}

:::MLPv0.5.0 ssd 1541710926.630448818 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 85, "value": 0.026916666666666672}

:::MLPv0.5.0 ssd 1541710927.052779913 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 86, "value": 0.02723333333333333}

:::MLPv0.5.0 ssd 1541710927.447825432 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 87, "value": 0.027550000000000005}

:::MLPv0.5.0 ssd 1541710927.819362402 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 88, "value": 0.02786666666666668}

:::MLPv0.5.0 ssd 1541710928.179524183 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 89, "value": 0.028183333333333338}

:::MLPv0.5.0 ssd 1541710928.541034222 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 90, "value": 0.02850000000000001}

:::MLPv0.5.0 ssd 1541710928.879191875 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 91, "value": 0.02881666666666667}

:::MLPv0.5.0 ssd 1541710929.213839769 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 92, "value": 0.029133333333333344}

:::MLPv0.5.0 ssd 1541710929.574274778 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 93, "value": 0.029450000000000004}

:::MLPv0.5.0 ssd 1541710929.889608145 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 94, "value": 0.029766666666666677}

:::MLPv0.5.0 ssd 1541710930.284600496 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 95, "value": 0.030083333333333337}

:::MLPv0.5.0 ssd 1541710930.632477283 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 96, "value": 0.03040000000000001}

:::MLPv0.5.0 ssd 1541710930.981804371 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 97, "value": 0.03071666666666667}

:::MLPv0.5.0 ssd 1541710931.278808117 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1541710931.316502810 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 98, "value": 0.031033333333333343}

:::MLPv0.5.0 ssd 1541710931.668245792 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 99, "value": 0.03135}

:::MLPv0.5.0 ssd 1541710932.007609606 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 100, "value": 0.031666666666666676}
Iteration:    100, Loss function: 8.860, Average Loss: 1.422, avg. samples / sec: 3387.35

:::MLPv0.5.0 ssd 1541710932.336146832 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 101, "value": 0.031983333333333336}

:::MLPv0.5.0 ssd 1541710932.677496672 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 102, "value": 0.03230000000000001}

:::MLPv0.5.0 ssd 1541710933.002325058 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 103, "value": 0.03261666666666667}

:::MLPv0.5.0 ssd 1541710933.338662863 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 104, "value": 0.032933333333333335}

:::MLPv0.5.0 ssd 1541710933.665269852 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 105, "value": 0.03325}

:::MLPv0.5.0 ssd 1541710934.040312290 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 106, "value": 0.03356666666666667}

:::MLPv0.5.0 ssd 1541710934.376401186 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 107, "value": 0.033883333333333335}

:::MLPv0.5.0 ssd 1541710934.725381851 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 108, "value": 0.03420000000000001}

:::MLPv0.5.0 ssd 1541710935.063666344 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 109, "value": 0.034516666666666675}

:::MLPv0.5.0 ssd 1541710935.401238441 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 110, "value": 0.03483333333333334}

:::MLPv0.5.0 ssd 1541710935.775882959 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 111, "value": 0.03515000000000001}

:::MLPv0.5.0 ssd 1541710936.117105722 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 112, "value": 0.035466666666666674}

:::MLPv0.5.0 ssd 1541710936.450512648 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 113, "value": 0.03578333333333334}

:::MLPv0.5.0 ssd 1541710936.773744106 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 114, "value": 0.03610000000000001}

:::MLPv0.5.0 ssd 1541710937.097232103 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 115, "value": 0.036416666666666674}

:::MLPv0.5.0 ssd 1541710937.409413815 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 116, "value": 0.03673333333333334}

:::MLPv0.5.0 ssd 1541710937.741853476 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 117, "value": 0.03705000000000001}

:::MLPv0.5.0 ssd 1541710938.086590290 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 118, "value": 0.03736666666666667}

:::MLPv0.5.0 ssd 1541710938.414450407 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 119, "value": 0.03768333333333334}

:::MLPv0.5.0 ssd 1541710938.804828882 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 120, "value": 0.038000000000000006}
Iteration:    120, Loss function: 8.945, Average Loss: 1.572, avg. samples / sec: 3578.57

:::MLPv0.5.0 ssd 1541710939.167438745 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 121, "value": 0.03831666666666667}

:::MLPv0.5.0 ssd 1541710939.503382444 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 122, "value": 0.03863333333333334}

:::MLPv0.5.0 ssd 1541710939.823219061 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 123, "value": 0.038950000000000005}

:::MLPv0.5.0 ssd 1541710940.146916151 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 124, "value": 0.03926666666666667}

:::MLPv0.5.0 ssd 1541710940.484063148 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 125, "value": 0.03958333333333334}

:::MLPv0.5.0 ssd 1541710940.808348417 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 126, "value": 0.039900000000000005}

:::MLPv0.5.0 ssd 1541710941.144943714 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 127, "value": 0.04021666666666667}

:::MLPv0.5.0 ssd 1541710941.478025913 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 128, "value": 0.04053333333333334}

:::MLPv0.5.0 ssd 1541710941.808706760 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 129, "value": 0.040850000000000004}

:::MLPv0.5.0 ssd 1541710942.149363756 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 130, "value": 0.04116666666666667}

:::MLPv0.5.0 ssd 1541710942.487397671 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 131, "value": 0.04148333333333334}

:::MLPv0.5.0 ssd 1541710942.831370354 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 132, "value": 0.041800000000000004}

:::MLPv0.5.0 ssd 1541710943.160163879 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 133, "value": 0.04211666666666667}

:::MLPv0.5.0 ssd 1541710943.484681129 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 134, "value": 0.04243333333333334}

:::MLPv0.5.0 ssd 1541710943.802741051 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 135, "value": 0.04275}

:::MLPv0.5.0 ssd 1541710944.133299112 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 136, "value": 0.04306666666666667}

:::MLPv0.5.0 ssd 1541710944.457009315 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 137, "value": 0.043383333333333336}

:::MLPv0.5.0 ssd 1541710944.782225370 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 138, "value": 0.0437}

:::MLPv0.5.0 ssd 1541710945.136532545 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 139, "value": 0.04401666666666667}

:::MLPv0.5.0 ssd 1541710945.486036062 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 140, "value": 0.044333333333333336}
Iteration:    140, Loss function: 9.066, Average Loss: 1.716, avg. samples / sec: 3640.44

:::MLPv0.5.0 ssd 1541710945.794298410 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 141, "value": 0.04465}

:::MLPv0.5.0 ssd 1541710946.127134800 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 142, "value": 0.04496666666666667}

:::MLPv0.5.0 ssd 1541710946.454616070 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 143, "value": 0.045283333333333335}

:::MLPv0.5.0 ssd 1541710946.806330204 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 144, "value": 0.0456}

:::MLPv0.5.0 ssd 1541710947.116931438 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 145, "value": 0.04591666666666667}

:::MLPv0.5.0 ssd 1541710947.503408432 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 146, "value": 0.046233333333333335}

:::MLPv0.5.0 ssd 1541710947.811302423 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 147, "value": 0.04655}

:::MLPv0.5.0 ssd 1541710948.170734406 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 148, "value": 0.04686666666666667}

:::MLPv0.5.0 ssd 1541710948.532097578 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 149, "value": 0.047183333333333334}

:::MLPv0.5.0 ssd 1541710948.859735250 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 150, "value": 0.0475}

:::MLPv0.5.0 ssd 1541710949.178847551 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 151, "value": 0.047816666666666674}

:::MLPv0.5.0 ssd 1541710949.524773121 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 152, "value": 0.04813333333333334}

:::MLPv0.5.0 ssd 1541710949.855480433 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 153, "value": 0.04845000000000001}

:::MLPv0.5.0 ssd 1541710950.195030212 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 154, "value": 0.04876666666666667}

:::MLPv0.5.0 ssd 1541710950.516734123 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 155, "value": 0.04908333333333334}

:::MLPv0.5.0 ssd 1541710950.859006882 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 156, "value": 0.049400000000000006}

:::MLPv0.5.0 ssd 1541710951.201619625 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 157, "value": 0.04971666666666667}

:::MLPv0.5.0 ssd 1541710951.565559626 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 158, "value": 0.05003333333333334}

:::MLPv0.5.0 ssd 1541710951.903578043 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 159, "value": 0.050350000000000006}

:::MLPv0.5.0 ssd 1541710952.231386185 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 160, "value": 0.05066666666666667}
Iteration:    160, Loss function: 8.611, Average Loss: 1.853, avg. samples / sec: 3604.91

:::MLPv0.5.0 ssd 1541710952.563031912 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 161, "value": 0.05098333333333334}

:::MLPv0.5.0 ssd 1541710952.912062883 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 162, "value": 0.051300000000000005}

:::MLPv0.5.0 ssd 1541710953.248314142 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 163, "value": 0.05161666666666667}

:::MLPv0.5.0 ssd 1541710953.575472355 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 164, "value": 0.05193333333333334}

:::MLPv0.5.0 ssd 1541710953.895277739 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 165, "value": 0.052250000000000005}

:::MLPv0.5.0 ssd 1541710954.221980333 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 166, "value": 0.05256666666666667}

:::MLPv0.5.0 ssd 1541710954.542929888 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 167, "value": 0.05288333333333334}

:::MLPv0.5.0 ssd 1541710954.855045080 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 168, "value": 0.053200000000000004}

:::MLPv0.5.0 ssd 1541710955.183919191 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 169, "value": 0.05351666666666667}

:::MLPv0.5.0 ssd 1541710955.485208035 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 170, "value": 0.05383333333333334}

:::MLPv0.5.0 ssd 1541710955.815086126 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 171, "value": 0.054150000000000004}

:::MLPv0.5.0 ssd 1541710956.130052567 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 172, "value": 0.05446666666666667}

:::MLPv0.5.0 ssd 1541710956.449901342 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 173, "value": 0.05478333333333334}

:::MLPv0.5.0 ssd 1541710956.771328926 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 174, "value": 0.0551}

:::MLPv0.5.0 ssd 1541710957.103233337 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 175, "value": 0.05541666666666667}

:::MLPv0.5.0 ssd 1541710957.424302340 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 176, "value": 0.055733333333333336}

:::MLPv0.5.0 ssd 1541710957.757530451 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 177, "value": 0.05605}

:::MLPv0.5.0 ssd 1541710958.078132629 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 178, "value": 0.05636666666666667}

:::MLPv0.5.0 ssd 1541710958.411222219 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 179, "value": 0.056683333333333336}

:::MLPv0.5.0 ssd 1541710958.736839533 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 180, "value": 0.057}
Iteration:    180, Loss function: 8.487, Average Loss: 1.985, avg. samples / sec: 3738.57

:::MLPv0.5.0 ssd 1541710959.057871342 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 181, "value": 0.05731666666666667}

:::MLPv0.5.0 ssd 1541710959.390277147 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 182, "value": 0.057633333333333335}

:::MLPv0.5.0 ssd 1541710959.702288389 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 183, "value": 0.05795}

:::MLPv0.5.0 ssd 1541710960.016713619 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 184, "value": 0.05826666666666667}

:::MLPv0.5.0 ssd 1541710960.322856665 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 185, "value": 0.058583333333333334}

:::MLPv0.5.0 ssd 1541710960.633779049 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 186, "value": 0.0589}

:::MLPv0.5.0 ssd 1541710960.951668262 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 187, "value": 0.05921666666666667}

:::MLPv0.5.0 ssd 1541710961.275677204 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 188, "value": 0.059533333333333334}

:::MLPv0.5.0 ssd 1541710961.595330477 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 189, "value": 0.05985}

:::MLPv0.5.0 ssd 1541710961.937093258 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 190, "value": 0.06016666666666667}

:::MLPv0.5.0 ssd 1541710962.251987696 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 191, "value": 0.06048333333333333}

:::MLPv0.5.0 ssd 1541710962.558199883 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 192, "value": 0.0608}

:::MLPv0.5.0 ssd 1541710962.895211458 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 193, "value": 0.061116666666666666}

:::MLPv0.5.0 ssd 1541710963.237954617 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 194, "value": 0.06143333333333334}

:::MLPv0.5.0 ssd 1541710963.517233610 (train.py:553) train_epoch: 2

:::MLPv0.5.0 ssd 1541710963.561187506 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 195, "value": 0.061750000000000006}

:::MLPv0.5.0 ssd 1541710963.871128082 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 196, "value": 0.06206666666666667}

:::MLPv0.5.0 ssd 1541710964.194949389 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 197, "value": 0.06238333333333334}

:::MLPv0.5.0 ssd 1541710964.507582426 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 198, "value": 0.0627}

:::MLPv0.5.0 ssd 1541710964.816790342 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 199, "value": 0.06301666666666667}

:::MLPv0.5.0 ssd 1541710965.128603458 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 200, "value": 0.06333333333333334}
Iteration:    200, Loss function: 8.186, Average Loss: 2.116, avg. samples / sec: 3804.98

:::MLPv0.5.0 ssd 1541710965.445495605 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 201, "value": 0.06365000000000001}

:::MLPv0.5.0 ssd 1541710965.782708645 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 202, "value": 0.06396666666666667}

:::MLPv0.5.0 ssd 1541710966.105257511 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 203, "value": 0.06428333333333333}

:::MLPv0.5.0 ssd 1541710966.433661699 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 204, "value": 0.0646}

:::MLPv0.5.0 ssd 1541710966.757493019 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 205, "value": 0.06491666666666668}

:::MLPv0.5.0 ssd 1541710967.066927433 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 206, "value": 0.06523333333333334}

:::MLPv0.5.0 ssd 1541710967.390369654 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 207, "value": 0.06555}

:::MLPv0.5.0 ssd 1541710967.704743147 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 208, "value": 0.06586666666666667}

:::MLPv0.5.0 ssd 1541710968.019782543 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 209, "value": 0.06618333333333334}

:::MLPv0.5.0 ssd 1541710968.339902639 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 210, "value": 0.0665}

:::MLPv0.5.0 ssd 1541710968.665086508 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 211, "value": 0.06681666666666666}

:::MLPv0.5.0 ssd 1541710968.992157221 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 212, "value": 0.06713333333333334}

:::MLPv0.5.0 ssd 1541710969.288668156 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 213, "value": 0.06745000000000001}

:::MLPv0.5.0 ssd 1541710969.604254723 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 214, "value": 0.06776666666666667}

:::MLPv0.5.0 ssd 1541710969.934848547 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 215, "value": 0.06808333333333333}

:::MLPv0.5.0 ssd 1541710970.248634577 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 216, "value": 0.0684}

:::MLPv0.5.0 ssd 1541710970.588681221 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 217, "value": 0.06871666666666668}

:::MLPv0.5.0 ssd 1541710970.891926050 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 218, "value": 0.06903333333333334}

:::MLPv0.5.0 ssd 1541710971.213508606 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 219, "value": 0.06935}

:::MLPv0.5.0 ssd 1541710971.523153067 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 220, "value": 0.06966666666666667}
Iteration:    220, Loss function: 8.348, Average Loss: 2.237, avg. samples / sec: 3803.35

:::MLPv0.5.0 ssd 1541710971.850239038 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 221, "value": 0.06998333333333334}

:::MLPv0.5.0 ssd 1541710972.150713205 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 222, "value": 0.0703}

:::MLPv0.5.0 ssd 1541710972.505237579 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 223, "value": 0.07061666666666666}

:::MLPv0.5.0 ssd 1541710972.810958624 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 224, "value": 0.07093333333333333}

:::MLPv0.5.0 ssd 1541710973.126654625 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 225, "value": 0.07125000000000001}

:::MLPv0.5.0 ssd 1541710973.480860472 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 226, "value": 0.07156666666666667}

:::MLPv0.5.0 ssd 1541710973.793247223 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 227, "value": 0.07188333333333334}

:::MLPv0.5.0 ssd 1541710974.098787785 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 228, "value": 0.0722}

:::MLPv0.5.0 ssd 1541710974.418646574 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 229, "value": 0.07251666666666667}

:::MLPv0.5.0 ssd 1541710974.730618000 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 230, "value": 0.07283333333333333}

:::MLPv0.5.0 ssd 1541710975.060333729 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 231, "value": 0.07315}

:::MLPv0.5.0 ssd 1541710975.374926329 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 232, "value": 0.07346666666666667}

:::MLPv0.5.0 ssd 1541710975.682644129 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 233, "value": 0.07378333333333334}

:::MLPv0.5.0 ssd 1541710975.993570089 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 234, "value": 0.0741}

:::MLPv0.5.0 ssd 1541710976.301566362 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 235, "value": 0.07441666666666667}

:::MLPv0.5.0 ssd 1541710976.623990059 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 236, "value": 0.07473333333333333}

:::MLPv0.5.0 ssd 1541710976.940241098 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 237, "value": 0.07505}

:::MLPv0.5.0 ssd 1541710977.265983820 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 238, "value": 0.07536666666666667}

:::MLPv0.5.0 ssd 1541710977.588621616 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 239, "value": 0.07568333333333334}

:::MLPv0.5.0 ssd 1541710977.899225712 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 240, "value": 0.076}
Iteration:    240, Loss function: 8.161, Average Loss: 2.359, avg. samples / sec: 3814.02

:::MLPv0.5.0 ssd 1541710978.200018883 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 241, "value": 0.07631666666666667}

:::MLPv0.5.0 ssd 1541710978.538957357 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 242, "value": 0.07663333333333333}

:::MLPv0.5.0 ssd 1541710978.851392984 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 243, "value": 0.07695}

:::MLPv0.5.0 ssd 1541710979.169850588 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 244, "value": 0.07726666666666666}

:::MLPv0.5.0 ssd 1541710979.489290953 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 245, "value": 0.07758333333333334}

:::MLPv0.5.0 ssd 1541710979.827744484 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 246, "value": 0.0779}

:::MLPv0.5.0 ssd 1541710980.148437023 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 247, "value": 0.07821666666666667}

:::MLPv0.5.0 ssd 1541710980.469949007 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 248, "value": 0.07853333333333334}

:::MLPv0.5.0 ssd 1541710980.793199301 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 249, "value": 0.07885}

:::MLPv0.5.0 ssd 1541710981.089806557 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 250, "value": 0.07916666666666666}

:::MLPv0.5.0 ssd 1541710981.388300896 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 251, "value": 0.07948333333333334}

:::MLPv0.5.0 ssd 1541710981.711876392 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 252, "value": 0.07980000000000001}

:::MLPv0.5.0 ssd 1541710982.026407003 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 253, "value": 0.08011666666666667}

:::MLPv0.5.0 ssd 1541710982.341983795 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 254, "value": 0.08043333333333333}

:::MLPv0.5.0 ssd 1541710982.648334026 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 255, "value": 0.08075}

:::MLPv0.5.0 ssd 1541710982.972711325 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 256, "value": 0.08106666666666668}

:::MLPv0.5.0 ssd 1541710983.277629614 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 257, "value": 0.08138333333333334}

:::MLPv0.5.0 ssd 1541710983.600710869 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 258, "value": 0.0817}

:::MLPv0.5.0 ssd 1541710983.921608686 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 259, "value": 0.08201666666666667}

:::MLPv0.5.0 ssd 1541710984.217895269 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 260, "value": 0.08233333333333334}
Iteration:    260, Loss function: 8.137, Average Loss: 2.472, avg. samples / sec: 3847.83

:::MLPv0.5.0 ssd 1541710984.538396597 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 261, "value": 0.08265}

:::MLPv0.5.0 ssd 1541710984.834636211 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 262, "value": 0.08296666666666666}

:::MLPv0.5.0 ssd 1541710985.148762941 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 263, "value": 0.08328333333333333}

:::MLPv0.5.0 ssd 1541710985.468674898 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 264, "value": 0.08360000000000001}

:::MLPv0.5.0 ssd 1541710985.804284573 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 265, "value": 0.08391666666666667}

:::MLPv0.5.0 ssd 1541710986.145263672 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 266, "value": 0.08423333333333334}

:::MLPv0.5.0 ssd 1541710986.460808516 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 267, "value": 0.08455}

:::MLPv0.5.0 ssd 1541710986.771880388 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 268, "value": 0.08486666666666667}

:::MLPv0.5.0 ssd 1541710987.081613064 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 269, "value": 0.08518333333333333}

:::MLPv0.5.0 ssd 1541710987.388945103 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 270, "value": 0.0855}

:::MLPv0.5.0 ssd 1541710987.701255083 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 271, "value": 0.08581666666666667}

:::MLPv0.5.0 ssd 1541710988.007872820 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 272, "value": 0.08613333333333334}

:::MLPv0.5.0 ssd 1541710988.324869156 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 273, "value": 0.08645}

:::MLPv0.5.0 ssd 1541710988.643911362 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 274, "value": 0.08676666666666667}

:::MLPv0.5.0 ssd 1541710988.970325232 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 275, "value": 0.08708333333333333}

:::MLPv0.5.0 ssd 1541710989.289041519 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 276, "value": 0.0874}

:::MLPv0.5.0 ssd 1541710989.605488777 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 277, "value": 0.08771666666666667}

:::MLPv0.5.0 ssd 1541710989.924791813 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 278, "value": 0.08803333333333334}

:::MLPv0.5.0 ssd 1541710990.219601631 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 279, "value": 0.08835}

:::MLPv0.5.0 ssd 1541710990.530250311 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 280, "value": 0.08866666666666667}
Iteration:    280, Loss function: 7.705, Average Loss: 2.577, avg. samples / sec: 3847.05

:::MLPv0.5.0 ssd 1541710990.839275122 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 281, "value": 0.08898333333333333}

:::MLPv0.5.0 ssd 1541710991.144859791 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 282, "value": 0.0893}

:::MLPv0.5.0 ssd 1541710991.459796667 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 283, "value": 0.08961666666666666}

:::MLPv0.5.0 ssd 1541710991.777026892 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 284, "value": 0.08993333333333334}

:::MLPv0.5.0 ssd 1541710992.096380234 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 285, "value": 0.09025}

:::MLPv0.5.0 ssd 1541710992.423108339 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 286, "value": 0.09056666666666667}

:::MLPv0.5.0 ssd 1541710992.728224993 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 287, "value": 0.09088333333333333}

:::MLPv0.5.0 ssd 1541710993.030804634 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 288, "value": 0.0912}

:::MLPv0.5.0 ssd 1541710993.341589689 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 289, "value": 0.09151666666666666}

:::MLPv0.5.0 ssd 1541710993.655040264 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 290, "value": 0.09183333333333334}

:::MLPv0.5.0 ssd 1541710993.968521357 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 291, "value": 0.09215}

:::MLPv0.5.0 ssd 1541710994.247577667 (train.py:553) train_epoch: 3

:::MLPv0.5.0 ssd 1541710994.262873411 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 292, "value": 0.09246666666666667}

:::MLPv0.5.0 ssd 1541710994.570974827 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 293, "value": 0.09278333333333333}

:::MLPv0.5.0 ssd 1541710994.881699800 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 294, "value": 0.0931}

:::MLPv0.5.0 ssd 1541710995.202228069 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 295, "value": 0.09341666666666666}

:::MLPv0.5.0 ssd 1541710995.510011911 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 296, "value": 0.09373333333333334}

:::MLPv0.5.0 ssd 1541710995.817469597 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 297, "value": 0.09405}

:::MLPv0.5.0 ssd 1541710996.118317127 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 298, "value": 0.09436666666666667}

:::MLPv0.5.0 ssd 1541710996.433991909 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 299, "value": 0.09468333333333333}
Iteration:    300, Loss function: 7.478, Average Loss: 2.675, avg. samples / sec: 3928.65
Iteration:    320, Loss function: 7.720, Average Loss: 2.771, avg. samples / sec: 3925.41
Iteration:    340, Loss function: 7.167, Average Loss: 2.862, avg. samples / sec: 3967.37
Iteration:    360, Loss function: 6.932, Average Loss: 2.944, avg. samples / sec: 3935.40
Iteration:    380, Loss function: 7.116, Average Loss: 3.023, avg. samples / sec: 3941.79

:::MLPv0.5.0 ssd 1541711024.464166164 (train.py:553) train_epoch: 4
Iteration:    400, Loss function: 6.794, Average Loss: 3.098, avg. samples / sec: 3991.37
Iteration:    420, Loss function: 6.420, Average Loss: 3.168, avg. samples / sec: 3972.82
Iteration:    440, Loss function: 6.289, Average Loss: 3.234, avg. samples / sec: 3971.80
Iteration:    460, Loss function: 6.375, Average Loss: 3.297, avg. samples / sec: 4005.25
Iteration:    480, Loss function: 6.394, Average Loss: 3.359, avg. samples / sec: 3983.70

:::MLPv0.5.0 ssd 1541711054.040020943 (train.py:553) train_epoch: 5
Iteration:    500, Loss function: 6.082, Average Loss: 3.416, avg. samples / sec: 4015.77
Iteration:    520, Loss function: 6.130, Average Loss: 3.469, avg. samples / sec: 4053.53
Iteration:    540, Loss function: 6.256, Average Loss: 3.525, avg. samples / sec: 3952.64
Iteration:    560, Loss function: 5.674, Average Loss: 3.571, avg. samples / sec: 4001.78
Iteration:    580, Loss function: 5.879, Average Loss: 3.616, avg. samples / sec: 3987.06

:::MLPv0.5.0 ssd 1541711083.493370056 (train.py:553) train_epoch: 6
Iteration:    600, Loss function: 5.851, Average Loss: 3.660, avg. samples / sec: 4018.01
Iteration:    620, Loss function: 5.536, Average Loss: 3.701, avg. samples / sec: 4045.32
Iteration:    640, Loss function: 5.483, Average Loss: 3.739, avg. samples / sec: 4000.46
Iteration:    660, Loss function: 5.745, Average Loss: 3.774, avg. samples / sec: 4003.34
Iteration:    680, Loss function: 5.420, Average Loss: 3.810, avg. samples / sec: 4033.83

:::MLPv0.5.0 ssd 1541711112.838420630 (train.py:553) train_epoch: 7
Iteration:    700, Loss function: 6.173, Average Loss: 3.845, avg. samples / sec: 4050.02
Iteration:    720, Loss function: 5.509, Average Loss: 3.878, avg. samples / sec: 4031.25
Iteration:    740, Loss function: 5.458, Average Loss: 3.906, avg. samples / sec: 4035.36
Iteration:    760, Loss function: 5.461, Average Loss: 3.933, avg. samples / sec: 4064.19

:::MLPv0.5.0 ssd 1541711142.348828554 (train.py:553) train_epoch: 8
Iteration:    780, Loss function: 5.115, Average Loss: 3.959, avg. samples / sec: 4019.93
Iteration:    800, Loss function: 5.225, Average Loss: 3.985, avg. samples / sec: 4029.04
Iteration:    820, Loss function: 4.875, Average Loss: 4.009, avg. samples / sec: 4043.89
Iteration:    840, Loss function: 5.112, Average Loss: 4.031, avg. samples / sec: 4044.45
Iteration:    860, Loss function: 5.037, Average Loss: 4.052, avg. samples / sec: 4029.74

:::MLPv0.5.0 ssd 1541711171.585869074 (train.py:553) train_epoch: 9
Iteration:    880, Loss function: 4.976, Average Loss: 4.075, avg. samples / sec: 4020.72
Iteration:    900, Loss function: 4.637, Average Loss: 4.092, avg. samples / sec: 4046.46
Iteration:    920, Loss function: 5.058, Average Loss: 4.109, avg. samples / sec: 4030.12
Iteration:    940, Loss function: 5.036, Average Loss: 4.129, avg. samples / sec: 4076.27
Iteration:    960, Loss function: 5.186, Average Loss: 4.151, avg. samples / sec: 4064.60

:::MLPv0.5.0 ssd 1541711200.694337368 (train.py:553) train_epoch: 10
Iteration:    980, Loss function: 4.917, Average Loss: 4.168, avg. samples / sec: 4049.28
Iteration:   1000, Loss function: 4.738, Average Loss: 4.181, avg. samples / sec: 4069.57
Iteration:   1020, Loss function: 4.696, Average Loss: 4.194, avg. samples / sec: 4068.36
Iteration:   1040, Loss function: 4.853, Average Loss: 4.207, avg. samples / sec: 4055.91
Iteration:   1060, Loss function: 4.804, Average Loss: 4.220, avg. samples / sec: 4062.74

:::MLPv0.5.0 ssd 1541711230.027462006 (train.py:553) train_epoch: 11
Iteration:   1080, Loss function: 4.680, Average Loss: 4.231, avg. samples / sec: 4069.39
Iteration:   1100, Loss function: 4.795, Average Loss: 4.243, avg. samples / sec: 4050.75
Iteration:   1120, Loss function: 4.844, Average Loss: 4.254, avg. samples / sec: 4084.01
Iteration:   1140, Loss function: 4.714, Average Loss: 4.263, avg. samples / sec: 4005.18
Iteration:   1160, Loss function: 4.750, Average Loss: 4.270, avg. samples / sec: 4029.90

:::MLPv0.5.0 ssd 1541711259.166961908 (train.py:553) train_epoch: 12
Iteration:   1180, Loss function: 4.890, Average Loss: 4.280, avg. samples / sec: 4046.27
Iteration:   1200, Loss function: 4.555, Average Loss: 4.290, avg. samples / sec: 4071.87
Iteration:   1220, Loss function: 4.471, Average Loss: 4.297, avg. samples / sec: 4064.41
Iteration:   1240, Loss function: 4.506, Average Loss: 4.302, avg. samples / sec: 4066.40
Iteration:   1260, Loss function: 4.703, Average Loss: 4.308, avg. samples / sec: 4053.48

:::MLPv0.5.0 ssd 1541711288.222500086 (train.py:553) train_epoch: 13
Iteration:   1280, Loss function: 4.834, Average Loss: 4.314, avg. samples / sec: 4062.28
Iteration:   1300, Loss function: 4.613, Average Loss: 4.320, avg. samples / sec: 4089.97
Iteration:   1320, Loss function: 4.694, Average Loss: 4.327, avg. samples / sec: 4074.82
Iteration:   1340, Loss function: 4.456, Average Loss: 4.332, avg. samples / sec: 4080.50
Iteration:   1360, Loss function: 4.524, Average Loss: 4.336, avg. samples / sec: 4062.61

:::MLPv0.5.0 ssd 1541711317.154487133 (train.py:553) train_epoch: 14
Iteration:   1380, Loss function: 4.370, Average Loss: 4.341, avg. samples / sec: 4066.94
Iteration:   1400, Loss function: 4.354, Average Loss: 4.344, avg. samples / sec: 4108.94
Iteration:   1420, Loss function: 4.516, Average Loss: 4.346, avg. samples / sec: 4084.38
Iteration:   1440, Loss function: 4.599, Average Loss: 4.350, avg. samples / sec: 4082.39

:::MLPv0.5.0 ssd 1541711346.344053030 (train.py:553) train_epoch: 15
Iteration:   1460, Loss function: 4.429, Average Loss: 4.352, avg. samples / sec: 4074.40
Iteration:   1480, Loss function: 4.420, Average Loss: 4.354, avg. samples / sec: 4084.18
Iteration:   1500, Loss function: 4.440, Average Loss: 4.355, avg. samples / sec: 4034.68
Iteration:   1520, Loss function: 4.418, Average Loss: 4.355, avg. samples / sec: 4101.05
Iteration:   1540, Loss function: 4.508, Average Loss: 4.355, avg. samples / sec: 4063.25

:::MLPv0.5.0 ssd 1541711375.309406042 (train.py:553) train_epoch: 16
Iteration:   1560, Loss function: 4.920, Average Loss: 4.360, avg. samples / sec: 4078.33
Iteration:   1580, Loss function: 4.426, Average Loss: 4.363, avg. samples / sec: 4102.45
Iteration:   1600, Loss function: 4.584, Average Loss: 4.363, avg. samples / sec: 4066.18
Iteration:   1620, Loss function: 4.458, Average Loss: 4.363, avg. samples / sec: 4085.21
Iteration:   1640, Loss function: 4.653, Average Loss: 4.364, avg. samples / sec: 4076.25

:::MLPv0.5.0 ssd 1541711404.251319408 (train.py:553) train_epoch: 17
Iteration:   1660, Loss function: 4.194, Average Loss: 4.365, avg. samples / sec: 4066.39
Iteration:   1680, Loss function: 4.425, Average Loss: 4.365, avg. samples / sec: 4079.95
Iteration:   1700, Loss function: 4.400, Average Loss: 4.365, avg. samples / sec: 4070.44
Iteration:   1720, Loss function: 4.662, Average Loss: 4.365, avg. samples / sec: 4084.80
Iteration:   1740, Loss function: 4.242, Average Loss: 4.364, avg. samples / sec: 4065.10

:::MLPv0.5.0 ssd 1541711433.183688641 (train.py:553) train_epoch: 18
Iteration:   1760, Loss function: 4.248, Average Loss: 4.364, avg. samples / sec: 4081.81
Iteration:   1780, Loss function: 4.267, Average Loss: 4.364, avg. samples / sec: 4083.22
Iteration:   1800, Loss function: 4.247, Average Loss: 4.363, avg. samples / sec: 4097.29
Iteration:   1820, Loss function: 4.451, Average Loss: 4.363, avg. samples / sec: 4092.58
Iteration:   1840, Loss function: 4.260, Average Loss: 4.361, avg. samples / sec: 4091.55

:::MLPv0.5.0 ssd 1541711462.300587416 (train.py:553) train_epoch: 19
Iteration:   1860, Loss function: 4.425, Average Loss: 4.359, avg. samples / sec: 4085.84
Iteration:   1880, Loss function: 4.243, Average Loss: 4.358, avg. samples / sec: 4078.31
Iteration:   1900, Loss function: 4.406, Average Loss: 4.357, avg. samples / sec: 4080.56
Iteration:   1920, Loss function: 4.344, Average Loss: 4.355, avg. samples / sec: 4086.78
Iteration:   1940, Loss function: 4.191, Average Loss: 4.352, avg. samples / sec: 4112.07

:::MLPv0.5.0 ssd 1541711491.133222103 (train.py:553) train_epoch: 20
Iteration:   1960, Loss function: 4.373, Average Loss: 4.351, avg. samples / sec: 4090.76
Iteration:   1980, Loss function: 4.070, Average Loss: 4.350, avg. samples / sec: 4107.82
Iteration:   2000, Loss function: 4.352, Average Loss: 4.348, avg. samples / sec: 4046.38
Iteration:   2020, Loss function: 4.184, Average Loss: 4.344, avg. samples / sec: 4094.14
Iteration:   2040, Loss function: 4.424, Average Loss: 4.340, avg. samples / sec: 4102.58

:::MLPv0.5.0 ssd 1541711520.006633997 (train.py:553) train_epoch: 21
Iteration:   2060, Loss function: 4.531, Average Loss: 4.339, avg. samples / sec: 4099.67
Iteration:   2080, Loss function: 4.273, Average Loss: 4.338, avg. samples / sec: 4124.86
Iteration:   2100, Loss function: 3.998, Average Loss: 4.335, avg. samples / sec: 4088.10
Iteration:   2120, Loss function: 4.228, Average Loss: 4.332, avg. samples / sec: 4057.42
Iteration:   2140, Loss function: 4.179, Average Loss: 4.329, avg. samples / sec: 4077.51

:::MLPv0.5.0 ssd 1541711549.143347263 (train.py:553) train_epoch: 22
Iteration:   2160, Loss function: 3.828, Average Loss: 4.325, avg. samples / sec: 4076.82
Iteration:   2180, Loss function: 3.935, Average Loss: 4.322, avg. samples / sec: 4065.19
Iteration:   2200, Loss function: 4.121, Average Loss: 4.319, avg. samples / sec: 4097.08
Iteration:   2220, Loss function: 4.262, Average Loss: 4.317, avg. samples / sec: 4078.39

:::MLPv0.5.0 ssd 1541711578.013175011 (train.py:553) train_epoch: 23
Iteration:   2240, Loss function: 3.866, Average Loss: 4.314, avg. samples / sec: 4109.56
Iteration:   2260, Loss function: 4.247, Average Loss: 4.310, avg. samples / sec: 4133.48
Iteration:   2280, Loss function: 4.127, Average Loss: 4.308, avg. samples / sec: 4131.88
Iteration:   2300, Loss function: 4.321, Average Loss: 4.303, avg. samples / sec: 4091.59
Iteration:   2320, Loss function: 4.184, Average Loss: 4.300, avg. samples / sec: 4074.40

:::MLPv0.5.0 ssd 1541711606.787228584 (train.py:553) train_epoch: 24
Iteration:   2340, Loss function: 4.066, Average Loss: 4.297, avg. samples / sec: 4061.52
Iteration:   2360, Loss function: 4.302, Average Loss: 4.293, avg. samples / sec: 4078.03
Iteration:   2380, Loss function: 3.923, Average Loss: 4.289, avg. samples / sec: 4116.75
Iteration:   2400, Loss function: 4.172, Average Loss: 4.285, avg. samples / sec: 4078.18
Iteration:   2420, Loss function: 4.774, Average Loss: 4.282, avg. samples / sec: 4110.66

:::MLPv0.5.0 ssd 1541711635.600970268 (train.py:553) train_epoch: 25
Iteration:   2440, Loss function: 3.985, Average Loss: 4.280, avg. samples / sec: 4091.66
Iteration:   2460, Loss function: 4.180, Average Loss: 4.276, avg. samples / sec: 4074.40
Iteration:   2480, Loss function: 3.995, Average Loss: 4.271, avg. samples / sec: 4105.35
Iteration:   2500, Loss function: 4.059, Average Loss: 4.266, avg. samples / sec: 4115.60
Iteration:   2520, Loss function: 4.179, Average Loss: 4.263, avg. samples / sec: 4062.09

:::MLPv0.5.0 ssd 1541711664.740514755 (train.py:553) train_epoch: 26
Iteration:   2540, Loss function: 4.238, Average Loss: 4.260, avg. samples / sec: 4107.08
Iteration:   2560, Loss function: 4.100, Average Loss: 4.256, avg. samples / sec: 4097.13
Iteration:   2580, Loss function: 4.200, Average Loss: 4.252, avg. samples / sec: 4090.23
Iteration:   2600, Loss function: 4.112, Average Loss: 4.249, avg. samples / sec: 4087.73
Iteration:   2620, Loss function: 4.096, Average Loss: 4.245, avg. samples / sec: 4083.99

:::MLPv0.5.0 ssd 1541711693.562308073 (train.py:553) train_epoch: 27
Iteration:   2640, Loss function: 4.094, Average Loss: 4.241, avg. samples / sec: 4105.58
Iteration:   2660, Loss function: 3.954, Average Loss: 4.236, avg. samples / sec: 4072.31
Iteration:   2680, Loss function: 4.303, Average Loss: 4.234, avg. samples / sec: 4079.96
Iteration:   2700, Loss function: 3.959, Average Loss: 4.233, avg. samples / sec: 4098.23
Iteration:   2720, Loss function: 4.052, Average Loss: 4.228, avg. samples / sec: 4093.89

:::MLPv0.5.0 ssd 1541711722.400241613 (train.py:553) train_epoch: 28
Iteration:   2740, Loss function: 4.395, Average Loss: 4.226, avg. samples / sec: 4102.23
Iteration:   2760, Loss function: 3.973, Average Loss: 4.223, avg. samples / sec: 4074.80
Iteration:   2780, Loss function: 3.939, Average Loss: 4.219, avg. samples / sec: 4084.66
Iteration:   2800, Loss function: 4.015, Average Loss: 4.214, avg. samples / sec: 4088.11
Iteration:   2820, Loss function: 3.957, Average Loss: 4.209, avg. samples / sec: 4082.53

:::MLPv0.5.0 ssd 1541711751.262484074 (train.py:553) train_epoch: 29
Iteration:   2840, Loss function: 4.001, Average Loss: 4.204, avg. samples / sec: 4111.75
Iteration:   2860, Loss function: 3.764, Average Loss: 4.201, avg. samples / sec: 4108.19
Iteration:   2880, Loss function: 3.968, Average Loss: 4.197, avg. samples / sec: 4082.18
Iteration:   2900, Loss function: 3.941, Average Loss: 4.193, avg. samples / sec: 4087.51

:::MLPv0.5.0 ssd 1541711780.326979637 (train.py:553) train_epoch: 30
Iteration:   2920, Loss function: 3.990, Average Loss: 4.191, avg. samples / sec: 4097.47
Iteration:   2940, Loss function: 4.149, Average Loss: 4.187, avg. samples / sec: 4108.25
Iteration:   2960, Loss function: 3.723, Average Loss: 4.182, avg. samples / sec: 4109.09
Iteration:   2980, Loss function: 3.925, Average Loss: 4.177, avg. samples / sec: 4088.30
Iteration:   3000, Loss function: 3.924, Average Loss: 4.173, avg. samples / sec: 4087.21

:::MLPv0.5.0 ssd 1541711809.132240295 (train.py:553) train_epoch: 31
Iteration:   3020, Loss function: 4.168, Average Loss: 4.169, avg. samples / sec: 4102.18
Iteration:   3040, Loss function: 3.945, Average Loss: 4.166, avg. samples / sec: 4098.72
Iteration:   3060, Loss function: 3.854, Average Loss: 4.162, avg. samples / sec: 4109.65
Iteration:   3080, Loss function: 3.848, Average Loss: 4.157, avg. samples / sec: 4085.64
Iteration:   3100, Loss function: 4.071, Average Loss: 4.153, avg. samples / sec: 4080.94

:::MLPv0.5.0 ssd 1541711837.901349068 (train.py:553) train_epoch: 32
Iteration:   3120, Loss function: 3.926, Average Loss: 4.150, avg. samples / sec: 4124.39
Iteration:   3140, Loss function: 4.179, Average Loss: 4.146, avg. samples / sec: 4093.67









:::MLPv0.5.0 ssd 1541711851.234427214 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711851.235242844 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711851.235990524 (train.py:220) eval_start: 32
Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3No object detected in idx: 153
Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Predicting Ended, total time: 39.77 s
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(370925, 7)
(370925, 7)
0/370925
0/370925
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(370925, 7)
(370925, 7)
(370925, 7)
Loading and preparing results...
(370925, 7)
0/370925
0/370925
0/370925
0/370925
Converting ndarray to lists...
(370925, 7)
0/370925
Loading and preparing results...
Converting ndarray to lists...
(370925, 7)
0/370925
DONE (t=2.54s)
creating index...
DONE (t=2.55s)
creating index...
DONE (t=2.58s)
creating index...
DONE (t=2.58s)
creating index...
DONE (t=2.58s)
creating index...
DONE (t=2.60s)
creating index...
DONE (t=2.61s)
creating index...
DONE (t=2.63s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=4.02s).
Accumulating evaluation results...
DONE (t=1.28s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.269
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.035
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.249
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.065
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.266
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.375
Current AP: 0.14445 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711899.172762156 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711899.173616648 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.14444855055604644}

:::MLPv0.5.0 ssd 1541711899.174415112 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.14444855055604644}

:::MLPv0.5.0 ssd 1541711899.175228834 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711899.176025867 (train.py:338) eval_stop: 32
Iteration:   3160, Loss function: 3.932, Average Loss: 4.142, avg. samples / sec: 444.99
Iteration:   3180, Loss function: 3.814, Average Loss: 4.138, avg. samples / sec: 4171.21
Iteration:   3200, Loss function: 4.179, Average Loss: 4.134, avg. samples / sec: 4142.75

:::MLPv0.5.0 ssd 1541711915.511204004 (train.py:553) train_epoch: 33
Iteration:   3220, Loss function: 3.986, Average Loss: 4.133, avg. samples / sec: 4129.77
Iteration:   3240, Loss function: 3.778, Average Loss: 4.129, avg. samples / sec: 4130.97
Iteration:   3260, Loss function: 3.992, Average Loss: 4.123, avg. samples / sec: 4117.32
Iteration:   3280, Loss function: 4.049, Average Loss: 4.119, avg. samples / sec: 4111.27
Iteration:   3300, Loss function: 3.945, Average Loss: 4.117, avg. samples / sec: 4111.47

:::MLPv0.5.0 ssd 1541711944.131878614 (train.py:553) train_epoch: 34
Iteration:   3320, Loss function: 3.985, Average Loss: 4.114, avg. samples / sec: 4105.78
Iteration:   3340, Loss function: 4.169, Average Loss: 4.111, avg. samples / sec: 4127.60
Iteration:   3360, Loss function: 3.893, Average Loss: 4.107, avg. samples / sec: 4104.16
Iteration:   3380, Loss function: 4.044, Average Loss: 4.104, avg. samples / sec: 4111.96
Iteration:   3400, Loss function: 3.678, Average Loss: 4.099, avg. samples / sec: 4119.55

:::MLPv0.5.0 ssd 1541711972.836083174 (train.py:553) train_epoch: 35
Iteration:   3420, Loss function: 3.987, Average Loss: 4.095, avg. samples / sec: 4096.64
Iteration:   3440, Loss function: 3.850, Average Loss: 4.093, avg. samples / sec: 4121.89
Iteration:   3460, Loss function: 4.090, Average Loss: 4.090, avg. samples / sec: 4106.71
Iteration:   3480, Loss function: 3.787, Average Loss: 4.087, avg. samples / sec: 4105.82
Iteration:   3500, Loss function: 3.949, Average Loss: 4.083, avg. samples / sec: 4101.10

:::MLPv0.5.0 ssd 1541712001.541036129 (train.py:553) train_epoch: 36
Iteration:   3520, Loss function: 3.931, Average Loss: 4.080, avg. samples / sec: 4078.95
Iteration:   3540, Loss function: 3.845, Average Loss: 4.078, avg. samples / sec: 4115.97
Iteration:   3560, Loss function: 3.689, Average Loss: 4.074, avg. samples / sec: 4065.86
Iteration:   3580, Loss function: 3.924, Average Loss: 4.070, avg. samples / sec: 4085.18

:::MLPv0.5.0 ssd 1541712030.675730705 (train.py:553) train_epoch: 37
Iteration:   3600, Loss function: 4.159, Average Loss: 4.066, avg. samples / sec: 4108.16
Iteration:   3620, Loss function: 3.875, Average Loss: 4.063, avg. samples / sec: 4094.21
Iteration:   3640, Loss function: 3.823, Average Loss: 4.060, avg. samples / sec: 4096.90
Iteration:   3660, Loss function: 3.713, Average Loss: 4.056, avg. samples / sec: 4100.97
Iteration:   3680, Loss function: 3.975, Average Loss: 4.052, avg. samples / sec: 4102.78

:::MLPv0.5.0 ssd 1541712059.467435598 (train.py:553) train_epoch: 38
Iteration:   3700, Loss function: 3.907, Average Loss: 4.050, avg. samples / sec: 4089.82
Iteration:   3720, Loss function: 3.606, Average Loss: 4.047, avg. samples / sec: 4113.94
Iteration:   3740, Loss function: 3.890, Average Loss: 4.043, avg. samples / sec: 4103.52
Iteration:   3760, Loss function: 4.065, Average Loss: 4.041, avg. samples / sec: 4131.26
Iteration:   3780, Loss function: 3.904, Average Loss: 4.039, avg. samples / sec: 4094.01

:::MLPv0.5.0 ssd 1541712088.190973282 (train.py:553) train_epoch: 39
Iteration:   3800, Loss function: 3.907, Average Loss: 4.035, avg. samples / sec: 4088.66
Iteration:   3820, Loss function: 3.794, Average Loss: 4.030, avg. samples / sec: 4118.28
Iteration:   3840, Loss function: 3.865, Average Loss: 4.027, avg. samples / sec: 4110.28
Iteration:   3860, Loss function: 3.764, Average Loss: 4.022, avg. samples / sec: 4084.84
Iteration:   3880, Loss function: 3.902, Average Loss: 4.019, avg. samples / sec: 4092.01

:::MLPv0.5.0 ssd 1541712117.287521362 (train.py:553) train_epoch: 40
Iteration:   3900, Loss function: 3.748, Average Loss: 4.015, avg. samples / sec: 4078.81
Iteration:   3920, Loss function: 3.685, Average Loss: 4.011, avg. samples / sec: 4098.73
Iteration:   3940, Loss function: 3.955, Average Loss: 4.007, avg. samples / sec: 4118.74
Iteration:   3960, Loss function: 3.654, Average Loss: 4.003, avg. samples / sec: 4130.17
Iteration:   3980, Loss function: 3.756, Average Loss: 4.000, avg. samples / sec: 4092.68

:::MLPv0.5.0 ssd 1541712145.999882460 (train.py:553) train_epoch: 41
Iteration:   4000, Loss function: 3.953, Average Loss: 3.997, avg. samples / sec: 4103.38
Iteration:   4020, Loss function: 3.621, Average Loss: 3.992, avg. samples / sec: 4125.39
Iteration:   4040, Loss function: 3.847, Average Loss: 3.989, avg. samples / sec: 4099.14
Iteration:   4060, Loss function: 4.305, Average Loss: 3.989, avg. samples / sec: 4116.74
Iteration:   4080, Loss function: 3.963, Average Loss: 3.986, avg. samples / sec: 4104.49

:::MLPv0.5.0 ssd 1541712174.686906099 (train.py:553) train_epoch: 42
Iteration:   4100, Loss function: 4.134, Average Loss: 3.983, avg. samples / sec: 4109.45
Iteration:   4120, Loss function: 3.709, Average Loss: 3.983, avg. samples / sec: 4106.56
Iteration:   4140, Loss function: 4.097, Average Loss: 3.981, avg. samples / sec: 4118.91
Iteration:   4160, Loss function: 4.311, Average Loss: 3.978, avg. samples / sec: 4073.02
Iteration:   4180, Loss function: 3.577, Average Loss: 3.973, avg. samples / sec: 4084.65

:::MLPv0.5.0 ssd 1541712203.467806816 (train.py:553) train_epoch: 43
Iteration:   4200, Loss function: 3.894, Average Loss: 3.972, avg. samples / sec: 4117.06
lr decay step #1

:::MLPv0.5.0 ssd 1541712211.480947256 (train.py:578) opt_learning_rate: 0.009500000000000001









:::MLPv0.5.0 ssd 1541712211.765238523 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541712211.766424894 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541712211.767411232 (train.py:220) eval_start: 43
Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3No object detected in idx: 153
Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Predicting Ended, total time: 32.53 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(275194, 7)
(275194, 7)
(275194, 7)
0/275194
0/275194
0/275194
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(275194, 7)
(275194, 7)
0/275194
0/275194
(275194, 7)
(275194, 7)
0/275194
0/275194
Loading and preparing results...
Converting ndarray to lists...
(275194, 7)
0/275194
DONE (t=1.65s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=3.18s).
Accumulating evaluation results...
DONE (t=1.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.149
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.290
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.141
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.234
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.254
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.071
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.269
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395
Current AP: 0.14935 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541712250.483741283 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541712250.484647274 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.14934995468285622}

:::MLPv0.5.0 ssd 1541712250.485371590 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.14934995468285622}

:::MLPv0.5.0 ssd 1541712250.486056805 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541712250.486815214 (train.py:338) eval_stop: 43
Iteration:   4220, Loss function: 3.561, Average Loss: 3.969, avg. samples / sec: 540.35
Iteration:   4240, Loss function: 3.189, Average Loss: 3.960, avg. samples / sec: 4142.96
Iteration:   4260, Loss function: 3.487, Average Loss: 3.949, avg. samples / sec: 4123.57
Iteration:   4280, Loss function: 3.238, Average Loss: 3.936, avg. samples / sec: 4124.71

:::MLPv0.5.0 ssd 1541712271.458151579 (train.py:553) train_epoch: 44
Iteration:   4300, Loss function: 3.267, Average Loss: 3.923, avg. samples / sec: 4140.36
Iteration:   4320, Loss function: 3.386, Average Loss: 3.910, avg. samples / sec: 4141.01
Iteration:   4340, Loss function: 3.229, Average Loss: 3.897, avg. samples / sec: 4108.98
Iteration:   4360, Loss function: 3.419, Average Loss: 3.884, avg. samples / sec: 4136.86

:::MLPv0.5.0 ssd 1541712300.029297829 (train.py:553) train_epoch: 45
Iteration:   4380, Loss function: 3.295, Average Loss: 3.870, avg. samples / sec: 4114.56
Iteration:   4400, Loss function: 3.517, Average Loss: 3.857, avg. samples / sec: 4100.77
Iteration:   4420, Loss function: 3.161, Average Loss: 3.845, avg. samples / sec: 4121.70
Iteration:   4440, Loss function: 3.154, Average Loss: 3.833, avg. samples / sec: 4117.40
Iteration:   4460, Loss function: 3.095, Average Loss: 3.820, avg. samples / sec: 4116.86

:::MLPv0.5.0 ssd 1541712328.705169916 (train.py:553) train_epoch: 46
Iteration:   4480, Loss function: 2.978, Average Loss: 3.806, avg. samples / sec: 4104.58
Iteration:   4500, Loss function: 2.974, Average Loss: 3.793, avg. samples / sec: 4126.58
Iteration:   4520, Loss function: 3.250, Average Loss: 3.781, avg. samples / sec: 4106.46
Iteration:   4540, Loss function: 3.196, Average Loss: 3.768, avg. samples / sec: 4103.19
Iteration:   4560, Loss function: 3.286, Average Loss: 3.756, avg. samples / sec: 4105.31

:::MLPv0.5.0 ssd 1541712357.409117460 (train.py:553) train_epoch: 47
Iteration:   4580, Loss function: 3.050, Average Loss: 3.743, avg. samples / sec: 4105.71
Iteration:   4600, Loss function: 3.182, Average Loss: 3.731, avg. samples / sec: 4108.02
Iteration:   4620, Loss function: 3.100, Average Loss: 3.719, avg. samples / sec: 4070.29
Iteration:   4640, Loss function: 3.104, Average Loss: 3.705, avg. samples / sec: 4127.84
Iteration:   4660, Loss function: 3.117, Average Loss: 3.693, avg. samples / sec: 4095.81

:::MLPv0.5.0 ssd 1541712386.475791693 (train.py:553) train_epoch: 48
Iteration:   4680, Loss function: 3.073, Average Loss: 3.681, avg. samples / sec: 4100.21
Iteration:   4700, Loss function: 2.994, Average Loss: 3.669, avg. samples / sec: 4116.32
Iteration:   4720, Loss function: 3.113, Average Loss: 3.657, avg. samples / sec: 4113.07









:::MLPv0.5.0 ssd 1541712406.312445879 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541712406.313320637 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541712406.314039469 (train.py:220) eval_start: 48
Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3No object detected in idx: 153
Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Predicting Ended, total time: 33.46 s
Loading and preparing results...
Converting ndarray to lists...
(301534, 7)
0/301534
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(301534, 7)
(301534, 7)
(301534, 7)
(301534, 7)
(301534, 7)
0/301534
0/301534
0/301534
0/301534
0/301534
Converting ndarray to lists...
(301534, 7)
0/301534
Loading and preparing results...
Converting ndarray to lists...
(301534, 7)
0/301534
DONE (t=1.98s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
index created!
index created!
DONE (t=2.15s)
creating index...
DONE (t=2.16s)
creating index...
index created!
index created!
index created!
DONE (t=2.20s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.52s).
Accumulating evaluation results...
DONE (t=1.11s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.372
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.054
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.308
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.348
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497
Current AP: 0.21554 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541712446.807245970 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541712446.808100939 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.21554185454392838}

:::MLPv0.5.0 ssd 1541712446.808913946 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.21554185454392838}

:::MLPv0.5.0 ssd 1541712446.809720278 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541712446.810460329 (train.py:338) eval_stop: 48

:::MLPv0.5.0 ssd 1541712448.427226067 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1541712448.427966833 (train.py:707) run_final
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2018-11-08 09:27:33 PM
RESULT,OBJECT_DETECTION,,1617,nvidia,2018-11-08 09:00:36 PM
