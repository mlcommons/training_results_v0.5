Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3
:::MLPv0.5.0 transformer 1541632775.703111649 (<string>:1) run_clear_caches
Launching on node sc-sdgx-395
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-395
+ srun -N 1 -n 1 -w sc-sdgx-395 docker exec -e DGXSYSTEM=DGX1 -e MULTI_NODE= -e SEED=10785 -e SLURM_JOB_ID=154763 -e SLURM_NTASKS_PER_NODE=8 -e MODE=TRAIN cont_154763 ./run_and_time.sh
Run vars: id 154763 gpus 8 mparams 
+ SEED=10785
+ MAX_TOKENS=5120
+ DATASET_DIR=/data
+ MODE=TRAIN
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START=1541632775
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2018-11-07 11:19:35 PM
++ START_FMT='2018-11-07 11:19:35 PM'
++ echo 'STARTING TIMING RUN AT 2018-11-07 11:19:35 PM'
++ python -m torch.distributed.launch --nproc_per_node 8 train.py /data --seed 10785 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 3000 --lr 6.4e-4 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 5120 --max-epoch 12 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --distributed-init-method env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 3
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 2
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 4
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 7
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 0
| distributed init (rank 0): env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 1
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 5
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 8, RANK: 6
| distributed init done!
| initialized host sc-sdgx-395 as rank 0 and device id 0
:::MLPv0.5.0 transformer 1541632779.062086582 (/workspace/translation/train.py:34) run_clear_caches
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLPv0.5.0 transformer 1541632786.706996679 (/workspace/translation/train.py:40) run_start
Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', data='/data', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=0, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=True, fuse_dropout_add=False, fuse_relu_dropout=False, gen_subset='test', ignore_case=True, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source='True', left_pad_target='False', lenpen=1, local_rank=0, log_format=None, log_interval=1000, log_translations=False, lr=[0.00064], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=12, max_len_a=0, max_len_b=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=5120, max_update=0, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_progress_bar=False, no_save=True, no_token_positional_embeddings=False, num_shards=1, online_eval=False, optimizer='adam', pad_sequence=1, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=None, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe=None, replace_unk=None, restore_file='checkpoint_last.pt', sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='checkpoints', save_interval=1, save_interval_updates=0, score_reference=False, seed=10785, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_bleu=25.0, target_lang=None, task='translation', train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=3000, weight_decay=0.0)
:::MLPv0.5.0 transformer 1541632786.708243608 (/workspace/translation/train.py:44) opt_name: "adam"
:::MLPv0.5.0 transformer 1541632786.708765745 (/workspace/translation/train.py:45) opt_learning_rate: [0.00064]
:::MLPv0.5.0 transformer 1541632786.709270477 (/workspace/translation/train.py:46) opt_hp_Adam_beta1: 0.9
:::MLPv0.5.0 transformer 1541632786.709645748 (/workspace/translation/train.py:47) opt_hp_Adam_beta2: 0.997
:::MLPv0.5.0 transformer 1541632786.709999561 (/workspace/translation/train.py:48) opt_hp_Adam_epsilon: 1e-09
:::MLPv0.5.0 transformer 1541632786.710928917 (/workspace/translation/train.py:53) run_set_random_seed: 10785
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
:::MLPv0.5.0 transformer 1541632786.792746067 (/workspace/translation/train.py:61) model_hp_sequence_beam_search: {"alpha": 1, "beam_size": 4, "extra_decode_length": 200, "vocab_size": 33712}
| /data train 4575616 examples
| Sentences are being padded to multiples of: 1
| /data valid 3000 examples
| Sentences are being padded to multiples of: 1
:::MLPv0.5.0 transformer 1541632788.323609114 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541632788.331259251 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541632788.457584381 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541632788.468373299 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541632788.478071451 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541632788.485764027 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541632788.514397860 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541632788.524154902 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541632788.534298182 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541632788.541146278 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541632788.547132254 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541632788.552365065 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541632788.613842726 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541632788.616587639 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541632788.629216194 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541632788.632349014 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541632791.160381794 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.161130428 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.173067093 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.194608927 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.197017670 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.212225199 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.216978550 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.228662968 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.230233908 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.282499790 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.283705711 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.285927057 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.290152550 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.306825399 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.315522909 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.316671371 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.321443081 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.327095509 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.327238083 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.331577301 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.336924076 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.354117632 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.356022120 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.414542198 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.419050217 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.451088667 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.453636885 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.455997944 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.474676132 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.477469921 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.481015205 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.497189522 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.500788927 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.523414373 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.536531687 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.544452906 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.545185089 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.551700592 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.561659098 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.561908960 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.562864065 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.564002514 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.565131664 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.566057444 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.566462755 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.567694187 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.571544409 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.587180614 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.587173700 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.613802671 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.622483015 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.629250526 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.633071899 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.634107828 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.635062456 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.635998964 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.637083292 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.638093710 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.649649620 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.650316954 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.650932789 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.651539803 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.652244329 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.652913332 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.655039549 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.655743837 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.657150269 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.658406496 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.659288168 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.660092592 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.660918951 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.661795616 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.662595272 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.665132284 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.666584015 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.667290211 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.668159485 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.668762922 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.669551134 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.670408964 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.671223879 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.673184872 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.681088686 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.688426018 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.689263821 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.690103769 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.690933228 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.691829205 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.691738129 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.691895247 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.692652225 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.698836327 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.699628115 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.700440645 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.701238871 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.701496363 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.702110529 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.702619791 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.702908516 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.709415436 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.712812662 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.717759132 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.723999977 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.730641603 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.736745834 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.736921787 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.737650633 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.738093853 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.738426685 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.739171267 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.740156651 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.741030455 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.745683432 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.750196934 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.751339197 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.757180452 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.772869110 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.787679195 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.789422035 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.793644428 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.800235271 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.803802252 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.811214924 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.825232744 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.825484037 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.825950146 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.826545000 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.827196121 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.827945471 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.828485727 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.828672886 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.832592487 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.833368540 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.833367348 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.833972216 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.834234953 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.834629297 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.835092545 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.835450411 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.835903883 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.836219549 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.836840391 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.837690592 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.840160608 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.840404034 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.842820644 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.843678951 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.844453812 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.845209837 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.846011400 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.846684456 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.846759319 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.849295855 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.856986284 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.861122608 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.861387968 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.862155437 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.862873554 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.863622665 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.864479780 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.865274191 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.872182846 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.873041391 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.873852253 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.874668360 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.875356197 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.875533581 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.876222610 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.877666473 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.884104252 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.885458469 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.886421204 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.893154860 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.894171715 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.894942045 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.895718336 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.896431208 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.897322416 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.898095131 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.901715994 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.908361197 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.913027763 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.917153835 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.920052052 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.921475410 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.923562765 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.929200411 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.933856487 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.934719086 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632791.935485363 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.936249971 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632791.937143564 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632791.937952518 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.945945263 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.948295593 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.952581644 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.956852913 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.983683586 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.984427691 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632791.986010313 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.987124681 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.993422031 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632791.993692398 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.015626669 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.016453266 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.017264366 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.018037558 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.018925428 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.019041300 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.019724607 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.019805431 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.020537853 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.021201134 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.021990061 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.022216797 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.022267580 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.022729635 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.024547338 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.024711370 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.025330544 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.026116848 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.026562452 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.026937246 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.027295828 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.027776241 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.028058529 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.028577089 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.028779268 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.029717922 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.030534506 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.031473160 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.034528732 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.039299488 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.040770769 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.054645777 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.055532455 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.055575371 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.055793762 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.056327581 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.056387424 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.056942225 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.056979895 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.057556629 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.057673693 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.058433771 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.058607101 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.059287071 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.067487001 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.068380833 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.069258928 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.071536779 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.075128555 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.077768564 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.088521719 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.089342356 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.090097189 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.090830326 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.091716528 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.092508316 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.097385406 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.102332354 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.102683306 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.103865147 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.106855392 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.111033916 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.111143351 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.117005825 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.129465580 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.130295038 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.131045341 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.131795406 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.132647991 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.133501291 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.138160706 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.140204430 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.144043684 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.146298647 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.167687893 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.175106049 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.177779198 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.180077314 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.180636883 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.186463118 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.197768450 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.198635578 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.199506283 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.200307131 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.201191902 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.202014208 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.204479218 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.206863880 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.207683563 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.208443165 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.209175587 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.209963560 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.210677862 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.211662531 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.212321758 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.213031054 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.213789225 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.213866472 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.214682341 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.214697123 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.215445518 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.218511581 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.219279289 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.219550133 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.219973087 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.220560312 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.221367836 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.221480370 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.222133398 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.226548433 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.232287884 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.236912012 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.237761259 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.238609076 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.239409924 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.240303278 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.241140366 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.247321844 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.248073578 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.248270988 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.248848677 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.249595642 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.250438213 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.250567675 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.251251221 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.251308918 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.257805109 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.261294127 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.263725996 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.269469976 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.280800581 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.281568527 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.282296896 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.283017874 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.283897638 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.284663439 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.285103083 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.286893606 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.291056633 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.294118166 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.294893026 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.298697710 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.302992582 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.308555365 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.321557760 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.323475599 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.324365854 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.325251341 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.325977564 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.326863766 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.327670097 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.332576990 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.338151932 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.338208675 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.351398945 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.358404875 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.370312691 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.372667551 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.374510050 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.378448725 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.385519266 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.397210598 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.398111343 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.398984432 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.399809361 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.400705099 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.401536465 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.406812191 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.408663988 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.409581900 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.410444736 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.411198378 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.412019014 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.412754536 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.413123369 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.413544893 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.423148870 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.425777912 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.426574707 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.427357197 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.428099155 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.428997278 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.429765224 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.435424805 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.436312914 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.437171936 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.437988758 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.438894510 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.439719677 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.440005064 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.441120625 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.441302061 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.441840410 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.442590714 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.443315029 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.444218159 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.444992542 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.449469805 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.451375246 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.455115557 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.458069324 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.458845854 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.459565639 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.460327625 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.460550547 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.461191654 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.461912155 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.471962690 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.478775263 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.483547211 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.486473799 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.487297058 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.492680550 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.492962599 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.493736982 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.494482517 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.495215893 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.496100903 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.496427059 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.496888876 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.506992102 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.509624720 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.518119097 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.519489527 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.531769514 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.537447214 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.538260698 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.539065123 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.539842844 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.540730476 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.541586876 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.544666529 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.548875093 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.551705122 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.552166462 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.560731173 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.581135035 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.581950665 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.582311153 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.582721710 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.583428860 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.584052801 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541632792.584939718 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.587532282 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.588019133 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.592334509 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.593198061 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.593980789 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.594755650 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.595455408 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541632792.601136446 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.602632761 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.603974104 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.617480516 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.619981527 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.620659351 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.621300459 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.621893167 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.622420788 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541632792.622495651 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.627175093 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.627916574 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.628299952 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.637826443 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.638848305 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.639876366 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.640890121 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.641788244 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541632792.644892454 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.645973682 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.646673918 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.646992445 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.648000002 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.648908138 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541632792.659886360 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.661267281 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.676343203 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.679833412 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.681885958 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.683182240 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.684374571 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.685556889 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.686622381 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541632792.686878204 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.688377619 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.689657688 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.708250284 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.714342356 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.715897560 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.717164516 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.735735655 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.738898754 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.744677544 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.746458769 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.746647596 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.747863054 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.748134375 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.751764536 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.752943516 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.753789663 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.766621351 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.769762278 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.771065235 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.772314072 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.773471117 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541632792.778557539 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.852690220 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.853224039 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.854036808 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.855064631 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.856140375 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.856874704 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.857510567 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.858136415 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.858693361 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541632792.873378992 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.883295059 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.883639812 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.886556149 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.888076782 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.911681652 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.918977499 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.942172766 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.942689419 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.943841219 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.949691296 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.958420992 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.965980768 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.974766254 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.975664854 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.976320505 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.977899313 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.978554964 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632792.979159355 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.979756832 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632792.980443478 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632792.981101513 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632792.987115860 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.987672329 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632792.992401123 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.008868456 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.036107302 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.038227081 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.038341045 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.039945126 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.048703909 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.049768925 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.050564528 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.058691740 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.062238693 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.070169449 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.071542501 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.071609020 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.072736740 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.084187746 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.089622974 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.091311932 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.113610744 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.115003824 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.115090609 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.115748405 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.116059303 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.116366148 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.116988182 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.117694616 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.118355751 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.120080471 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.120736361 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.121338844 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.121926308 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.122608185 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.123240948 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.129667521 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.134384632 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.134516478 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.141705751 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.142345905 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.142953634 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.143551588 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.144356251 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.145264864 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.149350405 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.154620409 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.165726185 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.166760683 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.167669296 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.168067694 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.172708750 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.173737288 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.174454212 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.176209450 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.177150011 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.179906368 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.185253382 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.188337803 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.189443827 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.190343857 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.199611664 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.208794355 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.209568262 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.210166931 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.210850716 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.211655140 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.212414742 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.217745066 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.221749544 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.223183155 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.240697622 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.243795872 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.252195358 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.254874468 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.255584955 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.256252050 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.256949663 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.257748842 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.258486032 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.263128281 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.264149904 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.264873505 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.265166044 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.265880108 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.269526243 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.275193691 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.275992393 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.276746273 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.276827335 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.277464628 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.278359652 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.279138327 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.287331581 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.288081884 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.288804054 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.289562941 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.290198803 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.290448189 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.291167259 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.292755365 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.301751614 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.303770304 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.308854103 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.309828520 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.310560703 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.321116686 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.324838877 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.326320648 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.327059507 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.327804327 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.328538179 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.329157114 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.329484940 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.330195189 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.330328465 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.331094265 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.331403255 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.332253218 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.333126068 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.333951235 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.334859610 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.335712671 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.338340521 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.341126204 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.341245651 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.342065811 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.342357397 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.343061686 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.345303535 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.353511333 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.354871988 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.355699539 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.356549025 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.357425451 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.358330011 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.359172821 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.361974716 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.368611336 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.371790171 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.372498989 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.373248577 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.373912096 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.374786854 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.375508547 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.379692793 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.380465508 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.380733252 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.381533623 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.381585836 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.382258654 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.386503220 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.390564203 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.392962933 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.400691509 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.401505232 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.402542353 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.403441668 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.412694216 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.419946194 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.423892021 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.424946547 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.425709009 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.431893587 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.435216188 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.436514616 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.456497669 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.469269037 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.469494820 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.469995737 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.470592737 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.471253395 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.472071648 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.472496271 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.472783327 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.478127003 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.482964754 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.491744757 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.503042221 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.504956961 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.505686283 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.506411076 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.507096767 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.507887363 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.508638620 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.513304949 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.517535210 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.519165277 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.521405220 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.522392511 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.523136139 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.526145697 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.526926517 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.527648926 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.528390646 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.529287338 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.530066490 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.534573555 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.538551331 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.539358377 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.539398909 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.540103912 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.540883780 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.540919065 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.541577816 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.542351246 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.544674635 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.546121836 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.547008038 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.547813416 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.548657656 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.549576759 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.550433397 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.552639723 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.558664083 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.559672117 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.560071230 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.560457230 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.570520878 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.570519209 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.571040869 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.571862459 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.572657824 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.573559046 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.574400187 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.576782942 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.577596426 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.578349113 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.578970432 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.579129934 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.579857588 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.580134869 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.580641508 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.581055403 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.585338354 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.587103128 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.590681791 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.591128349 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.591145754 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.591744661 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.592539072 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.597333431 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.598401546 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.599279642 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.602704048 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.608926058 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.613611460 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.619390011 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.620153666 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.620723009 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.620911121 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.621657372 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.621818781 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.622462749 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.622571468 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.623200178 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.629318476 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.630356550 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.631174564 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.632268190 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.634101391 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.641642809 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.648271322 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.662180185 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.670209169 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.671278238 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.672051907 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.678769827 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.678853273 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.682211637 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.686101913 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.701198816 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.715420961 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.717250824 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.717968464 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.718663454 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.719273806 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.719354391 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.720059633 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.720837593 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.726404428 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.731379032 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.743048429 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.748853683 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.751768827 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.752543449 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.753275633 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.754040003 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.754918337 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.755694628 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.755811930 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.756475210 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.757400751 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.758269072 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.758380651 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.759196520 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.760107517 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.766058445 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.767166615 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.770137310 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.774489641 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.775381804 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.775432110 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.776218414 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.776857376 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.777058363 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.777605534 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.778037310 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.778915882 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.782844782 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.783620834 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.784399986 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.785172224 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.786089659 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.786915779 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.788542509 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.789290428 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.789652824 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.797366858 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.798751116 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.799588203 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.800430298 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.801273823 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.802157640 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.802984715 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.804522276 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.805536985 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.806321859 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.808103085 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.809246063 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.810148239 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.813746214 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.816484928 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.820208549 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.822009325 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.822792768 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.823577642 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.824335337 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.825244665 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.825719118 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.826050997 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.826744795 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.827605009 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.831328154 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.835936069 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.836764336 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.837100029 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.837199450 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.837807178 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.847926855 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.848761797 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.849840879 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.850708246 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.860270262 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.863530397 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.864265919 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.865023851 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.865733624 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.866559982 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.866629601 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.867391586 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.874703646 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.875742912 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.876554966 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.878168583 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.886887312 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.890471697 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.894170046 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.908380747 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.914479971 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.915488005 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.916223764 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.924195766 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.928309202 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.930728912 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.937317371 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.956171036 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.961748123 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.963878632 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.968976021 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.969787359 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.970511436 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.971237183 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.972129107 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.972903967 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.972877026 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.982968807 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.985727072 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.986662388 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.987581015 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.988456011 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632793.989415884 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632793.990288973 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.994798183 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.996891975 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.997053146 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632793.997746468 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632793.998485565 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632793.999199390 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.000079393 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.000257254 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.000875711 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.004144192 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.005012274 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.005045652 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.005575657 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.006418943 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.007363319 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.008233070 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.011501789 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.018073082 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.021093130 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.022154570 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.022935390 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.028481722 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.029002190 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.029359102 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.029869318 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.030207634 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.030652046 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.031050444 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.031448364 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.032051563 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.032226086 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.032955885 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.032977343 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.034516811 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.036089182 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.039409876 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.040465355 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.041267872 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.042672873 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.044317722 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.050998449 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.051928043 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.051979780 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.052755356 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.053732634 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.054771185 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.055680275 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.063118219 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.065063953 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.069681883 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.070416927 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.071160078 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.071913481 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.072782755 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.073553562 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.076139212 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.077232361 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.078138113 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.079080105 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.084137917 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.084884405 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.085979700 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.086752653 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.087488413 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.097148180 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.111984015 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.112693071 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.112802505 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.113371611 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.114076614 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.115001440 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.115749598 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.122625351 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.123668194 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.124483347 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.126571894 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.130569696 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.132713318 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.135076523 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.138428688 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.155420065 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.162799835 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.163868904 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.164645195 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.173193455 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.174903393 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.183541059 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.197549582 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.199575901 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.203623056 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.210712433 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.214961052 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.215714455 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.216466188 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.217197418 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.218094587 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.218865633 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.219973326 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.227438450 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.228399754 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.229011059 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.229252815 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.230102062 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.230844736 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541632794.232578754 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.233371019 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.234108210 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.234882116 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.234973431 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.235778093 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.235980272 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.236596823 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.236887455 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.237696409 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.238410950 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541632794.244600058 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.247220993 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.251216888 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.253520966 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.254319906 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.255248785 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.256057501 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.256763220 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541632794.267165661 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.268478155 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.269279718 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.277127504 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.277959585 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.278720856 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.279480934 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.280361652 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.280557632 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.281144619 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.284737110 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.285794973 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.286877871 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.287696600 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.292428732 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.298152208 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.318050146 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.318792820 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.319409370 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.320014000 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.320730209 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.321399450 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.324256659 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.331392765 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.332467794 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.333191156 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.333433151 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.344610929 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.359185934 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.359890699 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.360486984 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.360747814 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.361082792 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.361646652 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541632794.374081373 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.375138998 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541632794.375833273 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.379540205 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.387475967 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.425684452 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.435857296 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.453690767 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.454914570 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.470368147 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.471116066 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.471716642 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.472306252 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.472898960 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
| model transformer_wmt_en_de_big_t2t, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 210808832
:::MLPv0.5.0 transformer 1541632794.488781452 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.489578485 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.490211725 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.490831137 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.491397142 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541632794.501474857 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.514506340 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541632794.536918640 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.537689924 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.538319111 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.539166212 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.539754152 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541632794.542279005 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.543095350 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541632794.543999672 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541632794.544847250 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541632794.545615911 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
| training on 8 GPUs
| max tokens per GPU = 5120 and max sentences per GPU = None
:::MLPv0.5.0 transformer 1541632794.987138510 (/workspace/translation/train.py:88) input_batch_size: 5120
:::MLPv0.5.0 transformer 1541632794.987715006 (/workspace/translation/train.py:89) input_order
:::MLPv0.5.0 transformer 1541632796.410884857 (/workspace/translation/fairseq/optim/lr_scheduler/inverse_square_root_schedule.py:42) opt_learning_rate_warmup_steps: 3000
:::MLPv0.5.0 transformer 1541632796.411478519 (/workspace/translation/train.py:114) train_loop
:::MLPv0.5.0 transformer 1541632796.411836863 (/workspace/translation/train.py:116) train_epoch: 0
generated batches in  1.57417893409729 s
got epoch iterator 1.5747337341308594
| WARNING: overflow detected, setting loss scale to: 64.0
| WARNING: overflow detected, setting loss scale to: 32.0
| WARNING: overflow detected, setting loss scale to: 16.0
| epoch 001:   1000 / 3935 loss=10.174, nll_loss=9.392, ppl=671.96, wps=239787, ups=6.5, wpb=36084, bsz=1146, num_updates=998, lr=0.000212907, gnorm=87499.395, clip=100%, oom=0, loss_scale=16.000, wall=154
| WARNING: overflow detected, setting loss scale to: 8.0
| epoch 001:   2000 / 3935 loss=8.426, nll_loss=7.360, ppl=164.31, wps=240070, ups=6.6, wpb=36127, bsz=1154, num_updates=1997, lr=0.000426027, gnorm=56930.997, clip=100%, oom=0, loss_scale=8.000, wall=304
| epoch 001:   3000 / 3935 loss=7.483, nll_loss=6.271, ppl=77.23, wps=240769, ups=6.6, wpb=36173, bsz=1159, num_updates=2997, lr=0.00063936, gnorm=42828.727, clip=100%, oom=0, loss_scale=8.000, wall=454
| epoch 001 | loss 6.951 | nll_loss 5.661 | ppl 50.61 | wps 240921 | ups 6.6 | wpb 36152 | bsz 1162 | num_updates 3931 | lr 0.000559099 | gnorm 37191.093 | clip 100% | oom 0 | loss_scale 16.000 | wall 593
epoch time  590.3958678245544
generated batches in  0.0007805824279785156 s
| epoch 001 | valid on 'valid' subset | valid_loss 4.72658 | valid_nll_loss 3.06822 | valid_ppl 8.39 | num_updates 3931
:::MLPv0.5.0 transformer 1541633388.714422464 (/workspace/translation/train.py:149) eval_start: -1
| /data test 3003 examples
| Sentences are being padded to multiples of: 1
generated batches in  0.0005788803100585938 s
| Translated 384 sentences (8325 tokens) in 4.4s (87.89 sentences/s, 1905.46 tokens/s)
| Generate test with beam=4: BLEU4 = 20.87, 53.6/26.4/15.0/8.9 (BP=1.000, ratio=1.028, syslen=66302, reflen=64503)
| Eval completed in: 7.62s
:::MLPv0.5.0 transformer 1541633396.339886189 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": -1, "value": 20.868463583565624}
:::MLPv0.5.0 transformer 1541633396.340537786 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541633396.340911388 (/workspace/translation/train.py:154) eval_stop: -1
validation and scoring  7.9583964347839355
:::MLPv0.5.0 transformer 1541633396.341399193 (/workspace/translation/train.py:116) train_epoch: 1
generated batches in  1.6539220809936523 s
got epoch iterator 1.7558953762054443
| WARNING: overflow detected, setting loss scale to: 8.0
| epoch 002:   1000 / 3935 loss=4.986, nll_loss=3.425, ppl=10.74, wps=241130, ups=6.3, wpb=36094, bsz=1160, num_updates=4931, lr=0.000499198, gnorm=32376.221, clip=100%, oom=0, loss_scale=8.000, wall=753
| epoch 002:   2000 / 3935 loss=4.916, nll_loss=3.350, ppl=10.19, wps=240881, ups=6.5, wpb=36131, bsz=1159, num_updates=5931, lr=0.000455173, gnorm=28340.244, clip=100%, oom=0, loss_scale=8.000, wall=903
| epoch 002:   3000 / 3935 loss=4.863, nll_loss=3.292, ppl=9.80, wps=241027, ups=6.5, wpb=36145, bsz=1159, num_updates=6931, lr=0.000421059, gnorm=26037.803, clip=100%, oom=0, loss_scale=16.000, wall=1053
| epoch 002 | loss 4.820 | nll_loss 3.246 | ppl 9.49 | wps 241096 | ups 6.6 | wpb 36152 | bsz 1162 | num_updates 7865 | lr 0.000395268 | gnorm 24711.130 | clip 100% | oom 0 | loss_scale 16.000 | wall 1193
epoch time  590.1265907287598
generated batches in  0.0007913112640380859 s
| epoch 002 | valid on 'valid' subset | valid_loss 4.27283 | valid_nll_loss 2.57562 | valid_ppl 5.96 | num_updates 7865
:::MLPv0.5.0 transformer 1541633988.605093956 (/workspace/translation/train.py:149) eval_start: 0
generated batches in  0.0005795955657958984 s
| Translated 384 sentences (8454 tokens) in 2.5s (154.88 sentences/s, 3409.74 tokens/s)
| Generate test with beam=4: BLEU4 = 23.93, 55.6/29.5/17.8/11.2 (BP=1.000, ratio=1.055, syslen=68048, reflen=64503)
| Eval completed in: 6.85s
:::MLPv0.5.0 transformer 1541633995.460966110 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": 0, "value": 23.925951403788357}
:::MLPv0.5.0 transformer 1541633995.461542368 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541633995.461899757 (/workspace/translation/train.py:154) eval_stop: 0
validation and scoring  7.237983226776123
:::MLPv0.5.0 transformer 1541633995.462322235 (/workspace/translation/train.py:116) train_epoch: 2
generated batches in  1.690915584564209 s
got epoch iterator 1.7938265800476074
| epoch 003:   1000 / 3935 loss=4.592, nll_loss=2.995, ppl=7.97, wps=240459, ups=6.3, wpb=36000, bsz=1154, num_updates=8866, lr=0.000372286, gnorm=24348.823, clip=100%, oom=0, loss_scale=32.000, wall=1352
| epoch 003:   2000 / 3935 loss=4.565, nll_loss=2.965, ppl=7.81, wps=240660, ups=6.5, wpb=36029, bsz=1160, num_updates=9866, lr=0.000352915, gnorm=24658.954, clip=100%, oom=0, loss_scale=32.000, wall=1502
| WARNING: overflow detected, setting loss scale to: 16.0
| epoch 003:   3000 / 3935 loss=4.546, nll_loss=2.945, ppl=7.70, wps=241448, ups=6.5, wpb=36124, bsz=1164, num_updates=10865, lr=0.000336299, gnorm=24142.806, clip=100%, oom=0, loss_scale=16.000, wall=1651
| epoch 003 | loss 4.536 | nll_loss 2.935 | ppl 7.65 | wps 241622 | ups 6.6 | wpb 36151 | bsz 1162 | num_updates 11799 | lr 0.000322714 | gnorm 23260.366 | clip 100% | oom 0 | loss_scale 16.000 | wall 1791
epoch time  588.7715694904327
generated batches in  0.0008084774017333984 s
| epoch 003 | valid on 'valid' subset | valid_loss 4.12823 | valid_nll_loss 2.4202 | valid_ppl 5.35 | num_updates 11799
:::MLPv0.5.0 transformer 1541634586.366379976 (/workspace/translation/train.py:149) eval_start: 1
generated batches in  0.0005970001220703125 s
| Translated 384 sentences (8457 tokens) in 2.9s (134.41 sentences/s, 2960.24 tokens/s)
| Generate test with beam=4: BLEU4 = 25.01, 56.7/30.6/18.7/12.0 (BP=1.000, ratio=1.050, syslen=67755, reflen=64503)
| Eval completed in: 6.80s
:::MLPv0.5.0 transformer 1541634593.172773838 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": 1, "value": 25.00897166336336}
:::MLPv0.5.0 transformer 1541634593.173291922 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541634593.173641443 (/workspace/translation/train.py:154) eval_stop: 1
validation and scoring  7.145890712738037
:::MLPv0.5.0 transformer 1541634593.174049377 (/workspace/translation/train.py:167) run_stop
:::MLPv0.5.0 transformer 1541634593.174516678 (/workspace/translation/train.py:168) run_final
| done training in 1796.8 seconds
+++ date +%s
++ END=1541634595
+++ date '+%Y-%m-%d %r'
ENDING TIMING RUN AT 2018-11-07 11:49:55 PM
RESULT,transformer,10785,1820,,2018-11-07 11:19:35 PM
++ END_FMT='2018-11-07 11:49:55 PM'
++ echo 'ENDING TIMING RUN AT 2018-11-07 11:49:55 PM'
++ RESULT=1820
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,10785,1820,,2018-11-07 11:19:35 PM'
+ set +x
