Beginning trial 1 of 1
Clearing caches
:::MLPv0.5.0 transformer 1541705569.108431339 (<string>:1) run_clear_caches
Launching on node xpl-dvt-55
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX2 -e MULTI_NODE= -e SEED=5715 -e SLURM_JOB_ID=1541705542 -e SLURM_NTASKS_PER_NODE= -e MODE=TRAIN cont_1541705542 ./run_and_time.sh
Run vars: id 1541705542 gpus 16 mparams 
+ SEED=5715
+ MAX_TOKENS=10240
+ DATASET_DIR=/data
+ MODE=TRAIN
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START=1541705569
+++ date '+%Y-%m-%d %r'
++ START_FMT='2018-11-08 07:32:49 PM'
++ echo 'STARTING TIMING RUN AT 2018-11-08 07:32:49 PM'
STARTING TIMING RUN AT 2018-11-08 07:32:49 PM
++ python -m torch.distributed.launch --nproc_per_node 16 train.py /data --seed 5715 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 1200 --lr 1.35e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 10240 --max-epoch 12 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --distributed-init-method env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 14
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 15
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 8
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 3
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 12
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 13
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 2
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 4
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 9
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 10
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 6
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 0
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 11
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 5
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 7
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 1
| distributed init done!
| distributed init done!
| initialized host xpl-dvt-55 as rank 0 and device id 0
| distributed init done!
| distributed init done!
:::MLPv0.5.0 transformer 1541705583.237518787 (/workspace/translation/train.py:34) run_clear_caches
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLPv0.5.0 transformer 1541705605.167887449 (/workspace/translation/train.py:40) run_start
Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', data='/data', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=0, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=0, distributed_world_size=16, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=True, fuse_dropout_add=False, fuse_relu_dropout=False, gen_subset='test', ignore_case=True, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source='True', left_pad_target='False', lenpen=1, local_rank=0, log_format=None, log_interval=1000, log_translations=False, lr=[0.00135], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=12, max_len_a=0, max_len_b=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=10240, max_update=0, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_progress_bar=False, no_save=True, no_token_positional_embeddings=False, num_shards=1, online_eval=False, optimizer='adam', pad_sequence=1, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=None, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe=None, replace_unk=None, restore_file='checkpoint_last.pt', sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='checkpoints', save_interval=1, save_interval_updates=0, score_reference=False, seed=5715, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_bleu=25.0, target_lang=None, task='translation', train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=1200, weight_decay=0.0)
:::MLPv0.5.0 transformer 1541705605.168709517 (/workspace/translation/train.py:44) opt_name: "adam"
:::MLPv0.5.0 transformer 1541705605.169014692 (/workspace/translation/train.py:45) opt_learning_rate: [0.00135]
:::MLPv0.5.0 transformer 1541705605.169342041 (/workspace/translation/train.py:46) opt_hp_Adam_beta1: 0.9
:::MLPv0.5.0 transformer 1541705605.169601440 (/workspace/translation/train.py:47) opt_hp_Adam_beta2: 0.997
:::MLPv0.5.0 transformer 1541705605.169840574 (/workspace/translation/train.py:48) opt_hp_Adam_epsilon: 1e-09
:::MLPv0.5.0 transformer 1541705605.171729565 (/workspace/translation/train.py:53) run_set_random_seed: 5715
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
:::MLPv0.5.0 transformer 1541705605.243907928 (/workspace/translation/train.py:61) model_hp_sequence_beam_search: {"alpha": 1, "beam_size": 4, "extra_decode_length": 200, "vocab_size": 33712}
| /data train 4575616 examples
| Sentences are being padded to multiples of: 1
| /data valid 3000 examples
| Sentences are being padded to multiples of: 1
:::MLPv0.5.0 transformer 1541705606.364179611 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.437241077 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705606.510165930 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.516168594 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.518664598 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.522093534 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.524438620 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705606.526475668 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705606.526775599 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705606.528965712 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705606.552831411 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.552908897 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.555302143 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.559421539 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705606.570366621 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.572773695 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705606.582730055 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705606.590479374 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705606.590761185 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.596288681 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705606.628142357 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.634203911 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705606.646552563 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.652665377 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705606.678720236 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.680340767 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.681990147 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.685575008 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705606.686611176 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705606.689006329 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705606.691667318 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541705606.698933363 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541705607.186720610 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.187544346 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.201386929 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.271468878 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.297502041 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.298205137 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.300714493 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.301523924 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.315738678 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.331407070 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.332127333 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.351482630 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.352309465 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.352489948 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.362879276 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.373427153 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.413922310 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.417680740 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.449740171 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.450468779 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.476897001 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.477597952 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.499660254 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.500366449 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.504820108 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.505514383 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.510482550 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.526755571 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.528428078 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.530129433 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.544576168 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.559686422 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.576809645 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.576936245 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.577748060 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.578274488 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.578485727 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.622271776 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.622984886 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.627532721 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.628593445 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.627529144 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.650770426 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.659096241 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.661979198 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.673372984 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.690336704 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.710714340 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.711417675 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.714531898 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.715382099 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.722095251 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.722800493 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.723651171 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.727741003 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.737522364 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.738240957 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.741715193 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.742264032 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.742607355 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.742761612 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.743158817 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.743239164 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705607.743770838 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.744249344 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.749235868 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.749756336 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.750374556 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.750878572 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705607.751518011 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.752001762 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.752089500 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.752753973 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.752913475 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.753539085 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.754096270 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.754660606 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705607.754812956 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.755363941 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.755963564 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.762099504 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.765770435 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.768967152 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.774753094 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.777906418 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.778609037 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.784107208 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.791219711 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.799015999 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.800490856 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.815294981 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.815875292 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.819855690 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.821107626 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.831562757 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.832042933 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.834231377 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.843970776 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.844373703 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.847331762 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.859176636 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.867542267 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.869087696 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.869722128 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.870280027 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.870827436 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705607.871480942 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.872074842 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.877227545 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.879313469 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.885119438 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.886323929 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.888271570 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.889400721 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.893187761 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.894025803 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.887034416 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.915214539 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.917839766 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.919167519 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.922180891 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.922191620 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.923439503 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.926609993 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.927440166 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.931936979 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.932592630 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.933161020 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.933718443 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705607.934373140 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.934969902 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.946790695 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.947284937 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.947495937 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.947704554 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.947871447 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.947992086 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.948107958 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705607.948409557 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.948387861 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.948582172 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.948817253 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705607.949011803 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.949296951 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.949730396 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.953372478 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.953858137 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.954269171 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.954585552 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.954674006 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705607.955445051 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.955967426 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.956677675 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.957211018 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.957369089 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.957463741 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.957651854 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.957866430 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.958011389 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.958082199 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705607.958593369 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.958588839 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.958641052 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.959055901 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.959159613 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705607.959824324 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.960422516 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.961846352 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.965294600 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.965594292 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.967237473 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.968305349 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.970902205 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.973369837 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.973648310 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.974134445 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.974548578 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.974952936 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705607.975477219 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.975636244 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.975946426 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.981342554 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.984867573 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.986459970 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.986935616 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.987394094 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.987822056 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705607.988244534 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.988335848 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.988765240 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.989753246 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.990550041 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.990915775 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.991019249 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.991469860 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.991907358 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705607.992030382 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.992424726 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.992540121 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.992591381 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.992859125 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.992948294 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.993069172 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.993349791 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705607.993474245 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.993808031 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.993863344 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705607.994223356 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.994325161 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705607.994741678 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.996577740 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.997627497 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705607.998162985 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.998470068 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.998953104 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705607.999397039 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.999760151 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705607.999834061 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.000341415 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.000786543 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.000995874 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.001469612 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.001693964 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.001863241 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.002246380 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.002704144 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.003021717 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.003132343 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.003611326 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.005469084 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.008437395 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.008932114 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.009342432 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.009552956 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.009742498 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.010213375 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.010645866 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.011907578 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.012109280 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.015893221 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.019636393 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.021460295 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.021951914 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.022450209 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.022865772 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.022877216 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.023296118 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.023802519 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.024133444 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.024278879 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.029457569 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.032639265 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.032678843 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.032977819 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.033132553 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.033990383 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.034601450 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.037243128 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.037728786 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.038134813 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.038534164 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.038668871 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.039003372 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.039477825 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.040743113 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.043007374 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.047197342 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.048371792 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.051295996 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.060870886 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.064231396 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.066053867 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.067400694 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.068130493 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.074287415 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.076576471 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.078994513 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.079381943 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.082665205 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.083082438 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.085104704 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.090876818 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.092148781 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.096637487 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.097523451 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.104212761 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.104705572 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.104931593 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.105110168 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.105504990 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.105993032 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.106431723 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.107991695 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.108510733 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.108912945 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.109306574 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.109773636 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.110199690 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.110500336 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.110836029 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.110969067 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.111407518 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.111832857 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.112329006 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.112758160 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.115352631 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.116179466 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.116691113 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.117090464 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.117329836 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.117482185 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.117821455 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.117945671 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.118237257 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.118371010 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.118638039 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.119112015 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.119148254 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.119598866 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.120198965 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.121706486 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.121720552 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.122179508 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.122576237 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.122964144 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.123485088 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.123594761 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.123951674 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.125073910 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.125779152 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.127258778 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.128546238 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.130142689 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.130623579 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.131032467 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.131477833 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.131985188 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.132465601 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.132813931 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.134264469 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.134274006 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.141333103 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.143382788 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.145344734 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.145816803 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.146217108 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.146563292 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.146609068 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.147066116 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.147538424 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.148789167 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.149253845 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.149652481 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.150044203 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.150200129 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.150510311 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.150474072 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.150658369 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.150934935 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.150939226 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.151054859 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.151462555 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.151495695 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.151903152 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.151998520 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.152333736 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.152457476 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.152804375 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.153007269 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.153224230 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.155246735 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.156610966 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.158541679 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.159501314 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.159913063 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.159920692 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.160001755 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.160443068 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.160827398 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.161285877 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.161321163 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.161705256 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.161723375 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.162092686 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.162219524 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.162637711 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.163029432 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.163553238 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.163825035 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.164027452 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.168409348 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.168726206 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.169193745 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.169587612 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.169974566 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.170446157 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.170484304 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.170863390 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.172565937 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.172870636 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.179759502 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.179975271 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.180479050 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.180943251 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.181339979 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.181727409 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.182184935 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.182602644 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.183915854 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.184309959 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.187738657 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.189933062 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.191155195 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.191502333 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.192450047 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.193229675 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.193387747 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.193458080 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.193918705 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.194311142 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.194695711 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.194971800 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.195173979 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.195636749 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.201784849 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.204137325 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.204533339 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.206040144 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.211487532 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.221110344 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.222701550 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.224450350 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.225824833 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.226711512 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.235309362 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.235881329 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.237905264 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.237881422 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.242009401 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.244720221 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.245976925 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.248567104 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.251784801 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.253823280 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.255406141 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.263028622 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.263571262 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.264014006 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.264354229 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.264464617 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.264930010 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.265351772 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.267331362 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.267623186 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.267856359 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.268296957 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.268717289 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.269195795 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.269628525 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.272182226 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.272681475 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.273084641 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.273482323 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.273793936 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.274013281 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.274178267 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.274281263 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.274460077 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.274678469 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.275064230 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.275581837 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.276037931 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.277048588 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.277520657 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.277928591 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.278333426 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.278612852 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.278812647 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.278927803 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.279289484 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.279453516 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.279889822 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.280328274 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.280798912 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.281221151 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.281171560 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.282413244 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.283427238 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.283836365 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.284823656 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.284975052 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.288322449 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.289599657 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.290082455 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.290095091 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.290491581 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.290893555 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.291405201 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.291872263 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.293433905 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.298263550 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.300845861 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.304438114 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.305428505 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.306507111 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.306983709 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.307445049 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.307608128 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.307880878 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.308125734 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.308430433 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.308568239 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.308862686 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.308981419 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.309029341 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.309461594 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.309501648 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.309947491 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.309952259 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.310034990 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.310285807 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.310378313 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.310544729 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.310875416 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.310980558 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.311341763 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.311420679 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.311920881 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.312388897 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.313681602 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.314954042 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.316122055 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.317933083 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.318695068 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.319143534 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.319209814 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.319658041 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.320084810 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.320266724 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.320512533 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.320590496 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.321032047 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.321294308 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.321527719 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.325428486 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.325787067 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.326281309 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.326711893 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.327150106 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.327663660 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.328130484 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.329877615 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.330058336 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.330369949 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.330764771 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.331185818 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.331722021 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.332184076 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.332626104 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.337228298 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.338953972 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.339068174 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.339493036 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.339928389 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.340353251 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.340842485 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.341196060 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.341284275 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.344514608 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.349139929 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.349588394 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.349825621 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.350194931 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.350509405 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.350933075 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.350983381 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.351433039 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.351858854 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.352178097 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.352375031 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.352615595 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.352793455 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.353121281 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.353200436 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.361537218 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.361571789 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.366212368 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.368570805 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.372447729 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.381058216 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.383631945 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.384314775 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.385456085 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.386585236 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.392579556 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.394871950 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.396643877 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.401955128 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.402217388 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.405858517 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.407473564 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.409073830 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.410023212 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.411200047 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.417171955 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.421751261 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.422243834 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.422661066 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.423054695 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.423582315 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.424055815 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.424387217 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.425697088 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.427500010 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.428020954 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.428467035 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.428869724 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.429349184 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.429782391 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.432737350 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.432856083 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.433221817 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.433615208 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.434001684 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.434456825 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.434869528 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.435145140 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.435672760 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.436114550 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.436438799 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.436554193 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.436590910 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.436916590 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.437074423 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.437326193 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.437482834 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.437726736 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.437878847 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.438200474 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.438354731 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.438641071 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.438758373 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.438791513 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.438769579 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.442412615 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.443584442 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.443709850 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.444763899 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.447729349 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.447886705 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.449620008 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.450113058 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.450521708 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.450946093 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.451461554 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.451930046 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.453245163 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.460890293 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.462687016 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.463500023 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.463795900 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.463877678 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.463989258 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.464075327 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.464581966 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.464977980 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.465367556 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.465823889 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.466242790 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.467617989 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.468121290 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.468555450 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.468800068 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.468957663 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.469271898 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.469436169 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.469676733 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.469875097 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.469913006 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.470075607 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.470395088 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.470546007 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.470686436 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.470805645 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.470976591 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.471235752 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.471736908 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.472194195 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.472799301 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.474380970 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.474851370 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.475015163 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.478416681 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.478796005 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.478924513 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.479360819 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.479481936 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.479503155 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.479786158 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.479937315 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.480283976 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.480742455 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.481140375 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.483069420 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.489235163 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.489594221 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.489721775 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.490124226 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.490205765 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.490514517 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.490705729 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.490979910 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.491130829 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.491505384 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.491588831 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.492095947 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.492435932 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.492558241 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.496884823 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.499660969 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.500114441 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.500458241 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.500506639 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.500879765 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.501322746 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.501498222 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.501731157 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.503736734 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.504507303 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.506080866 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.508141279 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.508275270 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.508755207 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.509149551 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.509533644 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.510411978 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.510704994 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.511265516 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.512536049 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.513252020 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.513386250 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.521044254 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.526186705 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.531666994 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.532167435 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.532662153 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.532973051 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.536686897 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.537348032 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.541270256 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.541824102 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.542090893 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.544937134 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.546238422 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.552287817 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.553733587 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.562561989 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.564253092 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.565191031 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.566725016 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.571594954 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.571753263 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.573465109 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.576804399 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.578857422 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.579369545 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.579811335 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.580243349 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.580765963 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.581197262 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.584298849 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.589993477 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.590389967 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.590878248 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.591332674 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.591759205 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.594022989 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.596815825 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.596927643 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.597300291 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.597706795 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.598102808 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.598574877 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.599004269 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.600359440 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.600940466 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.601412058 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.601814270 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.602206707 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.602709055 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.603015661 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.603146791 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.604496956 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.604867220 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.607803822 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.608265877 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.608369350 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.608654022 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.609033823 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.609379292 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705608.611810446 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.612460136 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.613281488 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.613940477 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.619227171 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.619731426 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.620167017 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.620593548 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.620647192 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.620975733 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705608.621145964 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.621414423 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.622461557 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.622979164 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.623278141 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.623913527 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.624410152 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.624992371 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.625521183 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.625513792 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.626176119 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.626892328 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.630589485 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.631146908 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.631622314 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.632072210 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.632861376 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.633365154 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.633362770 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.633440733 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.633609295 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.634082317 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.634097099 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.634529114 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.634649038 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.635077477 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.635220051 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.635630846 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.635874748 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.636473179 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.637086153 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.638033390 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.638609886 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.639116764 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.639366627 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.639623404 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.640061378 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705608.646796227 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.647861242 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.648520947 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.649235249 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.649867296 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.650423288 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.650967598 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.651626587 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.651817322 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.652050495 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.652226210 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.653355598 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.654988289 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.655709982 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.656537533 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.661722183 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.663156986 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.663802624 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.664363861 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.664921045 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.665278673 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.665575266 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.666177750 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.667372227 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.675522327 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.679183960 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.679340601 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.680349112 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.681112528 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.681658983 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.682760715 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.690562963 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.691835880 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.692464828 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.693017960 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.693561554 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.694201708 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.694796562 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.694844484 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.695776463 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.696412325 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.696407795 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.697573662 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.703478098 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.704316139 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.704867363 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.706371546 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.707814217 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.709683180 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.711608648 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.711957455 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.717229128 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.720431089 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.721057177 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.721609354 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.722151756 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.722633362 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705608.723060369 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.725107670 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.729260206 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.738872528 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.754956961 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.755963326 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.756541729 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.761097431 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.762711048 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.763382673 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.767512083 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.778012276 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.779040337 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.788314342 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.793656349 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.801356792 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.806825638 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.810659647 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.813241959 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.817165613 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.818834782 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.826774836 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.840162754 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.843650818 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.844238997 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.844728470 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.845206499 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.845636845 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705608.847565174 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.856151342 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.856783628 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.857331514 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.857507944 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.857870579 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.858350992 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705608.858376265 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.862654686 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.863143444 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.863255739 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.863753319 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.863765478 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.864256859 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.864262819 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.864708662 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705608.864748478 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.865336418 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.865953207 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.871957302 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.883009434 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.885066509 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.886314154 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.887098551 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.887825727 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.888507843 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.890111446 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.890681744 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.891193151 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.891687393 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.892126560 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705608.897128820 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705608.897284031 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.900956154 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.908540964 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.912671804 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.923674583 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.932717800 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.946719408 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.947315693 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.947815180 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.948302269 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.948739052 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705608.956077099 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.956746817 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.962249041 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.972863913 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.973529100 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.974109411 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.974681616 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705608.975522280 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.976215839 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.976279736 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.976639509 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.976968050 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.977237463 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.986124516 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.986748457 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.989667177 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705608.993257999 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705608.993972540 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.996905804 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.997551441 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705608.998120546 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705608.998681068 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.005801201 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.006373405 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.006867886 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.007421494 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.007862806 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705609.011855602 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.012133360 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.012501478 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.013319254 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.014566660 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.014816046 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.015225887 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.015794039 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.016345024 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.016833544 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705609.018907070 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705609.025633335 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.025731802 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.026548147 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.027425051 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.028015614 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.028519392 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.029016495 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.029600620 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.030134439 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.045986891 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.047549009 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.048209906 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.048773289 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.049328804 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.049832821 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705609.054074526 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.066366911 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.067227602 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.067850351 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.081100702 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.086060286 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.086750507 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.097875595 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.098715067 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.108295202 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.108516693 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.108849049 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.109294176 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705609.118495226 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.132296085 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.162444115 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.163022995 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.163532972 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.164022446 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.164594889 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.165118933 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.170810461 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.171506166 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.171716928 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.172322989 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.172411680 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.173033476 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.178022861 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.180860519 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.181685686 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.182244778 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.185194492 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.186104298 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.189002275 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.189878464 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.190487385 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.193852186 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.200768948 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.203170538 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.208328009 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.209120989 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.209655762 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.219343901 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.220282316 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.220902920 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.221552849 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.269602537 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.277945518 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.278582573 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.279766560 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.286638498 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.287496090 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.288443804 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.289072037 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.290404081 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.291319370 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.291936159 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.292810440 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.296055555 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.296867371 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.297558308 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.304082394 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.304774761 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.309024572 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.310753345 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.312231541 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.321382999 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.322285414 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.322337866 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.326910734 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.353007793 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.361636877 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.362927437 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.366186857 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.368836403 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.370747328 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.375464678 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.379245996 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.398985386 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.399927855 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.400568485 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.407905102 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.408808470 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.408793688 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.409436941 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.409447908 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.410016775 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.410576344 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.411274433 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.411281824 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.411883116 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.411953688 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.412522793 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.413081169 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.413581371 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705609.414032936 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.414700508 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.415301085 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.415884018 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.416565180 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.417189121 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.425699949 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.430215597 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.433925390 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.439600468 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.443287373 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.448422909 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.449076414 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.449649096 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.450210810 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.450878143 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.451496124 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.456928730 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.457814217 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.458441496 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.464060307 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.465226412 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.469428301 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.470431089 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.471064568 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.473834753 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.474692345 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.475587845 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.476218939 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.477181911 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.477984428 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.478557348 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.489320755 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.489678144 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.490862846 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.490969419 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.491569042 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.492076874 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.492573023 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.493162155 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.493695021 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.497832298 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.498538256 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.499833345 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.500471354 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.500580549 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.501067638 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.508338213 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.509207726 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.509829044 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.510334969 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.514394522 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.515042305 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.517169237 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.517573357 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.517821074 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.518483877 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.519085407 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.522858143 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.523072958 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.528300285 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.528837919 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.529285908 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.529717207 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.530213594 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.530221939 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.530695915 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.534673214 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.539707184 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.540994406 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.546940088 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.549591541 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.562800884 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.563037634 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.563556194 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.563939333 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.564051628 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.564558506 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.564898014 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.565418482 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.566191673 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.566244364 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.566700459 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.567151546 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.567589045 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.568097353 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.568562746 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.571585417 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.572299004 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.572788715 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.573336601 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.575033903 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.575587749 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.576052904 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.576336145 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.576508522 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.577037334 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.577524185 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.577556133 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.577809095 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.582252264 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.584162712 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.586871862 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.591458321 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.592582941 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.593651533 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.594420910 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.607226610 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.609830618 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.610216141 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.610457420 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.610530615 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.610882998 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.611063719 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.611514807 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.611943960 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.612449169 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.612905264 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.618937016 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.619157314 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.619518995 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.620230675 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.620733500 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.622730970 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.623440266 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.630538940 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.632768869 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.632945776 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.635078907 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.638831139 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.648073435 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.648552418 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.649058104 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.649493217 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.649918318 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.650418997 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.650880814 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.651799440 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.654911757 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.655689240 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.656182766 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.660047770 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.660276651 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.660795689 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.661238432 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.661181450 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.661667585 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.662173986 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.662635088 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.665385962 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.670070887 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.670715570 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.670971155 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.671297073 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.671860695 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.671943665 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.672512293 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.673110723 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.675973415 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.676608801 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.677165985 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.677716494 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.678358078 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.678946257 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.683075190 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.686128378 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.689067602 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.689716339 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.690279245 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.690833569 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.691491127 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.691862345 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.692095041 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.692474365 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.693146706 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.693626881 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.698600531 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.698845863 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.702216387 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.702373981 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.702618361 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.704169750 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.704809666 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.705254078 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.706475735 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.713818312 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.715895653 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.720601559 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.721245289 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.721672297 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.723447561 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.724291563 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.724934578 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.725368023 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.727301359 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.727772713 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.728178024 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.728577375 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.729033470 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.729452610 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.729711056 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.729935884 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.731560946 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.732089758 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.732537746 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.732955456 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.732913494 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.733019352 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.733432293 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.733676195 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.733872414 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.733996868 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.734124184 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.736567020 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.738031864 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.739103556 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.739996433 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.742507935 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.742710829 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.745767832 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.746460438 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.747067690 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.747673750 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.748375893 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.749016762 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.756173372 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.760240793 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.760733128 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.761138439 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.761547565 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.761571407 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.762029409 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.762075424 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.762462139 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.762479067 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.762866497 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.763359308 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.763797760 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.763926268 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.765034914 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.765089750 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.765510321 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.765484095 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.765918732 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.766214132 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.766335011 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.766816139 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.766851902 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.767266512 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.767519712 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.768273354 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.768624783 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.768964529 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.769252539 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.769679070 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.771111965 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.772419214 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.773306370 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.773972750 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.774420977 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.775879860 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.776819944 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.778263092 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.781301260 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.781779528 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.782185555 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.782580853 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.783058643 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.783090830 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.783528805 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.783679485 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.786167145 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.786902428 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.786867142 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.787590027 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.788238287 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.789000511 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.789678812 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.791584253 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.792510986 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.797890663 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.801749945 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.802427292 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.802476406 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.802873373 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.803205013 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.803879738 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.804361820 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.805386543 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.806692362 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.807305098 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.807730198 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.811562061 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.812123537 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.812649012 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.813047647 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.813147068 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.813436985 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.813906193 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.813868284 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.814323664 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.816477537 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.817788363 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.818862915 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.819654226 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.822749376 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.823411942 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.823443174 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.824119806 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.824611425 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.833471298 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.834772825 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.834858418 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.836123466 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.836994886 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.837509155 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.838312149 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.839286089 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.839378595 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.839789152 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.840146065 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.840220928 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.840635300 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.841089487 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.841506004 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.841728926 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.846603632 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.847552538 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.849354982 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.849586725 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.849845648 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.850261927 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.850285530 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.850671530 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.851136923 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.851595163 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.851628304 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.855560780 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.855649948 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.856301785 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.856773138 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.859906673 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.860437870 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.860476971 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.860848188 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.861250401 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.861724138 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.862146139 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.862962484 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.865642309 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.870949984 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.874648094 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.875142574 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.875560284 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.875968695 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.876454830 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.876880646 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.877239227 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.877301693 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.880788803 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.881105661 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.881249428 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.881657362 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.881692171 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.881753683 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.882047415 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.882185698 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.882510424 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.882928371 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.885519028 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.888084650 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.888575792 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.888973475 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.889362097 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.889822721 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.890245914 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.890760422 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.891617775 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.891719818 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.892407894 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.892845631 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.896754742 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.898699045 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.898969650 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.901615620 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.902280331 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.902948380 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.903293610 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.903456211 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.905419827 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.910145521 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.912307262 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.914826870 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.916260719 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.916884661 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.917310238 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.922483921 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.923113585 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.923585176 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.923784971 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.924251318 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.924652815 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.925037622 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.925498724 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.925562143 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.925914288 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.929723024 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.930409431 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.930502176 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.930853367 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.930986643 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.931447744 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.931891441 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.932267666 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.932297230 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.932426214 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.932869673 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.934560299 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.935033083 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.936724424 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.939363480 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.939579248 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.941714287 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.953748941 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.953903198 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.960115194 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.960363626 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.960621357 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.961038828 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.961441517 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.961924791 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.962343931 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.964136362 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.964550734 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.964654922 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.965034008 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.965072155 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.965195894 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.965451002 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.965480566 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.965739489 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.965845108 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.965867996 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.965963602 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.966309071 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.966348886 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.966427326 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.966774464 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.967172384 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.971122503 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.972800493 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.973465204 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.973911762 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.974929333 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.975078106 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.975116014 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.975590944 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.976385117 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.979003191 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.979517937 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.979932785 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.980339527 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.980818748 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.981243849 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.982702494 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.985390663 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.985860348 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.986264467 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.986666918 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.987141132 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.987447023 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.987598896 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.989912033 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.990165710 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.992240429 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.992722750 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705609.993119478 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.993536472 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705609.994012594 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705609.994479656 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705609.995397806 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705609.996345520 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.002370596 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.002427578 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.002404213 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.002874136 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.003069878 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.003167391 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.003318787 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.003570795 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.003727913 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.004199743 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.004628181 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.005928040 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.006602764 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.006832361 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.007047653 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.007465839 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.007898569 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.010791540 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.012421370 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.013302326 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.015341043 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.015786409 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.015867949 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.016154289 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.016305447 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.016724586 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.017208576 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.017635345 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.020799398 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.021456957 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.021899462 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.023419857 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.026551485 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.027169943 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.027854919 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.028356552 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.030669928 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.034030199 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.034272671 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.034663439 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.035087824 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.035933256 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.036443710 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.036835432 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.037181377 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.037219048 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.037671089 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.038088083 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.038119793 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.044386625 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.045001030 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.045039177 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.045476675 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.046404123 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.046868086 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.046858311 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.048515320 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.049005747 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.049413443 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.049815655 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.050294638 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.050712824 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.053083181 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.054212093 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.057998419 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.058666706 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.059112310 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.059324741 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.059573412 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.059850216 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.060033083 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.060291767 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.060708523 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.061182976 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.061614513 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.068048239 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.070096016 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.070487738 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.070564270 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.070963621 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.071390629 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.071874619 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.072316170 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.076888800 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.077927589 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.078112602 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.078586102 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.078589439 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.078686953 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.078984737 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.079022408 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.079301834 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.079405308 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.079876423 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.080303431 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.080949068 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.085092545 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.085567236 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.085963726 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.086351395 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.086804152 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.087260485 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.087658405 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.088966608 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.090939283 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.091643333 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.092117310 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.094416857 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.095061064 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.095885515 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.100622892 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.100894690 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.101932287 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.102583408 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.103041887 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.104810476 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.108233929 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.111688137 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.111839533 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.112393618 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.112833023 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.117402077 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.119937181 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.120189667 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.120609283 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.120669365 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.121047020 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.121064425 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.121450663 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.121524572 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.121913910 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.122331619 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.126711607 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.127340555 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.127765656 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.129656553 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.129879951 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.130353928 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.130766630 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.130937576 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.131201267 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.131719351 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.132193327 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.132461071 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.134533882 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.135977507 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.136392355 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.138659000 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.141133070 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.150790930 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.151693583 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.158025026 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.159644842 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.160149813 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.160577297 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.160975933 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.161451817 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.161457777 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.161892414 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.161896706 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.161946297 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.162364006 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.162556887 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.162777185 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.163003206 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.163288832 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.163751841 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.165709972 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.165740013 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.165966511 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.166440725 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.166839361 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.167260647 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.167770147 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.168231964 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.170866489 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.171847820 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.172738075 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.172956467 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.173641205 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.174103260 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.176912546 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.177052021 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.177181959 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.177394867 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.177523613 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.177798033 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.178191900 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.178658962 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.179086924 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.182916880 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.183062077 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.183593988 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.184032202 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.184471369 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.184933901 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.185291529 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.185360432 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.188018084 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.190716743 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.190774679 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.191303492 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.191743851 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.192171335 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.192666054 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.193088293 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.193799257 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.194190264 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.198418379 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.201863050 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.202284336 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.202923298 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.203405857 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.203954697 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.204746246 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.204835892 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.205219507 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.205307961 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.205700397 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.206086874 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.206540823 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.206979990 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.208322048 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.208641529 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.209002018 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.209443569 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.212341070 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.214302540 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.215934038 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.215903759 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.216439486 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.216836929 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.217240810 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.217712402 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.218150139 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.218247890 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.219176531 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.219881058 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.220375299 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.223620415 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.225475073 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.226110220 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.226547718 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.227022171 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.229181290 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.233031273 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.233690977 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.233703613 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.234168291 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.234193563 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.234584093 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.235016823 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.235137463 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.235421658 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.235532522 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.235912561 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.235993624 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.243160725 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.243359804 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.245052338 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.247339249 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.247786045 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.248055220 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.248537540 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.248730421 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.249218702 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.249610186 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.250021935 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.250489235 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.250920773 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.251979351 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.256584167 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.257466316 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.258778095 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.259525537 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.259865999 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.260013103 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.260359764 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.260853052 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.261250973 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.261666059 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.262142658 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.262564898 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.268553734 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.268914461 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.269035816 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.269438028 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.269833803 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.270323277 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.270765543 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.271481752 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.276486158 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.276991606 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.277134895 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.277090311 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.277558088 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.277569771 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.277953863 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.278341532 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.278808594 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.278918743 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.279299021 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.279652834 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.281637669 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.282107592 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.282507658 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.282654285 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.282896996 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.283396959 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.283856392 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.286386967 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.288533926 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.291480780 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.292166233 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.292639017 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.292796373 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.293636322 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.293893576 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.300130606 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.301474571 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.303280115 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.303974628 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.304464340 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.306025028 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.307272911 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.310865402 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.311579466 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.312069178 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.313323021 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.318725109 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.319220304 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.319669247 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.319823027 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.320096493 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.320534945 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.320603132 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.321023941 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.321058989 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.321175337 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.321654558 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.323986530 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.324662209 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.325101376 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.329792023 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.329806566 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.331237793 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.331759930 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.332202673 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.332639933 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.333144903 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.333590746 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.333912134 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.334189415 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.335250378 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.337307215 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.342528582 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.342881680 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.350667715 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.351778269 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.357913256 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.360403538 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.360887051 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.360903263 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.361289501 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.361542940 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.361688137 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.361972809 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.362157345 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.362424612 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.362581015 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.362892389 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.363331795 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.363762856 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.364158392 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705610.364930153 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.366272926 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.370204926 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.370597363 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.370684385 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.371086121 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.371443510 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.371517181 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.371995211 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.372442722 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.373636723 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.374335766 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.374794245 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.376953602 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.377431870 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.377833366 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.378234863 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.378195047 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.378588200 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705610.381057262 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.381165504 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.382923841 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.383424759 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.383767605 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.383844852 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.384246826 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.384732485 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.384748220 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.385155678 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.389993906 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.390461683 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.390861034 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.391279459 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.391772985 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.392223120 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.392358541 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.393381834 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.393810034 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.397325277 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.401010990 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.402901649 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.403610468 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.404088974 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.407901287 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.409060240 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.409521580 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.409916878 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.410305738 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.410773039 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.411224604 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.411811829 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.412513256 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.412961483 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.413041830 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.417538643 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.418049335 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.418460608 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.418862104 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.419376373 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.419842720 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.420021534 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.421638966 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.424433708 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.424812317 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.425455809 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.425897121 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.428896666 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.431672812 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.432366848 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.432808161 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.432967663 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.433330059 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.433428049 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.433819294 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.434202909 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.434552908 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.434686899 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.435147762 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.436845541 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.441482067 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.441947937 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.443842411 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.448451757 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.449524879 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.450003147 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.450397730 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.450793266 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.450793505 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.451300383 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.451492071 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.451773882 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.451952219 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.452127218 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.454410553 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.460233212 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.460593462 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.460667610 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.461028576 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.461478710 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.461936951 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.462429047 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.462843418 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.463281155 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.463797331 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.464265108 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.467093229 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.467606783 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.468024492 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.468428850 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.468805790 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705610.470420837 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.473232031 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.474488258 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.475270033 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.475731134 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.477261066 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.477601051 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.477733850 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.478130579 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.478520393 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.478875637 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705610.479547977 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.480033636 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.480450153 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.480837107 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.481298208 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.481721878 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.484369040 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.484619856 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.490189552 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.490420818 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.492197037 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.492860317 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.493306875 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.497599840 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.502084494 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.504625559 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.505318403 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.505615950 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.505784035 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.506276369 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.514606476 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.515373707 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.515844822 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.516248941 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.516644239 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.516993999 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705610.522027254 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.522887945 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.523370028 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.523694754 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.531407833 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.531934261 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.532035112 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.532392263 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.532800674 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.533180475 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705610.534654617 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.535244465 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.544050694 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.547462463 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.554871082 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.560427427 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.560914993 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.561319590 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.561725616 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.562085867 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705610.562806129 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.567306757 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.571379662 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.571873426 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.572287560 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.572690487 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.573153973 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.573596716 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.578731298 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.580012083 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.580499887 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.580897570 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.581288815 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.581755161 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.582184792 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.582438231 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.583570957 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.587930202 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.588423967 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.588823557 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.589214325 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.589673281 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.590099573 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.590837717 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.592661142 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.595041513 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.598730803 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.604420424 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.611174583 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.611659050 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.612066746 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.612473965 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.612841129 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705610.613117933 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.613785267 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.614233255 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.617805004 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.618295908 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.618695021 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.619081497 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.619505644 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705610.621650934 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.622300625 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.622745037 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.622882128 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.625043869 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.629319191 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.629504919 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.629970074 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.629982710 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541705610.630362272 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.630424500 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.630747557 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.631097555 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705610.631461620 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.636648655 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.639065742 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.650150537 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.650617123 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.651013851 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.651445389 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.651834011 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705610.652253628 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.661789894 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.662266731 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.662668467 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.663063288 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.663486004 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705610.677373648 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.677842140 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.678241491 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.678628683 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.679001331 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705610.686073542 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.694499254 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.705058575 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.744805336 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.751004934 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.761331797 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541705610.772065163 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.772529125 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.772928238 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.773318768 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.773696423 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705610.775996208 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.776454449 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.776851892 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.777241230 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.777597427 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541705610.786270857 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.786731005 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541705610.787139654 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541705610.787531614 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541705610.787907600 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
| model transformer_wmt_en_de_big_t2t, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 210808832
| training on 16 GPUs
| max tokens per GPU = 10240 and max sentences per GPU = None
:::MLPv0.5.0 transformer 1541705612.913171053 (/workspace/translation/train.py:88) input_batch_size: 10240
:::MLPv0.5.0 transformer 1541705612.913573265 (/workspace/translation/train.py:89) input_order
:::MLPv0.5.0 transformer 1541705619.744026423 (/workspace/translation/fairseq/optim/lr_scheduler/inverse_square_root_schedule.py:42) opt_learning_rate_warmup_steps: 1200
:::MLPv0.5.0 transformer 1541705619.744520426 (/workspace/translation/train.py:114) train_loop
:::MLPv0.5.0 transformer 1541705619.744777679 (/workspace/translation/train.py:116) train_epoch: 0
generated batches in  2.072326421737671 s
got epoch iterator 2.072754383087158
| WARNING: overflow detected, setting loss scale to: 64.0
| WARNING: overflow detected, setting loss scale to: 32.0
| WARNING: overflow detected, setting loss scale to: 16.0
| WARNING: overflow detected, setting loss scale to: 8.0
| epoch 001 | loss 8.056 | nll_loss 6.925 | ppl 121.50 | wps 534742 | ups 3.5 | wpb 146386 | bsz 4704 | num_updates 968 | lr 0.001089 | gnorm 60737.358 | clip 100% | oom 0 | loss_scale 8.000 | wall 274
epoch time  265.4864192008972
generated batches in  0.0006260871887207031 s
| epoch 001 | valid on 'valid' subset | valid_loss 5.18786 | valid_nll_loss 3.56906 | valid_ppl 11.87 | num_updates 968
:::MLPv0.5.0 transformer 1541705887.550140142 (/workspace/translation/train.py:149) eval_start: -1
| /data test 3003 examples
| Sentences are being padded to multiples of: 1
generated batches in  0.0005955696105957031 s
| Translated 192 sentences (4223 tokens) in 2.0s (93.84 sentences/s, 2063.99 tokens/s)
| Generate test with beam=4: BLEU4 = 17.46, 49.5/22.7/12.1/6.8 (BP=1.000, ratio=1.082, syslen=69760, reflen=64503)
| Eval completed in: 4.40s
:::MLPv0.5.0 transformer 1541705891.950219870 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": -1, "value": 17.460549052475073}
:::MLPv0.5.0 transformer 1541705891.950626612 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541705891.950861931 (/workspace/translation/train.py:154) eval_stop: -1
validation and scoring  4.6467742919921875
:::MLPv0.5.0 transformer 1541705891.951181173 (/workspace/translation/train.py:116) train_epoch: 1
generated batches in  2.0678632259368896 s
got epoch iterator 2.18205189704895
| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 002 | loss 5.132 | nll_loss 3.581 | ppl 11.97 | wps 536693 | ups 3.6 | wpb 146343 | bsz 4706 | num_updates 1939 | lr 0.00106203 | gnorm 36502.902 | clip 100% | oom 0 | loss_scale 4.000 | wall 546
epoch time  264.9622526168823
generated batches in  0.0006349086761474609 s
| epoch 002 | valid on 'valid' subset | valid_loss 4.47837 | valid_nll_loss 2.79629 | valid_ppl 6.95 | num_updates 1939
:::MLPv0.5.0 transformer 1541706159.358099222 (/workspace/translation/train.py:149) eval_start: 0
generated batches in  0.0006356239318847656 s
| Translated 192 sentences (4116 tokens) in 1.6s (118.22 sentences/s, 2534.38 tokens/s)
| Generate test with beam=4: BLEU4 = 22.40, 54.6/28.1/16.4/10.0 (BP=1.000, ratio=1.052, syslen=67850, reflen=64503)
| Eval completed in: 4.22s
:::MLPv0.5.0 transformer 1541706163.581676722 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": 0, "value": 22.39640559571337}
:::MLPv0.5.0 transformer 1541706163.582170725 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541706163.582404613 (/workspace/translation/train.py:154) eval_stop: 0
validation and scoring  4.486971378326416
:::MLPv0.5.0 transformer 1541706163.582766294 (/workspace/translation/train.py:116) train_epoch: 2
generated batches in  2.077072858810425 s
got epoch iterator 2.191390037536621
| epoch 003 | loss 4.712 | nll_loss 3.124 | ppl 8.72 | wps 537599 | ups 3.6 | wpb 146353 | bsz 4707 | num_updates 2911 | lr 0.000866769 | gnorm 26098.417 | clip 100% | oom 0 | loss_scale 4.000 | wall 818
epoch time  264.8625011444092
generated batches in  0.0006246566772460938 s
| epoch 003 | valid on 'valid' subset | valid_loss 4.26055 | valid_nll_loss 2.55908 | valid_ppl 5.89 | num_updates 2911
:::MLPv0.5.0 transformer 1541706430.894427061 (/workspace/translation/train.py:149) eval_start: 1
generated batches in  0.0006139278411865234 s
| Translated 192 sentences (4084 tokens) in 1.4s (132.65 sentences/s, 2821.54 tokens/s)
| Generate test with beam=4: BLEU4 = 24.04, 56.0/29.7/17.9/11.2 (BP=1.000, ratio=1.043, syslen=67295, reflen=64503)
| Eval completed in: 3.97s
:::MLPv0.5.0 transformer 1541706434.873998880 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": 1, "value": 24.03509861998914}
:::MLPv0.5.0 transformer 1541706434.874505520 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541706434.874740124 (/workspace/translation/train.py:154) eval_stop: 1
validation and scoring  4.238138437271118
:::MLPv0.5.0 transformer 1541706434.875100613 (/workspace/translation/train.py:116) train_epoch: 3
generated batches in  2.21793794631958 s
got epoch iterator 2.297994613647461
| epoch 004 | loss 4.523 | nll_loss 2.918 | ppl 7.56 | wps 537436 | ups 3.6 | wpb 146353 | bsz 4707 | num_updates 3883 | lr 0.000750483 | gnorm 21201.806 | clip 100% | oom 0 | loss_scale 8.000 | wall 1089
epoch time  264.9615135192871
generated batches in  0.0007643699645996094 s
| epoch 004 | valid on 'valid' subset | valid_loss 4.13487 | valid_nll_loss 2.43073 | valid_ppl 5.39 | num_updates 3883
:::MLPv0.5.0 transformer 1541706702.365066290 (/workspace/translation/train.py:149) eval_start: 2
generated batches in  0.0005915164947509766 s
| Translated 192 sentences (4116 tokens) in 1.4s (141.82 sentences/s, 3040.17 tokens/s)
| Generate test with beam=4: BLEU4 = 24.92, 56.8/30.6/18.7/11.9 (BP=1.000, ratio=1.049, syslen=67634, reflen=64503)
| Eval completed in: 4.01s
:::MLPv0.5.0 transformer 1541706706.382930756 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": 2, "value": 24.91603911800853}
:::MLPv0.5.0 transformer 1541706706.383505344 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541706706.383765697 (/workspace/translation/train.py:154) eval_stop: 2
validation and scoring  4.249211072921753
:::MLPv0.5.0 transformer 1541706706.384150505 (/workspace/translation/train.py:116) train_epoch: 4
generated batches in  2.033766984939575 s
got epoch iterator 2.1479358673095703
| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 005 | loss 4.404 | nll_loss 2.788 | ppl 6.90 | wps 536377 | ups 3.6 | wpb 146356 | bsz 4704 | num_updates 4854 | lr 0.000671235 | gnorm 18380.551 | clip 100% | oom 0 | loss_scale 4.000 | wall 1361
epoch time  265.36932373046875
generated batches in  0.0006303787231445312 s
| epoch 005 | valid on 'valid' subset | valid_loss 4.05403 | valid_nll_loss 2.34481 | valid_ppl 5.08 | num_updates 4854
:::MLPv0.5.0 transformer 1541706974.145942688 (/workspace/translation/train.py:149) eval_start: 3
generated batches in  0.000598907470703125 s
| Translated 192 sentences (4087 tokens) in 1.2s (153.87 sentences/s, 3275.37 tokens/s)
| Generate test with beam=4: BLEU4 = 25.72, 57.3/31.3/19.4/12.6 (BP=1.000, ratio=1.037, syslen=66911, reflen=64503)
| Eval completed in: 4.09s
:::MLPv0.5.0 transformer 1541706978.237958908 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": 3, "value": 25.716508624234823}
:::MLPv0.5.0 transformer 1541706978.238472223 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541706978.238705158 (/workspace/translation/train.py:154) eval_stop: 3
validation and scoring  4.3373541831970215
:::MLPv0.5.0 transformer 1541706978.239068985 (/workspace/translation/train.py:167) run_stop
:::MLPv0.5.0 transformer 1541706978.239339828 (/workspace/translation/train.py:168) run_final
| done training in 1358.5 seconds
+++ date +%s
++ END=1541706987
+++ date '+%Y-%m-%d %r'
++ END_FMT='2018-11-08 07:56:27 PM'
++ echo 'ENDING TIMING RUN AT 2018-11-08 07:56:27 PM'
++ RESULT=1418
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,5715,1418,,2018-11-08 07:32:49 PM'
+ set +x
ENDING TIMING RUN AT 2018-11-08 07:56:27 PM
RESULT,transformer,5715,1418,,2018-11-08 07:32:49 PM
