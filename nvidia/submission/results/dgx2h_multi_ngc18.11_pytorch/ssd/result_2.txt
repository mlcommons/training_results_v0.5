Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3

:::MLPv0.5.0 ssd 1541757117.204435110 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757116.267488241 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757114.690814734 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757119.039204836 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757119.954175949 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757116.473214149 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757118.094431162 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757112.471916437 (<string>:1) run_clear_caches
Launching on node circe-n005
+ pids+=($!)
+ set +x
Launching on node circe-n006
+ pids+=($!)
+ set +x
Launching on node circe-n008
+ pids+=($!)
+ set +x
Launching on node circe-n009
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n005
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n006
+ pids+=($!)
+ set +x
Launching on node circe-n019
+ srun -N 1 -n 1 -w circe-n005 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=0 --master_addr=10.0.1.5 --master_port=4242' -e SLURM_JOB_ID=35121 -e SLURM_NTASKS_PER_NODE=8 cont_35121 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n008
+ pids+=($!)
+ set +x
Launching on node circe-n020
+ srun -N 1 -n 1 -w circe-n006 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=1 --master_addr=10.0.1.5 --master_port=4242' -e SLURM_JOB_ID=35121 -e SLURM_NTASKS_PER_NODE=8 cont_35121 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n009
+ srun -N 1 -n 1 -w circe-n008 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=2 --master_addr=10.0.1.5 --master_port=4242' -e SLURM_JOB_ID=35121 -e SLURM_NTASKS_PER_NODE=8 cont_35121 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n021
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n019
+ srun -N 1 -n 1 -w circe-n009 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=3 --master_addr=10.0.1.5 --master_port=4242' -e SLURM_JOB_ID=35121 -e SLURM_NTASKS_PER_NODE=8 cont_35121 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n022
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n020
+ srun -N 1 -n 1 -w circe-n019 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=4 --master_addr=10.0.1.5 --master_port=4242' -e SLURM_JOB_ID=35121 -e SLURM_NTASKS_PER_NODE=8 cont_35121 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n021
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n022
+ srun -N 1 -n 1 -w circe-n020 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=5 --master_addr=10.0.1.5 --master_port=4242' -e SLURM_JOB_ID=35121 -e SLURM_NTASKS_PER_NODE=8 cont_35121 ./run_and_time.sh
+ srun -N 1 -n 1 -w circe-n021 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=6 --master_addr=10.0.1.5 --master_port=4242' -e SLURM_JOB_ID=35121 -e SLURM_NTASKS_PER_NODE=8 cont_35121 ./run_and_time.sh
+ srun -N 1 -n 1 -w circe-n022 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=7 --master_addr=10.0.1.5 --master_port=4242' -e SLURM_JOB_ID=35121 -e SLURM_NTASKS_PER_NODE=8 cont_35121 ./run_and_time.sh
Run vars: id 35121 gpus 8 mparams  --nnodes=8 --node_rank=6 --master_addr=10.0.1.5 --master_port=4242
Run vars: id 35121 gpus 8 mparams  --nnodes=8 --node_rank=3 --master_addr=10.0.1.5 --master_port=4242
Run vars: id 35121 gpus 8 mparams  --nnodes=8 --node_rank=4 --master_addr=10.0.1.5 --master_port=4242
Run vars: id 35121 gpus 8 mparams  --nnodes=8 --node_rank=0 --master_addr=10.0.1.5 --master_port=4242
Run vars: id 35121 gpus 8 mparams  --nnodes=8 --node_rank=2 --master_addr=10.0.1.5 --master_port=4242
Run vars: id 35121 gpus 8 mparams  --nnodes=8 --node_rank=7 --master_addr=10.0.1.5 --master_port=4242
Run vars: id 35121 gpus 8 mparams  --nnodes=8 --node_rank=5 --master_addr=10.0.1.5 --master_port=4242
STARTING TIMING RUN AT 2018-11-09 09:51:56 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=6 --master_addr=10.0.1.5 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:51:57 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=3 --master_addr=10.0.1.5 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
Run vars: id 35121 gpus 8 mparams  --nnodes=8 --node_rank=1 --master_addr=10.0.1.5 --master_port=4242
STARTING TIMING RUN AT 2018-11-09 09:51:54 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=4 --master_addr=10.0.1.5 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:52:00 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=0 --master_addr=10.0.1.5 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:51:58 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=2 --master_addr=10.0.1.5 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:51:59 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=7 --master_addr=10.0.1.5 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:51:56 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=5 --master_addr=10.0.1.5 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:51:52 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=1 --master_addr=10.0.1.5 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
0 Using seed = 3906909910
3 Using seed = 3906909913
4 Using seed = 3906909914
1 Using seed = 3906909911
2 Using seed = 3906909912
15 Using seed = 3906909925
13 Using seed = 3906909923
14 Using seed = 3906909924
8 Using seed = 3906909918
11 Using seed = 3906909921
9 Using seed = 3906909919
10 Using seed = 3906909920
12 Using seed = 3906909922
22 Using seed = 3906909932
23 Using seed = 3906909933
20 Using seed = 3906909930
16 Using seed = 3906909926
21 Using seed = 3906909931
18 Using seed = 3906909928
19 Using seed = 3906909929
17 Using seed = 3906909927
26 Using seed = 3906909936
30 Using seed = 3906909940
28 Using seed = 3906909938
27 Using seed = 3906909937
24 Using seed = 3906909934
29 Using seed = 3906909939
25 Using seed = 3906909935
31 Using seed = 3906909941
35 Using seed = 3906909945
37 Using seed = 3906909947
33 Using seed = 3906909943
32 Using seed = 3906909942
34 Using seed = 3906909944
38 Using seed = 3906909948
39 Using seed = 3906909949
36 Using seed = 3906909946
46 Using seed = 3906909956
43 Using seed = 3906909953
47 Using seed = 3906909957
44 Using seed = 3906909954
45 Using seed = 3906909955
42 Using seed = 3906909952
41 Using seed = 3906909951
40 Using seed = 3906909950
52 Using seed = 3906909962
48 Using seed = 3906909958
55 Using seed = 3906909965
54 Using seed = 3906909964
53 Using seed = 3906909963
51 Using seed = 3906909961
50 Using seed = 3906909960
49 Using seed = 3906909959
58 Using seed = 3906909968
63 Using seed = 3906909973
57 Using seed = 3906909967
59 Using seed = 3906909969
56 Using seed = 3906909966
61 Using seed = 3906909971
62 Using seed = 3906909972
60 Using seed = 3906909970
7 Using seed = 3906909917
6 Using seed = 3906909916
5 Using seed = 3906909915

:::MLPv0.5.0 ssd 1541757134.014666557 (train.py:371) run_start

:::MLPv0.5.0 ssd 1541757134.017028093 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1541757134.017547369 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1541757134.030933857 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1541757134.031420231 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1541757134.090329170 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1541757134.107518196 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1541757134.127527714 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1541757134.147482634 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1541757134.167497873 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1541757134.187548399 (train.py:382) input_size: 300
loading annotations into memory...
Done (t=0.42s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
index created!
index created!
index created!
index created!
Done (t=0.45s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
time_check a: 1541757127.530835629
time_check a: 1541757135.082548618
time_check a: 1541757134.172635078
time_check a: 1541757131.605056763
time_check a: 1541757129.835988045
time_check a: 1541757131.419447660
time_check a: 1541757132.357946873
time_check a: 1541757133.188075781
time_check b: 1541757157.042388439
time_check b: 1541757149.638910294
time_check b: 1541757156.323597670
time_check b: 1541757153.575086117
time_check b: 1541757153.806138515
time_check b: 1541757155.483520269
time_check b: 1541757152.410665512
time_check b: 1541757154.969959497

:::MLPv0.5.0 ssd 1541757158.198313713 (train.py:413) input_order

:::MLPv0.5.0 ssd 1541757158.204818726 (train.py:414) input_batch_size: 32

:::MLPv0.5.0 ssd 1541757159.392014265 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541757159.392571211 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541757159.418179274 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1541757161.176668644 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1541757161.177221537 (train.py:477) opt_learning_rate: 0.16

:::MLPv0.5.0 ssd 1541757161.177681208 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1541757161.178109169 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1541757161.178530216 (train.py:483) opt_learning_rate_warmup_steps: 900

:::MLPv0.5.0 ssd 1541757162.571326256 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541757162.571886539 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541757162.597238779 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss

:::MLPv0.5.0 ssd 1541757165.830963135 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1541757165.831528902 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1541757165.834320307 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 22.566, Average Loss: 0.023, avg. samples / sec: 17781.27
Iteration:      0, Loss function: 22.750, Average Loss: 0.023, avg. samples / sec: 25060.42
Iteration:      0, Loss function: 22.979, Average Loss: 0.023, avg. samples / sec: 22375.16
Iteration:      0, Loss function: 21.975, Average Loss: 0.022, avg. samples / sec: 19238.03
Iteration:      0, Loss function: 23.340, Average Loss: 0.023, avg. samples / sec: 31859.65
Iteration:      0, Loss function: 23.215, Average Loss: 0.023, avg. samples / sec: 52613.45
Iteration:      0, Loss function: 22.524, Average Loss: 0.023, avg. samples / sec: 20465.87
Iteration:      0, Loss function: 22.603, Average Loss: 0.023, avg. samples / sec: 24854.06

:::MLPv0.5.0 ssd 1541757168.356153488 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.0001777777777777767}

:::MLPv0.5.0 ssd 1541757168.720582962 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0003555555555555534}

:::MLPv0.5.0 ssd 1541757168.816665411 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0005333333333333301}

:::MLPv0.5.0 ssd 1541757168.912478209 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0007111111111111068}

:::MLPv0.5.0 ssd 1541757169.017990112 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0008888888888888835}

:::MLPv0.5.0 ssd 1541757169.110092402 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0010666666666666602}

:::MLPv0.5.0 ssd 1541757169.212032557 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.001244444444444437}

:::MLPv0.5.0 ssd 1541757169.311310530 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0014222222222222136}

:::MLPv0.5.0 ssd 1541757169.409733295 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0015999999999999903}

:::MLPv0.5.0 ssd 1541757169.499552011 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.001777777777777767}

:::MLPv0.5.0 ssd 1541757169.597708225 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0019555555555555437}

:::MLPv0.5.0 ssd 1541757169.691109419 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.0021333333333333204}

:::MLPv0.5.0 ssd 1541757169.778194189 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.002311111111111097}

:::MLPv0.5.0 ssd 1541757169.872115612 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.002488888888888874}

:::MLPv0.5.0 ssd 1541757169.963740110 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.0026666666666666505}

:::MLPv0.5.0 ssd 1541757170.055685997 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.0028444444444444272}

:::MLPv0.5.0 ssd 1541757170.146903992 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.0030222222222222317}

:::MLPv0.5.0 ssd 1541757170.236824512 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.0032000000000000084}

:::MLPv0.5.0 ssd 1541757170.327790737 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.003377777777777785}

:::MLPv0.5.0 ssd 1541757170.418332815 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.003555555555555562}
Iteration:     20, Loss function: 20.228, Average Loss: 0.437, avg. samples / sec: 8950.36
Iteration:     20, Loss function: 20.494, Average Loss: 0.439, avg. samples / sec: 8949.74
Iteration:     20, Loss function: 20.056, Average Loss: 0.441, avg. samples / sec: 8949.51
Iteration:     20, Loss function: 20.662, Average Loss: 0.440, avg. samples / sec: 8949.73
Iteration:     20, Loss function: 20.849, Average Loss: 0.442, avg. samples / sec: 8948.56
Iteration:     20, Loss function: 20.554, Average Loss: 0.439, avg. samples / sec: 8946.39
Iteration:     20, Loss function: 20.462, Average Loss: 0.439, avg. samples / sec: 8944.76
Iteration:     20, Loss function: 20.916, Average Loss: 0.441, avg. samples / sec: 8938.81

:::MLPv0.5.0 ssd 1541757170.508908987 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.0037333333333333385}

:::MLPv0.5.0 ssd 1541757170.600291252 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.003911111111111115}

:::MLPv0.5.0 ssd 1541757170.694751978 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.004088888888888892}

:::MLPv0.5.0 ssd 1541757170.783870459 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.004266666666666669}

:::MLPv0.5.0 ssd 1541757170.881377697 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.004444444444444445}

:::MLPv0.5.0 ssd 1541757170.972321510 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.004622222222222222}

:::MLPv0.5.0 ssd 1541757171.069904566 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.004799999999999999}

:::MLPv0.5.0 ssd 1541757171.158489943 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.004977777777777775}

:::MLPv0.5.0 ssd 1541757171.244891644 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.005155555555555552}

:::MLPv0.5.0 ssd 1541757171.333657026 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.005333333333333329}

:::MLPv0.5.0 ssd 1541757171.424475670 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.0055111111111111055}

:::MLPv0.5.0 ssd 1541757171.515162230 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.005688888888888882}

:::MLPv0.5.0 ssd 1541757171.605336666 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.005866666666666659}

:::MLPv0.5.0 ssd 1541757171.694383144 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.006044444444444436}

:::MLPv0.5.0 ssd 1541757171.790798187 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.006222222222222212}

:::MLPv0.5.0 ssd 1541757171.879028082 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.006399999999999989}

:::MLPv0.5.0 ssd 1541757171.966260910 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.006577777777777766}

:::MLPv0.5.0 ssd 1541757172.058787107 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.0067555555555555424}

:::MLPv0.5.0 ssd 1541757172.147409678 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.006933333333333319}

:::MLPv0.5.0 ssd 1541757172.234782219 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.007111111111111096}
Iteration:     40, Loss function: 15.561, Average Loss: 0.811, avg. samples / sec: 22635.55
Iteration:     40, Loss function: 15.005, Average Loss: 0.806, avg. samples / sec: 22559.06
Iteration:     40, Loss function: 15.526, Average Loss: 0.808, avg. samples / sec: 22574.52
Iteration:     40, Loss function: 15.218, Average Loss: 0.801, avg. samples / sec: 22580.02
Iteration:     40, Loss function: 15.450, Average Loss: 0.809, avg. samples / sec: 22548.30
Iteration:     40, Loss function: 15.788, Average Loss: 0.813, avg. samples / sec: 22540.94
Iteration:     40, Loss function: 15.264, Average Loss: 0.809, avg. samples / sec: 22526.48
Iteration:     40, Loss function: 15.767, Average Loss: 0.806, avg. samples / sec: 22528.60

:::MLPv0.5.0 ssd 1541757172.321823835 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.0072888888888888725}

:::MLPv0.5.0 ssd 1541757172.408252239 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.007466666666666649}

:::MLPv0.5.0 ssd 1541757172.500758886 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.007644444444444454}

:::MLPv0.5.0 ssd 1541757172.590740204 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.00782222222222223}

:::MLPv0.5.0 ssd 1541757172.680098057 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.008000000000000007}

:::MLPv0.5.0 ssd 1541757172.769512177 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.008177777777777784}

:::MLPv0.5.0 ssd 1541757172.860515594 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.00835555555555556}

:::MLPv0.5.0 ssd 1541757172.947970629 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.008533333333333337}

:::MLPv0.5.0 ssd 1541757173.035083055 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.008711111111111114}

:::MLPv0.5.0 ssd 1541757173.120167971 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.00888888888888889}

:::MLPv0.5.0 ssd 1541757173.207614899 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.009066666666666667}

:::MLPv0.5.0 ssd 1541757173.295116186 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.009244444444444444}

:::MLPv0.5.0 ssd 1541757173.388778210 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.00942222222222222}

:::MLPv0.5.0 ssd 1541757173.480576992 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.009599999999999997}

:::MLPv0.5.0 ssd 1541757173.565162420 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.009777777777777774}

:::MLPv0.5.0 ssd 1541757173.650950193 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.00995555555555555}

:::MLPv0.5.0 ssd 1541757173.739111423 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.010133333333333328}

:::MLPv0.5.0 ssd 1541757173.827002525 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1541757173.831123352 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 58, "value": 0.010311111111111104}

:::MLPv0.5.0 ssd 1541757173.916913986 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 59, "value": 0.010488888888888881}

:::MLPv0.5.0 ssd 1541757174.003007412 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 60, "value": 0.010666666666666658}
Iteration:     60, Loss function: 10.654, Average Loss: 1.050, avg. samples / sec: 23177.48
Iteration:     60, Loss function: 10.410, Average Loss: 1.050, avg. samples / sec: 23171.63
Iteration:     60, Loss function: 10.477, Average Loss: 1.056, avg. samples / sec: 23185.97
Iteration:     60, Loss function: 10.178, Average Loss: 1.054, avg. samples / sec: 23155.97
Iteration:     60, Loss function: 10.526, Average Loss: 1.052, avg. samples / sec: 23164.85
Iteration:     60, Loss function: 10.698, Average Loss: 1.048, avg. samples / sec: 23149.19
Iteration:     60, Loss function: 10.545, Average Loss: 1.052, avg. samples / sec: 23175.19
Iteration:     60, Loss function: 10.161, Average Loss: 1.051, avg. samples / sec: 23167.21

:::MLPv0.5.0 ssd 1541757174.088807344 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 61, "value": 0.010844444444444434}

:::MLPv0.5.0 ssd 1541757174.175606251 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 62, "value": 0.011022222222222211}

:::MLPv0.5.0 ssd 1541757174.263026953 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 63, "value": 0.011199999999999988}

:::MLPv0.5.0 ssd 1541757174.348172903 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 64, "value": 0.011377777777777764}

:::MLPv0.5.0 ssd 1541757174.434201956 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 65, "value": 0.011555555555555541}

:::MLPv0.5.0 ssd 1541757174.523273945 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 66, "value": 0.011733333333333318}

:::MLPv0.5.0 ssd 1541757174.608659983 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 67, "value": 0.011911111111111095}

:::MLPv0.5.0 ssd 1541757174.697172880 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 68, "value": 0.012088888888888899}

:::MLPv0.5.0 ssd 1541757174.784079075 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 69, "value": 0.012266666666666676}

:::MLPv0.5.0 ssd 1541757174.871078968 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 70, "value": 0.012444444444444452}

:::MLPv0.5.0 ssd 1541757174.956406832 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 71, "value": 0.012622222222222229}

:::MLPv0.5.0 ssd 1541757175.043983221 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 72, "value": 0.012800000000000006}

:::MLPv0.5.0 ssd 1541757175.130385637 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 73, "value": 0.012977777777777783}

:::MLPv0.5.0 ssd 1541757175.216077089 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 74, "value": 0.01315555555555556}

:::MLPv0.5.0 ssd 1541757175.302772522 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 75, "value": 0.013333333333333336}

:::MLPv0.5.0 ssd 1541757175.398486376 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 76, "value": 0.013511111111111113}

:::MLPv0.5.0 ssd 1541757175.486811876 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 77, "value": 0.01368888888888889}

:::MLPv0.5.0 ssd 1541757175.583232164 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 78, "value": 0.013866666666666666}

:::MLPv0.5.0 ssd 1541757175.668848038 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 79, "value": 0.014044444444444443}

:::MLPv0.5.0 ssd 1541757175.753418684 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 80, "value": 0.01422222222222222}
Iteration:     80, Loss function: 9.831, Average Loss: 1.240, avg. samples / sec: 23394.97
Iteration:     80, Loss function: 9.662, Average Loss: 1.237, avg. samples / sec: 23423.00
Iteration:     80, Loss function: 9.082, Average Loss: 1.241, avg. samples / sec: 23400.75
Iteration:     80, Loss function: 9.520, Average Loss: 1.243, avg. samples / sec: 23414.66
Iteration:     80, Loss function: 9.577, Average Loss: 1.242, avg. samples / sec: 23401.34
Iteration:     80, Loss function: 9.898, Average Loss: 1.244, avg. samples / sec: 23391.65
Iteration:     80, Loss function: 9.452, Average Loss: 1.235, avg. samples / sec: 23395.99
Iteration:     80, Loss function: 9.196, Average Loss: 1.238, avg. samples / sec: 23374.56

:::MLPv0.5.0 ssd 1541757175.838612556 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 81, "value": 0.014399999999999996}

:::MLPv0.5.0 ssd 1541757175.923531532 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 82, "value": 0.014577777777777773}

:::MLPv0.5.0 ssd 1541757176.009579182 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 83, "value": 0.01475555555555555}

:::MLPv0.5.0 ssd 1541757176.097042322 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 84, "value": 0.014933333333333326}

:::MLPv0.5.0 ssd 1541757176.184414148 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 85, "value": 0.015111111111111103}

:::MLPv0.5.0 ssd 1541757176.272587061 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 86, "value": 0.01528888888888888}

:::MLPv0.5.0 ssd 1541757176.358086109 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 87, "value": 0.015466666666666656}

:::MLPv0.5.0 ssd 1541757176.443988800 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 88, "value": 0.015644444444444433}

:::MLPv0.5.0 ssd 1541757176.533440351 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 89, "value": 0.01582222222222221}

:::MLPv0.5.0 ssd 1541757176.620201826 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 90, "value": 0.015999999999999986}

:::MLPv0.5.0 ssd 1541757176.703799725 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 91, "value": 0.016177777777777763}

:::MLPv0.5.0 ssd 1541757176.791602135 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 92, "value": 0.01635555555555554}

:::MLPv0.5.0 ssd 1541757176.876005888 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 93, "value": 0.016533333333333317}

:::MLPv0.5.0 ssd 1541757176.961707830 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 94, "value": 0.01671111111111112}

:::MLPv0.5.0 ssd 1541757177.048423529 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 95, "value": 0.016888888888888898}

:::MLPv0.5.0 ssd 1541757177.133266926 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 96, "value": 0.017066666666666674}

:::MLPv0.5.0 ssd 1541757177.220081806 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 97, "value": 0.01724444444444445}

:::MLPv0.5.0 ssd 1541757177.304197311 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 98, "value": 0.017422222222222228}

:::MLPv0.5.0 ssd 1541757177.392364502 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 99, "value": 0.017600000000000005}

:::MLPv0.5.0 ssd 1541757177.476698875 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 100, "value": 0.01777777777777778}
Iteration:    100, Loss function: 9.228, Average Loss: 1.401, avg. samples / sec: 23785.09
Iteration:    100, Loss function: 9.318, Average Loss: 1.398, avg. samples / sec: 23777.48
Iteration:    100, Loss function: 9.783, Average Loss: 1.401, avg. samples / sec: 23797.25
Iteration:    100, Loss function: 9.133, Average Loss: 1.405, avg. samples / sec: 23771.23
Iteration:    100, Loss function: 9.436, Average Loss: 1.406, avg. samples / sec: 23768.20
Iteration:    100, Loss function: 9.673, Average Loss: 1.406, avg. samples / sec: 23763.59
Iteration:    100, Loss function: 9.564, Average Loss: 1.404, avg. samples / sec: 23749.92
Iteration:    100, Loss function: 9.558, Average Loss: 1.397, avg. samples / sec: 23757.40

:::MLPv0.5.0 ssd 1541757177.562553644 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 101, "value": 0.017955555555555558}

:::MLPv0.5.0 ssd 1541757177.648707867 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 102, "value": 0.018133333333333335}

:::MLPv0.5.0 ssd 1541757177.733082771 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 103, "value": 0.01831111111111111}

:::MLPv0.5.0 ssd 1541757177.818752289 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 104, "value": 0.018488888888888888}

:::MLPv0.5.0 ssd 1541757177.906059980 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 105, "value": 0.018666666666666665}

:::MLPv0.5.0 ssd 1541757177.991086960 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 106, "value": 0.01884444444444444}

:::MLPv0.5.0 ssd 1541757178.077275038 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 107, "value": 0.019022222222222218}

:::MLPv0.5.0 ssd 1541757178.161259413 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 108, "value": 0.019199999999999995}

:::MLPv0.5.0 ssd 1541757178.245151758 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 109, "value": 0.01937777777777777}

:::MLPv0.5.0 ssd 1541757178.334594488 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 110, "value": 0.019555555555555548}

:::MLPv0.5.0 ssd 1541757178.421409369 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 111, "value": 0.019733333333333325}

:::MLPv0.5.0 ssd 1541757178.507624626 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 112, "value": 0.0199111111111111}

:::MLPv0.5.0 ssd 1541757178.591832638 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 113, "value": 0.02008888888888888}

:::MLPv0.5.0 ssd 1541757178.675791025 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 114, "value": 0.020266666666666655}

:::MLPv0.5.0 ssd 1541757178.765035868 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 115, "value": 0.020444444444444432}

:::MLPv0.5.0 ssd 1541757178.849555731 (train.py:553) train_epoch: 2

:::MLPv0.5.0 ssd 1541757178.853759766 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 116, "value": 0.02062222222222221}

:::MLPv0.5.0 ssd 1541757178.938862562 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 117, "value": 0.020799999999999985}

:::MLPv0.5.0 ssd 1541757179.024446011 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 118, "value": 0.020977777777777762}

:::MLPv0.5.0 ssd 1541757179.108766317 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 119, "value": 0.02115555555555554}

:::MLPv0.5.0 ssd 1541757179.198718071 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 120, "value": 0.021333333333333343}
Iteration:    120, Loss function: 8.513, Average Loss: 1.552, avg. samples / sec: 23788.39
Iteration:    120, Loss function: 8.317, Average Loss: 1.550, avg. samples / sec: 23781.90
Iteration:    120, Loss function: 8.713, Average Loss: 1.559, avg. samples / sec: 23800.87
Iteration:    120, Loss function: 8.946, Average Loss: 1.551, avg. samples / sec: 23768.35
Iteration:    120, Loss function: 8.498, Average Loss: 1.549, avg. samples / sec: 23808.92
Iteration:    120, Loss function: 9.092, Average Loss: 1.555, avg. samples / sec: 23774.38
Iteration:    120, Loss function: 9.316, Average Loss: 1.557, avg. samples / sec: 23781.71
Iteration:    120, Loss function: 9.242, Average Loss: 1.557, avg. samples / sec: 23748.19

:::MLPv0.5.0 ssd 1541757179.283425808 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 121, "value": 0.02151111111111112}

:::MLPv0.5.0 ssd 1541757179.369069576 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 122, "value": 0.021688888888888896}

:::MLPv0.5.0 ssd 1541757179.455537081 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 123, "value": 0.021866666666666673}

:::MLPv0.5.0 ssd 1541757179.539555788 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 124, "value": 0.02204444444444445}

:::MLPv0.5.0 ssd 1541757179.625945330 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 125, "value": 0.022222222222222227}

:::MLPv0.5.0 ssd 1541757179.711459875 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 126, "value": 0.022400000000000003}

:::MLPv0.5.0 ssd 1541757179.795187712 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 127, "value": 0.02257777777777778}

:::MLPv0.5.0 ssd 1541757179.881245613 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 128, "value": 0.022755555555555557}

:::MLPv0.5.0 ssd 1541757179.967251539 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 129, "value": 0.022933333333333333}

:::MLPv0.5.0 ssd 1541757180.051693916 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 130, "value": 0.02311111111111111}

:::MLPv0.5.0 ssd 1541757180.139238834 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 131, "value": 0.023288888888888887}

:::MLPv0.5.0 ssd 1541757180.223690748 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 132, "value": 0.023466666666666663}

:::MLPv0.5.0 ssd 1541757180.309645414 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 133, "value": 0.02364444444444444}

:::MLPv0.5.0 ssd 1541757180.394341707 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 134, "value": 0.023822222222222217}

:::MLPv0.5.0 ssd 1541757180.477733850 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 135, "value": 0.023999999999999994}

:::MLPv0.5.0 ssd 1541757180.562514782 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 136, "value": 0.02417777777777777}

:::MLPv0.5.0 ssd 1541757180.650477886 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 137, "value": 0.024355555555555547}

:::MLPv0.5.0 ssd 1541757180.736500740 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 138, "value": 0.024533333333333324}

:::MLPv0.5.0 ssd 1541757180.820273876 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 139, "value": 0.0247111111111111}

:::MLPv0.5.0 ssd 1541757180.906615973 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 140, "value": 0.024888888888888877}
Iteration:    140, Loss function: 8.763, Average Loss: 1.691, avg. samples / sec: 23992.21
Iteration:    140, Loss function: 9.335, Average Loss: 1.696, avg. samples / sec: 23995.90
Iteration:    140, Loss function: 9.509, Average Loss: 1.700, avg. samples / sec: 24006.70
Iteration:    140, Loss function: 9.772, Average Loss: 1.701, avg. samples / sec: 23985.95
Iteration:    140, Loss function: 8.738, Average Loss: 1.699, avg. samples / sec: 24010.12
Iteration:    140, Loss function: 8.869, Average Loss: 1.698, avg. samples / sec: 24029.21
Iteration:    140, Loss function: 9.185, Average Loss: 1.693, avg. samples / sec: 23968.79
Iteration:    140, Loss function: 8.889, Average Loss: 1.695, avg. samples / sec: 23944.92

:::MLPv0.5.0 ssd 1541757180.992933989 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 141, "value": 0.025066666666666654}

:::MLPv0.5.0 ssd 1541757181.077592850 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 142, "value": 0.02524444444444443}

:::MLPv0.5.0 ssd 1541757181.163885355 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 143, "value": 0.025422222222222207}

:::MLPv0.5.0 ssd 1541757181.248798609 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 144, "value": 0.025599999999999984}

:::MLPv0.5.0 ssd 1541757181.332879066 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 145, "value": 0.02577777777777779}

:::MLPv0.5.0 ssd 1541757181.416794538 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 146, "value": 0.025955555555555565}

:::MLPv0.5.0 ssd 1541757181.500471354 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 147, "value": 0.026133333333333342}

:::MLPv0.5.0 ssd 1541757181.584581375 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 148, "value": 0.02631111111111112}

:::MLPv0.5.0 ssd 1541757181.670073748 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 149, "value": 0.026488888888888895}

:::MLPv0.5.0 ssd 1541757181.754381418 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 150, "value": 0.026666666666666672}

:::MLPv0.5.0 ssd 1541757181.838442802 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 151, "value": 0.02684444444444445}

:::MLPv0.5.0 ssd 1541757181.923282146 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 152, "value": 0.027022222222222225}

:::MLPv0.5.0 ssd 1541757182.010121584 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 153, "value": 0.027200000000000002}

:::MLPv0.5.0 ssd 1541757182.095172405 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 154, "value": 0.02737777777777778}

:::MLPv0.5.0 ssd 1541757182.180592060 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 155, "value": 0.027555555555555555}

:::MLPv0.5.0 ssd 1541757182.266173363 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 156, "value": 0.027733333333333332}

:::MLPv0.5.0 ssd 1541757182.350245953 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 157, "value": 0.02791111111111111}

:::MLPv0.5.0 ssd 1541757182.434237957 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 158, "value": 0.028088888888888885}

:::MLPv0.5.0 ssd 1541757182.518552065 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 159, "value": 0.028266666666666662}

:::MLPv0.5.0 ssd 1541757182.602010965 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 160, "value": 0.02844444444444444}
Iteration:    160, Loss function: 8.249, Average Loss: 1.834, avg. samples / sec: 24156.98
Iteration:    160, Loss function: 8.300, Average Loss: 1.837, avg. samples / sec: 24160.11
Iteration:    160, Loss function: 7.878, Average Loss: 1.837, avg. samples / sec: 24156.65
Iteration:    160, Loss function: 8.482, Average Loss: 1.831, avg. samples / sec: 24180.66
Iteration:    160, Loss function: 8.258, Average Loss: 1.832, avg. samples / sec: 24189.92
Iteration:    160, Loss function: 7.921, Average Loss: 1.838, avg. samples / sec: 24152.17
Iteration:    160, Loss function: 8.385, Average Loss: 1.828, avg. samples / sec: 24125.93
Iteration:    160, Loss function: 8.731, Average Loss: 1.835, avg. samples / sec: 24123.78

:::MLPv0.5.0 ssd 1541757182.686925173 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 161, "value": 0.028622222222222216}

:::MLPv0.5.0 ssd 1541757182.772302866 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 162, "value": 0.028799999999999992}

:::MLPv0.5.0 ssd 1541757182.856761694 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 163, "value": 0.02897777777777777}

:::MLPv0.5.0 ssd 1541757182.942640543 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 164, "value": 0.029155555555555546}

:::MLPv0.5.0 ssd 1541757183.029599190 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 165, "value": 0.029333333333333322}

:::MLPv0.5.0 ssd 1541757183.113056421 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 166, "value": 0.0295111111111111}

:::MLPv0.5.0 ssd 1541757183.197486639 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 167, "value": 0.029688888888888876}

:::MLPv0.5.0 ssd 1541757183.282184124 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 168, "value": 0.029866666666666652}

:::MLPv0.5.0 ssd 1541757183.365926743 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 169, "value": 0.03004444444444443}

:::MLPv0.5.0 ssd 1541757183.451573610 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 170, "value": 0.030222222222222206}

:::MLPv0.5.0 ssd 1541757183.535707235 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 171, "value": 0.03040000000000001}

:::MLPv0.5.0 ssd 1541757183.619951963 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 172, "value": 0.030577777777777787}

:::MLPv0.5.0 ssd 1541757183.704311371 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 173, "value": 0.030755555555555564}

:::MLPv0.5.0 ssd 1541757183.785442829 (train.py:553) train_epoch: 3

:::MLPv0.5.0 ssd 1541757183.789624929 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 174, "value": 0.03093333333333334}

:::MLPv0.5.0 ssd 1541757183.878220558 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 175, "value": 0.031111111111111117}

:::MLPv0.5.0 ssd 1541757183.964774609 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 176, "value": 0.031288888888888894}

:::MLPv0.5.0 ssd 1541757184.049195051 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 177, "value": 0.03146666666666667}

:::MLPv0.5.0 ssd 1541757184.134262562 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 178, "value": 0.03164444444444445}

:::MLPv0.5.0 ssd 1541757184.218625069 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 179, "value": 0.031822222222222224}

:::MLPv0.5.0 ssd 1541757184.303367138 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 180, "value": 0.032}
Iteration:    180, Loss function: 7.842, Average Loss: 1.962, avg. samples / sec: 24079.06
Iteration:    180, Loss function: 8.157, Average Loss: 1.955, avg. samples / sec: 24100.84
Iteration:    180, Loss function: 7.885, Average Loss: 1.963, avg. samples / sec: 24069.62
Iteration:    180, Loss function: 8.687, Average Loss: 1.959, avg. samples / sec: 24061.16
Iteration:    180, Loss function: 7.899, Average Loss: 1.963, avg. samples / sec: 24063.04
Iteration:    180, Loss function: 7.325, Average Loss: 1.955, avg. samples / sec: 24032.45
Iteration:    180, Loss function: 7.898, Average Loss: 1.963, avg. samples / sec: 24021.18
Iteration:    180, Loss function: 7.793, Average Loss: 1.960, avg. samples / sec: 24064.85

:::MLPv0.5.0 ssd 1541757184.388298988 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 181, "value": 0.03217777777777778}

:::MLPv0.5.0 ssd 1541757184.472944736 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 182, "value": 0.032355555555555554}

:::MLPv0.5.0 ssd 1541757184.557675838 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 183, "value": 0.03253333333333333}

:::MLPv0.5.0 ssd 1541757184.641765356 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 184, "value": 0.03271111111111111}

:::MLPv0.5.0 ssd 1541757184.725639820 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 185, "value": 0.032888888888888884}

:::MLPv0.5.0 ssd 1541757184.809955835 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 186, "value": 0.03306666666666666}

:::MLPv0.5.0 ssd 1541757184.895922899 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 187, "value": 0.03324444444444444}

:::MLPv0.5.0 ssd 1541757184.981223822 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 188, "value": 0.033422222222222214}

:::MLPv0.5.0 ssd 1541757185.065136194 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 189, "value": 0.03359999999999999}

:::MLPv0.5.0 ssd 1541757185.149917364 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 190, "value": 0.03377777777777777}

:::MLPv0.5.0 ssd 1541757185.233968973 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 191, "value": 0.033955555555555544}

:::MLPv0.5.0 ssd 1541757185.317302227 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 192, "value": 0.03413333333333332}

:::MLPv0.5.0 ssd 1541757185.403321505 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 193, "value": 0.0343111111111111}

:::MLPv0.5.0 ssd 1541757185.486816168 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 194, "value": 0.034488888888888874}

:::MLPv0.5.0 ssd 1541757185.573267221 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 195, "value": 0.03466666666666665}

:::MLPv0.5.0 ssd 1541757185.657253742 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 196, "value": 0.03484444444444443}

:::MLPv0.5.0 ssd 1541757185.742533207 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 197, "value": 0.03502222222222222}

:::MLPv0.5.0 ssd 1541757185.826549292 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 198, "value": 0.035199999999999995}

:::MLPv0.5.0 ssd 1541757185.910654545 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 199, "value": 0.03537777777777777}

:::MLPv0.5.0 ssd 1541757185.994273424 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 200, "value": 0.03555555555555555}
Iteration:    200, Loss function: 8.248, Average Loss: 2.080, avg. samples / sec: 24249.81
Iteration:    200, Loss function: 8.131, Average Loss: 2.075, avg. samples / sec: 24275.00
Iteration:    200, Loss function: 8.775, Average Loss: 2.078, avg. samples / sec: 24227.08
Iteration:    200, Loss function: 7.892, Average Loss: 2.081, avg. samples / sec: 24273.05
Iteration:    200, Loss function: 8.278, Average Loss: 2.082, avg. samples / sec: 24235.50
Iteration:    200, Loss function: 8.218, Average Loss: 2.082, avg. samples / sec: 24266.39
Iteration:    200, Loss function: 8.123, Average Loss: 2.082, avg. samples / sec: 24210.44
Iteration:    200, Loss function: 8.449, Average Loss: 2.084, avg. samples / sec: 24178.51

:::MLPv0.5.0 ssd 1541757186.078830004 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 201, "value": 0.035733333333333325}

:::MLPv0.5.0 ssd 1541757186.162787437 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 202, "value": 0.0359111111111111}

:::MLPv0.5.0 ssd 1541757186.247321367 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 203, "value": 0.03608888888888889}

:::MLPv0.5.0 ssd 1541757186.334396601 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 204, "value": 0.03626666666666667}

:::MLPv0.5.0 ssd 1541757186.420073032 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 205, "value": 0.036444444444444446}

:::MLPv0.5.0 ssd 1541757186.503818989 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 206, "value": 0.03662222222222222}

:::MLPv0.5.0 ssd 1541757186.587578773 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 207, "value": 0.0368}

:::MLPv0.5.0 ssd 1541757186.671544075 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 208, "value": 0.036977777777777776}

:::MLPv0.5.0 ssd 1541757186.755340576 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 209, "value": 0.03715555555555555}

:::MLPv0.5.0 ssd 1541757186.840106487 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 210, "value": 0.03733333333333333}

:::MLPv0.5.0 ssd 1541757186.924028873 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 211, "value": 0.037511111111111106}

:::MLPv0.5.0 ssd 1541757187.008323669 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 212, "value": 0.03768888888888888}

:::MLPv0.5.0 ssd 1541757187.093594313 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 213, "value": 0.03786666666666666}

:::MLPv0.5.0 ssd 1541757187.178605318 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 214, "value": 0.038044444444444436}

:::MLPv0.5.0 ssd 1541757187.263405085 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 215, "value": 0.03822222222222221}

:::MLPv0.5.0 ssd 1541757187.347852468 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 216, "value": 0.038400000000000004}

:::MLPv0.5.0 ssd 1541757187.431937695 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 217, "value": 0.03857777777777778}

:::MLPv0.5.0 ssd 1541757187.518723488 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 218, "value": 0.03875555555555556}

:::MLPv0.5.0 ssd 1541757187.603597879 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 219, "value": 0.038933333333333334}

:::MLPv0.5.0 ssd 1541757187.687397718 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 220, "value": 0.03911111111111111}
Iteration:    220, Loss function: 8.123, Average Loss: 2.198, avg. samples / sec: 24209.80
Iteration:    220, Loss function: 8.173, Average Loss: 2.190, avg. samples / sec: 24180.98
Iteration:    220, Loss function: 7.835, Average Loss: 2.198, avg. samples / sec: 24194.81
Iteration:    220, Loss function: 8.350, Average Loss: 2.198, avg. samples / sec: 24195.93
Iteration:    220, Loss function: 8.158, Average Loss: 2.193, avg. samples / sec: 24176.05
Iteration:    220, Loss function: 7.824, Average Loss: 2.198, avg. samples / sec: 24161.22
Iteration:    220, Loss function: 8.252, Average Loss: 2.198, avg. samples / sec: 24169.06
Iteration:    220, Loss function: 7.813, Average Loss: 2.201, avg. samples / sec: 24184.00

:::MLPv0.5.0 ssd 1541757187.771616459 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 221, "value": 0.03928888888888889}

:::MLPv0.5.0 ssd 1541757187.855723381 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 222, "value": 0.039466666666666664}

:::MLPv0.5.0 ssd 1541757187.943923235 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 223, "value": 0.03964444444444444}

:::MLPv0.5.0 ssd 1541757188.027782440 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 224, "value": 0.03982222222222222}

:::MLPv0.5.0 ssd 1541757188.111677408 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 225, "value": 0.039999999999999994}

:::MLPv0.5.0 ssd 1541757188.200053692 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 226, "value": 0.04017777777777777}

:::MLPv0.5.0 ssd 1541757188.284849167 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 227, "value": 0.04035555555555555}

:::MLPv0.5.0 ssd 1541757188.369180441 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 228, "value": 0.04053333333333334}

:::MLPv0.5.0 ssd 1541757188.453276873 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 229, "value": 0.040711111111111115}

:::MLPv0.5.0 ssd 1541757188.536837339 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 230, "value": 0.04088888888888889}

:::MLPv0.5.0 ssd 1541757188.621199369 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 231, "value": 0.04106666666666667}

:::MLPv0.5.0 ssd 1541757188.703367710 (train.py:553) train_epoch: 4

:::MLPv0.5.0 ssd 1541757188.707473516 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 232, "value": 0.041244444444444445}

:::MLPv0.5.0 ssd 1541757188.794103861 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 233, "value": 0.04142222222222222}

:::MLPv0.5.0 ssd 1541757188.878178120 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 234, "value": 0.0416}

:::MLPv0.5.0 ssd 1541757188.962371111 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 235, "value": 0.041777777777777775}

:::MLPv0.5.0 ssd 1541757189.046582222 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 236, "value": 0.04195555555555555}

:::MLPv0.5.0 ssd 1541757189.130517483 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 237, "value": 0.04213333333333333}

:::MLPv0.5.0 ssd 1541757189.214796066 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 238, "value": 0.042311111111111105}

:::MLPv0.5.0 ssd 1541757189.298774719 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 239, "value": 0.04248888888888888}

:::MLPv0.5.0 ssd 1541757189.383878946 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 240, "value": 0.04266666666666666}
Iteration:    240, Loss function: 7.671, Average Loss: 2.312, avg. samples / sec: 24199.66
Iteration:    240, Loss function: 7.665, Average Loss: 2.304, avg. samples / sec: 24152.20
Iteration:    240, Loss function: 7.492, Average Loss: 2.308, avg. samples / sec: 24152.65
Iteration:    240, Loss function: 7.900, Average Loss: 2.310, avg. samples / sec: 24168.25
Iteration:    240, Loss function: 7.636, Average Loss: 2.306, avg. samples / sec: 24147.01
Iteration:    240, Loss function: 8.253, Average Loss: 2.310, avg. samples / sec: 24140.30
Iteration:    240, Loss function: 7.879, Average Loss: 2.311, avg. samples / sec: 24111.83
Iteration:    240, Loss function: 8.004, Average Loss: 2.311, avg. samples / sec: 24134.79

:::MLPv0.5.0 ssd 1541757189.467991352 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 241, "value": 0.04284444444444445}

:::MLPv0.5.0 ssd 1541757189.554361343 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 242, "value": 0.043022222222222226}

:::MLPv0.5.0 ssd 1541757189.639755249 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 243, "value": 0.0432}

:::MLPv0.5.0 ssd 1541757189.723811388 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 244, "value": 0.04337777777777778}

:::MLPv0.5.0 ssd 1541757189.808316231 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 245, "value": 0.043555555555555556}

:::MLPv0.5.0 ssd 1541757189.892538309 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 246, "value": 0.04373333333333333}

:::MLPv0.5.0 ssd 1541757189.976250410 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 247, "value": 0.04391111111111111}

:::MLPv0.5.0 ssd 1541757190.061328173 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 248, "value": 0.044088888888888886}

:::MLPv0.5.0 ssd 1541757190.145454645 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 249, "value": 0.04426666666666666}

:::MLPv0.5.0 ssd 1541757190.229321957 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 250, "value": 0.04444444444444444}

:::MLPv0.5.0 ssd 1541757190.313169003 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 251, "value": 0.044622222222222216}

:::MLPv0.5.0 ssd 1541757190.397547483 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 252, "value": 0.04479999999999999}

:::MLPv0.5.0 ssd 1541757190.481445312 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 253, "value": 0.04497777777777777}

:::MLPv0.5.0 ssd 1541757190.565060139 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 254, "value": 0.04515555555555556}

:::MLPv0.5.0 ssd 1541757190.649362326 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 255, "value": 0.04533333333333334}

:::MLPv0.5.0 ssd 1541757190.734445095 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 256, "value": 0.04551111111111111}

:::MLPv0.5.0 ssd 1541757190.819511890 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 257, "value": 0.04568888888888889}

:::MLPv0.5.0 ssd 1541757190.903704166 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 258, "value": 0.04586666666666667}

:::MLPv0.5.0 ssd 1541757190.988467216 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 259, "value": 0.04604444444444444}

:::MLPv0.5.0 ssd 1541757191.072484255 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 260, "value": 0.04622222222222222}
Iteration:    260, Loss function: 7.167, Average Loss: 2.415, avg. samples / sec: 24295.32
Iteration:    260, Loss function: 7.127, Average Loss: 2.412, avg. samples / sec: 24264.92
Iteration:    260, Loss function: 7.509, Average Loss: 2.414, avg. samples / sec: 24259.00
Iteration:    260, Loss function: 7.275, Average Loss: 2.413, avg. samples / sec: 24261.65
Iteration:    260, Loss function: 7.456, Average Loss: 2.418, avg. samples / sec: 24289.37
Iteration:    260, Loss function: 7.771, Average Loss: 2.409, avg. samples / sec: 24233.93
Iteration:    260, Loss function: 7.567, Average Loss: 2.412, avg. samples / sec: 24246.96
Iteration:    260, Loss function: 8.277, Average Loss: 2.415, avg. samples / sec: 24209.76

:::MLPv0.5.0 ssd 1541757191.158039331 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 261, "value": 0.0464}

:::MLPv0.5.0 ssd 1541757191.244249582 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 262, "value": 0.046577777777777774}

:::MLPv0.5.0 ssd 1541757191.329128981 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 263, "value": 0.04675555555555555}

:::MLPv0.5.0 ssd 1541757191.413611412 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 264, "value": 0.04693333333333333}

:::MLPv0.5.0 ssd 1541757191.497506380 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 265, "value": 0.047111111111111104}

:::MLPv0.5.0 ssd 1541757191.583613157 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 266, "value": 0.04728888888888888}

:::MLPv0.5.0 ssd 1541757191.667015076 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 267, "value": 0.04746666666666667}

:::MLPv0.5.0 ssd 1541757191.750864983 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 268, "value": 0.04764444444444445}

:::MLPv0.5.0 ssd 1541757191.835232973 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 269, "value": 0.047822222222222224}

:::MLPv0.5.0 ssd 1541757191.919260263 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 270, "value": 0.048}

:::MLPv0.5.0 ssd 1541757192.003308058 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 271, "value": 0.04817777777777778}

:::MLPv0.5.0 ssd 1541757192.087148190 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 272, "value": 0.048355555555555554}

:::MLPv0.5.0 ssd 1541757192.170853853 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 273, "value": 0.04853333333333333}

:::MLPv0.5.0 ssd 1541757192.254426718 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 274, "value": 0.04871111111111111}

:::MLPv0.5.0 ssd 1541757192.338674068 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 275, "value": 0.048888888888888885}

:::MLPv0.5.0 ssd 1541757192.422961473 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 276, "value": 0.04906666666666666}

:::MLPv0.5.0 ssd 1541757192.507370234 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 277, "value": 0.04924444444444444}

:::MLPv0.5.0 ssd 1541757192.592336178 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 278, "value": 0.049422222222222215}

:::MLPv0.5.0 ssd 1541757192.676538944 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 279, "value": 0.04959999999999999}

:::MLPv0.5.0 ssd 1541757192.760580540 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 280, "value": 0.04977777777777778}
Iteration:    280, Loss function: 7.604, Average Loss: 2.516, avg. samples / sec: 24293.40
Iteration:    280, Loss function: 7.397, Average Loss: 2.508, avg. samples / sec: 24261.69
Iteration:    280, Loss function: 6.740, Average Loss: 2.505, avg. samples / sec: 24291.63
Iteration:    280, Loss function: 7.300, Average Loss: 2.512, avg. samples / sec: 24266.56
Iteration:    280, Loss function: 6.619, Average Loss: 2.510, avg. samples / sec: 24244.78
Iteration:    280, Loss function: 7.153, Average Loss: 2.510, avg. samples / sec: 24262.67
Iteration:    280, Loss function: 6.870, Average Loss: 2.507, avg. samples / sec: 24268.56
Iteration:    280, Loss function: 6.709, Average Loss: 2.512, avg. samples / sec: 24258.29

:::MLPv0.5.0 ssd 1541757192.844935417 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 281, "value": 0.04995555555555556}

:::MLPv0.5.0 ssd 1541757192.929276943 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 282, "value": 0.050133333333333335}

:::MLPv0.5.0 ssd 1541757193.013186455 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 283, "value": 0.05031111111111111}

:::MLPv0.5.0 ssd 1541757193.097232103 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 284, "value": 0.05048888888888889}

:::MLPv0.5.0 ssd 1541757193.181607723 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 285, "value": 0.050666666666666665}

:::MLPv0.5.0 ssd 1541757193.265843153 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 286, "value": 0.05084444444444444}

:::MLPv0.5.0 ssd 1541757193.349838257 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 287, "value": 0.05102222222222222}

:::MLPv0.5.0 ssd 1541757193.433113098 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 288, "value": 0.051199999999999996}

:::MLPv0.5.0 ssd 1541757193.515048027 (train.py:553) train_epoch: 5

:::MLPv0.5.0 ssd 1541757193.519351959 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 289, "value": 0.05137777777777777}

:::MLPv0.5.0 ssd 1541757193.605501413 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 290, "value": 0.05155555555555555}

:::MLPv0.5.0 ssd 1541757193.689371586 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 291, "value": 0.051733333333333326}

:::MLPv0.5.0 ssd 1541757193.773332834 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 292, "value": 0.0519111111111111}

:::MLPv0.5.0 ssd 1541757193.857168913 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 293, "value": 0.05208888888888889}

:::MLPv0.5.0 ssd 1541757193.942537546 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 294, "value": 0.05226666666666667}

:::MLPv0.5.0 ssd 1541757194.026971340 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 295, "value": 0.052444444444444446}

:::MLPv0.5.0 ssd 1541757194.111014366 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 296, "value": 0.05262222222222222}

:::MLPv0.5.0 ssd 1541757194.195071697 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 297, "value": 0.0528}

:::MLPv0.5.0 ssd 1541757194.278884649 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 298, "value": 0.052977777777777776}

:::MLPv0.5.0 ssd 1541757194.362303734 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 299, "value": 0.05315555555555555}

:::MLPv0.5.0 ssd 1541757194.446505308 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 300, "value": 0.05333333333333333}
Iteration:    300, Loss function: 7.561, Average Loss: 2.599, avg. samples / sec: 24302.57
Iteration:    300, Loss function: 7.765, Average Loss: 2.608, avg. samples / sec: 24303.28
Iteration:    300, Loss function: 7.234, Average Loss: 2.605, avg. samples / sec: 24342.07
Iteration:    300, Loss function: 7.372, Average Loss: 2.603, avg. samples / sec: 24307.71
Iteration:    300, Loss function: 7.278, Average Loss: 2.606, avg. samples / sec: 24291.05
Iteration:    300, Loss function: 7.535, Average Loss: 2.609, avg. samples / sec: 24270.18
Iteration:    300, Loss function: 7.649, Average Loss: 2.604, avg. samples / sec: 24251.27
Iteration:    300, Loss function: 7.259, Average Loss: 2.604, avg. samples / sec: 24264.33

:::MLPv0.5.0 ssd 1541757194.531331301 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 301, "value": 0.053511111111111107}

:::MLPv0.5.0 ssd 1541757194.615384340 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 302, "value": 0.05368888888888888}

:::MLPv0.5.0 ssd 1541757194.701021910 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 303, "value": 0.05386666666666666}

:::MLPv0.5.0 ssd 1541757194.784749031 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 304, "value": 0.05404444444444444}

:::MLPv0.5.0 ssd 1541757194.869144917 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 305, "value": 0.05422222222222223}

:::MLPv0.5.0 ssd 1541757194.952779770 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 306, "value": 0.054400000000000004}

:::MLPv0.5.0 ssd 1541757195.037118673 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 307, "value": 0.05457777777777778}

:::MLPv0.5.0 ssd 1541757195.122199535 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 308, "value": 0.05475555555555556}

:::MLPv0.5.0 ssd 1541757195.206544638 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 309, "value": 0.054933333333333334}

:::MLPv0.5.0 ssd 1541757195.295032978 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 310, "value": 0.05511111111111111}

:::MLPv0.5.0 ssd 1541757195.378812790 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 311, "value": 0.05528888888888889}

:::MLPv0.5.0 ssd 1541757195.463363886 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 312, "value": 0.055466666666666664}

:::MLPv0.5.0 ssd 1541757195.547486782 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 313, "value": 0.05564444444444444}

:::MLPv0.5.0 ssd 1541757195.631674767 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 314, "value": 0.05582222222222222}

:::MLPv0.5.0 ssd 1541757195.715949297 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 315, "value": 0.055999999999999994}

:::MLPv0.5.0 ssd 1541757195.801015377 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 316, "value": 0.05617777777777777}

:::MLPv0.5.0 ssd 1541757195.885044575 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 317, "value": 0.05635555555555555}

:::MLPv0.5.0 ssd 1541757195.969192028 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 318, "value": 0.05653333333333334}

:::MLPv0.5.0 ssd 1541757196.053363562 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 319, "value": 0.056711111111111115}

:::MLPv0.5.0 ssd 1541757196.138099670 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 320, "value": 0.05688888888888889}
Iteration:    320, Loss function: 7.050, Average Loss: 2.696, avg. samples / sec: 24223.56
Iteration:    320, Loss function: 6.880, Average Loss: 2.691, avg. samples / sec: 24210.26
Iteration:    320, Loss function: 6.951, Average Loss: 2.695, avg. samples / sec: 24261.55
Iteration:    320, Loss function: 6.860, Average Loss: 2.694, avg. samples / sec: 24251.55
Iteration:    320, Loss function: 7.067, Average Loss: 2.698, avg. samples / sec: 24204.12
Iteration:    320, Loss function: 6.691, Average Loss: 2.696, avg. samples / sec: 24160.79
Iteration:    320, Loss function: 7.561, Average Loss: 2.693, avg. samples / sec: 24173.83
Iteration:    320, Loss function: 6.821, Average Loss: 2.695, avg. samples / sec: 24182.35

:::MLPv0.5.0 ssd 1541757196.222751379 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 321, "value": 0.05706666666666667}

:::MLPv0.5.0 ssd 1541757196.306483030 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 322, "value": 0.057244444444444445}

:::MLPv0.5.0 ssd 1541757196.390578747 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 323, "value": 0.05742222222222222}

:::MLPv0.5.0 ssd 1541757196.475255966 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 324, "value": 0.0576}

:::MLPv0.5.0 ssd 1541757196.559768915 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 325, "value": 0.057777777777777775}

:::MLPv0.5.0 ssd 1541757196.644321203 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 326, "value": 0.05795555555555555}

:::MLPv0.5.0 ssd 1541757196.727741241 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 327, "value": 0.05813333333333333}

:::MLPv0.5.0 ssd 1541757196.811661243 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 328, "value": 0.058311111111111105}

:::MLPv0.5.0 ssd 1541757196.896683216 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 329, "value": 0.05848888888888888}

:::MLPv0.5.0 ssd 1541757196.980976820 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 330, "value": 0.05866666666666666}

:::MLPv0.5.0 ssd 1541757197.065157652 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 331, "value": 0.05884444444444445}

:::MLPv0.5.0 ssd 1541757197.149398565 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 332, "value": 0.059022222222222226}

:::MLPv0.5.0 ssd 1541757197.233222246 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 333, "value": 0.0592}

:::MLPv0.5.0 ssd 1541757197.318186998 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 334, "value": 0.05937777777777778}

:::MLPv0.5.0 ssd 1541757197.402883053 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 335, "value": 0.059555555555555556}

:::MLPv0.5.0 ssd 1541757197.487369061 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 336, "value": 0.05973333333333333}

:::MLPv0.5.0 ssd 1541757197.572497606 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 337, "value": 0.05991111111111111}

:::MLPv0.5.0 ssd 1541757197.656427622 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 338, "value": 0.060088888888888886}

:::MLPv0.5.0 ssd 1541757197.740249634 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 339, "value": 0.06026666666666666}

:::MLPv0.5.0 ssd 1541757197.823859215 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 340, "value": 0.06044444444444444}
Iteration:    340, Loss function: 7.083, Average Loss: 2.777, avg. samples / sec: 24362.90
Iteration:    340, Loss function: 6.972, Average Loss: 2.772, avg. samples / sec: 24303.43
Iteration:    340, Loss function: 6.549, Average Loss: 2.780, avg. samples / sec: 24353.35
Iteration:    340, Loss function: 7.581, Average Loss: 2.777, avg. samples / sec: 24353.13
Iteration:    340, Loss function: 7.101, Average Loss: 2.775, avg. samples / sec: 24286.10
Iteration:    340, Loss function: 7.871, Average Loss: 2.781, avg. samples / sec: 24258.26
Iteration:    340, Loss function: 6.494, Average Loss: 2.781, avg. samples / sec: 24239.93
Iteration:    340, Loss function: 6.697, Average Loss: 2.781, avg. samples / sec: 24276.83

:::MLPv0.5.0 ssd 1541757197.907608271 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 341, "value": 0.060622222222222216}

:::MLPv0.5.0 ssd 1541757197.992320776 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 342, "value": 0.06079999999999999}

:::MLPv0.5.0 ssd 1541757198.078948259 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 343, "value": 0.06097777777777777}

:::MLPv0.5.0 ssd 1541757198.162638426 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 344, "value": 0.06115555555555556}

:::MLPv0.5.0 ssd 1541757198.246518612 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 345, "value": 0.06133333333333334}

:::MLPv0.5.0 ssd 1541757198.331422091 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 346, "value": 0.061511111111111114}

:::MLPv0.5.0 ssd 1541757198.413028955 (train.py:553) train_epoch: 6

:::MLPv0.5.0 ssd 1541757198.417294025 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 347, "value": 0.06168888888888889}

:::MLPv0.5.0 ssd 1541757198.501801491 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 348, "value": 0.06186666666666667}

:::MLPv0.5.0 ssd 1541757198.586165667 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 349, "value": 0.062044444444444444}

:::MLPv0.5.0 ssd 1541757198.670608997 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 350, "value": 0.06222222222222222}

:::MLPv0.5.0 ssd 1541757198.754659414 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 351, "value": 0.0624}

:::MLPv0.5.0 ssd 1541757198.839454412 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 352, "value": 0.06257777777777777}

:::MLPv0.5.0 ssd 1541757198.923650980 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 353, "value": 0.06275555555555555}

:::MLPv0.5.0 ssd 1541757199.007924557 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 354, "value": 0.06293333333333333}

:::MLPv0.5.0 ssd 1541757199.092704773 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 355, "value": 0.0631111111111111}

:::MLPv0.5.0 ssd 1541757199.176987171 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 356, "value": 0.0632888888888889}

:::MLPv0.5.0 ssd 1541757199.261347532 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 357, "value": 0.06346666666666667}

:::MLPv0.5.0 ssd 1541757199.345888853 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 358, "value": 0.06364444444444445}

:::MLPv0.5.0 ssd 1541757199.429799318 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 359, "value": 0.06382222222222222}

:::MLPv0.5.0 ssd 1541757199.513662815 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 360, "value": 0.064}
Iteration:    360, Loss function: 6.727, Average Loss: 2.861, avg. samples / sec: 24244.94
Iteration:    360, Loss function: 6.681, Average Loss: 2.862, avg. samples / sec: 24235.92
Iteration:    360, Loss function: 6.737, Average Loss: 2.856, avg. samples / sec: 24230.46
Iteration:    360, Loss function: 6.382, Average Loss: 2.861, avg. samples / sec: 24286.51
Iteration:    360, Loss function: 6.477, Average Loss: 2.865, avg. samples / sec: 24249.99
Iteration:    360, Loss function: 6.298, Average Loss: 2.863, avg. samples / sec: 24257.64
Iteration:    360, Loss function: 6.390, Average Loss: 2.859, avg. samples / sec: 24190.93
Iteration:    360, Loss function: 6.812, Average Loss: 2.858, avg. samples / sec: 24220.80

:::MLPv0.5.0 ssd 1541757199.598607302 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 361, "value": 0.06417777777777778}

:::MLPv0.5.0 ssd 1541757199.682553053 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 362, "value": 0.06435555555555555}

:::MLPv0.5.0 ssd 1541757199.767326355 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 363, "value": 0.06453333333333333}

:::MLPv0.5.0 ssd 1541757199.850780725 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 364, "value": 0.06471111111111111}

:::MLPv0.5.0 ssd 1541757199.934947729 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 365, "value": 0.06488888888888888}

:::MLPv0.5.0 ssd 1541757200.019174099 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 366, "value": 0.06506666666666666}

:::MLPv0.5.0 ssd 1541757200.103217840 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 367, "value": 0.06524444444444444}

:::MLPv0.5.0 ssd 1541757200.187680483 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 368, "value": 0.06542222222222221}

:::MLPv0.5.0 ssd 1541757200.271381617 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 369, "value": 0.0656}

:::MLPv0.5.0 ssd 1541757200.356482029 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 370, "value": 0.06577777777777778}

:::MLPv0.5.0 ssd 1541757200.440409660 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 371, "value": 0.06595555555555556}

:::MLPv0.5.0 ssd 1541757200.524046421 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 372, "value": 0.06613333333333334}

:::MLPv0.5.0 ssd 1541757200.607732058 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 373, "value": 0.06631111111111111}

:::MLPv0.5.0 ssd 1541757200.691610336 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 374, "value": 0.06648888888888889}

:::MLPv0.5.0 ssd 1541757200.776419878 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 375, "value": 0.06666666666666667}

:::MLPv0.5.0 ssd 1541757200.860383034 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 376, "value": 0.06684444444444444}

:::MLPv0.5.0 ssd 1541757200.944936037 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 377, "value": 0.06702222222222222}

:::MLPv0.5.0 ssd 1541757201.028725147 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 378, "value": 0.0672}

:::MLPv0.5.0 ssd 1541757201.112893343 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 379, "value": 0.06737777777777777}

:::MLPv0.5.0 ssd 1541757201.197022438 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 380, "value": 0.06755555555555555}
Iteration:    380, Loss function: 6.939, Average Loss: 2.933, avg. samples / sec: 24341.35
Iteration:    380, Loss function: 6.159, Average Loss: 2.935, avg. samples / sec: 24376.55
Iteration:    380, Loss function: 6.768, Average Loss: 2.938, avg. samples / sec: 24330.17
Iteration:    380, Loss function: 6.470, Average Loss: 2.937, avg. samples / sec: 24324.29
Iteration:    380, Loss function: 6.555, Average Loss: 2.939, avg. samples / sec: 24348.07
Iteration:    380, Loss function: 7.209, Average Loss: 2.936, avg. samples / sec: 24331.72
Iteration:    380, Loss function: 6.622, Average Loss: 2.937, avg. samples / sec: 24282.27
Iteration:    380, Loss function: 6.671, Average Loss: 2.943, avg. samples / sec: 24314.53

:::MLPv0.5.0 ssd 1541757201.281963110 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 381, "value": 0.06773333333333333}

:::MLPv0.5.0 ssd 1541757201.366044998 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 382, "value": 0.06791111111111112}

:::MLPv0.5.0 ssd 1541757201.450388670 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 383, "value": 0.0680888888888889}

:::MLPv0.5.0 ssd 1541757201.534342289 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 384, "value": 0.06826666666666667}

:::MLPv0.5.0 ssd 1541757201.618557215 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 385, "value": 0.06844444444444445}

:::MLPv0.5.0 ssd 1541757201.702651501 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 386, "value": 0.06862222222222222}

:::MLPv0.5.0 ssd 1541757201.787742138 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 387, "value": 0.0688}

:::MLPv0.5.0 ssd 1541757201.872033358 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 388, "value": 0.06897777777777778}

:::MLPv0.5.0 ssd 1541757201.955745935 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 389, "value": 0.06915555555555555}

:::MLPv0.5.0 ssd 1541757202.039436340 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 390, "value": 0.06933333333333333}

:::MLPv0.5.0 ssd 1541757202.123306036 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 391, "value": 0.0695111111111111}

:::MLPv0.5.0 ssd 1541757202.207028151 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 392, "value": 0.06968888888888888}

:::MLPv0.5.0 ssd 1541757202.292032003 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 393, "value": 0.06986666666666666}

:::MLPv0.5.0 ssd 1541757202.376025915 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 394, "value": 0.07004444444444444}

:::MLPv0.5.0 ssd 1541757202.460177422 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 395, "value": 0.07022222222222223}

:::MLPv0.5.0 ssd 1541757202.544232607 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 396, "value": 0.0704}

:::MLPv0.5.0 ssd 1541757202.628379822 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 397, "value": 0.07057777777777778}

:::MLPv0.5.0 ssd 1541757202.711971521 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 398, "value": 0.07075555555555556}

:::MLPv0.5.0 ssd 1541757202.796123981 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 399, "value": 0.07093333333333333}

:::MLPv0.5.0 ssd 1541757202.880027294 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 400, "value": 0.07111111111111111}
Iteration:    400, Loss function: 6.820, Average Loss: 3.015, avg. samples / sec: 24392.23
Iteration:    400, Loss function: 6.295, Average Loss: 3.011, avg. samples / sec: 24383.94
Iteration:    400, Loss function: 6.446, Average Loss: 3.007, avg. samples / sec: 24332.31
Iteration:    400, Loss function: 6.683, Average Loss: 3.014, avg. samples / sec: 24334.40
Iteration:    400, Loss function: 6.462, Average Loss: 3.011, avg. samples / sec: 24309.50
Iteration:    400, Loss function: 6.818, Average Loss: 3.011, avg. samples / sec: 24339.52
Iteration:    400, Loss function: 6.567, Average Loss: 3.011, avg. samples / sec: 24305.24
Iteration:    400, Loss function: 6.277, Average Loss: 3.009, avg. samples / sec: 24282.93

:::MLPv0.5.0 ssd 1541757202.963797092 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 401, "value": 0.07128888888888889}

:::MLPv0.5.0 ssd 1541757203.047762871 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 402, "value": 0.07146666666666666}

:::MLPv0.5.0 ssd 1541757203.132683277 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 403, "value": 0.07164444444444444}

:::MLPv0.5.0 ssd 1541757203.217392683 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 404, "value": 0.07182222222222222}

:::MLPv0.5.0 ssd 1541757203.299537897 (train.py:553) train_epoch: 7

:::MLPv0.5.0 ssd 1541757203.303820610 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 405, "value": 0.072}

:::MLPv0.5.0 ssd 1541757203.388367176 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 406, "value": 0.07217777777777777}

:::MLPv0.5.0 ssd 1541757203.472542286 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 407, "value": 0.07235555555555555}

:::MLPv0.5.0 ssd 1541757203.556201935 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 408, "value": 0.07253333333333334}

:::MLPv0.5.0 ssd 1541757203.640448093 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 409, "value": 0.07271111111111112}

:::MLPv0.5.0 ssd 1541757203.724496841 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 410, "value": 0.07288888888888889}

:::MLPv0.5.0 ssd 1541757203.808609724 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 411, "value": 0.07306666666666667}

:::MLPv0.5.0 ssd 1541757203.893088579 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 412, "value": 0.07324444444444445}

:::MLPv0.5.0 ssd 1541757203.976841211 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 413, "value": 0.07342222222222222}

:::MLPv0.5.0 ssd 1541757204.061041832 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 414, "value": 0.0736}

:::MLPv0.5.0 ssd 1541757204.144871235 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 415, "value": 0.07377777777777778}

:::MLPv0.5.0 ssd 1541757204.229572773 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 416, "value": 0.07395555555555555}

:::MLPv0.5.0 ssd 1541757204.313993216 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 417, "value": 0.07413333333333333}

:::MLPv0.5.0 ssd 1541757204.398782969 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 418, "value": 0.0743111111111111}

:::MLPv0.5.0 ssd 1541757204.482722521 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 419, "value": 0.07448888888888888}

:::MLPv0.5.0 ssd 1541757204.566965103 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 420, "value": 0.07466666666666666}
Iteration:    420, Loss function: 6.412, Average Loss: 3.080, avg. samples / sec: 24317.38
Iteration:    420, Loss function: 6.506, Average Loss: 3.075, avg. samples / sec: 24283.24
Iteration:    420, Loss function: 6.931, Average Loss: 3.081, avg. samples / sec: 24326.13
Iteration:    420, Loss function: 5.822, Average Loss: 3.080, avg. samples / sec: 24261.30
Iteration:    420, Loss function: 7.106, Average Loss: 3.079, avg. samples / sec: 24295.78
Iteration:    420, Loss function: 6.827, Average Loss: 3.084, avg. samples / sec: 24237.90
Iteration:    420, Loss function: 6.476, Average Loss: 3.084, avg. samples / sec: 24259.72
Iteration:    420, Loss function: 6.526, Average Loss: 3.081, avg. samples / sec: 24267.39

:::MLPv0.5.0 ssd 1541757204.651329279 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 421, "value": 0.07484444444444445}

:::MLPv0.5.0 ssd 1541757204.735949993 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 422, "value": 0.07502222222222223}

:::MLPv0.5.0 ssd 1541757204.820301056 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 423, "value": 0.0752}

:::MLPv0.5.0 ssd 1541757204.904036760 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 424, "value": 0.07537777777777778}

:::MLPv0.5.0 ssd 1541757204.988227606 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 425, "value": 0.07555555555555556}

:::MLPv0.5.0 ssd 1541757205.071770430 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 426, "value": 0.07573333333333333}

:::MLPv0.5.0 ssd 1541757205.155548334 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 427, "value": 0.07591111111111111}

:::MLPv0.5.0 ssd 1541757205.239481688 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 428, "value": 0.07608888888888889}

:::MLPv0.5.0 ssd 1541757205.323810101 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 429, "value": 0.07626666666666666}

:::MLPv0.5.0 ssd 1541757205.407794476 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 430, "value": 0.07644444444444444}

:::MLPv0.5.0 ssd 1541757205.492320061 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 431, "value": 0.07662222222222222}

:::MLPv0.5.0 ssd 1541757205.576661587 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 432, "value": 0.0768}

:::MLPv0.5.0 ssd 1541757205.660424232 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 433, "value": 0.07697777777777778}

:::MLPv0.5.0 ssd 1541757205.744586468 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 434, "value": 0.07715555555555556}

:::MLPv0.5.0 ssd 1541757205.828277349 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 435, "value": 0.07733333333333334}

:::MLPv0.5.0 ssd 1541757205.911966801 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 436, "value": 0.07751111111111111}

:::MLPv0.5.0 ssd 1541757205.995874643 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 437, "value": 0.07768888888888889}

:::MLPv0.5.0 ssd 1541757206.079599619 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 438, "value": 0.07786666666666667}

:::MLPv0.5.0 ssd 1541757206.163566589 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 439, "value": 0.07804444444444444}

:::MLPv0.5.0 ssd 1541757206.247527599 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 440, "value": 0.07822222222222222}
Iteration:    440, Loss function: 7.231, Average Loss: 3.151, avg. samples / sec: 24376.05
Iteration:    440, Loss function: 6.910, Average Loss: 3.150, avg. samples / sec: 24408.95
Iteration:    440, Loss function: 7.343, Average Loss: 3.156, avg. samples / sec: 24417.85
Iteration:    440, Loss function: 6.907, Average Loss: 3.157, avg. samples / sec: 24391.81
Iteration:    440, Loss function: 7.089, Average Loss: 3.147, avg. samples / sec: 24351.50
Iteration:    440, Loss function: 6.928, Average Loss: 3.157, avg. samples / sec: 24350.73
Iteration:    440, Loss function: 6.894, Average Loss: 3.158, avg. samples / sec: 24374.67
Iteration:    440, Loss function: 7.097, Average Loss: 3.156, avg. samples / sec: 24333.41

:::MLPv0.5.0 ssd 1541757206.330903769 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 441, "value": 0.0784}

:::MLPv0.5.0 ssd 1541757206.414669752 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 442, "value": 0.07857777777777777}

:::MLPv0.5.0 ssd 1541757206.498646498 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 443, "value": 0.07875555555555555}

:::MLPv0.5.0 ssd 1541757206.582472563 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 444, "value": 0.07893333333333333}

:::MLPv0.5.0 ssd 1541757206.666412592 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 445, "value": 0.0791111111111111}

:::MLPv0.5.0 ssd 1541757206.750057936 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 446, "value": 0.0792888888888889}

:::MLPv0.5.0 ssd 1541757206.833729506 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 447, "value": 0.07946666666666667}

:::MLPv0.5.0 ssd 1541757206.917729855 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 448, "value": 0.07964444444444445}

:::MLPv0.5.0 ssd 1541757207.001876116 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 449, "value": 0.07982222222222222}

:::MLPv0.5.0 ssd 1541757207.086134434 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 450, "value": 0.08}

:::MLPv0.5.0 ssd 1541757207.170397758 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 451, "value": 0.08017777777777778}

:::MLPv0.5.0 ssd 1541757207.254951715 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 452, "value": 0.08035555555555556}

:::MLPv0.5.0 ssd 1541757207.338319302 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 453, "value": 0.08053333333333333}

:::MLPv0.5.0 ssd 1541757207.424542904 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 454, "value": 0.08071111111111111}

:::MLPv0.5.0 ssd 1541757207.508611441 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 455, "value": 0.08088888888888889}

:::MLPv0.5.0 ssd 1541757207.592973471 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 456, "value": 0.08106666666666666}

:::MLPv0.5.0 ssd 1541757207.676865101 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 457, "value": 0.08124444444444444}

:::MLPv0.5.0 ssd 1541757207.761504650 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 458, "value": 0.08142222222222222}

:::MLPv0.5.0 ssd 1541757207.844799757 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 459, "value": 0.0816}

:::MLPv0.5.0 ssd 1541757207.928671837 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 460, "value": 0.08177777777777778}
Iteration:    460, Loss function: 6.264, Average Loss: 3.218, avg. samples / sec: 24362.27
Iteration:    460, Loss function: 6.320, Average Loss: 3.225, avg. samples / sec: 24410.48
Iteration:    460, Loss function: 6.805, Average Loss: 3.217, avg. samples / sec: 24381.08
Iteration:    460, Loss function: 6.318, Average Loss: 3.224, avg. samples / sec: 24401.82
Iteration:    460, Loss function: 6.613, Average Loss: 3.220, avg. samples / sec: 24342.11
Iteration:    460, Loss function: 5.742, Average Loss: 3.225, avg. samples / sec: 24380.86
Iteration:    460, Loss function: 6.252, Average Loss: 3.224, avg. samples / sec: 24349.84
Iteration:    460, Loss function: 6.347, Average Loss: 3.223, avg. samples / sec: 24338.54

:::MLPv0.5.0 ssd 1541757208.012634277 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 461, "value": 0.08195555555555556}

:::MLPv0.5.0 ssd 1541757208.096676826 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 462, "value": 0.08213333333333334}

:::MLPv0.5.0 ssd 1541757208.177900791 (train.py:553) train_epoch: 8

:::MLPv0.5.0 ssd 1541757208.182039022 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 463, "value": 0.08231111111111111}

:::MLPv0.5.0 ssd 1541757208.265710592 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 464, "value": 0.08248888888888889}

:::MLPv0.5.0 ssd 1541757208.349697828 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 465, "value": 0.08266666666666667}

:::MLPv0.5.0 ssd 1541757208.433431387 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 466, "value": 0.08284444444444444}

:::MLPv0.5.0 ssd 1541757208.517339468 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 467, "value": 0.08302222222222222}

:::MLPv0.5.0 ssd 1541757208.601368904 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 468, "value": 0.0832}

:::MLPv0.5.0 ssd 1541757208.686177969 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 469, "value": 0.08337777777777777}

:::MLPv0.5.0 ssd 1541757208.770710230 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 470, "value": 0.08355555555555555}

:::MLPv0.5.0 ssd 1541757208.855814457 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 471, "value": 0.08373333333333333}

:::MLPv0.5.0 ssd 1541757208.939717054 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 472, "value": 0.08391111111111112}

:::MLPv0.5.0 ssd 1541757209.023675919 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 473, "value": 0.0840888888888889}

:::MLPv0.5.0 ssd 1541757209.107724905 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 474, "value": 0.08426666666666667}

:::MLPv0.5.0 ssd 1541757209.192557812 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 475, "value": 0.08444444444444445}

:::MLPv0.5.0 ssd 1541757209.276443243 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 476, "value": 0.08462222222222222}

:::MLPv0.5.0 ssd 1541757209.363041162 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 477, "value": 0.0848}

:::MLPv0.5.0 ssd 1541757209.446700573 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 478, "value": 0.08497777777777778}

:::MLPv0.5.0 ssd 1541757209.530873299 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 479, "value": 0.08515555555555555}

:::MLPv0.5.0 ssd 1541757209.615366936 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 480, "value": 0.08533333333333333}
Iteration:    480, Loss function: 5.754, Average Loss: 3.271, avg. samples / sec: 24301.24
Iteration:    480, Loss function: 5.992, Average Loss: 3.281, avg. samples / sec: 24315.46
Iteration:    480, Loss function: 5.572, Average Loss: 3.275, avg. samples / sec: 24288.89
Iteration:    480, Loss function: 6.667, Average Loss: 3.282, avg. samples / sec: 24258.25
Iteration:    480, Loss function: 6.497, Average Loss: 3.280, avg. samples / sec: 24283.94
Iteration:    480, Loss function: 5.803, Average Loss: 3.275, avg. samples / sec: 24243.03
Iteration:    480, Loss function: 5.428, Average Loss: 3.282, avg. samples / sec: 24271.39
Iteration:    480, Loss function: 5.783, Average Loss: 3.281, avg. samples / sec: 24236.69

:::MLPv0.5.0 ssd 1541757209.698995113 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 481, "value": 0.08551111111111111}

:::MLPv0.5.0 ssd 1541757209.782757998 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 482, "value": 0.08568888888888888}

:::MLPv0.5.0 ssd 1541757209.866597414 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 483, "value": 0.08586666666666666}

:::MLPv0.5.0 ssd 1541757209.950656176 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 484, "value": 0.08604444444444445}

:::MLPv0.5.0 ssd 1541757210.035211086 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 485, "value": 0.08622222222222223}

:::MLPv0.5.0 ssd 1541757210.119670630 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 486, "value": 0.0864}

:::MLPv0.5.0 ssd 1541757210.203648090 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 487, "value": 0.08657777777777778}

:::MLPv0.5.0 ssd 1541757210.288061142 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 488, "value": 0.08675555555555556}

:::MLPv0.5.0 ssd 1541757210.372358322 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 489, "value": 0.08693333333333333}

:::MLPv0.5.0 ssd 1541757210.456732988 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 490, "value": 0.08711111111111111}

:::MLPv0.5.0 ssd 1541757210.540915728 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 491, "value": 0.08728888888888889}

:::MLPv0.5.0 ssd 1541757210.625018120 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 492, "value": 0.08746666666666666}

:::MLPv0.5.0 ssd 1541757210.709435940 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 493, "value": 0.08764444444444444}

:::MLPv0.5.0 ssd 1541757210.793625355 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 494, "value": 0.08782222222222222}

:::MLPv0.5.0 ssd 1541757210.877606153 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 495, "value": 0.088}

:::MLPv0.5.0 ssd 1541757210.961485386 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 496, "value": 0.08817777777777777}

:::MLPv0.5.0 ssd 1541757211.045452118 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 497, "value": 0.08835555555555556}

:::MLPv0.5.0 ssd 1541757211.129472971 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 498, "value": 0.08853333333333334}

:::MLPv0.5.0 ssd 1541757211.213490725 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 499, "value": 0.08871111111111112}

:::MLPv0.5.0 ssd 1541757211.297627449 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 500, "value": 0.08888888888888889}
Iteration:    500, Loss function: 5.722, Average Loss: 3.340, avg. samples / sec: 24356.27
Iteration:    500, Loss function: 5.858, Average Loss: 3.335, avg. samples / sec: 24375.12
Iteration:    500, Loss function: 5.803, Average Loss: 3.332, avg. samples / sec: 24391.86
Iteration:    500, Loss function: 5.860, Average Loss: 3.338, avg. samples / sec: 24400.68
Iteration:    500, Loss function: 6.372, Average Loss: 3.331, avg. samples / sec: 24325.35
Iteration:    500, Loss function: 6.119, Average Loss: 3.342, avg. samples / sec: 24358.75
Iteration:    500, Loss function: 5.497, Average Loss: 3.342, avg. samples / sec: 24359.01
Iteration:    500, Loss function: 6.056, Average Loss: 3.339, avg. samples / sec: 24334.12

:::MLPv0.5.0 ssd 1541757211.381669998 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 501, "value": 0.08906666666666667}

:::MLPv0.5.0 ssd 1541757211.465808392 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 502, "value": 0.08924444444444445}

:::MLPv0.5.0 ssd 1541757211.549515963 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 503, "value": 0.08942222222222222}

:::MLPv0.5.0 ssd 1541757211.633405685 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 504, "value": 0.0896}

:::MLPv0.5.0 ssd 1541757211.717777491 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 505, "value": 0.08977777777777778}

:::MLPv0.5.0 ssd 1541757211.802028894 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 506, "value": 0.08995555555555555}

:::MLPv0.5.0 ssd 1541757211.886182070 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 507, "value": 0.09013333333333333}

:::MLPv0.5.0 ssd 1541757211.970891476 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 508, "value": 0.0903111111111111}

:::MLPv0.5.0 ssd 1541757212.054766655 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 509, "value": 0.09048888888888888}

:::MLPv0.5.0 ssd 1541757212.139067650 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 510, "value": 0.09066666666666667}

:::MLPv0.5.0 ssd 1541757212.222833157 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 511, "value": 0.09084444444444445}

:::MLPv0.5.0 ssd 1541757212.307234764 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 512, "value": 0.09102222222222223}

:::MLPv0.5.0 ssd 1541757212.391363859 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 513, "value": 0.0912}

:::MLPv0.5.0 ssd 1541757212.475189447 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 514, "value": 0.09137777777777778}

:::MLPv0.5.0 ssd 1541757212.559572220 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 515, "value": 0.09155555555555556}

:::MLPv0.5.0 ssd 1541757212.643193245 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 516, "value": 0.09173333333333333}

:::MLPv0.5.0 ssd 1541757212.727673531 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 517, "value": 0.09191111111111111}

:::MLPv0.5.0 ssd 1541757212.811588049 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 518, "value": 0.09208888888888889}

:::MLPv0.5.0 ssd 1541757212.896205664 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 519, "value": 0.09226666666666666}

:::MLPv0.5.0 ssd 1541757212.976989508 (train.py:553) train_epoch: 9

:::MLPv0.5.0 ssd 1541757212.981302261 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 520, "value": 0.09244444444444444}
Iteration:    520, Loss function: 5.552, Average Loss: 3.388, avg. samples / sec: 24330.76
Iteration:    520, Loss function: 5.918, Average Loss: 3.392, avg. samples / sec: 24323.13
Iteration:    520, Loss function: 5.594, Average Loss: 3.390, avg. samples / sec: 24367.11
Iteration:    520, Loss function: 5.678, Average Loss: 3.391, avg. samples / sec: 24335.15
Iteration:    520, Loss function: 6.054, Average Loss: 3.383, avg. samples / sec: 24319.03
Iteration:    520, Loss function: 5.729, Average Loss: 3.387, avg. samples / sec: 24289.15
Iteration:    520, Loss function: 6.051, Average Loss: 3.386, avg. samples / sec: 24287.50
Iteration:    520, Loss function: 5.490, Average Loss: 3.392, avg. samples / sec: 24307.86

:::MLPv0.5.0 ssd 1541757213.065674782 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 521, "value": 0.09262222222222222}

:::MLPv0.5.0 ssd 1541757213.149795294 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 522, "value": 0.0928}

:::MLPv0.5.0 ssd 1541757213.233916521 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 523, "value": 0.09297777777777778}

:::MLPv0.5.0 ssd 1541757213.317697048 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 524, "value": 0.09315555555555556}

:::MLPv0.5.0 ssd 1541757213.400944471 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 525, "value": 0.09333333333333334}

:::MLPv0.5.0 ssd 1541757213.485365391 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 526, "value": 0.09351111111111111}

:::MLPv0.5.0 ssd 1541757213.569256306 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 527, "value": 0.09368888888888889}

:::MLPv0.5.0 ssd 1541757213.653234720 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 528, "value": 0.09386666666666667}

:::MLPv0.5.0 ssd 1541757213.737142563 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 529, "value": 0.09404444444444444}

:::MLPv0.5.0 ssd 1541757213.820701122 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 530, "value": 0.09422222222222222}

:::MLPv0.5.0 ssd 1541757213.904384613 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 531, "value": 0.0944}

:::MLPv0.5.0 ssd 1541757213.989193916 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 532, "value": 0.09457777777777777}

:::MLPv0.5.0 ssd 1541757214.073504925 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 533, "value": 0.09475555555555555}

:::MLPv0.5.0 ssd 1541757214.157227993 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 534, "value": 0.09493333333333333}

:::MLPv0.5.0 ssd 1541757214.241454124 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 535, "value": 0.0951111111111111}

:::MLPv0.5.0 ssd 1541757214.325762749 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 536, "value": 0.0952888888888889}

:::MLPv0.5.0 ssd 1541757214.410459280 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 537, "value": 0.09546666666666667}

:::MLPv0.5.0 ssd 1541757214.494482756 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 538, "value": 0.09564444444444445}

:::MLPv0.5.0 ssd 1541757214.578295231 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 539, "value": 0.09582222222222223}

:::MLPv0.5.0 ssd 1541757214.661924362 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 540, "value": 0.096}
Iteration:    540, Loss function: 6.535, Average Loss: 3.444, avg. samples / sec: 24419.57
Iteration:    540, Loss function: 5.648, Average Loss: 3.450, avg. samples / sec: 24373.16
Iteration:    540, Loss function: 6.379, Average Loss: 3.444, avg. samples / sec: 24365.33
Iteration:    540, Loss function: 6.710, Average Loss: 3.447, avg. samples / sec: 24382.90
Iteration:    540, Loss function: 6.636, Average Loss: 3.443, avg. samples / sec: 24403.67
Iteration:    540, Loss function: 5.627, Average Loss: 3.450, avg. samples / sec: 24425.69
Iteration:    540, Loss function: 6.265, Average Loss: 3.441, avg. samples / sec: 24386.03
Iteration:    540, Loss function: 6.299, Average Loss: 3.449, avg. samples / sec: 24373.16

:::MLPv0.5.0 ssd 1541757214.745952606 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 541, "value": 0.09617777777777778}

:::MLPv0.5.0 ssd 1541757214.830254078 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 542, "value": 0.09635555555555556}

:::MLPv0.5.0 ssd 1541757214.915614605 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 543, "value": 0.09653333333333333}

:::MLPv0.5.0 ssd 1541757214.999305725 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 544, "value": 0.09671111111111111}

:::MLPv0.5.0 ssd 1541757215.082954168 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 545, "value": 0.09688888888888889}

:::MLPv0.5.0 ssd 1541757215.166582584 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 546, "value": 0.09706666666666666}

:::MLPv0.5.0 ssd 1541757215.250924110 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 547, "value": 0.09724444444444444}

:::MLPv0.5.0 ssd 1541757215.335378408 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 548, "value": 0.09742222222222222}

:::MLPv0.5.0 ssd 1541757215.419305563 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 549, "value": 0.09759999999999999}

:::MLPv0.5.0 ssd 1541757215.503251553 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 550, "value": 0.09777777777777777}

:::MLPv0.5.0 ssd 1541757215.587545156 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 551, "value": 0.09795555555555555}

:::MLPv0.5.0 ssd 1541757215.671543121 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 552, "value": 0.09813333333333334}

:::MLPv0.5.0 ssd 1541757215.755324841 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 553, "value": 0.09831111111111111}

:::MLPv0.5.0 ssd 1541757215.838475704 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 554, "value": 0.09848888888888889}

:::MLPv0.5.0 ssd 1541757215.923025370 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 555, "value": 0.09866666666666667}

:::MLPv0.5.0 ssd 1541757216.007180214 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 556, "value": 0.09884444444444444}

:::MLPv0.5.0 ssd 1541757216.090939760 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 557, "value": 0.09902222222222222}

:::MLPv0.5.0 ssd 1541757216.174824238 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 558, "value": 0.09920000000000001}

:::MLPv0.5.0 ssd 1541757216.259585381 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 559, "value": 0.09937777777777779}

:::MLPv0.5.0 ssd 1541757216.343543530 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 560, "value": 0.09955555555555556}
Iteration:    560, Loss function: 5.881, Average Loss: 3.493, avg. samples / sec: 24361.78
Iteration:    560, Loss function: 5.550, Average Loss: 3.499, avg. samples / sec: 24358.50
Iteration:    560, Loss function: 5.520, Average Loss: 3.490, avg. samples / sec: 24370.02
Iteration:    560, Loss function: 5.937, Average Loss: 3.490, avg. samples / sec: 24353.33
Iteration:    560, Loss function: 5.826, Average Loss: 3.497, avg. samples / sec: 24359.84
Iteration:    560, Loss function: 5.445, Average Loss: 3.493, avg. samples / sec: 24320.87
Iteration:    560, Loss function: 5.480, Average Loss: 3.494, avg. samples / sec: 24313.86
Iteration:    560, Loss function: 6.362, Average Loss: 3.499, avg. samples / sec: 24302.59

:::MLPv0.5.0 ssd 1541757216.427923679 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 561, "value": 0.09973333333333334}

:::MLPv0.5.0 ssd 1541757216.512345791 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 562, "value": 0.09991111111111112}

:::MLPv0.5.0 ssd 1541757216.596891403 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 563, "value": 0.1000888888888889}

:::MLPv0.5.0 ssd 1541757216.681399107 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 564, "value": 0.10026666666666667}

:::MLPv0.5.0 ssd 1541757216.765190125 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 565, "value": 0.10044444444444445}

:::MLPv0.5.0 ssd 1541757216.849376202 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 566, "value": 0.10062222222222222}

:::MLPv0.5.0 ssd 1541757216.933382511 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 567, "value": 0.1008}

:::MLPv0.5.0 ssd 1541757217.017618418 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 568, "value": 0.10097777777777778}

:::MLPv0.5.0 ssd 1541757217.101478100 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 569, "value": 0.10115555555555555}

:::MLPv0.5.0 ssd 1541757217.185981274 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 570, "value": 0.10133333333333333}

:::MLPv0.5.0 ssd 1541757217.269696712 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 571, "value": 0.10151111111111111}

:::MLPv0.5.0 ssd 1541757217.354234934 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 572, "value": 0.10168888888888888}

:::MLPv0.5.0 ssd 1541757217.438061714 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 573, "value": 0.10186666666666666}

:::MLPv0.5.0 ssd 1541757217.522196293 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 574, "value": 0.10204444444444444}

:::MLPv0.5.0 ssd 1541757217.605839968 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 575, "value": 0.10222222222222221}

:::MLPv0.5.0 ssd 1541757217.690096140 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 576, "value": 0.10239999999999999}

:::MLPv0.5.0 ssd 1541757217.773693323 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 577, "value": 0.10257777777777778}

:::MLPv0.5.0 ssd 1541757217.855400562 (train.py:553) train_epoch: 10

:::MLPv0.5.0 ssd 1541757217.859627247 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 578, "value": 0.10275555555555556}

:::MLPv0.5.0 ssd 1541757217.943904161 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 579, "value": 0.10293333333333334}

:::MLPv0.5.0 ssd 1541757218.028125763 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 580, "value": 0.10311111111111111}
Iteration:    580, Loss function: 5.581, Average Loss: 3.534, avg. samples / sec: 24323.08
Iteration:    580, Loss function: 5.174, Average Loss: 3.540, avg. samples / sec: 24320.34
Iteration:    580, Loss function: 5.667, Average Loss: 3.535, avg. samples / sec: 24316.64
Iteration:    580, Loss function: 5.565, Average Loss: 3.542, avg. samples / sec: 24381.48
Iteration:    580, Loss function: 5.946, Average Loss: 3.540, avg. samples / sec: 24304.43
Iteration:    580, Loss function: 5.874, Average Loss: 3.536, avg. samples / sec: 24331.26
Iteration:    580, Loss function: 6.066, Average Loss: 3.534, avg. samples / sec: 24265.49
Iteration:    580, Loss function: 5.613, Average Loss: 3.537, avg. samples / sec: 24305.44

:::MLPv0.5.0 ssd 1541757218.111871243 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 581, "value": 0.10328888888888889}

:::MLPv0.5.0 ssd 1541757218.195546150 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 582, "value": 0.10346666666666667}

:::MLPv0.5.0 ssd 1541757218.280146599 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 583, "value": 0.10364444444444444}

:::MLPv0.5.0 ssd 1541757218.364171982 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 584, "value": 0.10382222222222223}

:::MLPv0.5.0 ssd 1541757218.448414564 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 585, "value": 0.10400000000000001}

:::MLPv0.5.0 ssd 1541757218.532577515 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 586, "value": 0.10417777777777779}

:::MLPv0.5.0 ssd 1541757218.616371870 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 587, "value": 0.10435555555555556}

:::MLPv0.5.0 ssd 1541757218.700366259 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 588, "value": 0.10453333333333334}

:::MLPv0.5.0 ssd 1541757218.784041882 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 589, "value": 0.10471111111111112}

:::MLPv0.5.0 ssd 1541757218.868945122 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 590, "value": 0.10488888888888889}

:::MLPv0.5.0 ssd 1541757218.952806473 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 591, "value": 0.10506666666666667}

:::MLPv0.5.0 ssd 1541757219.037045956 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 592, "value": 0.10524444444444445}

:::MLPv0.5.0 ssd 1541757219.121438026 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 593, "value": 0.10542222222222222}

:::MLPv0.5.0 ssd 1541757219.205416203 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 594, "value": 0.1056}

:::MLPv0.5.0 ssd 1541757219.289574862 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 595, "value": 0.10577777777777778}

:::MLPv0.5.0 ssd 1541757219.373212337 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 596, "value": 0.10595555555555555}

:::MLPv0.5.0 ssd 1541757219.456896782 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 597, "value": 0.10613333333333333}

:::MLPv0.5.0 ssd 1541757219.541100025 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 598, "value": 0.1063111111111111}

:::MLPv0.5.0 ssd 1541757219.625174284 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 599, "value": 0.10648888888888888}

:::MLPv0.5.0 ssd 1541757219.709395885 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 600, "value": 0.10666666666666666}
Iteration:    600, Loss function: 6.236, Average Loss: 3.582, avg. samples / sec: 24363.53
Iteration:    600, Loss function: 6.498, Average Loss: 3.580, avg. samples / sec: 24361.45
Iteration:    600, Loss function: 6.532, Average Loss: 3.585, avg. samples / sec: 24358.12
Iteration:    600, Loss function: 6.280, Average Loss: 3.579, avg. samples / sec: 24410.33
Iteration:    600, Loss function: 5.968, Average Loss: 3.583, avg. samples / sec: 24385.24
Iteration:    600, Loss function: 6.143, Average Loss: 3.582, avg. samples / sec: 24393.81
Iteration:    600, Loss function: 6.790, Average Loss: 3.587, avg. samples / sec: 24332.56
Iteration:    600, Loss function: 6.362, Average Loss: 3.580, avg. samples / sec: 24345.16

:::MLPv0.5.0 ssd 1541757219.793164253 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 601, "value": 0.10684444444444444}

:::MLPv0.5.0 ssd 1541757219.876928806 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 602, "value": 0.10702222222222221}

:::MLPv0.5.0 ssd 1541757219.960705996 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 603, "value": 0.1072}

:::MLPv0.5.0 ssd 1541757220.044730663 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 604, "value": 0.10737777777777778}

:::MLPv0.5.0 ssd 1541757220.128776550 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 605, "value": 0.10755555555555556}

:::MLPv0.5.0 ssd 1541757220.212185860 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 606, "value": 0.10773333333333333}

:::MLPv0.5.0 ssd 1541757220.296570778 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 607, "value": 0.10791111111111111}

:::MLPv0.5.0 ssd 1541757220.381935596 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 608, "value": 0.10808888888888889}

:::MLPv0.5.0 ssd 1541757220.465801716 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 609, "value": 0.10826666666666668}

:::MLPv0.5.0 ssd 1541757220.549718857 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 610, "value": 0.10844444444444445}

:::MLPv0.5.0 ssd 1541757220.633761883 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 611, "value": 0.10862222222222223}

:::MLPv0.5.0 ssd 1541757220.718858242 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 612, "value": 0.10880000000000001}

:::MLPv0.5.0 ssd 1541757220.803013086 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 613, "value": 0.10897777777777778}

:::MLPv0.5.0 ssd 1541757220.887065411 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 614, "value": 0.10915555555555556}

:::MLPv0.5.0 ssd 1541757220.971572638 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 615, "value": 0.10933333333333334}

:::MLPv0.5.0 ssd 1541757221.055959225 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 616, "value": 0.10951111111111111}

:::MLPv0.5.0 ssd 1541757221.140637398 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 617, "value": 0.10968888888888889}

:::MLPv0.5.0 ssd 1541757221.224568367 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 618, "value": 0.10986666666666667}

:::MLPv0.5.0 ssd 1541757221.308372736 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 619, "value": 0.11004444444444444}

:::MLPv0.5.0 ssd 1541757221.392343521 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 620, "value": 0.11022222222222222}
Iteration:    620, Loss function: 6.066, Average Loss: 3.636, avg. samples / sec: 24376.83
Iteration:    620, Loss function: 5.888, Average Loss: 3.630, avg. samples / sec: 24339.63
Iteration:    620, Loss function: 5.517, Average Loss: 3.624, avg. samples / sec: 24344.81
Iteration:    620, Loss function: 5.451, Average Loss: 3.628, avg. samples / sec: 24339.17
Iteration:    620, Loss function: 5.706, Average Loss: 3.628, avg. samples / sec: 24345.95
Iteration:    620, Loss function: 5.326, Average Loss: 3.629, avg. samples / sec: 24306.53
Iteration:    620, Loss function: 5.221, Average Loss: 3.626, avg. samples / sec: 24349.03
Iteration:    620, Loss function: 5.618, Average Loss: 3.632, avg. samples / sec: 24296.62

:::MLPv0.5.0 ssd 1541757221.476152658 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 621, "value": 0.1104}

:::MLPv0.5.0 ssd 1541757221.560494900 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 622, "value": 0.11057777777777777}

:::MLPv0.5.0 ssd 1541757221.644355297 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 623, "value": 0.11075555555555555}

:::MLPv0.5.0 ssd 1541757221.728381157 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 624, "value": 0.11093333333333333}

:::MLPv0.5.0 ssd 1541757221.812421322 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 625, "value": 0.1111111111111111}

:::MLPv0.5.0 ssd 1541757221.896285295 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 626, "value": 0.11128888888888888}

:::MLPv0.5.0 ssd 1541757221.980163574 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 627, "value": 0.11146666666666666}

:::MLPv0.5.0 ssd 1541757222.064347029 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 628, "value": 0.11164444444444445}

:::MLPv0.5.0 ssd 1541757222.147976160 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 629, "value": 0.11182222222222223}

:::MLPv0.5.0 ssd 1541757222.232044458 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 630, "value": 0.112}

:::MLPv0.5.0 ssd 1541757222.315973997 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 631, "value": 0.11217777777777778}

:::MLPv0.5.0 ssd 1541757222.400575638 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 632, "value": 0.11235555555555556}

:::MLPv0.5.0 ssd 1541757222.484765768 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 633, "value": 0.11253333333333333}

:::MLPv0.5.0 ssd 1541757222.568364620 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 634, "value": 0.11271111111111111}

:::MLPv0.5.0 ssd 1541757222.652795076 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 635, "value": 0.1128888888888889}

:::MLPv0.5.0 ssd 1541757222.734735966 (train.py:553) train_epoch: 11

:::MLPv0.5.0 ssd 1541757222.738913298 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 636, "value": 0.11306666666666668}

:::MLPv0.5.0 ssd 1541757222.822815657 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 637, "value": 0.11324444444444445}

:::MLPv0.5.0 ssd 1541757222.907272816 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 638, "value": 0.11342222222222223}

:::MLPv0.5.0 ssd 1541757222.991466284 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 639, "value": 0.1136}

:::MLPv0.5.0 ssd 1541757223.075688601 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 640, "value": 0.11377777777777778}
Iteration:    640, Loss function: 5.433, Average Loss: 3.666, avg. samples / sec: 24333.30
Iteration:    640, Loss function: 4.569, Average Loss: 3.671, avg. samples / sec: 24328.83
Iteration:    640, Loss function: 5.761, Average Loss: 3.665, avg. samples / sec: 24373.41
Iteration:    640, Loss function: 5.378, Average Loss: 3.662, avg. samples / sec: 24329.85
Iteration:    640, Loss function: 5.256, Average Loss: 3.666, avg. samples / sec: 24337.88
Iteration:    640, Loss function: 4.961, Average Loss: 3.665, avg. samples / sec: 24321.64
Iteration:    640, Loss function: 5.603, Average Loss: 3.663, avg. samples / sec: 24332.40
Iteration:    640, Loss function: 5.625, Average Loss: 3.669, avg. samples / sec: 24327.58

:::MLPv0.5.0 ssd 1541757223.159740448 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 641, "value": 0.11395555555555556}

:::MLPv0.5.0 ssd 1541757223.243463993 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 642, "value": 0.11413333333333334}

:::MLPv0.5.0 ssd 1541757223.326997280 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 643, "value": 0.11431111111111111}

:::MLPv0.5.0 ssd 1541757223.410892487 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 644, "value": 0.11448888888888889}

:::MLPv0.5.0 ssd 1541757223.495474577 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 645, "value": 0.11466666666666667}

:::MLPv0.5.0 ssd 1541757223.578836679 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 646, "value": 0.11484444444444444}

:::MLPv0.5.0 ssd 1541757223.662997007 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 647, "value": 0.11502222222222222}

:::MLPv0.5.0 ssd 1541757223.747678280 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 648, "value": 0.1152}

:::MLPv0.5.0 ssd 1541757223.831806183 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 649, "value": 0.11537777777777777}

:::MLPv0.5.0 ssd 1541757223.915459394 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 650, "value": 0.11555555555555555}

:::MLPv0.5.0 ssd 1541757223.999281168 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 651, "value": 0.11573333333333333}

:::MLPv0.5.0 ssd 1541757224.083893061 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 652, "value": 0.1159111111111111}

:::MLPv0.5.0 ssd 1541757224.167854071 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 653, "value": 0.11608888888888888}

:::MLPv0.5.0 ssd 1541757224.251564264 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 654, "value": 0.11626666666666667}

:::MLPv0.5.0 ssd 1541757224.335640669 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 655, "value": 0.11644444444444445}

:::MLPv0.5.0 ssd 1541757224.420648098 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 656, "value": 0.11662222222222222}

:::MLPv0.5.0 ssd 1541757224.504526377 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 657, "value": 0.1168}

:::MLPv0.5.0 ssd 1541757224.589018106 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 658, "value": 0.11697777777777778}

:::MLPv0.5.0 ssd 1541757224.673290968 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 659, "value": 0.11715555555555555}

:::MLPv0.5.0 ssd 1541757224.757380962 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 660, "value": 0.11733333333333333}
Iteration:    660, Loss function: 5.345, Average Loss: 3.705, avg. samples / sec: 24363.46
Iteration:    660, Loss function: 5.412, Average Loss: 3.703, avg. samples / sec: 24357.77
Iteration:    660, Loss function: 4.983, Average Loss: 3.698, avg. samples / sec: 24361.25
Iteration:    660, Loss function: 5.484, Average Loss: 3.704, avg. samples / sec: 24407.62
Iteration:    660, Loss function: 5.326, Average Loss: 3.701, avg. samples / sec: 24346.75
Iteration:    660, Loss function: 5.473, Average Loss: 3.706, avg. samples / sec: 24313.38
Iteration:    660, Loss function: 5.516, Average Loss: 3.701, avg. samples / sec: 24345.44
Iteration:    660, Loss function: 5.882, Average Loss: 3.700, avg. samples / sec: 24353.51

:::MLPv0.5.0 ssd 1541757224.841465712 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 661, "value": 0.11751111111111112}

:::MLPv0.5.0 ssd 1541757224.925627470 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 662, "value": 0.1176888888888889}

:::MLPv0.5.0 ssd 1541757225.009578466 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 663, "value": 0.11786666666666668}

:::MLPv0.5.0 ssd 1541757225.093407154 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 664, "value": 0.11804444444444445}

:::MLPv0.5.0 ssd 1541757225.177731037 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 665, "value": 0.11822222222222223}

:::MLPv0.5.0 ssd 1541757225.261396408 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 666, "value": 0.1184}

:::MLPv0.5.0 ssd 1541757225.345509291 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 667, "value": 0.11857777777777778}

:::MLPv0.5.0 ssd 1541757225.429968357 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 668, "value": 0.11875555555555556}

:::MLPv0.5.0 ssd 1541757225.514065504 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 669, "value": 0.11893333333333334}

:::MLPv0.5.0 ssd 1541757225.607532024 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 670, "value": 0.11911111111111111}

:::MLPv0.5.0 ssd 1541757225.691803217 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 671, "value": 0.11928888888888889}

:::MLPv0.5.0 ssd 1541757225.775874138 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 672, "value": 0.11946666666666667}

:::MLPv0.5.0 ssd 1541757225.859873533 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 673, "value": 0.11964444444444444}

:::MLPv0.5.0 ssd 1541757225.943723917 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 674, "value": 0.11982222222222222}

:::MLPv0.5.0 ssd 1541757226.028177738 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 675, "value": 0.12}

:::MLPv0.5.0 ssd 1541757226.112511635 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 676, "value": 0.12017777777777777}

:::MLPv0.5.0 ssd 1541757226.196719170 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 677, "value": 0.12035555555555555}

:::MLPv0.5.0 ssd 1541757226.281160355 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 678, "value": 0.12053333333333333}

:::MLPv0.5.0 ssd 1541757226.365312576 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 679, "value": 0.1207111111111111}

:::MLPv0.5.0 ssd 1541757226.449409485 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 680, "value": 0.12088888888888889}
Iteration:    680, Loss function: 5.267, Average Loss: 3.736, avg. samples / sec: 24208.75
Iteration:    680, Loss function: 5.999, Average Loss: 3.735, avg. samples / sec: 24226.65
Iteration:    680, Loss function: 5.205, Average Loss: 3.739, avg. samples / sec: 24173.04
Iteration:    680, Loss function: 5.905, Average Loss: 3.732, avg. samples / sec: 24175.77
Iteration:    680, Loss function: 5.944, Average Loss: 3.742, avg. samples / sec: 24218.96
Iteration:    680, Loss function: 5.286, Average Loss: 3.739, avg. samples / sec: 24175.93
Iteration:    680, Loss function: 5.407, Average Loss: 3.735, avg. samples / sec: 24217.52
Iteration:    680, Loss function: 5.129, Average Loss: 3.735, avg. samples / sec: 24189.95

:::MLPv0.5.0 ssd 1541757226.533666611 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 681, "value": 0.12106666666666667}

:::MLPv0.5.0 ssd 1541757226.618181705 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 682, "value": 0.12124444444444445}

:::MLPv0.5.0 ssd 1541757226.702344656 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 683, "value": 0.12142222222222222}

:::MLPv0.5.0 ssd 1541757226.786257982 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 684, "value": 0.1216}

:::MLPv0.5.0 ssd 1541757226.872009277 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 685, "value": 0.12177777777777778}

:::MLPv0.5.0 ssd 1541757226.956326723 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 686, "value": 0.12195555555555557}

:::MLPv0.5.0 ssd 1541757227.040397406 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 687, "value": 0.12213333333333334}

:::MLPv0.5.0 ssd 1541757227.124458313 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 688, "value": 0.12231111111111112}

:::MLPv0.5.0 ssd 1541757227.208391666 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 689, "value": 0.1224888888888889}

:::MLPv0.5.0 ssd 1541757227.292441607 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 690, "value": 0.12266666666666667}

:::MLPv0.5.0 ssd 1541757227.376492500 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 691, "value": 0.12284444444444445}

:::MLPv0.5.0 ssd 1541757227.461118221 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 692, "value": 0.12302222222222223}

:::MLPv0.5.0 ssd 1541757227.545169115 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 693, "value": 0.1232}

:::MLPv0.5.0 ssd 1541757227.626600504 (train.py:553) train_epoch: 12

:::MLPv0.5.0 ssd 1541757227.630801439 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 694, "value": 0.12337777777777778}

:::MLPv0.5.0 ssd 1541757227.715309620 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 695, "value": 0.12355555555555556}

:::MLPv0.5.0 ssd 1541757227.799288034 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 696, "value": 0.12373333333333333}

:::MLPv0.5.0 ssd 1541757227.883639097 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 697, "value": 0.12391111111111111}

:::MLPv0.5.0 ssd 1541757227.967968464 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 698, "value": 0.12408888888888889}

:::MLPv0.5.0 ssd 1541757228.052176476 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 699, "value": 0.12426666666666666}

:::MLPv0.5.0 ssd 1541757228.136205912 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 700, "value": 0.12444444444444444}
Iteration:    700, Loss function: 5.646, Average Loss: 3.776, avg. samples / sec: 24285.61
Iteration:    700, Loss function: 5.396, Average Loss: 3.776, avg. samples / sec: 24322.17
Iteration:    700, Loss function: 5.960, Average Loss: 3.775, avg. samples / sec: 24279.81
Iteration:    700, Loss function: 5.413, Average Loss: 3.771, avg. samples / sec: 24290.19
Iteration:    700, Loss function: 6.041, Average Loss: 3.782, avg. samples / sec: 24279.77
Iteration:    700, Loss function: 5.805, Average Loss: 3.782, avg. samples / sec: 24271.99
Iteration:    700, Loss function: 5.891, Average Loss: 3.776, avg. samples / sec: 24297.94
Iteration:    700, Loss function: 6.428, Average Loss: 3.781, avg. samples / sec: 24259.06

:::MLPv0.5.0 ssd 1541757228.220115900 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 701, "value": 0.12462222222222222}

:::MLPv0.5.0 ssd 1541757228.303902149 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 702, "value": 0.1248}

:::MLPv0.5.0 ssd 1541757228.388474941 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 703, "value": 0.12497777777777777}

:::MLPv0.5.0 ssd 1541757228.472474813 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 704, "value": 0.12515555555555555}

:::MLPv0.5.0 ssd 1541757228.556616306 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 705, "value": 0.12533333333333335}

:::MLPv0.5.0 ssd 1541757228.640801191 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 706, "value": 0.12551111111111113}

:::MLPv0.5.0 ssd 1541757228.725033522 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 707, "value": 0.1256888888888889}

:::MLPv0.5.0 ssd 1541757228.808987379 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 708, "value": 0.12586666666666668}

:::MLPv0.5.0 ssd 1541757228.893002987 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 709, "value": 0.12604444444444446}

:::MLPv0.5.0 ssd 1541757228.977685452 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 710, "value": 0.12622222222222224}

:::MLPv0.5.0 ssd 1541757229.062004328 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 711, "value": 0.1264}

:::MLPv0.5.0 ssd 1541757229.145714521 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 712, "value": 0.1265777777777778}

:::MLPv0.5.0 ssd 1541757229.229539156 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 713, "value": 0.12675555555555557}

:::MLPv0.5.0 ssd 1541757229.313716650 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 714, "value": 0.12693333333333334}

:::MLPv0.5.0 ssd 1541757229.397987843 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 715, "value": 0.12711111111111112}

:::MLPv0.5.0 ssd 1541757229.481993914 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 716, "value": 0.1272888888888889}

:::MLPv0.5.0 ssd 1541757229.566039085 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 717, "value": 0.12746666666666667}

:::MLPv0.5.0 ssd 1541757229.649943829 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 718, "value": 0.12764444444444445}

:::MLPv0.5.0 ssd 1541757229.734811544 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 719, "value": 0.12782222222222223}

:::MLPv0.5.0 ssd 1541757229.819189072 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 720, "value": 0.128}
Iteration:    720, Loss function: 5.076, Average Loss: 3.803, avg. samples / sec: 24371.42
Iteration:    720, Loss function: 5.290, Average Loss: 3.808, avg. samples / sec: 24334.02
Iteration:    720, Loss function: 5.225, Average Loss: 3.812, avg. samples / sec: 24378.00
Iteration:    720, Loss function: 5.257, Average Loss: 3.809, avg. samples / sec: 24332.20
Iteration:    720, Loss function: 4.949, Average Loss: 3.809, avg. samples / sec: 24373.83
Iteration:    720, Loss function: 4.913, Average Loss: 3.816, avg. samples / sec: 24366.79
Iteration:    720, Loss function: 5.486, Average Loss: 3.807, avg. samples / sec: 24319.37
Iteration:    720, Loss function: 5.537, Average Loss: 3.813, avg. samples / sec: 24349.01

:::MLPv0.5.0 ssd 1541757229.903684616 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 721, "value": 0.12817777777777778}

:::MLPv0.5.0 ssd 1541757229.987824917 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 722, "value": 0.12835555555555556}

:::MLPv0.5.0 ssd 1541757230.071851730 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 723, "value": 0.12853333333333333}

:::MLPv0.5.0 ssd 1541757230.156096458 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 724, "value": 0.1287111111111111}

:::MLPv0.5.0 ssd 1541757230.240377903 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 725, "value": 0.1288888888888889}

:::MLPv0.5.0 ssd 1541757230.324182987 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 726, "value": 0.12906666666666666}

:::MLPv0.5.0 ssd 1541757230.409883499 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 727, "value": 0.12924444444444444}

:::MLPv0.5.0 ssd 1541757230.493962288 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 728, "value": 0.12942222222222222}

:::MLPv0.5.0 ssd 1541757230.577767849 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 729, "value": 0.1296}

:::MLPv0.5.0 ssd 1541757230.661828756 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 730, "value": 0.12977777777777777}

:::MLPv0.5.0 ssd 1541757230.746068716 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 731, "value": 0.12995555555555555}

:::MLPv0.5.0 ssd 1541757230.830243349 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 732, "value": 0.13013333333333332}

:::MLPv0.5.0 ssd 1541757230.913988590 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 733, "value": 0.1303111111111111}

:::MLPv0.5.0 ssd 1541757230.998347521 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 734, "value": 0.13048888888888888}

:::MLPv0.5.0 ssd 1541757231.082607269 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 735, "value": 0.13066666666666665}

:::MLPv0.5.0 ssd 1541757231.167174339 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 736, "value": 0.13084444444444446}

:::MLPv0.5.0 ssd 1541757231.251756430 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 737, "value": 0.13102222222222223}

:::MLPv0.5.0 ssd 1541757231.335852623 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 738, "value": 0.1312}

:::MLPv0.5.0 ssd 1541757231.419605255 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 739, "value": 0.1313777777777778}

:::MLPv0.5.0 ssd 1541757231.503622055 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 740, "value": 0.13155555555555556}
Iteration:    740, Loss function: 4.859, Average Loss: 3.842, avg. samples / sec: 24322.56
Iteration:    740, Loss function: 5.695, Average Loss: 3.847, avg. samples / sec: 24332.07
Iteration:    740, Loss function: 4.798, Average Loss: 3.837, avg. samples / sec: 24327.37
Iteration:    740, Loss function: 5.313, Average Loss: 3.840, avg. samples / sec: 24298.78
Iteration:    740, Loss function: 5.301, Average Loss: 3.841, avg. samples / sec: 24305.01
Iteration:    740, Loss function: 5.673, Average Loss: 3.839, avg. samples / sec: 24307.39
Iteration:    740, Loss function: 4.882, Average Loss: 3.833, avg. samples / sec: 24280.89
Iteration:    740, Loss function: 5.502, Average Loss: 3.843, avg. samples / sec: 24327.13

:::MLPv0.5.0 ssd 1541757231.587686300 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 741, "value": 0.13173333333333334}

:::MLPv0.5.0 ssd 1541757231.671829700 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 742, "value": 0.13191111111111112}

:::MLPv0.5.0 ssd 1541757231.755853653 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 743, "value": 0.1320888888888889}

:::MLPv0.5.0 ssd 1541757231.839425802 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 744, "value": 0.13226666666666667}

:::MLPv0.5.0 ssd 1541757231.923575401 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 745, "value": 0.13244444444444445}

:::MLPv0.5.0 ssd 1541757232.008124590 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 746, "value": 0.13262222222222222}

:::MLPv0.5.0 ssd 1541757232.092546701 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 747, "value": 0.1328}

:::MLPv0.5.0 ssd 1541757232.176584482 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 748, "value": 0.13297777777777778}

:::MLPv0.5.0 ssd 1541757232.260544300 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 749, "value": 0.13315555555555555}

:::MLPv0.5.0 ssd 1541757232.344405413 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 750, "value": 0.13333333333333333}

:::MLPv0.5.0 ssd 1541757232.425610304 (train.py:553) train_epoch: 13

:::MLPv0.5.0 ssd 1541757232.429860115 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 751, "value": 0.1335111111111111}

:::MLPv0.5.0 ssd 1541757232.513803959 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 752, "value": 0.13368888888888888}

:::MLPv0.5.0 ssd 1541757232.598006487 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 753, "value": 0.13386666666666666}

:::MLPv0.5.0 ssd 1541757232.681829929 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 754, "value": 0.13404444444444444}

:::MLPv0.5.0 ssd 1541757232.765142202 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 755, "value": 0.13422222222222221}

:::MLPv0.5.0 ssd 1541757232.848993301 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 756, "value": 0.1344}

:::MLPv0.5.0 ssd 1541757232.933173418 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 757, "value": 0.13457777777777777}

:::MLPv0.5.0 ssd 1541757233.016946793 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 758, "value": 0.13475555555555557}

:::MLPv0.5.0 ssd 1541757233.100842237 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 759, "value": 0.13493333333333335}

:::MLPv0.5.0 ssd 1541757233.184947968 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 760, "value": 0.13511111111111113}
Iteration:    760, Loss function: 5.310, Average Loss: 3.868, avg. samples / sec: 24357.14
Iteration:    760, Loss function: 4.908, Average Loss: 3.861, avg. samples / sec: 24383.02
Iteration:    760, Loss function: 5.511, Average Loss: 3.873, avg. samples / sec: 24351.78
Iteration:    760, Loss function: 5.199, Average Loss: 3.867, avg. samples / sec: 24349.05
Iteration:    760, Loss function: 6.017, Average Loss: 3.869, avg. samples / sec: 24360.66
Iteration:    760, Loss function: 5.450, Average Loss: 3.868, avg. samples / sec: 24349.80
Iteration:    760, Loss function: 5.473, Average Loss: 3.871, avg. samples / sec: 24355.51
Iteration:    760, Loss function: 5.764, Average Loss: 3.869, avg. samples / sec: 24330.81

:::MLPv0.5.0 ssd 1541757233.268830776 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 761, "value": 0.1352888888888889}

:::MLPv0.5.0 ssd 1541757233.353035450 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 762, "value": 0.13546666666666668}

:::MLPv0.5.0 ssd 1541757233.437371969 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 763, "value": 0.13564444444444446}

:::MLPv0.5.0 ssd 1541757233.521718979 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 764, "value": 0.13582222222222223}

:::MLPv0.5.0 ssd 1541757233.605813980 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 765, "value": 0.136}

:::MLPv0.5.0 ssd 1541757233.689715385 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 766, "value": 0.1361777777777778}

:::MLPv0.5.0 ssd 1541757233.773274422 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 767, "value": 0.13635555555555556}

:::MLPv0.5.0 ssd 1541757233.857582808 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 768, "value": 0.13653333333333334}

:::MLPv0.5.0 ssd 1541757233.941629887 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 769, "value": 0.13671111111111112}

:::MLPv0.5.0 ssd 1541757234.026310205 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 770, "value": 0.1368888888888889}

:::MLPv0.5.0 ssd 1541757234.110673189 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 771, "value": 0.13706666666666667}

:::MLPv0.5.0 ssd 1541757234.194518328 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 772, "value": 0.13724444444444445}

:::MLPv0.5.0 ssd 1541757234.278265715 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 773, "value": 0.13742222222222222}

:::MLPv0.5.0 ssd 1541757234.362320423 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 774, "value": 0.1376}

:::MLPv0.5.0 ssd 1541757234.446282387 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 775, "value": 0.13777777777777778}

:::MLPv0.5.0 ssd 1541757234.530137062 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 776, "value": 0.13795555555555555}

:::MLPv0.5.0 ssd 1541757234.614529133 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 777, "value": 0.13813333333333333}

:::MLPv0.5.0 ssd 1541757234.699535370 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 778, "value": 0.1383111111111111}

:::MLPv0.5.0 ssd 1541757234.784349442 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 779, "value": 0.13848888888888888}

:::MLPv0.5.0 ssd 1541757234.869125605 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 780, "value": 0.13866666666666666}
Iteration:    780, Loss function: 5.166, Average Loss: 3.895, avg. samples / sec: 24350.82
Iteration:    780, Loss function: 4.685, Average Loss: 3.895, avg. samples / sec: 24373.69
Iteration:    780, Loss function: 5.124, Average Loss: 3.892, avg. samples / sec: 24324.38
Iteration:    780, Loss function: 5.384, Average Loss: 3.896, avg. samples / sec: 24354.99
Iteration:    780, Loss function: 4.836, Average Loss: 3.888, avg. samples / sec: 24311.42
Iteration:    780, Loss function: 4.906, Average Loss: 3.894, avg. samples / sec: 24325.01
Iteration:    780, Loss function: 5.175, Average Loss: 3.893, avg. samples / sec: 24316.11
Iteration:    780, Loss function: 4.563, Average Loss: 3.896, avg. samples / sec: 24291.98

:::MLPv0.5.0 ssd 1541757234.953749657 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 781, "value": 0.13884444444444444}

:::MLPv0.5.0 ssd 1541757235.037718058 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 782, "value": 0.1390222222222222}

:::MLPv0.5.0 ssd 1541757235.121993542 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 783, "value": 0.1392}

:::MLPv0.5.0 ssd 1541757235.206290960 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 784, "value": 0.13937777777777777}

:::MLPv0.5.0 ssd 1541757235.290233135 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 785, "value": 0.13955555555555554}

:::MLPv0.5.0 ssd 1541757235.374888182 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 786, "value": 0.13973333333333332}

:::MLPv0.5.0 ssd 1541757235.458641291 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 787, "value": 0.13991111111111112}

:::MLPv0.5.0 ssd 1541757235.542531729 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 788, "value": 0.1400888888888889}

:::MLPv0.5.0 ssd 1541757235.626507998 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 789, "value": 0.14026666666666668}

:::MLPv0.5.0 ssd 1541757235.710399151 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 790, "value": 0.14044444444444446}

:::MLPv0.5.0 ssd 1541757235.795032263 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 791, "value": 0.14062222222222223}

:::MLPv0.5.0 ssd 1541757235.879051924 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 792, "value": 0.1408}

:::MLPv0.5.0 ssd 1541757235.963429451 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 793, "value": 0.14097777777777779}

:::MLPv0.5.0 ssd 1541757236.047353268 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 794, "value": 0.14115555555555556}

:::MLPv0.5.0 ssd 1541757236.131212234 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 795, "value": 0.14133333333333334}

:::MLPv0.5.0 ssd 1541757236.215513706 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 796, "value": 0.14151111111111112}

:::MLPv0.5.0 ssd 1541757236.299569845 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 797, "value": 0.1416888888888889}

:::MLPv0.5.0 ssd 1541757236.383545876 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 798, "value": 0.14186666666666667}

:::MLPv0.5.0 ssd 1541757236.467942238 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 799, "value": 0.14204444444444445}

:::MLPv0.5.0 ssd 1541757236.552306175 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 800, "value": 0.14222222222222222}
Iteration:    800, Loss function: 4.900, Average Loss: 3.922, avg. samples / sec: 24340.81
Iteration:    800, Loss function: 5.713, Average Loss: 3.917, avg. samples / sec: 24374.04
Iteration:    800, Loss function: 5.428, Average Loss: 3.920, avg. samples / sec: 24362.05
Iteration:    800, Loss function: 5.110, Average Loss: 3.914, avg. samples / sec: 24348.37
Iteration:    800, Loss function: 4.977, Average Loss: 3.923, avg. samples / sec: 24355.81
Iteration:    800, Loss function: 5.617, Average Loss: 3.923, avg. samples / sec: 24298.01
Iteration:    800, Loss function: 5.148, Average Loss: 3.921, avg. samples / sec: 24284.36
Iteration:    800, Loss function: 5.063, Average Loss: 3.918, avg. samples / sec: 24283.14

:::MLPv0.5.0 ssd 1541757236.636141777 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 801, "value": 0.1424}

:::MLPv0.5.0 ssd 1541757236.720595598 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 802, "value": 0.14257777777777778}

:::MLPv0.5.0 ssd 1541757236.805141211 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 803, "value": 0.14275555555555555}

:::MLPv0.5.0 ssd 1541757236.889650583 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 804, "value": 0.14293333333333333}

:::MLPv0.5.0 ssd 1541757236.974029303 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 805, "value": 0.1431111111111111}

:::MLPv0.5.0 ssd 1541757237.057988882 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 806, "value": 0.14328888888888888}

:::MLPv0.5.0 ssd 1541757237.142030239 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 807, "value": 0.14346666666666666}

:::MLPv0.5.0 ssd 1541757237.225887299 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 808, "value": 0.14364444444444444}

:::MLPv0.5.0 ssd 1541757237.307025909 (train.py:553) train_epoch: 14

:::MLPv0.5.0 ssd 1541757237.311231852 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 809, "value": 0.14382222222222224}

:::MLPv0.5.0 ssd 1541757237.395516157 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 810, "value": 0.14400000000000002}

:::MLPv0.5.0 ssd 1541757237.479903936 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 811, "value": 0.1441777777777778}

:::MLPv0.5.0 ssd 1541757237.564234495 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 812, "value": 0.14435555555555557}

:::MLPv0.5.0 ssd 1541757237.648265839 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 813, "value": 0.14453333333333335}

:::MLPv0.5.0 ssd 1541757237.732077360 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 814, "value": 0.14471111111111112}

:::MLPv0.5.0 ssd 1541757237.816178799 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 815, "value": 0.1448888888888889}

:::MLPv0.5.0 ssd 1541757237.900185347 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 816, "value": 0.14506666666666668}

:::MLPv0.5.0 ssd 1541757237.983992815 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 817, "value": 0.14524444444444445}

:::MLPv0.5.0 ssd 1541757238.067919254 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 818, "value": 0.14542222222222223}

:::MLPv0.5.0 ssd 1541757238.152014017 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 819, "value": 0.1456}

:::MLPv0.5.0 ssd 1541757238.236405849 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 820, "value": 0.14577777777777778}
Iteration:    820, Loss function: 5.318, Average Loss: 3.950, avg. samples / sec: 24323.93
Iteration:    820, Loss function: 4.647, Average Loss: 3.944, avg. samples / sec: 24325.78
Iteration:    820, Loss function: 4.685, Average Loss: 3.946, avg. samples / sec: 24317.35
Iteration:    820, Loss function: 5.004, Average Loss: 3.948, avg. samples / sec: 24360.97
Iteration:    820, Loss function: 5.564, Average Loss: 3.939, avg. samples / sec: 24316.66
Iteration:    820, Loss function: 4.796, Average Loss: 3.947, avg. samples / sec: 24352.57
Iteration:    820, Loss function: 5.662, Average Loss: 3.950, avg. samples / sec: 24321.14
Iteration:    820, Loss function: 5.355, Average Loss: 3.942, avg. samples / sec: 24317.51

:::MLPv0.5.0 ssd 1541757238.320956707 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 821, "value": 0.14595555555555556}

:::MLPv0.5.0 ssd 1541757238.404703140 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 822, "value": 0.14613333333333334}

:::MLPv0.5.0 ssd 1541757238.488316298 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 823, "value": 0.14631111111111111}

:::MLPv0.5.0 ssd 1541757238.572461367 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 824, "value": 0.1464888888888889}

:::MLPv0.5.0 ssd 1541757238.656572342 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 825, "value": 0.14666666666666667}

:::MLPv0.5.0 ssd 1541757238.740532637 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 826, "value": 0.14684444444444444}

:::MLPv0.5.0 ssd 1541757238.824107170 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 827, "value": 0.14702222222222222}

:::MLPv0.5.0 ssd 1541757238.908756495 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 828, "value": 0.1472}

:::MLPv0.5.0 ssd 1541757238.992463589 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 829, "value": 0.14737777777777777}

:::MLPv0.5.0 ssd 1541757239.076602697 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 830, "value": 0.14755555555555555}

:::MLPv0.5.0 ssd 1541757239.160686493 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 831, "value": 0.14773333333333333}

:::MLPv0.5.0 ssd 1541757239.244619370 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 832, "value": 0.1479111111111111}

:::MLPv0.5.0 ssd 1541757239.328425169 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 833, "value": 0.14808888888888888}

:::MLPv0.5.0 ssd 1541757239.413003683 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 834, "value": 0.14826666666666666}

:::MLPv0.5.0 ssd 1541757239.497435093 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 835, "value": 0.14844444444444443}

:::MLPv0.5.0 ssd 1541757239.581845760 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 836, "value": 0.1486222222222222}

:::MLPv0.5.0 ssd 1541757239.666179895 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 837, "value": 0.14880000000000002}

:::MLPv0.5.0 ssd 1541757239.750759363 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 838, "value": 0.1489777777777778}

:::MLPv0.5.0 ssd 1541757239.834663391 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 839, "value": 0.14915555555555557}

:::MLPv0.5.0 ssd 1541757239.918771029 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 840, "value": 0.14933333333333335}
Iteration:    840, Loss function: 5.063, Average Loss: 3.966, avg. samples / sec: 24346.38
Iteration:    840, Loss function: 5.373, Average Loss: 3.975, avg. samples / sec: 24341.52
Iteration:    840, Loss function: 5.551, Average Loss: 3.972, avg. samples / sec: 24367.86
Iteration:    840, Loss function: 5.375, Average Loss: 3.966, avg. samples / sec: 24404.03
Iteration:    840, Loss function: 5.438, Average Loss: 3.970, avg. samples / sec: 24346.45
Iteration:    840, Loss function: 5.586, Average Loss: 3.964, avg. samples / sec: 24330.39
Iteration:    840, Loss function: 4.995, Average Loss: 3.974, avg. samples / sec: 24325.98
Iteration:    840, Loss function: 5.092, Average Loss: 3.972, avg. samples / sec: 24297.64

:::MLPv0.5.0 ssd 1541757240.003125668 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 841, "value": 0.14951111111111112}

:::MLPv0.5.0 ssd 1541757240.086941242 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 842, "value": 0.1496888888888889}

:::MLPv0.5.0 ssd 1541757240.170379877 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 843, "value": 0.14986666666666668}

:::MLPv0.5.0 ssd 1541757240.254566908 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 844, "value": 0.15004444444444445}

:::MLPv0.5.0 ssd 1541757240.338964939 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 845, "value": 0.15022222222222223}

:::MLPv0.5.0 ssd 1541757240.422917604 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 846, "value": 0.1504}

:::MLPv0.5.0 ssd 1541757240.507254362 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 847, "value": 0.15057777777777778}

:::MLPv0.5.0 ssd 1541757240.591039181 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 848, "value": 0.15075555555555556}

:::MLPv0.5.0 ssd 1541757240.674108982 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 849, "value": 0.15093333333333334}

:::MLPv0.5.0 ssd 1541757240.757950068 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 850, "value": 0.1511111111111111}

:::MLPv0.5.0 ssd 1541757240.841661453 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 851, "value": 0.1512888888888889}

:::MLPv0.5.0 ssd 1541757240.925891161 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 852, "value": 0.15146666666666667}

:::MLPv0.5.0 ssd 1541757241.009831667 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 853, "value": 0.15164444444444444}

:::MLPv0.5.0 ssd 1541757241.093364000 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 854, "value": 0.15182222222222222}

:::MLPv0.5.0 ssd 1541757241.177068233 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 855, "value": 0.152}

:::MLPv0.5.0 ssd 1541757241.261130810 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 856, "value": 0.15217777777777777}

:::MLPv0.5.0 ssd 1541757241.345303059 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 857, "value": 0.15235555555555555}

:::MLPv0.5.0 ssd 1541757241.429584026 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 858, "value": 0.15253333333333333}

:::MLPv0.5.0 ssd 1541757241.513752222 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 859, "value": 0.1527111111111111}

:::MLPv0.5.0 ssd 1541757241.598213434 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 860, "value": 0.15288888888888888}
Iteration:    860, Loss function: 5.278, Average Loss: 3.998, avg. samples / sec: 24395.56
Iteration:    860, Loss function: 5.236, Average Loss: 3.988, avg. samples / sec: 24389.51
Iteration:    860, Loss function: 5.205, Average Loss: 3.991, avg. samples / sec: 24388.16
Iteration:    860, Loss function: 4.412, Average Loss: 3.998, avg. samples / sec: 24415.26
Iteration:    860, Loss function: 4.861, Average Loss: 3.992, avg. samples / sec: 24351.03
Iteration:    860, Loss function: 5.427, Average Loss: 3.990, avg. samples / sec: 24366.95
Iteration:    860, Loss function: 5.337, Average Loss: 3.996, avg. samples / sec: 24392.11
Iteration:    860, Loss function: 5.153, Average Loss: 3.994, avg. samples / sec: 24322.17

:::MLPv0.5.0 ssd 1541757241.682705641 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 861, "value": 0.15306666666666666}

:::MLPv0.5.0 ssd 1541757241.767744780 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 862, "value": 0.15324444444444446}

:::MLPv0.5.0 ssd 1541757241.851500034 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 863, "value": 0.15342222222222224}

:::MLPv0.5.0 ssd 1541757241.935654163 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 864, "value": 0.15360000000000001}

:::MLPv0.5.0 ssd 1541757242.019591570 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 865, "value": 0.1537777777777778}

:::MLPv0.5.0 ssd 1541757242.103913069 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 866, "value": 0.15395555555555557}

:::MLPv0.5.0 ssd 1541757242.185485125 (train.py:553) train_epoch: 15

:::MLPv0.5.0 ssd 1541757242.189670801 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 867, "value": 0.15413333333333334}

:::MLPv0.5.0 ssd 1541757242.273872137 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 868, "value": 0.15431111111111112}

:::MLPv0.5.0 ssd 1541757242.358510733 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 869, "value": 0.1544888888888889}

:::MLPv0.5.0 ssd 1541757242.443336964 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 870, "value": 0.15466666666666667}

:::MLPv0.5.0 ssd 1541757242.527349234 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 871, "value": 0.15484444444444445}

:::MLPv0.5.0 ssd 1541757242.611380577 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 872, "value": 0.15502222222222223}

:::MLPv0.5.0 ssd 1541757242.695384979 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 873, "value": 0.1552}

:::MLPv0.5.0 ssd 1541757242.779215097 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 874, "value": 0.15537777777777778}

:::MLPv0.5.0 ssd 1541757242.863688707 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 875, "value": 0.15555555555555556}

:::MLPv0.5.0 ssd 1541757242.947915792 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 876, "value": 0.15573333333333333}

:::MLPv0.5.0 ssd 1541757243.032108068 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 877, "value": 0.1559111111111111}

:::MLPv0.5.0 ssd 1541757243.116819382 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 878, "value": 0.1560888888888889}

:::MLPv0.5.0 ssd 1541757243.200872421 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 879, "value": 0.15626666666666666}

:::MLPv0.5.0 ssd 1541757243.284509897 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 880, "value": 0.15644444444444444}
Iteration:    880, Loss function: 5.148, Average Loss: 4.018, avg. samples / sec: 24289.24
Iteration:    880, Loss function: 5.425, Average Loss: 4.013, avg. samples / sec: 24334.64
Iteration:    880, Loss function: 4.729, Average Loss: 4.014, avg. samples / sec: 24355.31
Iteration:    880, Loss function: 5.027, Average Loss: 4.014, avg. samples / sec: 24289.95
Iteration:    880, Loss function: 4.827, Average Loss: 4.008, avg. samples / sec: 24280.74
Iteration:    880, Loss function: 5.599, Average Loss: 4.014, avg. samples / sec: 24338.63
Iteration:    880, Loss function: 4.889, Average Loss: 4.017, avg. samples / sec: 24306.09
Iteration:    880, Loss function: 4.498, Average Loss: 4.019, avg. samples / sec: 24257.57

:::MLPv0.5.0 ssd 1541757243.369958878 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 881, "value": 0.15662222222222222}

:::MLPv0.5.0 ssd 1541757243.453727245 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 882, "value": 0.1568}

:::MLPv0.5.0 ssd 1541757243.537970066 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 883, "value": 0.15697777777777777}

:::MLPv0.5.0 ssd 1541757243.622467518 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 884, "value": 0.15715555555555555}

:::MLPv0.5.0 ssd 1541757243.705755711 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 885, "value": 0.15733333333333333}

:::MLPv0.5.0 ssd 1541757243.790541887 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 886, "value": 0.1575111111111111}

:::MLPv0.5.0 ssd 1541757243.874548912 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 887, "value": 0.15768888888888888}

:::MLPv0.5.0 ssd 1541757243.959334612 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 888, "value": 0.15786666666666668}

:::MLPv0.5.0 ssd 1541757244.042848349 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 889, "value": 0.15804444444444446}

:::MLPv0.5.0 ssd 1541757244.126686573 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 890, "value": 0.15822222222222224}

:::MLPv0.5.0 ssd 1541757244.211865902 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 891, "value": 0.1584}

:::MLPv0.5.0 ssd 1541757244.296047688 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 892, "value": 0.1585777777777778}

:::MLPv0.5.0 ssd 1541757244.380015850 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 893, "value": 0.15875555555555557}

:::MLPv0.5.0 ssd 1541757244.464784145 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 894, "value": 0.15893333333333334}

:::MLPv0.5.0 ssd 1541757244.549074650 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 895, "value": 0.15911111111111112}

:::MLPv0.5.0 ssd 1541757244.632965326 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 896, "value": 0.1592888888888889}

:::MLPv0.5.0 ssd 1541757244.718362570 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 897, "value": 0.15946666666666667}

:::MLPv0.5.0 ssd 1541757244.802550793 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 898, "value": 0.15964444444444445}

:::MLPv0.5.0 ssd 1541757244.886483669 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 899, "value": 0.15982222222222223}
Iteration:    900, Loss function: 5.491, Average Loss: 4.028, avg. samples / sec: 24319.96
Iteration:    900, Loss function: 5.687, Average Loss: 4.040, avg. samples / sec: 24294.09
Iteration:    900, Loss function: 5.726, Average Loss: 4.039, avg. samples / sec: 24287.40
Iteration:    900, Loss function: 4.956, Average Loss: 4.035, avg. samples / sec: 24278.53
Iteration:    900, Loss function: 5.739, Average Loss: 4.040, avg. samples / sec: 24274.44
Iteration:    900, Loss function: 5.471, Average Loss: 4.040, avg. samples / sec: 24308.34
Iteration:    900, Loss function: 5.192, Average Loss: 4.043, avg. samples / sec: 24315.49
Iteration:    900, Loss function: 4.802, Average Loss: 4.035, avg. samples / sec: 24261.50
Iteration:    920, Loss function: 5.191, Average Loss: 4.060, avg. samples / sec: 24590.03
Iteration:    920, Loss function: 4.735, Average Loss: 4.048, avg. samples / sec: 24570.41
Iteration:    920, Loss function: 4.934, Average Loss: 4.056, avg. samples / sec: 24603.98
Iteration:    920, Loss function: 4.839, Average Loss: 4.060, avg. samples / sec: 24616.30
Iteration:    920, Loss function: 4.668, Average Loss: 4.058, avg. samples / sec: 24605.25
Iteration:    920, Loss function: 5.498, Average Loss: 4.060, avg. samples / sec: 24581.22
Iteration:    920, Loss function: 4.937, Average Loss: 4.061, avg. samples / sec: 24572.29
Iteration:    920, Loss function: 5.315, Average Loss: 4.057, avg. samples / sec: 24572.71

:::MLPv0.5.0 ssd 1541757247.051590919 (train.py:553) train_epoch: 16
Iteration:    940, Loss function: 4.490, Average Loss: 4.075, avg. samples / sec: 24565.28
Iteration:    940, Loss function: 4.521, Average Loss: 4.073, avg. samples / sec: 24614.83
Iteration:    940, Loss function: 5.153, Average Loss: 4.067, avg. samples / sec: 24554.13
Iteration:    940, Loss function: 5.134, Average Loss: 4.075, avg. samples / sec: 24558.85
Iteration:    940, Loss function: 4.699, Average Loss: 4.073, avg. samples / sec: 24554.62
Iteration:    940, Loss function: 4.500, Average Loss: 4.075, avg. samples / sec: 24553.61
Iteration:    940, Loss function: 5.010, Average Loss: 4.076, avg. samples / sec: 24518.63
Iteration:    940, Loss function: 4.836, Average Loss: 4.076, avg. samples / sec: 24551.73
Iteration:    960, Loss function: 4.709, Average Loss: 4.094, avg. samples / sec: 24532.68
Iteration:    960, Loss function: 5.485, Average Loss: 4.092, avg. samples / sec: 24535.79
Iteration:    960, Loss function: 5.139, Average Loss: 4.094, avg. samples / sec: 24542.87
Iteration:    960, Loss function: 5.140, Average Loss: 4.086, avg. samples / sec: 24540.22
Iteration:    960, Loss function: 4.942, Average Loss: 4.097, avg. samples / sec: 24579.22
Iteration:    960, Loss function: 5.260, Average Loss: 4.093, avg. samples / sec: 24540.16
Iteration:    960, Loss function: 5.172, Average Loss: 4.095, avg. samples / sec: 24540.52
Iteration:    960, Loss function: 4.972, Average Loss: 4.095, avg. samples / sec: 24563.52
Iteration:    980, Loss function: 4.652, Average Loss: 4.109, avg. samples / sec: 24580.32
Iteration:    980, Loss function: 5.082, Average Loss: 4.108, avg. samples / sec: 24588.92
Iteration:    980, Loss function: 5.055, Average Loss: 4.100, avg. samples / sec: 24581.99
Iteration:    980, Loss function: 4.836, Average Loss: 4.105, avg. samples / sec: 24588.46
Iteration:    980, Loss function: 4.966, Average Loss: 4.110, avg. samples / sec: 24574.44
Iteration:    980, Loss function: 4.374, Average Loss: 4.109, avg. samples / sec: 24562.64
Iteration:    980, Loss function: 4.360, Average Loss: 4.109, avg. samples / sec: 24533.16
Iteration:    980, Loss function: 4.617, Average Loss: 4.105, avg. samples / sec: 24523.44

:::MLPv0.5.0 ssd 1541757251.805097818 (train.py:553) train_epoch: 17
Iteration:   1000, Loss function: 4.541, Average Loss: 4.124, avg. samples / sec: 24609.36
Iteration:   1000, Loss function: 5.483, Average Loss: 4.116, avg. samples / sec: 24609.25
Iteration:   1000, Loss function: 4.743, Average Loss: 4.118, avg. samples / sec: 24604.55
Iteration:   1000, Loss function: 5.368, Average Loss: 4.125, avg. samples / sec: 24633.62
Iteration:   1000, Loss function: 5.008, Average Loss: 4.121, avg. samples / sec: 24586.00
Iteration:   1000, Loss function: 4.723, Average Loss: 4.122, avg. samples / sec: 24639.38
Iteration:   1000, Loss function: 4.837, Average Loss: 4.118, avg. samples / sec: 24621.34
Iteration:   1000, Loss function: 4.585, Average Loss: 4.124, avg. samples / sec: 24601.56
Iteration:   1020, Loss function: 5.077, Average Loss: 4.141, avg. samples / sec: 24599.08
Iteration:   1020, Loss function: 4.761, Average Loss: 4.135, avg. samples / sec: 24600.75
Iteration:   1020, Loss function: 5.061, Average Loss: 4.133, avg. samples / sec: 24591.70
Iteration:   1020, Loss function: 4.787, Average Loss: 4.141, avg. samples / sec: 24608.83
Iteration:   1020, Loss function: 5.409, Average Loss: 4.140, avg. samples / sec: 24576.95
Iteration:   1020, Loss function: 4.540, Average Loss: 4.136, avg. samples / sec: 24596.88
Iteration:   1020, Loss function: 5.190, Average Loss: 4.140, avg. samples / sec: 24595.61
Iteration:   1020, Loss function: 4.665, Average Loss: 4.144, avg. samples / sec: 24556.95

:::MLPv0.5.0 ssd 1541757256.633329868 (train.py:553) train_epoch: 18
Iteration:   1040, Loss function: 4.441, Average Loss: 4.157, avg. samples / sec: 24573.97
Iteration:   1040, Loss function: 4.969, Average Loss: 4.156, avg. samples / sec: 24629.40
Iteration:   1040, Loss function: 3.751, Average Loss: 4.153, avg. samples / sec: 24623.96
Iteration:   1040, Loss function: 4.540, Average Loss: 4.156, avg. samples / sec: 24613.92
Iteration:   1040, Loss function: 4.688, Average Loss: 4.151, avg. samples / sec: 24576.06
Iteration:   1040, Loss function: 4.747, Average Loss: 4.160, avg. samples / sec: 24535.92
Iteration:   1040, Loss function: 4.836, Average Loss: 4.160, avg. samples / sec: 24585.06
Iteration:   1040, Loss function: 4.417, Average Loss: 4.151, avg. samples / sec: 24516.60
Iteration:   1060, Loss function: 4.484, Average Loss: 4.166, avg. samples / sec: 24574.28
Iteration:   1060, Loss function: 4.853, Average Loss: 4.168, avg. samples / sec: 24571.49
Iteration:   1060, Loss function: 4.729, Average Loss: 4.164, avg. samples / sec: 24568.32
Iteration:   1060, Loss function: 4.920, Average Loss: 4.169, avg. samples / sec: 24568.92
Iteration:   1060, Loss function: 5.126, Average Loss: 4.170, avg. samples / sec: 24615.37
Iteration:   1060, Loss function: 5.244, Average Loss: 4.162, avg. samples / sec: 24623.14
Iteration:   1060, Loss function: 4.466, Average Loss: 4.161, avg. samples / sec: 24568.71
Iteration:   1060, Loss function: 4.758, Average Loss: 4.172, avg. samples / sec: 24567.47
Iteration:   1080, Loss function: 4.731, Average Loss: 4.180, avg. samples / sec: 24555.92
Iteration:   1080, Loss function: 4.392, Average Loss: 4.173, avg. samples / sec: 24558.87
Iteration:   1080, Loss function: 4.423, Average Loss: 4.173, avg. samples / sec: 24563.41
Iteration:   1080, Loss function: 4.431, Average Loss: 4.182, avg. samples / sec: 24556.10
Iteration:   1080, Loss function: 4.406, Average Loss: 4.173, avg. samples / sec: 24558.13
Iteration:   1080, Loss function: 4.486, Average Loss: 4.175, avg. samples / sec: 24523.45
Iteration:   1080, Loss function: 5.024, Average Loss: 4.180, avg. samples / sec: 24525.54
Iteration:   1080, Loss function: 4.937, Average Loss: 4.183, avg. samples / sec: 24549.73

:::MLPv0.5.0 ssd 1541757261.470898867 (train.py:553) train_epoch: 19
Iteration:   1100, Loss function: 4.956, Average Loss: 4.192, avg. samples / sec: 24543.08
Iteration:   1100, Loss function: 4.313, Average Loss: 4.184, avg. samples / sec: 24543.83
Iteration:   1100, Loss function: 5.256, Average Loss: 4.187, avg. samples / sec: 24546.01
Iteration:   1100, Loss function: 4.807, Average Loss: 4.185, avg. samples / sec: 24541.85
Iteration:   1100, Loss function: 4.855, Average Loss: 4.192, avg. samples / sec: 24532.18
Iteration:   1100, Loss function: 4.824, Average Loss: 4.194, avg. samples / sec: 24567.87
Iteration:   1100, Loss function: 4.300, Average Loss: 4.185, avg. samples / sec: 24523.33
Iteration:   1100, Loss function: 4.548, Average Loss: 4.190, avg. samples / sec: 24524.38
Iteration:   1120, Loss function: 5.417, Average Loss: 4.204, avg. samples / sec: 24630.70
Iteration:   1120, Loss function: 4.281, Average Loss: 4.200, avg. samples / sec: 24641.30
Iteration:   1120, Loss function: 4.705, Average Loss: 4.199, avg. samples / sec: 24678.16
Iteration:   1120, Loss function: 4.892, Average Loss: 4.203, avg. samples / sec: 24632.45
Iteration:   1120, Loss function: 4.739, Average Loss: 4.193, avg. samples / sec: 24605.51
Iteration:   1120, Loss function: 5.106, Average Loss: 4.197, avg. samples / sec: 24590.48
Iteration:   1120, Loss function: 4.640, Average Loss: 4.194, avg. samples / sec: 24631.84
Iteration:   1120, Loss function: 5.569, Average Loss: 4.194, avg. samples / sec: 24576.00
Iteration:   1140, Loss function: 4.229, Average Loss: 4.211, avg. samples / sec: 24580.29
Iteration:   1140, Loss function: 4.529, Average Loss: 4.212, avg. samples / sec: 24571.88
Iteration:   1140, Loss function: 4.322, Average Loss: 4.207, avg. samples / sec: 24609.62
Iteration:   1140, Loss function: 4.475, Average Loss: 4.211, avg. samples / sec: 24576.07
Iteration:   1140, Loss function: 4.663, Average Loss: 4.202, avg. samples / sec: 24594.76
Iteration:   1140, Loss function: 4.896, Average Loss: 4.208, avg. samples / sec: 24536.54
Iteration:   1140, Loss function: 4.356, Average Loss: 4.203, avg. samples / sec: 24563.56
Iteration:   1140, Loss function: 4.854, Average Loss: 4.201, avg. samples / sec: 24548.52

:::MLPv0.5.0 ssd 1541757266.299229622 (train.py:553) train_epoch: 20
Iteration:   1160, Loss function: 4.783, Average Loss: 4.214, avg. samples / sec: 24649.33
Iteration:   1160, Loss function: 4.451, Average Loss: 4.210, avg. samples / sec: 24644.78
Iteration:   1160, Loss function: 4.830, Average Loss: 4.216, avg. samples / sec: 24586.85
Iteration:   1160, Loss function: 4.842, Average Loss: 4.219, avg. samples / sec: 24579.76
Iteration:   1160, Loss function: 4.162, Average Loss: 4.222, avg. samples / sec: 24575.73
Iteration:   1160, Loss function: 5.224, Average Loss: 4.212, avg. samples / sec: 24605.90
Iteration:   1160, Loss function: 4.596, Average Loss: 4.219, avg. samples / sec: 24604.51
Iteration:   1160, Loss function: 4.673, Average Loss: 4.219, avg. samples / sec: 24573.54
Iteration:   1180, Loss function: 4.664, Average Loss: 4.232, avg. samples / sec: 24580.59
Iteration:   1180, Loss function: 4.628, Average Loss: 4.218, avg. samples / sec: 24561.72
Iteration:   1180, Loss function: 4.631, Average Loss: 4.219, avg. samples / sec: 24572.06
Iteration:   1180, Loss function: 4.235, Average Loss: 4.228, avg. samples / sec: 24562.06
Iteration:   1180, Loss function: 4.400, Average Loss: 4.230, avg. samples / sec: 24597.20
Iteration:   1180, Loss function: 4.209, Average Loss: 4.221, avg. samples / sec: 24556.22
Iteration:   1180, Loss function: 3.827, Average Loss: 4.223, avg. samples / sec: 24527.86
Iteration:   1180, Loss function: 4.183, Average Loss: 4.226, avg. samples / sec: 24539.21
Iteration:   1200, Loss function: 4.597, Average Loss: 4.235, avg. samples / sec: 24625.55
Iteration:   1200, Loss function: 4.164, Average Loss: 4.233, avg. samples / sec: 24637.63
Iteration:   1200, Loss function: 4.124, Average Loss: 4.221, avg. samples / sec: 24629.45
Iteration:   1200, Loss function: 4.452, Average Loss: 4.226, avg. samples / sec: 24636.66
Iteration:   1200, Loss function: 4.418, Average Loss: 4.227, avg. samples / sec: 24635.37
Iteration:   1200, Loss function: 4.162, Average Loss: 4.231, avg. samples / sec: 24651.89
Iteration:   1200, Loss function: 4.650, Average Loss: 4.238, avg. samples / sec: 24609.76
Iteration:   1200, Loss function: 4.479, Average Loss: 4.222, avg. samples / sec: 24591.29

:::MLPv0.5.0 ssd 1541757271.047782183 (train.py:553) train_epoch: 21
Iteration:   1220, Loss function: 4.454, Average Loss: 4.243, avg. samples / sec: 24570.30
Iteration:   1220, Loss function: 4.714, Average Loss: 4.228, avg. samples / sec: 24585.47
Iteration:   1220, Loss function: 3.771, Average Loss: 4.225, avg. samples / sec: 24543.02
Iteration:   1220, Loss function: 4.730, Average Loss: 4.233, avg. samples / sec: 24544.30
Iteration:   1220, Loss function: 4.135, Average Loss: 4.238, avg. samples / sec: 24511.25
Iteration:   1220, Loss function: 4.634, Average Loss: 4.232, avg. samples / sec: 24514.71
Iteration:   1220, Loss function: 4.546, Average Loss: 4.240, avg. samples / sec: 24500.43
Iteration:   1220, Loss function: 4.850, Average Loss: 4.240, avg. samples / sec: 24519.35
Iteration:   1240, Loss function: 4.765, Average Loss: 4.252, avg. samples / sec: 24619.92
Iteration:   1240, Loss function: 4.590, Average Loss: 4.238, avg. samples / sec: 24611.59
Iteration:   1240, Loss function: 5.458, Average Loss: 4.243, avg. samples / sec: 24633.48
Iteration:   1240, Loss function: 5.185, Average Loss: 4.241, avg. samples / sec: 24640.70
Iteration:   1240, Loss function: 5.077, Average Loss: 4.232, avg. samples / sec: 24608.13
Iteration:   1240, Loss function: 4.909, Average Loss: 4.248, avg. samples / sec: 24614.11
Iteration:   1240, Loss function: 4.949, Average Loss: 4.248, avg. samples / sec: 24621.60
Iteration:   1240, Loss function: 4.643, Average Loss: 4.250, avg. samples / sec: 24584.56
Iteration:   1260, Loss function: 4.732, Average Loss: 4.257, avg. samples / sec: 24561.99
Iteration:   1260, Loss function: 4.209, Average Loss: 4.239, avg. samples / sec: 24536.42
Iteration:   1260, Loss function: 4.574, Average Loss: 4.255, avg. samples / sec: 24554.75
Iteration:   1260, Loss function: 4.568, Average Loss: 4.244, avg. samples / sec: 24522.46
Iteration:   1260, Loss function: 4.395, Average Loss: 4.258, avg. samples / sec: 24570.32
Iteration:   1260, Loss function: 3.921, Average Loss: 4.247, avg. samples / sec: 24485.22
Iteration:   1260, Loss function: 4.540, Average Loss: 4.257, avg. samples / sec: 24477.48
Iteration:   1260, Loss function: 4.194, Average Loss: 4.248, avg. samples / sec: 24478.32

:::MLPv0.5.0 ssd 1541757275.886303663 (train.py:553) train_epoch: 22
Iteration:   1280, Loss function: 4.056, Average Loss: 4.250, avg. samples / sec: 24556.77
Iteration:   1280, Loss function: 4.482, Average Loss: 4.261, avg. samples / sec: 24552.19
Iteration:   1280, Loss function: 4.322, Average Loss: 4.243, avg. samples / sec: 24541.82
Iteration:   1280, Loss function: 4.548, Average Loss: 4.262, avg. samples / sec: 24512.19
Iteration:   1280, Loss function: 4.320, Average Loss: 4.253, avg. samples / sec: 24560.93
Iteration:   1280, Loss function: 4.712, Average Loss: 4.256, avg. samples / sec: 24548.84
Iteration:   1280, Loss function: 4.888, Average Loss: 4.266, avg. samples / sec: 24530.22
Iteration:   1280, Loss function: 4.867, Average Loss: 4.262, avg. samples / sec: 24533.15
Iteration:   1300, Loss function: 4.790, Average Loss: 4.269, avg. samples / sec: 24640.16
Iteration:   1300, Loss function: 3.544, Average Loss: 4.263, avg. samples / sec: 24636.69
Iteration:   1300, Loss function: 4.697, Average Loss: 4.256, avg. samples / sec: 24615.86
Iteration:   1300, Loss function: 3.883, Average Loss: 4.255, avg. samples / sec: 24574.40
Iteration:   1300, Loss function: 4.801, Average Loss: 4.263, avg. samples / sec: 24572.50
Iteration:   1300, Loss function: 4.416, Average Loss: 4.248, avg. samples / sec: 24572.11
Iteration:   1300, Loss function: 4.248, Average Loss: 4.261, avg. samples / sec: 24589.50
Iteration:   1300, Loss function: 4.665, Average Loss: 4.265, avg. samples / sec: 24560.42
Iteration:   1320, Loss function: 4.595, Average Loss: 4.273, avg. samples / sec: 24601.24
Iteration:   1320, Loss function: 4.963, Average Loss: 4.263, avg. samples / sec: 24597.15
Iteration:   1320, Loss function: 4.687, Average Loss: 4.264, avg. samples / sec: 24594.26
Iteration:   1320, Loss function: 5.372, Average Loss: 4.274, avg. samples / sec: 24640.20
Iteration:   1320, Loss function: 5.030, Average Loss: 4.269, avg. samples / sec: 24586.79
Iteration:   1320, Loss function: 5.029, Average Loss: 4.271, avg. samples / sec: 24553.85
Iteration:   1320, Loss function: 5.130, Average Loss: 4.277, avg. samples / sec: 24538.69
Iteration:   1320, Loss function: 4.687, Average Loss: 4.254, avg. samples / sec: 24549.22

:::MLPv0.5.0 ssd 1541757280.717672348 (train.py:553) train_epoch: 23
Iteration:   1340, Loss function: 4.400, Average Loss: 4.275, avg. samples / sec: 24626.47
Iteration:   1340, Loss function: 4.764, Average Loss: 4.261, avg. samples / sec: 24641.76
Iteration:   1340, Loss function: 4.138, Average Loss: 4.268, avg. samples / sec: 24585.22
Iteration:   1340, Loss function: 4.388, Average Loss: 4.272, avg. samples / sec: 24627.63
Iteration:   1340, Loss function: 4.638, Average Loss: 4.270, avg. samples / sec: 24584.03
Iteration:   1340, Loss function: 3.763, Average Loss: 4.280, avg. samples / sec: 24604.71
Iteration:   1340, Loss function: 3.826, Average Loss: 4.277, avg. samples / sec: 24540.07
Iteration:   1340, Loss function: 4.093, Average Loss: 4.278, avg. samples / sec: 24541.46
Iteration:   1360, Loss function: 4.582, Average Loss: 4.282, avg. samples / sec: 24649.36
Iteration:   1360, Loss function: 4.480, Average Loss: 4.262, avg. samples / sec: 24596.14
Iteration:   1360, Loss function: 4.185, Average Loss: 4.271, avg. samples / sec: 24601.69
Iteration:   1360, Loss function: 4.741, Average Loss: 4.272, avg. samples / sec: 24596.56
Iteration:   1360, Loss function: 4.678, Average Loss: 4.273, avg. samples / sec: 24588.69
Iteration:   1360, Loss function: 4.063, Average Loss: 4.279, avg. samples / sec: 24617.53
Iteration:   1360, Loss function: 4.406, Average Loss: 4.279, avg. samples / sec: 24567.04
Iteration:   1360, Loss function: 4.319, Average Loss: 4.282, avg. samples / sec: 24595.85
Iteration:   1380, Loss function: 4.616, Average Loss: 4.283, avg. samples / sec: 24608.11
Iteration:   1380, Loss function: 4.468, Average Loss: 4.265, avg. samples / sec: 24573.87
Iteration:   1380, Loss function: 4.281, Average Loss: 4.282, avg. samples / sec: 24598.68
Iteration:   1380, Loss function: 4.472, Average Loss: 4.276, avg. samples / sec: 24551.55
Iteration:   1380, Loss function: 4.207, Average Loss: 4.282, avg. samples / sec: 24569.45
Iteration:   1380, Loss function: 4.554, Average Loss: 4.285, avg. samples / sec: 24542.19
Iteration:   1380, Loss function: 4.913, Average Loss: 4.279, avg. samples / sec: 24545.14
Iteration:   1380, Loss function: 4.499, Average Loss: 4.276, avg. samples / sec: 24521.30

:::MLPv0.5.0 ssd 1541757285.549102068 (train.py:553) train_epoch: 24
Iteration:   1400, Loss function: 4.929, Average Loss: 4.290, avg. samples / sec: 24636.04
Iteration:   1400, Loss function: 4.627, Average Loss: 4.275, avg. samples / sec: 24612.79
Iteration:   1400, Loss function: 5.243, Average Loss: 4.294, avg. samples / sec: 24614.26
Iteration:   1400, Loss function: 4.757, Average Loss: 4.295, avg. samples / sec: 24634.84
Iteration:   1400, Loss function: 5.012, Average Loss: 4.294, avg. samples / sec: 24602.97
Iteration:   1400, Loss function: 4.248, Average Loss: 4.290, avg. samples / sec: 24642.25
Iteration:   1400, Loss function: 4.695, Average Loss: 4.290, avg. samples / sec: 24620.68
Iteration:   1400, Loss function: 4.917, Average Loss: 4.287, avg. samples / sec: 24590.86
Iteration:   1420, Loss function: 4.802, Average Loss: 4.297, avg. samples / sec: 24579.16
Iteration:   1420, Loss function: 4.547, Average Loss: 4.294, avg. samples / sec: 24640.61
Iteration:   1420, Loss function: 4.297, Average Loss: 4.294, avg. samples / sec: 24565.59
Iteration:   1420, Loss function: 4.013, Average Loss: 4.301, avg. samples / sec: 24561.72
Iteration:   1420, Loss function: 4.458, Average Loss: 4.281, avg. samples / sec: 24550.03
Iteration:   1420, Loss function: 4.805, Average Loss: 4.297, avg. samples / sec: 24547.05
Iteration:   1420, Loss function: 4.669, Average Loss: 4.303, avg. samples / sec: 24525.68
Iteration:   1420, Loss function: 4.134, Average Loss: 4.302, avg. samples / sec: 24523.13
Iteration:   1440, Loss function: 4.143, Average Loss: 4.298, avg. samples / sec: 24662.57
Iteration:   1440, Loss function: 4.461, Average Loss: 4.295, avg. samples / sec: 24623.18
Iteration:   1440, Loss function: 4.932, Average Loss: 4.297, avg. samples / sec: 24624.24
Iteration:   1440, Loss function: 4.303, Average Loss: 4.303, avg. samples / sec: 24662.39
Iteration:   1440, Loss function: 4.576, Average Loss: 4.303, avg. samples / sec: 24666.42
Iteration:   1440, Loss function: 4.160, Average Loss: 4.280, avg. samples / sec: 24633.33
Iteration:   1440, Loss function: 4.591, Average Loss: 4.296, avg. samples / sec: 24615.64
Iteration:   1440, Loss function: 4.083, Average Loss: 4.302, avg. samples / sec: 24625.67

:::MLPv0.5.0 ssd 1541757290.296892643 (train.py:553) train_epoch: 25
Iteration:   1460, Loss function: 4.075, Average Loss: 4.305, avg. samples / sec: 24613.19
Iteration:   1460, Loss function: 4.576, Average Loss: 4.299, avg. samples / sec: 24602.19
Iteration:   1460, Loss function: 4.614, Average Loss: 4.303, avg. samples / sec: 24613.34
Iteration:   1460, Loss function: 4.315, Average Loss: 4.281, avg. samples / sec: 24609.02
Iteration:   1460, Loss function: 4.618, Average Loss: 4.297, avg. samples / sec: 24593.78
Iteration:   1460, Loss function: 4.147, Average Loss: 4.296, avg. samples / sec: 24558.51
Iteration:   1460, Loss function: 4.805, Average Loss: 4.297, avg. samples / sec: 24545.32
Iteration:   1460, Loss function: 4.551, Average Loss: 4.302, avg. samples / sec: 24538.42
Iteration:   1480, Loss function: 4.460, Average Loss: 4.302, avg. samples / sec: 24571.30
Iteration:   1480, Loss function: 4.374, Average Loss: 4.296, avg. samples / sec: 24574.15
Iteration:   1480, Loss function: 4.357, Average Loss: 4.304, avg. samples / sec: 24566.22
Iteration:   1480, Loss function: 4.383, Average Loss: 4.283, avg. samples / sec: 24564.37
Iteration:   1480, Loss function: 4.418, Average Loss: 4.305, avg. samples / sec: 24603.72
Iteration:   1480, Loss function: 4.170, Average Loss: 4.305, avg. samples / sec: 24530.11
Iteration:   1480, Loss function: 4.261, Average Loss: 4.300, avg. samples / sec: 24578.51
Iteration:   1480, Loss function: 4.850, Average Loss: 4.297, avg. samples / sec: 24571.21
Iteration:   1500, Loss function: 4.578, Average Loss: 4.310, avg. samples / sec: 24717.81
Iteration:   1500, Loss function: 4.075, Average Loss: 4.299, avg. samples / sec: 24679.91
Iteration:   1500, Loss function: 4.821, Average Loss: 4.307, avg. samples / sec: 24678.74
Iteration:   1500, Loss function: 4.669, Average Loss: 4.299, avg. samples / sec: 24722.73
Iteration:   1500, Loss function: 4.696, Average Loss: 4.303, avg. samples / sec: 24668.36
Iteration:   1500, Loss function: 4.436, Average Loss: 4.301, avg. samples / sec: 24718.99
Iteration:   1500, Loss function: 3.894, Average Loss: 4.286, avg. samples / sec: 24676.66
Iteration:   1500, Loss function: 4.409, Average Loss: 4.305, avg. samples / sec: 24680.20

:::MLPv0.5.0 ssd 1541757295.122244835 (train.py:553) train_epoch: 26
Iteration:   1520, Loss function: 4.221, Average Loss: 4.305, avg. samples / sec: 24622.07
Iteration:   1520, Loss function: 4.201, Average Loss: 4.308, avg. samples / sec: 24617.40
Iteration:   1520, Loss function: 4.470, Average Loss: 4.298, avg. samples / sec: 24610.67
Iteration:   1520, Loss function: 3.885, Average Loss: 4.287, avg. samples / sec: 24615.81
Iteration:   1520, Loss function: 3.841, Average Loss: 4.304, avg. samples / sec: 24628.08
Iteration:   1520, Loss function: 3.544, Average Loss: 4.298, avg. samples / sec: 24595.22
Iteration:   1520, Loss function: 4.502, Average Loss: 4.311, avg. samples / sec: 24582.97
Iteration:   1520, Loss function: 4.210, Average Loss: 4.300, avg. samples / sec: 24580.49
Iteration:   1540, Loss function: 4.307, Average Loss: 4.300, avg. samples / sec: 24657.98
Iteration:   1540, Loss function: 4.552, Average Loss: 4.288, avg. samples / sec: 24657.62
Iteration:   1540, Loss function: 4.425, Average Loss: 4.302, avg. samples / sec: 24668.36
Iteration:   1540, Loss function: 4.515, Average Loss: 4.307, avg. samples / sec: 24643.28
Iteration:   1540, Loss function: 4.349, Average Loss: 4.296, avg. samples / sec: 24668.71
Iteration:   1540, Loss function: 4.458, Average Loss: 4.300, avg. samples / sec: 24651.66
Iteration:   1540, Loss function: 3.641, Average Loss: 4.309, avg. samples / sec: 24590.12
Iteration:   1540, Loss function: 4.587, Average Loss: 4.311, avg. samples / sec: 24620.55

:::MLPv0.5.0 ssd 1541757299.944476366 (train.py:553) train_epoch: 27
Iteration:   1560, Loss function: 3.456, Average Loss: 4.305, avg. samples / sec: 24613.16
Iteration:   1560, Loss function: 4.372, Average Loss: 4.296, avg. samples / sec: 24610.38
Iteration:   1560, Loss function: 4.312, Average Loss: 4.300, avg. samples / sec: 24596.30
Iteration:   1560, Loss function: 4.463, Average Loss: 4.301, avg. samples / sec: 24632.82
Iteration:   1560, Loss function: 4.846, Average Loss: 4.311, avg. samples / sec: 24636.28
Iteration:   1560, Loss function: 3.569, Average Loss: 4.304, avg. samples / sec: 24569.28
Iteration:   1560, Loss function: 4.615, Average Loss: 4.309, avg. samples / sec: 24625.04
Iteration:   1560, Loss function: 4.114, Average Loss: 4.288, avg. samples / sec: 24544.07
Iteration:   1580, Loss function: 4.717, Average Loss: 4.301, avg. samples / sec: 24587.89
Iteration:   1580, Loss function: 4.541, Average Loss: 4.307, avg. samples / sec: 24576.72
Iteration:   1580, Loss function: 4.681, Average Loss: 4.300, avg. samples / sec: 24585.87
Iteration:   1580, Loss function: 4.212, Average Loss: 4.292, avg. samples / sec: 24637.27
Iteration:   1580, Loss function: 4.540, Average Loss: 4.296, avg. samples / sec: 24578.70
Iteration:   1580, Loss function: 4.379, Average Loss: 4.313, avg. samples / sec: 24574.68
Iteration:   1580, Loss function: 4.279, Average Loss: 4.306, avg. samples / sec: 24571.96
Iteration:   1580, Loss function: 4.368, Average Loss: 4.310, avg. samples / sec: 24565.66
Iteration:   1600, Loss function: 3.946, Average Loss: 4.308, avg. samples / sec: 24681.26
Iteration:   1600, Loss function: 4.747, Average Loss: 4.305, avg. samples / sec: 24722.16
Iteration:   1600, Loss function: 3.749, Average Loss: 4.298, avg. samples / sec: 24674.29
Iteration:   1600, Loss function: 3.849, Average Loss: 4.302, avg. samples / sec: 24672.53
Iteration:   1600, Loss function: 4.725, Average Loss: 4.292, avg. samples / sec: 24676.13
Iteration:   1600, Loss function: 4.265, Average Loss: 4.296, avg. samples / sec: 24672.95
Iteration:   1600, Loss function: 4.404, Average Loss: 4.309, avg. samples / sec: 24701.74
Iteration:   1600, Loss function: 4.477, Average Loss: 4.313, avg. samples / sec: 24664.73

:::MLPv0.5.0 ssd 1541757304.770292521 (train.py:553) train_epoch: 28
Iteration:   1620, Loss function: 4.160, Average Loss: 4.308, avg. samples / sec: 24604.26
Iteration:   1620, Loss function: 4.185, Average Loss: 4.299, avg. samples / sec: 24604.61
Iteration:   1620, Loss function: 4.428, Average Loss: 4.302, avg. samples / sec: 24605.42
Iteration:   1620, Loss function: 4.796, Average Loss: 4.298, avg. samples / sec: 24605.56
Iteration:   1620, Loss function: 4.683, Average Loss: 4.311, avg. samples / sec: 24617.99
Iteration:   1620, Loss function: 4.624, Average Loss: 4.315, avg. samples / sec: 24632.36
Iteration:   1620, Loss function: 4.416, Average Loss: 4.306, avg. samples / sec: 24569.37
Iteration:   1620, Loss function: 4.196, Average Loss: 4.291, avg. samples / sec: 24551.75
Iteration:   1640, Loss function: 4.079, Average Loss: 4.310, avg. samples / sec: 24632.90
Iteration:   1640, Loss function: 3.936, Average Loss: 4.291, avg. samples / sec: 24688.58
Iteration:   1640, Loss function: 4.500, Average Loss: 4.300, avg. samples / sec: 24635.83
Iteration:   1640, Loss function: 4.110, Average Loss: 4.303, avg. samples / sec: 24633.03
Iteration:   1640, Loss function: 3.469, Average Loss: 4.301, avg. samples / sec: 24631.60
Iteration:   1640, Loss function: 4.256, Average Loss: 4.307, avg. samples / sec: 24635.42
Iteration:   1640, Loss function: 4.289, Average Loss: 4.314, avg. samples / sec: 24615.21
Iteration:   1640, Loss function: 4.520, Average Loss: 4.315, avg. samples / sec: 24596.10
Iteration:   1660, Loss function: 4.399, Average Loss: 4.299, avg. samples / sec: 24618.66
Iteration:   1660, Loss function: 4.467, Average Loss: 4.290, avg. samples / sec: 24605.15
Iteration:   1660, Loss function: 4.107, Average Loss: 4.305, avg. samples / sec: 24619.43
Iteration:   1660, Loss function: 4.839, Average Loss: 4.311, avg. samples / sec: 24614.37
Iteration:   1660, Loss function: 4.731, Average Loss: 4.308, avg. samples / sec: 24572.40
Iteration:   1660, Loss function: 4.355, Average Loss: 4.300, avg. samples / sec: 24581.38
Iteration:   1660, Loss function: 3.477, Average Loss: 4.296, avg. samples / sec: 24567.56
Iteration:   1660, Loss function: 4.323, Average Loss: 4.312, avg. samples / sec: 24614.07

:::MLPv0.5.0 ssd 1541757309.511460066 (train.py:553) train_epoch: 29
Iteration:   1680, Loss function: 4.340, Average Loss: 4.312, avg. samples / sec: 24613.92
Iteration:   1680, Loss function: 4.020, Average Loss: 4.308, avg. samples / sec: 24620.57
Iteration:   1680, Loss function: 3.967, Average Loss: 4.298, avg. samples / sec: 24582.38
Iteration:   1680, Loss function: 3.900, Average Loss: 4.290, avg. samples / sec: 24590.68
Iteration:   1680, Loss function: 3.485, Average Loss: 4.300, avg. samples / sec: 24615.92
Iteration:   1680, Loss function: 3.810, Average Loss: 4.297, avg. samples / sec: 24621.97
Iteration:   1680, Loss function: 3.876, Average Loss: 4.306, avg. samples / sec: 24583.14
Iteration:   1680, Loss function: 4.489, Average Loss: 4.314, avg. samples / sec: 24598.55
Iteration:   1700, Loss function: 3.781, Average Loss: 4.286, avg. samples / sec: 24592.36
Iteration:   1700, Loss function: 4.549, Average Loss: 4.302, avg. samples / sec: 24590.88
Iteration:   1700, Loss function: 4.351, Average Loss: 4.297, avg. samples / sec: 24584.64
Iteration:   1700, Loss function: 3.987, Average Loss: 4.308, avg. samples / sec: 24609.74
Iteration:   1700, Loss function: 3.838, Average Loss: 4.312, avg. samples / sec: 24555.45
Iteration:   1700, Loss function: 3.971, Average Loss: 4.307, avg. samples / sec: 24552.51
Iteration:   1700, Loss function: 3.915, Average Loss: 4.294, avg. samples / sec: 24553.42
Iteration:   1700, Loss function: 4.351, Average Loss: 4.312, avg. samples / sec: 24564.23
Iteration:   1720, Loss function: 3.670, Average Loss: 4.283, avg. samples / sec: 24648.58
Iteration:   1720, Loss function: 4.061, Average Loss: 4.294, avg. samples / sec: 24650.70
Iteration:   1720, Loss function: 4.063, Average Loss: 4.299, avg. samples / sec: 24633.83
Iteration:   1720, Loss function: 3.807, Average Loss: 4.289, avg. samples / sec: 24670.27
Iteration:   1720, Loss function: 4.252, Average Loss: 4.308, avg. samples / sec: 24625.03
Iteration:   1720, Loss function: 4.186, Average Loss: 4.310, avg. samples / sec: 24625.81
Iteration:   1720, Loss function: 4.357, Average Loss: 4.307, avg. samples / sec: 24654.28
Iteration:   1720, Loss function: 3.662, Average Loss: 4.306, avg. samples / sec: 24619.88

:::MLPv0.5.0 ssd 1541757314.338830233 (train.py:553) train_epoch: 30
Iteration:   1740, Loss function: 3.976, Average Loss: 4.280, avg. samples / sec: 24585.25
Iteration:   1740, Loss function: 4.083, Average Loss: 4.306, avg. samples / sec: 24637.46
Iteration:   1740, Loss function: 4.797, Average Loss: 4.295, avg. samples / sec: 24582.26
Iteration:   1740, Loss function: 4.245, Average Loss: 4.297, avg. samples / sec: 24593.97
Iteration:   1740, Loss function: 4.095, Average Loss: 4.304, avg. samples / sec: 24633.11
Iteration:   1740, Loss function: 4.877, Average Loss: 4.287, avg. samples / sec: 24597.17
Iteration:   1740, Loss function: 4.532, Average Loss: 4.302, avg. samples / sec: 24635.36
Iteration:   1740, Loss function: 4.045, Average Loss: 4.307, avg. samples / sec: 24589.93
Iteration:   1760, Loss function: 3.832, Average Loss: 4.297, avg. samples / sec: 24630.12
Iteration:   1760, Loss function: 4.421, Average Loss: 4.287, avg. samples / sec: 24629.86
Iteration:   1760, Loss function: 4.448, Average Loss: 4.295, avg. samples / sec: 24615.27
Iteration:   1760, Loss function: 4.505, Average Loss: 4.306, avg. samples / sec: 24598.31
Iteration:   1760, Loss function: 4.411, Average Loss: 4.281, avg. samples / sec: 24583.29
Iteration:   1760, Loss function: 4.458, Average Loss: 4.306, avg. samples / sec: 24607.45
Iteration:   1760, Loss function: 4.165, Average Loss: 4.305, avg. samples / sec: 24590.27
Iteration:   1760, Loss function: 4.524, Average Loss: 4.302, avg. samples / sec: 24564.18
Iteration:   1780, Loss function: 3.634, Average Loss: 4.296, avg. samples / sec: 24613.29
Iteration:   1780, Loss function: 4.400, Average Loss: 4.280, avg. samples / sec: 24649.80
Iteration:   1780, Loss function: 4.369, Average Loss: 4.290, avg. samples / sec: 24616.98
Iteration:   1780, Loss function: 4.349, Average Loss: 4.305, avg. samples / sec: 24653.81
Iteration:   1780, Loss function: 4.142, Average Loss: 4.297, avg. samples / sec: 24617.81
Iteration:   1780, Loss function: 4.458, Average Loss: 4.306, avg. samples / sec: 24619.16
Iteration:   1780, Loss function: 4.018, Average Loss: 4.306, avg. samples / sec: 24581.19
Iteration:   1780, Loss function: 4.028, Average Loss: 4.302, avg. samples / sec: 24608.03

:::MLPv0.5.0 ssd 1541757319.169445276 (train.py:553) train_epoch: 31
Iteration:   1800, Loss function: 4.446, Average Loss: 4.297, avg. samples / sec: 24587.88
Iteration:   1800, Loss function: 4.190, Average Loss: 4.294, avg. samples / sec: 24579.04
Iteration:   1800, Loss function: 3.972, Average Loss: 4.290, avg. samples / sec: 24581.22
Iteration:   1800, Loss function: 4.184, Average Loss: 4.300, avg. samples / sec: 24643.74
Iteration:   1800, Loss function: 4.831, Average Loss: 4.303, avg. samples / sec: 24574.89
Iteration:   1800, Loss function: 4.537, Average Loss: 4.305, avg. samples / sec: 24619.47
Iteration:   1800, Loss function: 4.521, Average Loss: 4.305, avg. samples / sec: 24584.61
Iteration:   1800, Loss function: 4.634, Average Loss: 4.284, avg. samples / sec: 24549.20
Iteration:   1820, Loss function: 3.918, Average Loss: 4.287, avg. samples / sec: 24623.75
Iteration:   1820, Loss function: 5.043, Average Loss: 4.301, avg. samples / sec: 24588.19
Iteration:   1820, Loss function: 3.890, Average Loss: 4.299, avg. samples / sec: 24595.35
Iteration:   1820, Loss function: 3.878, Average Loss: 4.303, avg. samples / sec: 24615.85
Iteration:   1820, Loss function: 4.359, Average Loss: 4.296, avg. samples / sec: 24579.31
Iteration:   1820, Loss function: 4.445, Average Loss: 4.305, avg. samples / sec: 24574.37
Iteration:   1820, Loss function: 4.234, Average Loss: 4.289, avg. samples / sec: 24551.86
Iteration:   1820, Loss function: 4.485, Average Loss: 4.304, avg. samples / sec: 24529.07
Iteration:   1840, Loss function: 4.185, Average Loss: 4.297, avg. samples / sec: 24590.50
Iteration:   1840, Loss function: 3.916, Average Loss: 4.295, avg. samples / sec: 24590.53
Iteration:   1840, Loss function: 4.324, Average Loss: 4.285, avg. samples / sec: 24575.21
Iteration:   1840, Loss function: 4.292, Average Loss: 4.289, avg. samples / sec: 24609.47
Iteration:   1840, Loss function: 4.192, Average Loss: 4.298, avg. samples / sec: 24560.17
Iteration:   1840, Loss function: 4.137, Average Loss: 4.303, avg. samples / sec: 24624.88
Iteration:   1840, Loss function: 4.090, Average Loss: 4.300, avg. samples / sec: 24543.33
Iteration:   1840, Loss function: 4.058, Average Loss: 4.304, avg. samples / sec: 24570.49

:::MLPv0.5.0 ssd 1541757324.000841141 (train.py:553) train_epoch: 32
Iteration:   1860, Loss function: 4.196, Average Loss: 4.301, avg. samples / sec: 24632.47
Iteration:   1860, Loss function: 3.696, Average Loss: 4.300, avg. samples / sec: 24608.26
Iteration:   1860, Loss function: 4.197, Average Loss: 4.294, avg. samples / sec: 24579.34
Iteration:   1860, Loss function: 4.213, Average Loss: 4.285, avg. samples / sec: 24569.02
Iteration:   1860, Loss function: 4.229, Average Loss: 4.285, avg. samples / sec: 24576.01
Iteration:   1860, Loss function: 4.645, Average Loss: 4.299, avg. samples / sec: 24599.35
Iteration:   1860, Loss function: 5.067, Average Loss: 4.296, avg. samples / sec: 24548.06
Iteration:   1860, Loss function: 3.861, Average Loss: 4.295, avg. samples / sec: 24551.95

































































:::MLPv0.5.0 ssd 1541757326.247182369 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757326.247705460 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757326.248156071 (train.py:220) eval_start: 32
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 6
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 70
Predicting Ended, total time: 6.03 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.03 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.03 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.03 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.03 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.03 s
Predicting Ended, total time: 6.03 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.03 s
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(246363, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/246363
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(246363, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(246363, 7)
Loading and preparing results...
Converting ndarray to lists...
(246363, 7)
(246363, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(246363, 7)
Converting ndarray to lists...
0/246363
Converting ndarray to lists...
Converting ndarray to lists...
(246363, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(246363, 7)
(246363, 7)
(246363, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(246363, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(246363, 7)
Loading and preparing results...
Converting ndarray to lists...
0/246363
Loading and preparing results...
(246363, 7)
Converting ndarray to lists...
(246363, 7)
(246363, 7)
Loading and preparing results...
0/246363
(246363, 7)
0/246363
Converting ndarray to lists...
0/246363
0/246363
Converting ndarray to lists...
(246363, 7)
Converting ndarray to lists...
(246363, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(246363, 7)
0/246363
(246363, 7)
(246363, 7)
Loading and preparing results...
0/246363
(246363, 7)
Loading and preparing results...
0/246363
Converting ndarray to lists...
0/246363
0/246363
(246363, 7)
(246363, 7)
(246363, 7)
Converting ndarray to lists...
0/246363
0/246363
(246363, 7)
Converting ndarray to lists...
0/246363
(246363, 7)
Loading and preparing results...
Loading and preparing results...
0/246363
Converting ndarray to lists...
(246363, 7)
Loading and preparing results...
0/246363
(246363, 7)
0/246363
Converting ndarray to lists...
Converting ndarray to lists...
0/246363
(246363, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(246363, 7)
(246363, 7)
0/246363
0/246363
0/246363
Loading and preparing results...
0/246363
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/246363
Converting ndarray to lists...
(246363, 7)
Converting ndarray to lists...
Loading and preparing results...
(246363, 7)
Converting ndarray to lists...
(246363, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/246363
(246363, 7)
(246363, 7)
0/246363
0/246363
0/246363
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(246363, 7)
(246363, 7)
Converting ndarray to lists...
0/246363
Converting ndarray to lists...
Converting ndarray to lists...
0/246363
0/246363
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
0/246363
0/246363
Loading and preparing results...
0/246363
Converting ndarray to lists...
(246363, 7)
(246363, 7)
(246363, 7)
(246363, 7)
0/246363
0/246363
(246363, 7)
Loading and preparing results...
(246363, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(246363, 7)
0/246363
0/246363
(246363, 7)
0/246363
Converting ndarray to lists...
(246363, 7)
(246363, 7)
0/246363
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/246363
(246363, 7)
(246363, 7)
0/246363
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(246363, 7)
0/246363
(246363, 7)
(246363, 7)
(246363, 7)
(246363, 7)
(246363, 7)
0/246363
0/246363
(246363, 7)
Converting ndarray to lists...
0/246363
0/246363
0/246363
0/246363
(246363, 7)
Converting ndarray to lists...
0/246363
(246363, 7)
0/246363
0/246363
0/246363
0/246363
0/246363
0/246363
(246363, 7)
0/246363
(246363, 7)
0/246363
0/246363
(246363, 7)
0/246363
(246363, 7)
0/246363
0/246363
0/246363
0/246363
DONE (t=1.66s)
creating index...
DONE (t=1.66s)
creating index...
DONE (t=1.67s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
DONE (t=1.71s)
creating index...
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
DONE (t=3.14s).
Accumulating evaluation results...
DONE (t=3.13s).
Accumulating evaluation results...
DONE (t=3.13s).
Accumulating evaluation results...
DONE (t=3.15s).
Accumulating evaluation results...
DONE (t=3.15s).
Accumulating evaluation results...
DONE (t=3.17s).
Accumulating evaluation results...
DONE (t=3.16s).
Accumulating evaluation results...
DONE (t=3.18s).
Accumulating evaluation results...
DONE (t=0.95s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.125
DONE (t=0.96s).
DONE (t=0.96s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.237
DONE (t=0.95s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.125
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.119
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.125
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.125
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.132
DONE (t=0.96s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.119
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.119
DONE (t=0.95s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.195
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.119
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.147
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.125
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.132
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.125
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
Current AP: 0.12506 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.132
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.132
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.237
DONE (t=0.97s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.195
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.195
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.147
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.195
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.119
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.147
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.147
DONE (t=0.97s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
Current AP: 0.12506 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.208
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.119
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
Current AP: 0.12506 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.125
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
Current AP: 0.12506 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.132
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.125
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.132
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.195
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.119
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.147
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.195
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
Current AP: 0.12506 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.147
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.119
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
Current AP: 0.12506 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.132
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.195
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.132
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.147
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.195
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
Current AP: 0.12506 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.147
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
Current AP: 0.12506 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757338.348753214 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757338.349370003 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.12506227442865678}

:::MLPv0.5.0 ssd 1541757338.349831104 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.12506227442865678}

:::MLPv0.5.0 ssd 1541757338.350275993 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757338.350713015 (train.py:338) eval_stop: 32
Iteration:   1880, Loss function: 3.859, Average Loss: 4.284, avg. samples / sec: 2846.77
Iteration:   1880, Loss function: 3.894, Average Loss: 4.296, avg. samples / sec: 2846.75
Iteration:   1880, Loss function: 3.970, Average Loss: 4.293, avg. samples / sec: 2846.69
Iteration:   1880, Loss function: 3.735, Average Loss: 4.289, avg. samples / sec: 2846.39
Iteration:   1880, Loss function: 4.076, Average Loss: 4.284, avg. samples / sec: 2846.53
Iteration:   1880, Loss function: 3.517, Average Loss: 4.297, avg. samples / sec: 2846.30
Iteration:   1880, Loss function: 3.685, Average Loss: 4.294, avg. samples / sec: 2846.76
Iteration:   1880, Loss function: 4.347, Average Loss: 4.299, avg. samples / sec: 2845.58
Iteration:   1900, Loss function: 4.004, Average Loss: 4.290, avg. samples / sec: 24612.58
Iteration:   1900, Loss function: 4.248, Average Loss: 4.281, avg. samples / sec: 24599.75
Iteration:   1900, Loss function: 4.059, Average Loss: 4.292, avg. samples / sec: 24603.90
Iteration:   1900, Loss function: 4.033, Average Loss: 4.295, avg. samples / sec: 24612.78
Iteration:   1900, Loss function: 4.309, Average Loss: 4.292, avg. samples / sec: 24625.57
Iteration:   1900, Loss function: 4.493, Average Loss: 4.297, avg. samples / sec: 24651.15
Iteration:   1900, Loss function: 4.115, Average Loss: 4.282, avg. samples / sec: 24588.36
Iteration:   1900, Loss function: 4.231, Average Loss: 4.288, avg. samples / sec: 24559.93

:::MLPv0.5.0 ssd 1541757341.471316814 (train.py:553) train_epoch: 33
Iteration:   1920, Loss function: 4.481, Average Loss: 4.292, avg. samples / sec: 24576.45
Iteration:   1920, Loss function: 4.690, Average Loss: 4.291, avg. samples / sec: 24574.61
Iteration:   1920, Loss function: 4.548, Average Loss: 4.280, avg. samples / sec: 24574.63
Iteration:   1920, Loss function: 3.841, Average Loss: 4.279, avg. samples / sec: 24594.06
Iteration:   1920, Loss function: 3.862, Average Loss: 4.293, avg. samples / sec: 24575.07
Iteration:   1920, Loss function: 4.414, Average Loss: 4.295, avg. samples / sec: 24576.74
Iteration:   1920, Loss function: 4.183, Average Loss: 4.284, avg. samples / sec: 24595.50
Iteration:   1920, Loss function: 3.948, Average Loss: 4.289, avg. samples / sec: 24512.05
Iteration:   1940, Loss function: 4.275, Average Loss: 4.289, avg. samples / sec: 24536.55
Iteration:   1940, Loss function: 4.050, Average Loss: 4.278, avg. samples / sec: 24540.86
Iteration:   1940, Loss function: 3.747, Average Loss: 4.278, avg. samples / sec: 24533.54
Iteration:   1940, Loss function: 4.055, Average Loss: 4.293, avg. samples / sec: 24544.51
Iteration:   1940, Loss function: 4.545, Average Loss: 4.287, avg. samples / sec: 24528.60
Iteration:   1940, Loss function: 4.026, Average Loss: 4.280, avg. samples / sec: 24561.50
Iteration:   1940, Loss function: 4.724, Average Loss: 4.288, avg. samples / sec: 24596.47
Iteration:   1940, Loss function: 4.108, Average Loss: 4.291, avg. samples / sec: 24480.87
Iteration:   1960, Loss function: 3.703, Average Loss: 4.277, avg. samples / sec: 24570.72
Iteration:   1960, Loss function: 3.972, Average Loss: 4.276, avg. samples / sec: 24567.44
Iteration:   1960, Loss function: 4.691, Average Loss: 4.288, avg. samples / sec: 24561.08
Iteration:   1960, Loss function: 4.279, Average Loss: 4.283, avg. samples / sec: 24572.78
Iteration:   1960, Loss function: 4.188, Average Loss: 4.292, avg. samples / sec: 24557.09
Iteration:   1960, Loss function: 4.047, Average Loss: 4.289, avg. samples / sec: 24604.48
Iteration:   1960, Loss function: 3.967, Average Loss: 4.278, avg. samples / sec: 24513.50
Iteration:   1960, Loss function: 4.295, Average Loss: 4.282, avg. samples / sec: 24510.93

:::MLPv0.5.0 ssd 1541757346.309545517 (train.py:553) train_epoch: 34
Iteration:   1980, Loss function: 3.967, Average Loss: 4.286, avg. samples / sec: 24485.01
Iteration:   1980, Loss function: 3.937, Average Loss: 4.276, avg. samples / sec: 24473.90
Iteration:   1980, Loss function: 4.150, Average Loss: 4.287, avg. samples / sec: 24492.84
Iteration:   1980, Loss function: 4.690, Average Loss: 4.285, avg. samples / sec: 24474.65
Iteration:   1980, Loss function: 4.157, Average Loss: 4.280, avg. samples / sec: 24528.13
Iteration:   1980, Loss function: 4.556, Average Loss: 4.291, avg. samples / sec: 24479.71
Iteration:   1980, Loss function: 4.077, Average Loss: 4.278, avg. samples / sec: 24496.28
Iteration:   1980, Loss function: 4.167, Average Loss: 4.270, avg. samples / sec: 24428.77
Iteration:   2000, Loss function: 4.332, Average Loss: 4.265, avg. samples / sec: 24614.46
Iteration:   2000, Loss function: 4.290, Average Loss: 4.285, avg. samples / sec: 24561.02
Iteration:   2000, Loss function: 3.857, Average Loss: 4.288, avg. samples / sec: 24571.23
Iteration:   2000, Loss function: 3.780, Average Loss: 4.272, avg. samples / sec: 24564.68
Iteration:   2000, Loss function: 4.422, Average Loss: 4.272, avg. samples / sec: 24599.06
Iteration:   2000, Loss function: 4.325, Average Loss: 4.283, avg. samples / sec: 24568.83
Iteration:   2000, Loss function: 4.119, Average Loss: 4.275, avg. samples / sec: 24541.10
Iteration:   2000, Loss function: 4.085, Average Loss: 4.286, avg. samples / sec: 24534.90
Iteration:   2020, Loss function: 4.590, Average Loss: 4.281, avg. samples / sec: 24578.92
Iteration:   2020, Loss function: 3.471, Average Loss: 4.270, avg. samples / sec: 24578.86
Iteration:   2020, Loss function: 3.980, Average Loss: 4.261, avg. samples / sec: 24572.64
Iteration:   2020, Loss function: 4.230, Average Loss: 4.273, avg. samples / sec: 24604.59
Iteration:   2020, Loss function: 3.798, Average Loss: 4.269, avg. samples / sec: 24578.28
Iteration:   2020, Loss function: 3.825, Average Loss: 4.283, avg. samples / sec: 24608.06
Iteration:   2020, Loss function: 4.176, Average Loss: 4.285, avg. samples / sec: 24563.27
Iteration:   2020, Loss function: 3.925, Average Loss: 4.282, avg. samples / sec: 24557.70

:::MLPv0.5.0 ssd 1541757351.151206255 (train.py:553) train_epoch: 35
Iteration:   2040, Loss function: 4.273, Average Loss: 4.279, avg. samples / sec: 24540.76
Iteration:   2040, Loss function: 3.867, Average Loss: 4.269, avg. samples / sec: 24548.15
Iteration:   2040, Loss function: 4.033, Average Loss: 4.258, avg. samples / sec: 24544.74
Iteration:   2040, Loss function: 4.245, Average Loss: 4.265, avg. samples / sec: 24536.91
Iteration:   2040, Loss function: 4.288, Average Loss: 4.280, avg. samples / sec: 24560.41
Iteration:   2040, Loss function: 4.341, Average Loss: 4.266, avg. samples / sec: 24540.30
Iteration:   2040, Loss function: 4.903, Average Loss: 4.282, avg. samples / sec: 24548.58
Iteration:   2040, Loss function: 3.787, Average Loss: 4.277, avg. samples / sec: 24484.16
Iteration:   2060, Loss function: 4.490, Average Loss: 4.258, avg. samples / sec: 24595.37
Iteration:   2060, Loss function: 3.995, Average Loss: 4.281, avg. samples / sec: 24591.11
Iteration:   2060, Loss function: 4.095, Average Loss: 4.280, avg. samples / sec: 24594.37
Iteration:   2060, Loss function: 3.954, Average Loss: 4.283, avg. samples / sec: 24597.30
Iteration:   2060, Loss function: 4.396, Average Loss: 4.270, avg. samples / sec: 24586.79
Iteration:   2060, Loss function: 4.599, Average Loss: 4.266, avg. samples / sec: 24595.84
Iteration:   2060, Loss function: 4.249, Average Loss: 4.267, avg. samples / sec: 24561.68
Iteration:   2060, Loss function: 4.402, Average Loss: 4.278, avg. samples / sec: 24593.08

:::MLPv0.5.0 ssd 1541757355.983225822 (train.py:553) train_epoch: 36
Iteration:   2080, Loss function: 4.048, Average Loss: 4.256, avg. samples / sec: 24578.66
Iteration:   2080, Loss function: 3.825, Average Loss: 4.264, avg. samples / sec: 24583.01
Iteration:   2080, Loss function: 4.560, Average Loss: 4.267, avg. samples / sec: 24615.84
Iteration:   2080, Loss function: 4.484, Average Loss: 4.270, avg. samples / sec: 24580.49
Iteration:   2080, Loss function: 4.151, Average Loss: 4.276, avg. samples / sec: 24636.69
Iteration:   2080, Loss function: 4.254, Average Loss: 4.283, avg. samples / sec: 24576.74
Iteration:   2080, Loss function: 4.614, Average Loss: 4.281, avg. samples / sec: 24571.14
Iteration:   2080, Loss function: 4.492, Average Loss: 4.279, avg. samples / sec: 24573.57
Iteration:   2100, Loss function: 4.002, Average Loss: 4.279, avg. samples / sec: 24563.16
Iteration:   2100, Loss function: 4.099, Average Loss: 4.261, avg. samples / sec: 24552.41
Iteration:   2100, Loss function: 3.767, Average Loss: 4.278, avg. samples / sec: 24556.61
Iteration:   2100, Loss function: 3.695, Average Loss: 4.261, avg. samples / sec: 24548.01
Iteration:   2100, Loss function: 4.579, Average Loss: 4.267, avg. samples / sec: 24551.44
Iteration:   2100, Loss function: 3.691, Average Loss: 4.275, avg. samples / sec: 24544.48
Iteration:   2100, Loss function: 4.735, Average Loss: 4.276, avg. samples / sec: 24522.57
Iteration:   2100, Loss function: 3.985, Average Loss: 4.255, avg. samples / sec: 24494.39
Iteration:   2120, Loss function: 4.109, Average Loss: 4.275, avg. samples / sec: 24642.21
Iteration:   2120, Loss function: 3.716, Average Loss: 4.255, avg. samples / sec: 24644.58
Iteration:   2120, Loss function: 4.655, Average Loss: 4.258, avg. samples / sec: 24636.68
Iteration:   2120, Loss function: 3.943, Average Loss: 4.265, avg. samples / sec: 24641.86
Iteration:   2120, Loss function: 3.956, Average Loss: 4.274, avg. samples / sec: 24648.14
Iteration:   2120, Loss function: 4.017, Average Loss: 4.277, avg. samples / sec: 24631.64
Iteration:   2120, Loss function: 3.951, Average Loss: 4.275, avg. samples / sec: 24667.14
Iteration:   2120, Loss function: 3.854, Average Loss: 4.253, avg. samples / sec: 24662.85

:::MLPv0.5.0 ssd 1541757360.818375826 (train.py:553) train_epoch: 37
Iteration:   2140, Loss function: 3.410, Average Loss: 4.271, avg. samples / sec: 24523.93
Iteration:   2140, Loss function: 4.254, Average Loss: 4.255, avg. samples / sec: 24524.49
Iteration:   2140, Loss function: 4.880, Average Loss: 4.273, avg. samples / sec: 24530.73
Iteration:   2140, Loss function: 3.730, Average Loss: 4.268, avg. samples / sec: 24519.30
Iteration:   2140, Loss function: 4.317, Average Loss: 4.271, avg. samples / sec: 24521.80
Iteration:   2140, Loss function: 4.182, Average Loss: 4.249, avg. samples / sec: 24523.86
Iteration:   2140, Loss function: 4.552, Average Loss: 4.262, avg. samples / sec: 24483.18
Iteration:   2140, Loss function: 4.017, Average Loss: 4.251, avg. samples / sec: 24470.97
Iteration:   2160, Loss function: 3.955, Average Loss: 4.266, avg. samples / sec: 24592.26
Iteration:   2160, Loss function: 4.330, Average Loss: 4.251, avg. samples / sec: 24588.10
Iteration:   2160, Loss function: 4.537, Average Loss: 4.263, avg. samples / sec: 24629.76
Iteration:   2160, Loss function: 4.044, Average Loss: 4.270, avg. samples / sec: 24583.95
Iteration:   2160, Loss function: 4.111, Average Loss: 4.246, avg. samples / sec: 24618.09
Iteration:   2160, Loss function: 4.165, Average Loss: 4.249, avg. samples / sec: 24634.38
Iteration:   2160, Loss function: 3.969, Average Loss: 4.267, avg. samples / sec: 24591.02
Iteration:   2160, Loss function: 3.756, Average Loss: 4.265, avg. samples / sec: 24586.92
Iteration:   2180, Loss function: 3.818, Average Loss: 4.246, avg. samples / sec: 24616.12
Iteration:   2180, Loss function: 4.014, Average Loss: 4.262, avg. samples / sec: 24607.96
Iteration:   2180, Loss function: 3.956, Average Loss: 4.263, avg. samples / sec: 24622.07
Iteration:   2180, Loss function: 3.828, Average Loss: 4.258, avg. samples / sec: 24614.01
Iteration:   2180, Loss function: 4.259, Average Loss: 4.269, avg. samples / sec: 24610.18
Iteration:   2180, Loss function: 3.444, Average Loss: 4.244, avg. samples / sec: 24593.79
Iteration:   2180, Loss function: 4.268, Average Loss: 4.244, avg. samples / sec: 24584.76
Iteration:   2180, Loss function: 4.630, Average Loss: 4.265, avg. samples / sec: 24576.11

:::MLPv0.5.0 ssd 1541757365.566325426 (train.py:553) train_epoch: 38
Iteration:   2200, Loss function: 4.222, Average Loss: 4.241, avg. samples / sec: 24601.84
Iteration:   2200, Loss function: 3.968, Average Loss: 4.241, avg. samples / sec: 24566.88
Iteration:   2200, Loss function: 3.805, Average Loss: 4.264, avg. samples / sec: 24574.84
Iteration:   2200, Loss function: 3.999, Average Loss: 4.257, avg. samples / sec: 24563.84
Iteration:   2200, Loss function: 3.990, Average Loss: 4.259, avg. samples / sec: 24609.34
Iteration:   2200, Loss function: 4.463, Average Loss: 4.253, avg. samples / sec: 24563.57
Iteration:   2200, Loss function: 3.855, Average Loss: 4.257, avg. samples / sec: 24527.20
Iteration:   2200, Loss function: 3.650, Average Loss: 4.239, avg. samples / sec: 24536.98
Iteration:   2220, Loss function: 3.714, Average Loss: 4.254, avg. samples / sec: 24602.81
Iteration:   2220, Loss function: 4.298, Average Loss: 4.239, avg. samples / sec: 24593.87
Iteration:   2220, Loss function: 4.115, Average Loss: 4.238, avg. samples / sec: 24593.55
Iteration:   2220, Loss function: 3.716, Average Loss: 4.234, avg. samples / sec: 24644.66
Iteration:   2220, Loss function: 4.580, Average Loss: 4.261, avg. samples / sec: 24587.66
Iteration:   2220, Loss function: 3.695, Average Loss: 4.251, avg. samples / sec: 24628.60
Iteration:   2220, Loss function: 4.313, Average Loss: 4.257, avg. samples / sec: 24588.50
Iteration:   2220, Loss function: 3.985, Average Loss: 4.252, avg. samples / sec: 24567.93
Iteration:   2240, Loss function: 3.433, Average Loss: 4.250, avg. samples / sec: 24632.20
Iteration:   2240, Loss function: 4.429, Average Loss: 4.235, avg. samples / sec: 24630.48
Iteration:   2240, Loss function: 3.936, Average Loss: 4.233, avg. samples / sec: 24626.16
Iteration:   2240, Loss function: 4.221, Average Loss: 4.229, avg. samples / sec: 24634.08
Iteration:   2240, Loss function: 4.398, Average Loss: 4.247, avg. samples / sec: 24636.80
Iteration:   2240, Loss function: 3.853, Average Loss: 4.254, avg. samples / sec: 24630.76
Iteration:   2240, Loss function: 4.245, Average Loss: 4.250, avg. samples / sec: 24624.36
Iteration:   2240, Loss function: 3.523, Average Loss: 4.247, avg. samples / sec: 24614.28

:::MLPv0.5.0 ssd 1541757370.394909382 (train.py:553) train_epoch: 39
Iteration:   2260, Loss function: 4.399, Average Loss: 4.247, avg. samples / sec: 24550.06
Iteration:   2260, Loss function: 4.745, Average Loss: 4.234, avg. samples / sec: 24559.25
Iteration:   2260, Loss function: 4.300, Average Loss: 4.237, avg. samples / sec: 24555.13
Iteration:   2260, Loss function: 4.156, Average Loss: 4.253, avg. samples / sec: 24557.26
Iteration:   2260, Loss function: 4.103, Average Loss: 4.246, avg. samples / sec: 24601.44
Iteration:   2260, Loss function: 4.663, Average Loss: 4.253, avg. samples / sec: 24564.57
Iteration:   2260, Loss function: 4.978, Average Loss: 4.246, avg. samples / sec: 24546.16
Iteration:   2260, Loss function: 4.750, Average Loss: 4.231, avg. samples / sec: 24533.20
Iteration:   2280, Loss function: 4.024, Average Loss: 4.252, avg. samples / sec: 24571.90
Iteration:   2280, Loss function: 4.072, Average Loss: 4.246, avg. samples / sec: 24564.98
Iteration:   2280, Loss function: 4.072, Average Loss: 4.236, avg. samples / sec: 24562.85
Iteration:   2280, Loss function: 3.857, Average Loss: 4.252, avg. samples / sec: 24569.51
Iteration:   2280, Loss function: 4.047, Average Loss: 4.238, avg. samples / sec: 24557.52
Iteration:   2280, Loss function: 4.377, Average Loss: 4.247, avg. samples / sec: 24567.39
Iteration:   2280, Loss function: 4.083, Average Loss: 4.240, avg. samples / sec: 24552.28
Iteration:   2280, Loss function: 3.787, Average Loss: 4.232, avg. samples / sec: 24529.83
Iteration:   2300, Loss function: 4.311, Average Loss: 4.243, avg. samples / sec: 24603.17
Iteration:   2300, Loss function: 4.004, Average Loss: 4.231, avg. samples / sec: 24599.55
Iteration:   2300, Loss function: 4.174, Average Loss: 4.234, avg. samples / sec: 24604.30
Iteration:   2300, Loss function: 3.772, Average Loss: 4.247, avg. samples / sec: 24600.41
Iteration:   2300, Loss function: 3.951, Average Loss: 4.236, avg. samples / sec: 24613.86
Iteration:   2300, Loss function: 3.842, Average Loss: 4.229, avg. samples / sec: 24653.86
Iteration:   2300, Loss function: 3.946, Average Loss: 4.246, avg. samples / sec: 24585.90
Iteration:   2300, Loss function: 4.483, Average Loss: 4.241, avg. samples / sec: 24594.94

:::MLPv0.5.0 ssd 1541757375.229403973 (train.py:553) train_epoch: 40
Iteration:   2320, Loss function: 3.988, Average Loss: 4.238, avg. samples / sec: 24542.49
Iteration:   2320, Loss function: 4.466, Average Loss: 4.242, avg. samples / sec: 24554.34
Iteration:   2320, Loss function: 4.269, Average Loss: 4.239, avg. samples / sec: 24552.36
Iteration:   2320, Loss function: 4.204, Average Loss: 4.233, avg. samples / sec: 24539.16
Iteration:   2320, Loss function: 4.019, Average Loss: 4.225, avg. samples / sec: 24536.76
Iteration:   2320, Loss function: 4.399, Average Loss: 4.230, avg. samples / sec: 24533.24
Iteration:   2320, Loss function: 4.650, Average Loss: 4.241, avg. samples / sec: 24533.53
Iteration:   2320, Loss function: 3.743, Average Loss: 4.234, avg. samples / sec: 24515.82
Iteration:   2340, Loss function: 3.875, Average Loss: 4.233, avg. samples / sec: 24558.11
Iteration:   2340, Loss function: 3.939, Average Loss: 4.238, avg. samples / sec: 24564.00
Iteration:   2340, Loss function: 4.128, Average Loss: 4.237, avg. samples / sec: 24570.28
Iteration:   2340, Loss function: 3.768, Average Loss: 4.230, avg. samples / sec: 24588.01
Iteration:   2340, Loss function: 3.662, Average Loss: 4.226, avg. samples / sec: 24564.73
Iteration:   2340, Loss function: 4.232, Average Loss: 4.221, avg. samples / sec: 24562.98
Iteration:   2340, Loss function: 3.690, Average Loss: 4.229, avg. samples / sec: 24519.88
Iteration:   2340, Loss function: 4.196, Average Loss: 4.235, avg. samples / sec: 24490.12
Iteration:   2360, Loss function: 4.279, Average Loss: 4.217, avg. samples / sec: 24553.32
Iteration:   2360, Loss function: 3.863, Average Loss: 4.236, avg. samples / sec: 24535.25
Iteration:   2360, Loss function: 4.158, Average Loss: 4.227, avg. samples / sec: 24586.00
Iteration:   2360, Loss function: 3.882, Average Loss: 4.224, avg. samples / sec: 24543.16
Iteration:   2360, Loss function: 4.292, Average Loss: 4.232, avg. samples / sec: 24533.82
Iteration:   2360, Loss function: 4.243, Average Loss: 4.233, avg. samples / sec: 24609.05
Iteration:   2360, Loss function: 3.921, Average Loss: 4.235, avg. samples / sec: 24534.84
Iteration:   2360, Loss function: 3.674, Average Loss: 4.227, avg. samples / sec: 24491.59

:::MLPv0.5.0 ssd 1541757380.071883202 (train.py:553) train_epoch: 41
Iteration:   2380, Loss function: 3.983, Average Loss: 4.233, avg. samples / sec: 24525.93
Iteration:   2380, Loss function: 3.910, Average Loss: 4.221, avg. samples / sec: 24523.61
Iteration:   2380, Loss function: 4.377, Average Loss: 4.220, avg. samples / sec: 24523.71
Iteration:   2380, Loss function: 3.998, Average Loss: 4.228, avg. samples / sec: 24525.01
Iteration:   2380, Loss function: 3.861, Average Loss: 4.224, avg. samples / sec: 24522.71
Iteration:   2380, Loss function: 4.072, Average Loss: 4.213, avg. samples / sec: 24512.34
Iteration:   2380, Loss function: 3.805, Average Loss: 4.224, avg. samples / sec: 24567.92
Iteration:   2380, Loss function: 3.887, Average Loss: 4.231, avg. samples / sec: 24474.26
Iteration:   2400, Loss function: 3.947, Average Loss: 4.217, avg. samples / sec: 24522.96
Iteration:   2400, Loss function: 4.621, Average Loss: 4.229, avg. samples / sec: 24516.18
Iteration:   2400, Loss function: 3.963, Average Loss: 4.209, avg. samples / sec: 24521.22
Iteration:   2400, Loss function: 3.966, Average Loss: 4.212, avg. samples / sec: 24501.64
Iteration:   2400, Loss function: 3.987, Average Loss: 4.219, avg. samples / sec: 24478.00
Iteration:   2400, Loss function: 3.674, Average Loss: 4.224, avg. samples / sec: 24467.50
Iteration:   2400, Loss function: 4.338, Average Loss: 4.221, avg. samples / sec: 24463.28
Iteration:   2400, Loss function: 4.068, Average Loss: 4.226, avg. samples / sec: 24517.85
Iteration:   2420, Loss function: 4.491, Average Loss: 4.228, avg. samples / sec: 24569.34
Iteration:   2420, Loss function: 3.928, Average Loss: 4.207, avg. samples / sec: 24569.29
Iteration:   2420, Loss function: 3.738, Average Loss: 4.220, avg. samples / sec: 24618.44
Iteration:   2420, Loss function: 3.868, Average Loss: 4.224, avg. samples / sec: 24620.78
Iteration:   2420, Loss function: 3.472, Average Loss: 4.218, avg. samples / sec: 24621.47
Iteration:   2420, Loss function: 4.324, Average Loss: 4.214, avg. samples / sec: 24611.72
Iteration:   2420, Loss function: 3.852, Average Loss: 4.208, avg. samples / sec: 24576.45
Iteration:   2420, Loss function: 4.633, Average Loss: 4.215, avg. samples / sec: 24549.94

:::MLPv0.5.0 ssd 1541757384.826278925 (train.py:553) train_epoch: 42
Iteration:   2440, Loss function: 4.616, Average Loss: 4.226, avg. samples / sec: 24545.45
Iteration:   2440, Loss function: 4.267, Average Loss: 4.207, avg. samples / sec: 24545.38
Iteration:   2440, Loss function: 4.739, Average Loss: 4.215, avg. samples / sec: 24547.77
Iteration:   2440, Loss function: 4.396, Average Loss: 4.214, avg. samples / sec: 24534.93
Iteration:   2440, Loss function: 3.452, Average Loss: 4.203, avg. samples / sec: 24522.36
Iteration:   2440, Loss function: 4.252, Average Loss: 4.215, avg. samples / sec: 24498.08
Iteration:   2440, Loss function: 3.577, Average Loss: 4.220, avg. samples / sec: 24495.44
Iteration:   2440, Loss function: 3.933, Average Loss: 4.210, avg. samples / sec: 24486.26
Iteration:   2460, Loss function: 3.822, Average Loss: 4.221, avg. samples / sec: 24555.32
Iteration:   2460, Loss function: 4.132, Average Loss: 4.200, avg. samples / sec: 24562.12
Iteration:   2460, Loss function: 4.064, Average Loss: 4.217, avg. samples / sec: 24603.44
Iteration:   2460, Loss function: 4.257, Average Loss: 4.210, avg. samples / sec: 24560.60
Iteration:   2460, Loss function: 4.129, Average Loss: 4.198, avg. samples / sec: 24565.10
Iteration:   2460, Loss function: 4.413, Average Loss: 4.216, avg. samples / sec: 24553.38
Iteration:   2460, Loss function: 4.362, Average Loss: 4.211, avg. samples / sec: 24578.52
Iteration:   2460, Loss function: 4.069, Average Loss: 4.205, avg. samples / sec: 24578.86
Iteration:   2480, Loss function: 4.152, Average Loss: 4.218, avg. samples / sec: 24621.61
Iteration:   2480, Loss function: 3.668, Average Loss: 4.198, avg. samples / sec: 24621.50
Iteration:   2480, Loss function: 4.037, Average Loss: 4.213, avg. samples / sec: 24632.87
Iteration:   2480, Loss function: 3.462, Average Loss: 4.213, avg. samples / sec: 24619.85
Iteration:   2480, Loss function: 4.015, Average Loss: 4.196, avg. samples / sec: 24623.45
Iteration:   2480, Loss function: 3.940, Average Loss: 4.207, avg. samples / sec: 24635.08
Iteration:   2480, Loss function: 4.063, Average Loss: 4.207, avg. samples / sec: 24607.96
Iteration:   2480, Loss function: 4.244, Average Loss: 4.201, avg. samples / sec: 24635.70

:::MLPv0.5.0 ssd 1541757389.661684275 (train.py:553) train_epoch: 43
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1

:::MLPv0.5.0 ssd 1541757391.000697613 (train.py:578) opt_learning_rate: 0.016
Iteration:   2500, Loss function: 3.700, Average Loss: 4.214, avg. samples / sec: 24519.95
Iteration:   2500, Loss function: 3.610, Average Loss: 4.189, avg. samples / sec: 24527.72
Iteration:   2500, Loss function: 3.952, Average Loss: 4.202, avg. samples / sec: 24522.33
Iteration:   2500, Loss function: 4.105, Average Loss: 4.191, avg. samples / sec: 24534.58
Iteration:   2500, Loss function: 3.978, Average Loss: 4.191, avg. samples / sec: 24511.16
Iteration:   2500, Loss function: 4.131, Average Loss: 4.203, avg. samples / sec: 24522.01
Iteration:   2500, Loss function: 3.512, Average Loss: 4.207, avg. samples / sec: 24497.08
Iteration:   2500, Loss function: 3.697, Average Loss: 4.206, avg. samples / sec: 24472.80

































































:::MLPv0.5.0 ssd 1541757391.083799601 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757391.084343195 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757391.084800959 (train.py:220) eval_start: 43
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.48 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.48 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.48 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.48 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.48 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.48 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.48 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.48 s
Loading and preparing results...
Converting ndarray to lists...
(382285, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(382285, 7)
(382285, 7)
(382285, 7)
Converting ndarray to lists...
0/382285
0/382285
0/382285
Loading and preparing results...
(382285, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(382285, 7)
(382285, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/382285
0/382285
(382285, 7)
Converting ndarray to lists...
Loading and preparing results...
(382285, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/382285
0/382285
(382285, 7)
(382285, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(382285, 7)
0/382285
0/382285
(382285, 7)
0/382285
0/382285
Converting ndarray to lists...
(382285, 7)
Loading and preparing results...
Converting ndarray to lists...
(382285, 7)
Loading and preparing results...
Converting ndarray to lists...
(382285, 7)
(382285, 7)
Loading and preparing results...
(382285, 7)
0/382285
Loading and preparing results...
(382285, 7)
0/382285
Loading and preparing results...
(382285, 7)
Loading and preparing results...
Loading and preparing results...
0/382285
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/382285
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/382285
(382285, 7)
Loading and preparing results...
(382285, 7)
Converting ndarray to lists...
(382285, 7)
0/382285
Loading and preparing results...
0/382285
0/382285
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(382285, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(382285, 7)
0/382285
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(382285, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/382285
Loading and preparing results...
(382285, 7)
0/382285
Converting ndarray to lists...
Converting ndarray to lists...
(382285, 7)
Converting ndarray to lists...
(382285, 7)
0/382285
Converting ndarray to lists...
(382285, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(382285, 7)
(382285, 7)
Loading and preparing results...
0/382285
(382285, 7)
Loading and preparing results...
(382285, 7)
Converting ndarray to lists...
0/382285
0/382285
0/382285
0/382285
Converting ndarray to lists...
0/382285
Converting ndarray to lists...
(382285, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/382285
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(382285, 7)
(382285, 7)
0/382285
Converting ndarray to lists...
0/382285
Loading and preparing results...
Converting ndarray to lists...
(382285, 7)
(382285, 7)
(382285, 7)
(382285, 7)
Converting ndarray to lists...
0/382285
0/382285
Loading and preparing results...
0/382285
(382285, 7)
(382285, 7)
(382285, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(382285, 7)
0/382285
Converting ndarray to lists...
0/382285
(382285, 7)
(382285, 7)
(382285, 7)
0/382285
Loading and preparing results...
Converting ndarray to lists...
0/382285
Loading and preparing results...
(382285, 7)
(382285, 7)
0/382285
(382285, 7)
(382285, 7)
0/382285
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/382285
0/382285
Converting ndarray to lists...
0/382285
(382285, 7)
0/382285
Converting ndarray to lists...
Converting ndarray to lists...
0/382285
Converting ndarray to lists...
(382285, 7)
0/382285
0/382285
0/382285
Converting ndarray to lists...
0/382285
0/382285
0/382285
(382285, 7)
(382285, 7)
Converting ndarray to lists...
(382285, 7)
Converting ndarray to lists...
0/382285
0/382285
(382285, 7)
(382285, 7)
0/382285
(382285, 7)
0/382285
(382285, 7)
Converting ndarray to lists...
0/382285
0/382285
(382285, 7)
0/382285
0/382285
0/382285
0/382285
(382285, 7)
(382285, 7)
0/382285
0/382285
DONE (t=2.13s)
creating index...
DONE (t=2.13s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
DONE (t=3.46s).
Accumulating evaluation results...
DONE (t=3.45s).
Accumulating evaluation results...
DONE (t=3.47s).
Accumulating evaluation results...
DONE (t=3.47s).
Accumulating evaluation results...
DONE (t=3.49s).
Accumulating evaluation results...
DONE (t=3.49s).
Accumulating evaluation results...
DONE (t=3.48s).
Accumulating evaluation results...
DONE (t=3.49s).
Accumulating evaluation results...
DONE (t=1.31s).
DONE (t=1.31s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.158
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.158
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.155
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.155
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
DONE (t=1.32s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.167
DONE (t=1.32s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.167
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.158
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.247
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.247
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.158
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.251
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
DONE (t=1.33s).
DONE (t=1.32s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.251
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.263
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.069
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.277
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403
Current AP: 0.15813 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.263
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.069
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.277
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403
Current AP: 0.15813 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.155
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.158
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.158
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.155
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
DONE (t=1.33s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.167
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.158
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.167
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.155
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.247
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.155
DONE (t=1.33s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.247
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.251
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.158
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.263
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.069
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.277
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403
Current AP: 0.15813 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.251
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.167
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.167
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.155
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.263
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.069
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.277
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403
Current AP: 0.15813 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.247
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.247
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.155
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.251
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.251
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.167
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.263
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.069
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.277
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403
Current AP: 0.15813 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.263
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.069
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.277
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403
Current AP: 0.15813 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.247
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.167
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.251
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.247
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.263
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.069
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.277
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403
Current AP: 0.15813 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.172
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.251
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.263
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.069
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.277
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.403
Current AP: 0.15813 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757402.827082157 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757402.827726126 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.15813015636917743}

:::MLPv0.5.0 ssd 1541757402.828186512 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.15813015636917743}

:::MLPv0.5.0 ssd 1541757402.828630924 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757402.829071045 (train.py:338) eval_stop: 43
Iteration:   2520, Loss function: 3.614, Average Loss: 4.203, avg. samples / sec: 2951.38
Iteration:   2520, Loss function: 3.892, Average Loss: 4.192, avg. samples / sec: 2951.43
Iteration:   2520, Loss function: 3.547, Average Loss: 4.182, avg. samples / sec: 2951.44
Iteration:   2520, Loss function: 3.469, Average Loss: 4.195, avg. samples / sec: 2951.67
Iteration:   2520, Loss function: 3.033, Average Loss: 4.176, avg. samples / sec: 2951.22
Iteration:   2520, Loss function: 4.202, Average Loss: 4.197, avg. samples / sec: 2951.48
Iteration:   2520, Loss function: 3.017, Average Loss: 4.181, avg. samples / sec: 2950.62
Iteration:   2520, Loss function: 3.817, Average Loss: 4.193, avg. samples / sec: 2950.49
Iteration:   2540, Loss function: 3.163, Average Loss: 4.191, avg. samples / sec: 24581.65
Iteration:   2540, Loss function: 4.464, Average Loss: 4.183, avg. samples / sec: 24657.39
Iteration:   2540, Loss function: 3.427, Average Loss: 4.169, avg. samples / sec: 24639.45
Iteration:   2540, Loss function: 3.224, Average Loss: 4.161, avg. samples / sec: 24587.35
Iteration:   2540, Loss function: 3.950, Average Loss: 4.178, avg. samples / sec: 24578.40
Iteration:   2540, Loss function: 3.505, Average Loss: 4.183, avg. samples / sec: 24569.89
Iteration:   2540, Loss function: 3.593, Average Loss: 4.166, avg. samples / sec: 24534.06
Iteration:   2540, Loss function: 2.814, Average Loss: 4.182, avg. samples / sec: 24530.01

:::MLPv0.5.0 ssd 1541757406.709891319 (train.py:553) train_epoch: 44
Iteration:   2560, Loss function: 3.994, Average Loss: 4.176, avg. samples / sec: 24547.41
Iteration:   2560, Loss function: 3.563, Average Loss: 4.155, avg. samples / sec: 24552.99
Iteration:   2560, Loss function: 3.366, Average Loss: 4.170, avg. samples / sec: 24600.97
Iteration:   2560, Loss function: 3.580, Average Loss: 4.162, avg. samples / sec: 24556.37
Iteration:   2560, Loss function: 3.637, Average Loss: 4.152, avg. samples / sec: 24596.48
Iteration:   2560, Loss function: 3.063, Average Loss: 4.168, avg. samples / sec: 24602.47
Iteration:   2560, Loss function: 3.606, Average Loss: 4.148, avg. samples / sec: 24545.56
Iteration:   2560, Loss function: 3.437, Average Loss: 4.167, avg. samples / sec: 24521.79
Iteration:   2580, Loss function: 2.966, Average Loss: 4.146, avg. samples / sec: 24590.16
Iteration:   2580, Loss function: 3.356, Average Loss: 4.137, avg. samples / sec: 24588.93
Iteration:   2580, Loss function: 2.999, Average Loss: 4.154, avg. samples / sec: 24581.63
Iteration:   2580, Loss function: 3.743, Average Loss: 4.152, avg. samples / sec: 24606.66
Iteration:   2580, Loss function: 3.199, Average Loss: 4.138, avg. samples / sec: 24578.96
Iteration:   2580, Loss function: 3.237, Average Loss: 4.131, avg. samples / sec: 24587.14
Iteration:   2580, Loss function: 3.233, Average Loss: 4.154, avg. samples / sec: 24581.66
Iteration:   2580, Loss function: 3.832, Average Loss: 4.160, avg. samples / sec: 24553.30

:::MLPv0.5.0 ssd 1541757411.546175480 (train.py:553) train_epoch: 45
Iteration:   2600, Loss function: 3.092, Average Loss: 4.131, avg. samples / sec: 24534.87
Iteration:   2600, Loss function: 3.337, Average Loss: 4.125, avg. samples / sec: 24536.68
Iteration:   2600, Loss function: 3.477, Average Loss: 4.136, avg. samples / sec: 24535.44
Iteration:   2600, Loss function: 3.579, Average Loss: 4.117, avg. samples / sec: 24534.24
Iteration:   2600, Loss function: 3.436, Average Loss: 4.122, avg. samples / sec: 24521.70
Iteration:   2600, Loss function: 3.830, Average Loss: 4.143, avg. samples / sec: 24527.73
Iteration:   2600, Loss function: 3.715, Average Loss: 4.138, avg. samples / sec: 24487.49
Iteration:   2600, Loss function: 3.820, Average Loss: 4.138, avg. samples / sec: 24488.12
Iteration:   2620, Loss function: 3.281, Average Loss: 4.123, avg. samples / sec: 24665.36
Iteration:   2620, Loss function: 3.411, Average Loss: 4.110, avg. samples / sec: 24609.16
Iteration:   2620, Loss function: 3.473, Average Loss: 4.121, avg. samples / sec: 24610.23
Iteration:   2620, Loss function: 3.876, Average Loss: 4.115, avg. samples / sec: 24599.12
Iteration:   2620, Loss function: 3.548, Average Loss: 4.127, avg. samples / sec: 24625.76
Iteration:   2620, Loss function: 3.218, Average Loss: 4.108, avg. samples / sec: 24583.56
Iteration:   2620, Loss function: 3.434, Average Loss: 4.104, avg. samples / sec: 24573.04
Iteration:   2620, Loss function: 3.316, Average Loss: 4.123, avg. samples / sec: 24612.93
Iteration:   2640, Loss function: 3.216, Average Loss: 4.110, avg. samples / sec: 24724.83
Iteration:   2640, Loss function: 3.088, Average Loss: 4.089, avg. samples / sec: 24745.57
Iteration:   2640, Loss function: 3.542, Average Loss: 4.107, avg. samples / sec: 24702.12
Iteration:   2640, Loss function: 3.364, Average Loss: 4.099, avg. samples / sec: 24701.34
Iteration:   2640, Loss function: 3.777, Average Loss: 4.096, avg. samples / sec: 24694.34
Iteration:   2640, Loss function: 3.138, Average Loss: 4.106, avg. samples / sec: 24662.94
Iteration:   2640, Loss function: 3.179, Average Loss: 4.092, avg. samples / sec: 24694.95
Iteration:   2640, Loss function: 3.196, Average Loss: 4.107, avg. samples / sec: 24695.78

:::MLPv0.5.0 ssd 1541757416.281486511 (train.py:553) train_epoch: 46
Iteration:   2660, Loss function: 2.985, Average Loss: 4.089, avg. samples / sec: 24676.41
Iteration:   2660, Loss function: 2.880, Average Loss: 4.081, avg. samples / sec: 24647.31
Iteration:   2660, Loss function: 2.878, Average Loss: 4.071, avg. samples / sec: 24638.02
Iteration:   2660, Loss function: 3.592, Average Loss: 4.082, avg. samples / sec: 24642.64
Iteration:   2660, Loss function: 3.419, Average Loss: 4.090, avg. samples / sec: 24691.07
Iteration:   2660, Loss function: 3.582, Average Loss: 4.094, avg. samples / sec: 24615.65
Iteration:   2660, Loss function: 3.365, Average Loss: 4.090, avg. samples / sec: 24615.58
Iteration:   2660, Loss function: 2.875, Average Loss: 4.077, avg. samples / sec: 24643.69
Iteration:   2680, Loss function: 2.892, Average Loss: 4.064, avg. samples / sec: 24581.25
Iteration:   2680, Loss function: 2.896, Average Loss: 4.074, avg. samples / sec: 24575.49
Iteration:   2680, Loss function: 3.506, Average Loss: 4.076, avg. samples / sec: 24587.64
Iteration:   2680, Loss function: 3.638, Average Loss: 4.057, avg. samples / sec: 24579.57
Iteration:   2680, Loss function: 3.560, Average Loss: 4.078, avg. samples / sec: 24572.69
Iteration:   2680, Loss function: 3.137, Average Loss: 4.076, avg. samples / sec: 24577.64
Iteration:   2680, Loss function: 3.246, Average Loss: 4.067, avg. samples / sec: 24552.57
Iteration:   2680, Loss function: 3.271, Average Loss: 4.061, avg. samples / sec: 24575.33
Iteration:   2700, Loss function: 2.829, Average Loss: 4.048, avg. samples / sec: 24565.46
Iteration:   2700, Loss function: 2.807, Average Loss: 4.041, avg. samples / sec: 24564.36
Iteration:   2700, Loss function: 3.695, Average Loss: 4.062, avg. samples / sec: 24591.33
Iteration:   2700, Loss function: 3.060, Average Loss: 4.058, avg. samples / sec: 24561.47
Iteration:   2700, Loss function: 2.875, Average Loss: 4.061, avg. samples / sec: 24559.86
Iteration:   2700, Loss function: 3.196, Average Loss: 4.047, avg. samples / sec: 24607.41
Iteration:   2700, Loss function: 3.424, Average Loss: 4.052, avg. samples / sec: 24589.42
Iteration:   2700, Loss function: 3.673, Average Loss: 4.062, avg. samples / sec: 24564.85

:::MLPv0.5.0 ssd 1541757421.112895966 (train.py:553) train_epoch: 47
Iteration:   2720, Loss function: 2.931, Average Loss: 4.048, avg. samples / sec: 24644.85
Iteration:   2720, Loss function: 2.783, Average Loss: 4.036, avg. samples / sec: 24650.66
Iteration:   2720, Loss function: 3.110, Average Loss: 4.033, avg. samples / sec: 24636.69
Iteration:   2720, Loss function: 3.255, Average Loss: 4.045, avg. samples / sec: 24636.80
Iteration:   2720, Loss function: 3.315, Average Loss: 4.035, avg. samples / sec: 24635.47
Iteration:   2720, Loss function: 3.553, Average Loss: 4.044, avg. samples / sec: 24626.21
Iteration:   2720, Loss function: 3.308, Average Loss: 4.047, avg. samples / sec: 24641.33
Iteration:   2720, Loss function: 3.300, Average Loss: 4.026, avg. samples / sec: 24580.46
Iteration:   2740, Loss function: 3.330, Average Loss: 4.019, avg. samples / sec: 24559.76
Iteration:   2740, Loss function: 3.274, Average Loss: 4.031, avg. samples / sec: 24552.39
Iteration:   2740, Loss function: 3.429, Average Loss: 4.016, avg. samples / sec: 24614.82
Iteration:   2740, Loss function: 3.418, Average Loss: 4.029, avg. samples / sec: 24567.82
Iteration:   2740, Loss function: 3.210, Average Loss: 4.030, avg. samples / sec: 24556.28
Iteration:   2740, Loss function: 3.376, Average Loss: 4.031, avg. samples / sec: 24555.54
Iteration:   2740, Loss function: 3.600, Average Loss: 4.021, avg. samples / sec: 24502.58
Iteration:   2740, Loss function: 4.027, Average Loss: 4.021, avg. samples / sec: 24505.62
Iteration:   2760, Loss function: 3.400, Average Loss: 4.014, avg. samples / sec: 24543.69
Iteration:   2760, Loss function: 3.375, Average Loss: 4.005, avg. samples / sec: 24531.27
Iteration:   2760, Loss function: 3.433, Average Loss: 4.016, avg. samples / sec: 24539.54
Iteration:   2760, Loss function: 3.058, Average Loss: 4.007, avg. samples / sec: 24579.85
Iteration:   2760, Loss function: 3.695, Average Loss: 4.017, avg. samples / sec: 24549.14
Iteration:   2760, Loss function: 2.861, Average Loss: 4.009, avg. samples / sec: 24578.10
Iteration:   2760, Loss function: 3.162, Average Loss: 4.004, avg. samples / sec: 24516.49
Iteration:   2760, Loss function: 3.243, Average Loss: 4.015, avg. samples / sec: 24483.46

:::MLPv0.5.0 ssd 1541757425.949912310 (train.py:553) train_epoch: 48
Iteration:   2780, Loss function: 3.129, Average Loss: 4.001, avg. samples / sec: 24562.01
Iteration:   2780, Loss function: 3.356, Average Loss: 4.001, avg. samples / sec: 24580.16
Iteration:   2780, Loss function: 3.135, Average Loss: 3.990, avg. samples / sec: 24581.22
Iteration:   2780, Loss function: 3.394, Average Loss: 3.994, avg. samples / sec: 24566.71
Iteration:   2780, Loss function: 3.778, Average Loss: 4.001, avg. samples / sec: 24558.63
Iteration:   2780, Loss function: 3.419, Average Loss: 3.994, avg. samples / sec: 24559.70
Iteration:   2780, Loss function: 3.067, Average Loss: 3.989, avg. samples / sec: 24541.23
Iteration:   2780, Loss function: 3.502, Average Loss: 3.999, avg. samples / sec: 24577.87
Iteration:   2800, Loss function: 3.507, Average Loss: 3.986, avg. samples / sec: 24562.59
Iteration:   2800, Loss function: 3.373, Average Loss: 3.984, avg. samples / sec: 24558.25
Iteration:   2800, Loss function: 3.564, Average Loss: 3.978, avg. samples / sec: 24580.05
Iteration:   2800, Loss function: 2.913, Average Loss: 3.987, avg. samples / sec: 24566.92
Iteration:   2800, Loss function: 3.404, Average Loss: 3.976, avg. samples / sec: 24579.44
Iteration:   2800, Loss function: 3.466, Average Loss: 3.978, avg. samples / sec: 24558.96
Iteration:   2800, Loss function: 3.436, Average Loss: 3.984, avg. samples / sec: 24551.49
Iteration:   2800, Loss function: 3.162, Average Loss: 3.976, avg. samples / sec: 24510.02

































































:::MLPv0.5.0 ssd 1541757429.287007093 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757429.287560463 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757429.288017035 (train.py:220) eval_start: 48
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(300283, 7)
Loading and preparing results...
(300283, 7)
Loading and preparing results...
0/300283
0/300283
Converting ndarray to lists...
Converting ndarray to lists...
(300283, 7)
Converting ndarray to lists...
0/300283
Loading and preparing results...
(300283, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/300283
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(300283, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(300283, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/300283
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(300283, 7)
(300283, 7)
(300283, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(300283, 7)
0/300283
(300283, 7)
Loading and preparing results...
(300283, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(300283, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(300283, 7)
(300283, 7)
0/300283
0/300283
(300283, 7)
0/300283
Converting ndarray to lists...
(300283, 7)
Converting ndarray to lists...
0/300283
Converting ndarray to lists...
0/300283
(300283, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(300283, 7)
0/300283
Converting ndarray to lists...
0/300283
0/300283
(300283, 7)
Converting ndarray to lists...
(300283, 7)
0/300283
0/300283
Loading and preparing results...
(300283, 7)
(300283, 7)
Loading and preparing results...
Converting ndarray to lists...
(300283, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(300283, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/300283
Loading and preparing results...
0/300283
Loading and preparing results...
(300283, 7)
Loading and preparing results...
Converting ndarray to lists...
0/300283
(300283, 7)
Loading and preparing results...
Loading and preparing results...
0/300283
0/300283
Converting ndarray to lists...
(300283, 7)
(300283, 7)
Loading and preparing results...
Converting ndarray to lists...
0/300283
0/300283
0/300283
(300283, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(300283, 7)
(300283, 7)
Converting ndarray to lists...
0/300283
Converting ndarray to lists...
0/300283
0/300283
(300283, 7)
Loading and preparing results...
0/300283
(300283, 7)
(300283, 7)
0/300283
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(300283, 7)
Converting ndarray to lists...
(300283, 7)
0/300283
Loading and preparing results...
Converting ndarray to lists...
(300283, 7)
(300283, 7)
Converting ndarray to lists...
(300283, 7)
Converting ndarray to lists...
Loading and preparing results...
(300283, 7)
Loading and preparing results...
0/300283
(300283, 7)
Converting ndarray to lists...
(300283, 7)
Converting ndarray to lists...
(300283, 7)
0/300283
(300283, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(300283, 7)
0/300283
0/300283
0/300283
Converting ndarray to lists...
Loading and preparing results...
0/300283
(300283, 7)
Loading and preparing results...
0/300283
0/300283
0/300283
(300283, 7)
0/300283
0/300283
(300283, 7)
0/300283
(300283, 7)
0/300283
(300283, 7)
(300283, 7)
0/300283
Converting ndarray to lists...
0/300283
0/300283
Converting ndarray to lists...
0/300283
(300283, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(300283, 7)
(300283, 7)
0/300283
0/300283
0/300283
0/300283
Converting ndarray to lists...
(300283, 7)
0/300283
(300283, 7)
0/300283
0/300283
Loading and preparing results...
0/300283
Loading and preparing results...
(300283, 7)
0/300283
(300283, 7)
0/300283
0/300283
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(300283, 7)
(300283, 7)
0/300283
0/300283
0/300283
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(300283, 7)
(300283, 7)
(300283, 7)
0/300283
0/300283
0/300283
DONE (t=1.80s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.87s)
creating index...
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.01s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.09s)
creating index...
DONE (t=2.09s)
creating index...
DONE (t=2.09s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.11s)
creating index...
DONE (t=2.11s)
creating index...
DONE (t=2.11s)
creating index...
DONE (t=2.11s)
creating index...
DONE (t=2.11s)
creating index...
DONE (t=2.12s)
creating index...
DONE (t=2.12s)
creating index...
DONE (t=2.12s)
creating index...
index created!
DONE (t=2.12s)
creating index...
DONE (t=2.12s)
creating index...
DONE (t=2.13s)
creating index...
DONE (t=2.13s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
index created!
DONE (t=2.16s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=3.24s).
Accumulating evaluation results...
DONE (t=3.24s).
Accumulating evaluation results...
DONE (t=3.27s).
Accumulating evaluation results...
DONE (t=3.27s).
Accumulating evaluation results...
DONE (t=3.27s).
Accumulating evaluation results...
DONE (t=3.29s).
Accumulating evaluation results...
DONE (t=3.27s).
Accumulating evaluation results...
DONE (t=3.31s).
Accumulating evaluation results...
DONE (t=1.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
DONE (t=1.04s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21250 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21250 AP goal: 0.21200
DONE (t=1.03s).
DONE (t=1.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
DONE (t=1.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
DONE (t=1.06s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.341
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.341
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21250 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21250 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21250 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21250 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
DONE (t=1.06s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21250 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.367
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21250 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757440.189124346 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757440.189730883 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.2124975785784847}

:::MLPv0.5.0 ssd 1541757440.190181017 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.2124975785784847}

:::MLPv0.5.0 ssd 1541757440.190614939 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757440.191048145 (train.py:338) eval_stop: 48

:::MLPv0.5.0 ssd 1541757441.279154778 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1541757441.279710293 (train.py:707) run_final
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2018-11-09 09:57:25 AM
RESULT,OBJECT_DETECTION,,327,nvidia,2018-11-09 09:51:58 AM
ENDING TIMING RUN AT 2018-11-09 09:57:26 AM
RESULT,OBJECT_DETECTION,,327,nvidia,2018-11-09 09:51:59 AM
ENDING TIMING RUN AT 2018-11-09 09:57:24 AM
RESULT,OBJECT_DETECTION,,328,nvidia,2018-11-09 09:51:56 AM
ENDING TIMING RUN AT 2018-11-09 09:57:27 AM
RESULT,OBJECT_DETECTION,,327,nvidia,2018-11-09 09:52:00 AM
ENDING TIMING RUN AT 2018-11-09 09:57:24 AM
RESULT,OBJECT_DETECTION,,328,nvidia,2018-11-09 09:51:56 AM
ENDING TIMING RUN AT 2018-11-09 09:57:22 AM
RESULT,OBJECT_DETECTION,,328,nvidia,2018-11-09 09:51:54 AM
ENDING TIMING RUN AT 2018-11-09 09:57:25 AM
RESULT,OBJECT_DETECTION,,328,nvidia,2018-11-09 09:51:57 AM
ENDING TIMING RUN AT 2018-11-09 09:57:20 AM
RESULT,OBJECT_DETECTION,,328,nvidia,2018-11-09 09:51:52 AM
