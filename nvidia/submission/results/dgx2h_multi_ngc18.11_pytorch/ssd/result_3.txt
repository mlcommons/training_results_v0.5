Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3

:::MLPv0.5.0 ssd 1541757356.383246660 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757351.802471638 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757356.788552999 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757351.427782774 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757361.076337099 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757355.679361105 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757359.824322701 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757357.543928146 (<string>:1) run_clear_caches
Launching on node circe-n023
+ pids+=($!)
+ set +x
Launching on node circe-n024
+ pids+=($!)
+ set +x
Launching on node circe-n025
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
Launching on node circe-n026
+++ echo srun -N 1 -n 1 -w circe-n023
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n024
+ pids+=($!)
+ set +x
Launching on node circe-n027
++ eval echo srun -N 1 -n 1 -w '$hostn'
+ srun -N 1 -n 1 -w circe-n023 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=0 --master_addr=10.0.1.23 --master_port=4242' -e SLURM_JOB_ID=35123 -e SLURM_NTASKS_PER_NODE=8 cont_35123 ./run_and_time.sh
+++ echo srun -N 1 -n 1 -w circe-n025
+ srun -N 1 -n 1 -w circe-n024 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=1 --master_addr=10.0.1.23 --master_port=4242' -e SLURM_JOB_ID=35123 -e SLURM_NTASKS_PER_NODE=8 cont_35123 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n028
+ srun -N 1 -n 1 -w circe-n025 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=2 --master_addr=10.0.1.23 --master_port=4242' -e SLURM_JOB_ID=35123 -e SLURM_NTASKS_PER_NODE=8 cont_35123 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n026
+ pids+=($!)
+ set +x
Launching on node circe-n029
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n027
+ srun -N 1 -n 1 -w circe-n026 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=3 --master_addr=10.0.1.23 --master_port=4242' -e SLURM_JOB_ID=35123 -e SLURM_NTASKS_PER_NODE=8 cont_35123 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n030
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n028
+ srun -N 1 -n 1 -w circe-n027 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=4 --master_addr=10.0.1.23 --master_port=4242' -e SLURM_JOB_ID=35123 -e SLURM_NTASKS_PER_NODE=8 cont_35123 ./run_and_time.sh
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n029
+ srun -N 1 -n 1 -w circe-n028 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=5 --master_addr=10.0.1.23 --master_port=4242' -e SLURM_JOB_ID=35123 -e SLURM_NTASKS_PER_NODE=8 cont_35123 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n030
+ srun -N 1 -n 1 -w circe-n029 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=6 --master_addr=10.0.1.23 --master_port=4242' -e SLURM_JOB_ID=35123 -e SLURM_NTASKS_PER_NODE=8 cont_35123 ./run_and_time.sh
+ srun -N 1 -n 1 -w circe-n030 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=7 --master_addr=10.0.1.23 --master_port=4242' -e SLURM_JOB_ID=35123 -e SLURM_NTASKS_PER_NODE=8 cont_35123 ./run_and_time.sh
Run vars: id 35123 gpus 8 mparams  --nnodes=8 --node_rank=7 --master_addr=10.0.1.23 --master_port=4242
Run vars: id 35123 gpus 8 mparams  --nnodes=8 --node_rank=0 --master_addr=10.0.1.23 --master_port=4242
Run vars: id 35123 gpus 8 mparams  --nnodes=8 --node_rank=1 --master_addr=10.0.1.23 --master_port=4242
Run vars: id 35123 gpus 8 mparams  --nnodes=8 --node_rank=6 --master_addr=10.0.1.23 --master_port=4242
Run vars: id 35123 gpus 8 mparams  --nnodes=8 --node_rank=2 --master_addr=10.0.1.23 --master_port=4242
Run vars: id 35123 gpus 8 mparams  --nnodes=8 --node_rank=3 --master_addr=10.0.1.23 --master_port=4242
Run vars: id 35123 gpus 8 mparams  --nnodes=8 --node_rank=5 --master_addr=10.0.1.23 --master_port=4242
STARTING TIMING RUN AT 2018-11-09 09:55:57 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=7 --master_addr=10.0.1.23 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
Run vars: id 35123 gpus 8 mparams  --nnodes=8 --node_rank=4 --master_addr=10.0.1.23 --master_port=4242
STARTING TIMING RUN AT 2018-11-09 09:55:56 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=0 --master_addr=10.0.1.23 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:55:52 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=1 --master_addr=10.0.1.23 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:55:51 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=6 --master_addr=10.0.1.23 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:56:01 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=2 --master_addr=10.0.1.23 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:56:00 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=3 --master_addr=10.0.1.23 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:55:57 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=5 --master_addr=10.0.1.23 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:55:55 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=4 --master_addr=10.0.1.23 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
0 Using seed = 2923176485
1 Using seed = 2923176486
3 Using seed = 2923176488
2 Using seed = 2923176487
4 Using seed = 2923176489
11 Using seed = 2923176496
10 Using seed = 2923176495
9 Using seed = 2923176494
13 Using seed = 2923176498
12 Using seed = 2923176497
8 Using seed = 2923176493
15 Using seed = 2923176500
14 Using seed = 2923176499
22 Using seed = 2923176507
23 Using seed = 2923176508
21 Using seed = 2923176506
19 Using seed = 2923176504
16 Using seed = 2923176501
17 Using seed = 2923176502
20 Using seed = 2923176505
18 Using seed = 2923176503
31 Using seed = 2923176516
30 Using seed = 2923176515
27 Using seed = 2923176512
29 Using seed = 2923176514
25 Using seed = 2923176510
24 Using seed = 2923176509
26 Using seed = 2923176511
28 Using seed = 2923176513
38 Using seed = 2923176523
32 Using seed = 2923176517
39 Using seed = 2923176524
35 Using seed = 2923176520
34 Using seed = 2923176519
37 Using seed = 2923176522
33 Using seed = 2923176518
36 Using seed = 2923176521
40 Using seed = 2923176525
41 Using seed = 2923176526
45 Using seed = 2923176530
47 Using seed = 2923176532
42 Using seed = 2923176527
43 Using seed = 2923176528
46 Using seed = 2923176531
44 Using seed = 2923176529
53 Using seed = 2923176538
55 Using seed = 2923176540
54 Using seed = 2923176539
52 Using seed = 2923176537
50 Using seed = 2923176535
51 Using seed = 2923176536
48 Using seed = 2923176533
49 Using seed = 2923176534
61 Using seed = 2923176546
63 Using seed = 2923176548
58 Using seed = 2923176543
62 Using seed = 2923176547
56 Using seed = 2923176541
57 Using seed = 2923176542
59 Using seed = 2923176544
60 Using seed = 2923176545
5 Using seed = 2923176490
6 Using seed = 2923176491
7 Using seed = 2923176492

:::MLPv0.5.0 ssd 1541757370.494724035 (train.py:371) run_start

:::MLPv0.5.0 ssd 1541757370.497907877 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1541757370.498385668 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1541757370.526068211 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1541757370.526523590 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1541757370.586133242 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1541757370.606101036 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1541757370.628726721 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1541757370.654508352 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1541757370.666239500 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1541757370.692671299 (train.py:382) input_size: 300
loading annotations into memory...
Done (t=0.42s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
Done (t=0.43s)
creating index...
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
Done (t=0.43s)
creating index...
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
index created!
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Done (t=0.46s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
time_check a: 1541757371.578126431
time_check a: 1541757371.926912785
time_check a: 1541757370.817241192
time_check a: 1541757372.673768759
time_check a: 1541757374.957400799
time_check a: 1541757366.574661016
time_check a: 1541757376.224422693
time_check a: 1541757367.026577234
time_check b: 1541757393.642692089
time_check b: 1541757398.324200869
time_check b: 1541757388.686558723
time_check b: 1541757392.943989515
time_check b: 1541757389.138015270
time_check b: 1541757394.838126659
time_check b: 1541757394.198193550
time_check b: 1541757397.272561312

:::MLPv0.5.0 ssd 1541757394.729985952 (train.py:413) input_order

:::MLPv0.5.0 ssd 1541757394.735129595 (train.py:414) input_batch_size: 32

:::MLPv0.5.0 ssd 1541757395.927947283 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541757395.928575516 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541757395.958303690 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1541757397.789150476 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1541757397.789765358 (train.py:477) opt_learning_rate: 0.16

:::MLPv0.5.0 ssd 1541757397.790210962 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1541757397.790624857 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1541757397.791035175 (train.py:483) opt_learning_rate_warmup_steps: 900

:::MLPv0.5.0 ssd 1541757399.186652184 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541757399.187216043 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541757399.216812611 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss

:::MLPv0.5.0 ssd 1541757402.483798981 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1541757402.484337807 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1541757402.487223625 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 22.555, Average Loss: 0.023, avg. samples / sec: 21718.63
Iteration:      0, Loss function: 23.163, Average Loss: 0.023, avg. samples / sec: 22387.29
Iteration:      0, Loss function: 22.953, Average Loss: 0.023, avg. samples / sec: 32430.02
Iteration:      0, Loss function: 22.781, Average Loss: 0.023, avg. samples / sec: 19428.04
Iteration:      0, Loss function: 22.356, Average Loss: 0.022, avg. samples / sec: 25501.15
Iteration:      0, Loss function: 22.935, Average Loss: 0.023, avg. samples / sec: 29231.09
Iteration:      0, Loss function: 22.529, Average Loss: 0.023, avg. samples / sec: 14748.52
Iteration:      0, Loss function: 22.573, Average Loss: 0.023, avg. samples / sec: 7986.71

:::MLPv0.5.0 ssd 1541757404.970719337 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.0001777777777777767}

:::MLPv0.5.0 ssd 1541757405.334935665 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0003555555555555534}

:::MLPv0.5.0 ssd 1541757405.436816454 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0005333333333333301}

:::MLPv0.5.0 ssd 1541757405.542357206 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0007111111111111068}

:::MLPv0.5.0 ssd 1541757405.637266636 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0008888888888888835}

:::MLPv0.5.0 ssd 1541757405.740699053 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0010666666666666602}

:::MLPv0.5.0 ssd 1541757405.843023539 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.001244444444444437}

:::MLPv0.5.0 ssd 1541757405.935050011 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0014222222222222136}

:::MLPv0.5.0 ssd 1541757406.025810242 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0015999999999999903}

:::MLPv0.5.0 ssd 1541757406.122370481 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.001777777777777767}

:::MLPv0.5.0 ssd 1541757406.213010073 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0019555555555555437}

:::MLPv0.5.0 ssd 1541757406.308824539 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.0021333333333333204}

:::MLPv0.5.0 ssd 1541757406.399976730 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.002311111111111097}

:::MLPv0.5.0 ssd 1541757406.489213705 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.002488888888888874}

:::MLPv0.5.0 ssd 1541757406.581056833 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.0026666666666666505}

:::MLPv0.5.0 ssd 1541757406.672433376 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.0028444444444444272}

:::MLPv0.5.0 ssd 1541757406.762448788 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.0030222222222222317}

:::MLPv0.5.0 ssd 1541757406.851026058 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.0032000000000000084}

:::MLPv0.5.0 ssd 1541757406.943284750 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.003377777777777785}

:::MLPv0.5.0 ssd 1541757407.034317017 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.003555555555555562}
Iteration:     20, Loss function: 20.700, Average Loss: 0.443, avg. samples / sec: 9024.61
Iteration:     20, Loss function: 20.866, Average Loss: 0.440, avg. samples / sec: 9023.12
Iteration:     20, Loss function: 19.276, Average Loss: 0.442, avg. samples / sec: 9023.35
Iteration:     20, Loss function: 20.807, Average Loss: 0.440, avg. samples / sec: 9022.27
Iteration:     20, Loss function: 20.646, Average Loss: 0.442, avg. samples / sec: 9022.16
Iteration:     20, Loss function: 20.666, Average Loss: 0.441, avg. samples / sec: 9020.12
Iteration:     20, Loss function: 20.687, Average Loss: 0.436, avg. samples / sec: 9020.07
Iteration:     20, Loss function: 20.608, Average Loss: 0.441, avg. samples / sec: 9019.06

:::MLPv0.5.0 ssd 1541757407.124324322 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.0037333333333333385}

:::MLPv0.5.0 ssd 1541757407.215472937 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.003911111111111115}

:::MLPv0.5.0 ssd 1541757407.303436041 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.004088888888888892}

:::MLPv0.5.0 ssd 1541757407.392968893 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.004266666666666669}

:::MLPv0.5.0 ssd 1541757407.482800722 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.004444444444444445}

:::MLPv0.5.0 ssd 1541757407.573868513 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.004622222222222222}

:::MLPv0.5.0 ssd 1541757407.661876678 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.004799999999999999}

:::MLPv0.5.0 ssd 1541757407.749634266 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.004977777777777775}

:::MLPv0.5.0 ssd 1541757407.838836432 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.005155555555555552}

:::MLPv0.5.0 ssd 1541757407.927757025 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.005333333333333329}

:::MLPv0.5.0 ssd 1541757408.014362097 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.0055111111111111055}

:::MLPv0.5.0 ssd 1541757408.102695227 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.005688888888888882}

:::MLPv0.5.0 ssd 1541757408.194351673 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.005866666666666659}

:::MLPv0.5.0 ssd 1541757408.291601658 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.006044444444444436}

:::MLPv0.5.0 ssd 1541757408.382517815 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.006222222222222212}

:::MLPv0.5.0 ssd 1541757408.481111526 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.006399999999999989}

:::MLPv0.5.0 ssd 1541757408.568675280 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.006577777777777766}

:::MLPv0.5.0 ssd 1541757408.657569647 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.0067555555555555424}

:::MLPv0.5.0 ssd 1541757408.752209187 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.006933333333333319}

:::MLPv0.5.0 ssd 1541757408.840330839 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.007111111111111096}
Iteration:     40, Loss function: 16.074, Average Loss: 0.820, avg. samples / sec: 22679.58
Iteration:     40, Loss function: 15.676, Average Loss: 0.816, avg. samples / sec: 22696.17
Iteration:     40, Loss function: 15.554, Average Loss: 0.817, avg. samples / sec: 22687.59
Iteration:     40, Loss function: 16.014, Average Loss: 0.815, avg. samples / sec: 22700.19
Iteration:     40, Loss function: 16.642, Average Loss: 0.818, avg. samples / sec: 22707.81
Iteration:     40, Loss function: 16.276, Average Loss: 0.816, avg. samples / sec: 22672.83
Iteration:     40, Loss function: 16.323, Average Loss: 0.811, avg. samples / sec: 22679.80
Iteration:     40, Loss function: 16.361, Average Loss: 0.818, avg. samples / sec: 22655.17

:::MLPv0.5.0 ssd 1541757408.929387569 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.0072888888888888725}

:::MLPv0.5.0 ssd 1541757409.018827677 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.007466666666666649}

:::MLPv0.5.0 ssd 1541757409.107328415 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.007644444444444454}

:::MLPv0.5.0 ssd 1541757409.193267345 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.00782222222222223}

:::MLPv0.5.0 ssd 1541757409.279765606 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.008000000000000007}

:::MLPv0.5.0 ssd 1541757409.374118805 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.008177777777777784}

:::MLPv0.5.0 ssd 1541757409.464107037 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.00835555555555556}

:::MLPv0.5.0 ssd 1541757409.552504063 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.008533333333333337}

:::MLPv0.5.0 ssd 1541757409.641041517 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.008711111111111114}

:::MLPv0.5.0 ssd 1541757409.729168653 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.00888888888888889}

:::MLPv0.5.0 ssd 1541757409.816003084 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.009066666666666667}

:::MLPv0.5.0 ssd 1541757409.902065516 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.009244444444444444}

:::MLPv0.5.0 ssd 1541757409.989183903 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.00942222222222222}

:::MLPv0.5.0 ssd 1541757410.078554153 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.009599999999999997}

:::MLPv0.5.0 ssd 1541757410.164106131 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.009777777777777774}

:::MLPv0.5.0 ssd 1541757410.252842188 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.00995555555555555}

:::MLPv0.5.0 ssd 1541757410.347890139 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.010133333333333328}

:::MLPv0.5.0 ssd 1541757410.434037685 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1541757410.438347578 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 58, "value": 0.010311111111111104}

:::MLPv0.5.0 ssd 1541757410.525133133 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 59, "value": 0.010488888888888881}

:::MLPv0.5.0 ssd 1541757410.613923788 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 60, "value": 0.010666666666666658}
Iteration:     60, Loss function: 11.087, Average Loss: 1.048, avg. samples / sec: 23093.86
Iteration:     60, Loss function: 11.009, Average Loss: 1.047, avg. samples / sec: 23102.33
Iteration:     60, Loss function: 10.947, Average Loss: 1.046, avg. samples / sec: 23092.39
Iteration:     60, Loss function: 11.266, Average Loss: 1.045, avg. samples / sec: 23093.32
Iteration:     60, Loss function: 10.230, Average Loss: 1.037, avg. samples / sec: 23109.07
Iteration:     60, Loss function: 10.691, Average Loss: 1.048, avg. samples / sec: 23110.75
Iteration:     60, Loss function: 10.884, Average Loss: 1.044, avg. samples / sec: 23075.08
Iteration:     60, Loss function: 11.323, Average Loss: 1.046, avg. samples / sec: 23024.17

:::MLPv0.5.0 ssd 1541757410.699217558 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 61, "value": 0.010844444444444434}

:::MLPv0.5.0 ssd 1541757410.784335852 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 62, "value": 0.011022222222222211}

:::MLPv0.5.0 ssd 1541757410.872067690 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 63, "value": 0.011199999999999988}

:::MLPv0.5.0 ssd 1541757410.958202362 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 64, "value": 0.011377777777777764}

:::MLPv0.5.0 ssd 1541757411.043290377 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 65, "value": 0.011555555555555541}

:::MLPv0.5.0 ssd 1541757411.128391027 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 66, "value": 0.011733333333333318}

:::MLPv0.5.0 ssd 1541757411.216689348 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 67, "value": 0.011911111111111095}

:::MLPv0.5.0 ssd 1541757411.306993484 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 68, "value": 0.012088888888888899}

:::MLPv0.5.0 ssd 1541757411.392267942 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 69, "value": 0.012266666666666676}

:::MLPv0.5.0 ssd 1541757411.479552746 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 70, "value": 0.012444444444444452}

:::MLPv0.5.0 ssd 1541757411.565173388 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 71, "value": 0.012622222222222229}

:::MLPv0.5.0 ssd 1541757411.649554491 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 72, "value": 0.012800000000000006}

:::MLPv0.5.0 ssd 1541757411.735856533 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 73, "value": 0.012977777777777783}

:::MLPv0.5.0 ssd 1541757411.821666002 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 74, "value": 0.01315555555555556}

:::MLPv0.5.0 ssd 1541757411.908346415 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 75, "value": 0.013333333333333336}

:::MLPv0.5.0 ssd 1541757411.996342897 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 76, "value": 0.013511111111111113}

:::MLPv0.5.0 ssd 1541757412.081896544 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 77, "value": 0.01368888888888889}

:::MLPv0.5.0 ssd 1541757412.171374798 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 78, "value": 0.013866666666666666}

:::MLPv0.5.0 ssd 1541757412.255750418 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 79, "value": 0.014044444444444443}

:::MLPv0.5.0 ssd 1541757412.350301266 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 80, "value": 0.01422222222222222}
Iteration:     80, Loss function: 10.845, Average Loss: 1.234, avg. samples / sec: 23596.85
Iteration:     80, Loss function: 11.047, Average Loss: 1.233, avg. samples / sec: 23660.91
Iteration:     80, Loss function: 10.777, Average Loss: 1.231, avg. samples / sec: 23614.50
Iteration:     80, Loss function: 10.590, Average Loss: 1.234, avg. samples / sec: 23572.89
Iteration:     80, Loss function: 11.019, Average Loss: 1.235, avg. samples / sec: 23574.55
Iteration:     80, Loss function: 11.070, Average Loss: 1.223, avg. samples / sec: 23582.93
Iteration:     80, Loss function: 11.074, Average Loss: 1.236, avg. samples / sec: 23554.92
Iteration:     80, Loss function: 10.518, Average Loss: 1.235, avg. samples / sec: 23572.27

:::MLPv0.5.0 ssd 1541757412.438514709 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 81, "value": 0.014399999999999996}

:::MLPv0.5.0 ssd 1541757412.526175737 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 82, "value": 0.014577777777777773}

:::MLPv0.5.0 ssd 1541757412.613422155 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 83, "value": 0.01475555555555555}

:::MLPv0.5.0 ssd 1541757412.697599888 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 84, "value": 0.014933333333333326}

:::MLPv0.5.0 ssd 1541757412.783072948 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 85, "value": 0.015111111111111103}

:::MLPv0.5.0 ssd 1541757412.869340420 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 86, "value": 0.01528888888888888}

:::MLPv0.5.0 ssd 1541757412.955803394 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 87, "value": 0.015466666666666656}

:::MLPv0.5.0 ssd 1541757413.042296410 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 88, "value": 0.015644444444444433}

:::MLPv0.5.0 ssd 1541757413.127745390 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 89, "value": 0.01582222222222221}

:::MLPv0.5.0 ssd 1541757413.219162226 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 90, "value": 0.015999999999999986}

:::MLPv0.5.0 ssd 1541757413.305332899 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 91, "value": 0.016177777777777763}

:::MLPv0.5.0 ssd 1541757413.392238140 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 92, "value": 0.01635555555555554}

:::MLPv0.5.0 ssd 1541757413.476913929 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 93, "value": 0.016533333333333317}

:::MLPv0.5.0 ssd 1541757413.562834501 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 94, "value": 0.01671111111111112}

:::MLPv0.5.0 ssd 1541757413.649092436 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 95, "value": 0.016888888888888898}

:::MLPv0.5.0 ssd 1541757413.737360477 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 96, "value": 0.017066666666666674}

:::MLPv0.5.0 ssd 1541757413.823328972 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 97, "value": 0.01724444444444445}

:::MLPv0.5.0 ssd 1541757413.907582998 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 98, "value": 0.017422222222222228}

:::MLPv0.5.0 ssd 1541757413.994209528 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 99, "value": 0.017600000000000005}

:::MLPv0.5.0 ssd 1541757414.079075575 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 100, "value": 0.01777777777777778}
Iteration:    100, Loss function: 8.840, Average Loss: 1.396, avg. samples / sec: 23713.25
Iteration:    100, Loss function: 8.898, Average Loss: 1.399, avg. samples / sec: 23711.10
Iteration:    100, Loss function: 9.094, Average Loss: 1.400, avg. samples / sec: 23722.79
Iteration:    100, Loss function: 9.103, Average Loss: 1.401, avg. samples / sec: 23715.82
Iteration:    100, Loss function: 9.482, Average Loss: 1.390, avg. samples / sec: 23693.73
Iteration:    100, Loss function: 9.151, Average Loss: 1.400, avg. samples / sec: 23670.93
Iteration:    100, Loss function: 8.972, Average Loss: 1.398, avg. samples / sec: 23655.17
Iteration:    100, Loss function: 9.396, Average Loss: 1.394, avg. samples / sec: 23658.78

:::MLPv0.5.0 ssd 1541757414.166202784 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 101, "value": 0.017955555555555558}

:::MLPv0.5.0 ssd 1541757414.251540184 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 102, "value": 0.018133333333333335}

:::MLPv0.5.0 ssd 1541757414.335228443 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 103, "value": 0.01831111111111111}

:::MLPv0.5.0 ssd 1541757414.425316334 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 104, "value": 0.018488888888888888}

:::MLPv0.5.0 ssd 1541757414.510897875 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 105, "value": 0.018666666666666665}

:::MLPv0.5.0 ssd 1541757414.598064661 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 106, "value": 0.01884444444444444}

:::MLPv0.5.0 ssd 1541757414.683324814 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 107, "value": 0.019022222222222218}

:::MLPv0.5.0 ssd 1541757414.768525362 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 108, "value": 0.019199999999999995}

:::MLPv0.5.0 ssd 1541757414.854597569 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 109, "value": 0.01937777777777777}

:::MLPv0.5.0 ssd 1541757414.942879438 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 110, "value": 0.019555555555555548}

:::MLPv0.5.0 ssd 1541757415.030407429 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 111, "value": 0.019733333333333325}

:::MLPv0.5.0 ssd 1541757415.114788771 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 112, "value": 0.0199111111111111}

:::MLPv0.5.0 ssd 1541757415.200729609 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 113, "value": 0.02008888888888888}

:::MLPv0.5.0 ssd 1541757415.286402225 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 114, "value": 0.020266666666666655}

:::MLPv0.5.0 ssd 1541757415.370192051 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 115, "value": 0.020444444444444432}

:::MLPv0.5.0 ssd 1541757415.454057455 (train.py:553) train_epoch: 2

:::MLPv0.5.0 ssd 1541757415.458411455 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 116, "value": 0.02062222222222221}

:::MLPv0.5.0 ssd 1541757415.545134306 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 117, "value": 0.020799999999999985}

:::MLPv0.5.0 ssd 1541757415.630728245 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 118, "value": 0.020977777777777762}

:::MLPv0.5.0 ssd 1541757415.715222597 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 119, "value": 0.02115555555555554}

:::MLPv0.5.0 ssd 1541757415.801050186 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 120, "value": 0.021333333333333343}
Iteration:    120, Loss function: 8.705, Average Loss: 1.546, avg. samples / sec: 23796.16
Iteration:    120, Loss function: 9.214, Average Loss: 1.543, avg. samples / sec: 23836.66
Iteration:    120, Loss function: 9.360, Average Loss: 1.546, avg. samples / sec: 23818.24
Iteration:    120, Loss function: 8.862, Average Loss: 1.550, avg. samples / sec: 23789.05
Iteration:    120, Loss function: 8.479, Average Loss: 1.539, avg. samples / sec: 23782.79
Iteration:    120, Loss function: 9.173, Average Loss: 1.544, avg. samples / sec: 23756.06
Iteration:    120, Loss function: 8.814, Average Loss: 1.549, avg. samples / sec: 23768.14
Iteration:    120, Loss function: 8.988, Average Loss: 1.548, avg. samples / sec: 23747.70

:::MLPv0.5.0 ssd 1541757415.887409687 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 121, "value": 0.02151111111111112}

:::MLPv0.5.0 ssd 1541757415.973663092 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 122, "value": 0.021688888888888896}

:::MLPv0.5.0 ssd 1541757416.061119080 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 123, "value": 0.021866666666666673}

:::MLPv0.5.0 ssd 1541757416.147429228 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 124, "value": 0.02204444444444445}

:::MLPv0.5.0 ssd 1541757416.232841969 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 125, "value": 0.022222222222222227}

:::MLPv0.5.0 ssd 1541757416.316856384 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 126, "value": 0.022400000000000003}

:::MLPv0.5.0 ssd 1541757416.401197195 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 127, "value": 0.02257777777777778}

:::MLPv0.5.0 ssd 1541757416.486928463 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 128, "value": 0.022755555555555557}

:::MLPv0.5.0 ssd 1541757416.573870659 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 129, "value": 0.022933333333333333}

:::MLPv0.5.0 ssd 1541757416.658993959 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 130, "value": 0.02311111111111111}

:::MLPv0.5.0 ssd 1541757416.746002197 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 131, "value": 0.023288888888888887}

:::MLPv0.5.0 ssd 1541757416.832894325 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 132, "value": 0.023466666666666663}

:::MLPv0.5.0 ssd 1541757416.917497396 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 133, "value": 0.02364444444444444}

:::MLPv0.5.0 ssd 1541757417.004189014 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 134, "value": 0.023822222222222217}

:::MLPv0.5.0 ssd 1541757417.090945721 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 135, "value": 0.023999999999999994}

:::MLPv0.5.0 ssd 1541757417.175629616 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 136, "value": 0.02417777777777777}

:::MLPv0.5.0 ssd 1541757417.260940552 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 137, "value": 0.024355555555555547}

:::MLPv0.5.0 ssd 1541757417.345048666 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 138, "value": 0.024533333333333324}

:::MLPv0.5.0 ssd 1541757417.429969311 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 139, "value": 0.0247111111111111}

:::MLPv0.5.0 ssd 1541757417.516257048 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 140, "value": 0.024888888888888877}
Iteration:    140, Loss function: 7.858, Average Loss: 1.685, avg. samples / sec: 23872.09
Iteration:    140, Loss function: 8.131, Average Loss: 1.695, avg. samples / sec: 23889.34
Iteration:    140, Loss function: 8.140, Average Loss: 1.692, avg. samples / sec: 23904.21
Iteration:    140, Loss function: 8.616, Average Loss: 1.690, avg. samples / sec: 23881.61
Iteration:    140, Loss function: 8.222, Average Loss: 1.685, avg. samples / sec: 23883.09
Iteration:    140, Loss function: 8.599, Average Loss: 1.683, avg. samples / sec: 23865.39
Iteration:    140, Loss function: 8.878, Average Loss: 1.689, avg. samples / sec: 23830.08
Iteration:    140, Loss function: 8.870, Average Loss: 1.691, avg. samples / sec: 23889.95

:::MLPv0.5.0 ssd 1541757417.601890087 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 141, "value": 0.025066666666666654}

:::MLPv0.5.0 ssd 1541757417.688218117 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 142, "value": 0.02524444444444443}

:::MLPv0.5.0 ssd 1541757417.773786068 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 143, "value": 0.025422222222222207}

:::MLPv0.5.0 ssd 1541757417.858252764 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 144, "value": 0.025599999999999984}

:::MLPv0.5.0 ssd 1541757417.945016861 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 145, "value": 0.02577777777777779}

:::MLPv0.5.0 ssd 1541757418.029444695 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 146, "value": 0.025955555555555565}

:::MLPv0.5.0 ssd 1541757418.113588333 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 147, "value": 0.026133333333333342}

:::MLPv0.5.0 ssd 1541757418.198233604 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 148, "value": 0.02631111111111112}

:::MLPv0.5.0 ssd 1541757418.284137249 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 149, "value": 0.026488888888888895}

:::MLPv0.5.0 ssd 1541757418.367742300 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 150, "value": 0.026666666666666672}

:::MLPv0.5.0 ssd 1541757418.452141523 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 151, "value": 0.02684444444444445}

:::MLPv0.5.0 ssd 1541757418.537666321 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 152, "value": 0.027022222222222225}

:::MLPv0.5.0 ssd 1541757418.628049612 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 153, "value": 0.027200000000000002}

:::MLPv0.5.0 ssd 1541757418.713535309 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 154, "value": 0.02737777777777778}

:::MLPv0.5.0 ssd 1541757418.796899557 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 155, "value": 0.027555555555555555}

:::MLPv0.5.0 ssd 1541757418.881728172 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 156, "value": 0.027733333333333332}

:::MLPv0.5.0 ssd 1541757418.966975689 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 157, "value": 0.02791111111111111}

:::MLPv0.5.0 ssd 1541757419.051972628 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 158, "value": 0.028088888888888885}

:::MLPv0.5.0 ssd 1541757419.137710810 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 159, "value": 0.028266666666666662}

:::MLPv0.5.0 ssd 1541757419.222082615 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 160, "value": 0.02844444444444444}
Iteration:    160, Loss function: 8.265, Average Loss: 1.823, avg. samples / sec: 24023.66
Iteration:    160, Loss function: 8.221, Average Loss: 1.819, avg. samples / sec: 24017.95
Iteration:    160, Loss function: 8.056, Average Loss: 1.822, avg. samples / sec: 24044.48
Iteration:    160, Loss function: 8.474, Average Loss: 1.819, avg. samples / sec: 24025.64
Iteration:    160, Loss function: 8.262, Average Loss: 1.823, avg. samples / sec: 24001.20
Iteration:    160, Loss function: 7.972, Average Loss: 1.816, avg. samples / sec: 24029.32
Iteration:    160, Loss function: 8.035, Average Loss: 1.827, avg. samples / sec: 23987.00
Iteration:    160, Loss function: 7.895, Average Loss: 1.822, avg. samples / sec: 24012.07

:::MLPv0.5.0 ssd 1541757419.305827379 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 161, "value": 0.028622222222222216}

:::MLPv0.5.0 ssd 1541757419.389605284 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 162, "value": 0.028799999999999992}

:::MLPv0.5.0 ssd 1541757419.476310253 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 163, "value": 0.02897777777777777}

:::MLPv0.5.0 ssd 1541757419.560924530 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 164, "value": 0.029155555555555546}

:::MLPv0.5.0 ssd 1541757419.645351648 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 165, "value": 0.029333333333333322}

:::MLPv0.5.0 ssd 1541757419.729145050 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 166, "value": 0.0295111111111111}

:::MLPv0.5.0 ssd 1541757419.813593149 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 167, "value": 0.029688888888888876}

:::MLPv0.5.0 ssd 1541757419.897322893 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 168, "value": 0.029866666666666652}

:::MLPv0.5.0 ssd 1541757419.982632399 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 169, "value": 0.03004444444444443}

:::MLPv0.5.0 ssd 1541757420.066716433 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 170, "value": 0.030222222222222206}

:::MLPv0.5.0 ssd 1541757420.151432037 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 171, "value": 0.03040000000000001}

:::MLPv0.5.0 ssd 1541757420.237252951 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 172, "value": 0.030577777777777787}

:::MLPv0.5.0 ssd 1541757420.321413040 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 173, "value": 0.030755555555555564}

:::MLPv0.5.0 ssd 1541757420.403809786 (train.py:553) train_epoch: 3

:::MLPv0.5.0 ssd 1541757420.407966614 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 174, "value": 0.03093333333333334}

:::MLPv0.5.0 ssd 1541757420.493115187 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 175, "value": 0.031111111111111117}

:::MLPv0.5.0 ssd 1541757420.577383995 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 176, "value": 0.031288888888888894}

:::MLPv0.5.0 ssd 1541757420.662659883 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 177, "value": 0.03146666666666667}

:::MLPv0.5.0 ssd 1541757420.747334957 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 178, "value": 0.03164444444444445}

:::MLPv0.5.0 ssd 1541757420.830772161 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 179, "value": 0.031822222222222224}

:::MLPv0.5.0 ssd 1541757420.917147875 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 180, "value": 0.032}
Iteration:    180, Loss function: 7.969, Average Loss: 1.950, avg. samples / sec: 24162.32
Iteration:    180, Loss function: 8.325, Average Loss: 1.947, avg. samples / sec: 24160.77
Iteration:    180, Loss function: 8.061, Average Loss: 1.944, avg. samples / sec: 24191.35
Iteration:    180, Loss function: 8.544, Average Loss: 1.953, avg. samples / sec: 24189.69
Iteration:    180, Loss function: 8.500, Average Loss: 1.946, avg. samples / sec: 24176.67
Iteration:    180, Loss function: 8.246, Average Loss: 1.952, avg. samples / sec: 24163.24
Iteration:    180, Loss function: 8.382, Average Loss: 1.948, avg. samples / sec: 24145.93
Iteration:    180, Loss function: 8.127, Average Loss: 1.952, avg. samples / sec: 24168.30

:::MLPv0.5.0 ssd 1541757421.000542879 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 181, "value": 0.03217777777777778}

:::MLPv0.5.0 ssd 1541757421.084516287 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 182, "value": 0.032355555555555554}

:::MLPv0.5.0 ssd 1541757421.169956684 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 183, "value": 0.03253333333333333}

:::MLPv0.5.0 ssd 1541757421.255097389 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 184, "value": 0.03271111111111111}

:::MLPv0.5.0 ssd 1541757421.338756800 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 185, "value": 0.032888888888888884}

:::MLPv0.5.0 ssd 1541757421.423607111 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 186, "value": 0.03306666666666666}

:::MLPv0.5.0 ssd 1541757421.507364035 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 187, "value": 0.03324444444444444}

:::MLPv0.5.0 ssd 1541757421.592183113 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 188, "value": 0.033422222222222214}

:::MLPv0.5.0 ssd 1541757421.676130772 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 189, "value": 0.03359999999999999}

:::MLPv0.5.0 ssd 1541757421.760687828 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 190, "value": 0.03377777777777777}

:::MLPv0.5.0 ssd 1541757421.844596624 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 191, "value": 0.033955555555555544}

:::MLPv0.5.0 ssd 1541757421.928637505 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 192, "value": 0.03413333333333332}

:::MLPv0.5.0 ssd 1541757422.012732506 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 193, "value": 0.0343111111111111}

:::MLPv0.5.0 ssd 1541757422.096943855 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 194, "value": 0.034488888888888874}

:::MLPv0.5.0 ssd 1541757422.180924892 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 195, "value": 0.03466666666666665}

:::MLPv0.5.0 ssd 1541757422.264962912 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 196, "value": 0.03484444444444443}

:::MLPv0.5.0 ssd 1541757422.353507042 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 197, "value": 0.03502222222222222}

:::MLPv0.5.0 ssd 1541757422.437961817 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 198, "value": 0.035199999999999995}

:::MLPv0.5.0 ssd 1541757422.521821260 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 199, "value": 0.03537777777777777}

:::MLPv0.5.0 ssd 1541757422.605575323 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 200, "value": 0.03555555555555555}
Iteration:    200, Loss function: 8.083, Average Loss: 2.073, avg. samples / sec: 24316.66
Iteration:    200, Loss function: 8.030, Average Loss: 2.074, avg. samples / sec: 24258.44
Iteration:    200, Loss function: 7.484, Average Loss: 2.071, avg. samples / sec: 24261.46
Iteration:    200, Loss function: 8.074, Average Loss: 2.072, avg. samples / sec: 24289.65
Iteration:    200, Loss function: 7.739, Average Loss: 2.075, avg. samples / sec: 24259.34
Iteration:    200, Loss function: 7.619, Average Loss: 2.067, avg. samples / sec: 24237.36
Iteration:    200, Loss function: 8.136, Average Loss: 2.069, avg. samples / sec: 24234.82
Iteration:    200, Loss function: 8.473, Average Loss: 2.077, avg. samples / sec: 24248.81

:::MLPv0.5.0 ssd 1541757422.689465046 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 201, "value": 0.035733333333333325}

:::MLPv0.5.0 ssd 1541757422.774525166 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 202, "value": 0.0359111111111111}

:::MLPv0.5.0 ssd 1541757422.858656168 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 203, "value": 0.03608888888888889}

:::MLPv0.5.0 ssd 1541757422.942563295 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 204, "value": 0.03626666666666667}

:::MLPv0.5.0 ssd 1541757423.026769638 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 205, "value": 0.036444444444444446}

:::MLPv0.5.0 ssd 1541757423.110898972 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 206, "value": 0.03662222222222222}

:::MLPv0.5.0 ssd 1541757423.194699526 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 207, "value": 0.0368}

:::MLPv0.5.0 ssd 1541757423.287106991 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 208, "value": 0.036977777777777776}

:::MLPv0.5.0 ssd 1541757423.371314764 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 209, "value": 0.03715555555555555}

:::MLPv0.5.0 ssd 1541757423.454800129 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 210, "value": 0.03733333333333333}

:::MLPv0.5.0 ssd 1541757423.538902521 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 211, "value": 0.037511111111111106}

:::MLPv0.5.0 ssd 1541757423.623435736 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 212, "value": 0.03768888888888888}

:::MLPv0.5.0 ssd 1541757423.707350016 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 213, "value": 0.03786666666666666}

:::MLPv0.5.0 ssd 1541757423.791462421 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 214, "value": 0.038044444444444436}

:::MLPv0.5.0 ssd 1541757423.874994755 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 215, "value": 0.03822222222222221}

:::MLPv0.5.0 ssd 1541757423.958887577 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 216, "value": 0.038400000000000004}

:::MLPv0.5.0 ssd 1541757424.042679310 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 217, "value": 0.03857777777777778}

:::MLPv0.5.0 ssd 1541757424.127592087 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 218, "value": 0.03875555555555556}

:::MLPv0.5.0 ssd 1541757424.212053776 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 219, "value": 0.038933333333333334}

:::MLPv0.5.0 ssd 1541757424.296255350 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 220, "value": 0.03911111111111111}
Iteration:    220, Loss function: 7.370, Average Loss: 2.184, avg. samples / sec: 24237.13
Iteration:    220, Loss function: 7.765, Average Loss: 2.184, avg. samples / sec: 24230.15
Iteration:    220, Loss function: 7.452, Average Loss: 2.181, avg. samples / sec: 24242.69
Iteration:    220, Loss function: 7.672, Average Loss: 2.182, avg. samples / sec: 24246.72
Iteration:    220, Loss function: 7.780, Average Loss: 2.185, avg. samples / sec: 24218.50
Iteration:    220, Loss function: 7.401, Average Loss: 2.185, avg. samples / sec: 24214.31
Iteration:    220, Loss function: 7.236, Average Loss: 2.188, avg. samples / sec: 24220.18
Iteration:    220, Loss function: 7.819, Average Loss: 2.184, avg. samples / sec: 24178.14

:::MLPv0.5.0 ssd 1541757424.380072594 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 221, "value": 0.03928888888888889}

:::MLPv0.5.0 ssd 1541757424.464256763 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 222, "value": 0.039466666666666664}

:::MLPv0.5.0 ssd 1541757424.548155069 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 223, "value": 0.03964444444444444}

:::MLPv0.5.0 ssd 1541757424.632817745 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 224, "value": 0.03982222222222222}

:::MLPv0.5.0 ssd 1541757424.718950987 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 225, "value": 0.039999999999999994}

:::MLPv0.5.0 ssd 1541757424.804280281 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 226, "value": 0.04017777777777777}

:::MLPv0.5.0 ssd 1541757424.888414383 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 227, "value": 0.04035555555555555}

:::MLPv0.5.0 ssd 1541757424.973070145 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 228, "value": 0.04053333333333334}

:::MLPv0.5.0 ssd 1541757425.063305378 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 229, "value": 0.040711111111111115}

:::MLPv0.5.0 ssd 1541757425.148131847 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 230, "value": 0.04088888888888889}

:::MLPv0.5.0 ssd 1541757425.232672930 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 231, "value": 0.04106666666666667}

:::MLPv0.5.0 ssd 1541757425.313843012 (train.py:553) train_epoch: 4

:::MLPv0.5.0 ssd 1541757425.318009853 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 232, "value": 0.041244444444444445}

:::MLPv0.5.0 ssd 1541757425.401976585 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 233, "value": 0.04142222222222222}

:::MLPv0.5.0 ssd 1541757425.485855818 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 234, "value": 0.0416}

:::MLPv0.5.0 ssd 1541757425.571826935 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 235, "value": 0.041777777777777775}

:::MLPv0.5.0 ssd 1541757425.656031132 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 236, "value": 0.04195555555555555}

:::MLPv0.5.0 ssd 1541757425.741738081 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 237, "value": 0.04213333333333333}

:::MLPv0.5.0 ssd 1541757425.826087952 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 238, "value": 0.042311111111111105}

:::MLPv0.5.0 ssd 1541757425.910949945 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 239, "value": 0.04248888888888888}

:::MLPv0.5.0 ssd 1541757425.995191097 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 240, "value": 0.04266666666666666}
Iteration:    240, Loss function: 7.571, Average Loss: 2.291, avg. samples / sec: 24114.28
Iteration:    240, Loss function: 7.591, Average Loss: 2.286, avg. samples / sec: 24113.74
Iteration:    240, Loss function: 7.361, Average Loss: 2.294, avg. samples / sec: 24141.46
Iteration:    240, Loss function: 7.918, Average Loss: 2.291, avg. samples / sec: 24147.33
Iteration:    240, Loss function: 7.749, Average Loss: 2.294, avg. samples / sec: 24103.48
Iteration:    240, Loss function: 7.823, Average Loss: 2.290, avg. samples / sec: 24087.86
Iteration:    240, Loss function: 7.709, Average Loss: 2.291, avg. samples / sec: 24064.82
Iteration:    240, Loss function: 7.638, Average Loss: 2.286, avg. samples / sec: 24068.47

:::MLPv0.5.0 ssd 1541757426.081019402 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 241, "value": 0.04284444444444445}

:::MLPv0.5.0 ssd 1541757426.165727377 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 242, "value": 0.043022222222222226}

:::MLPv0.5.0 ssd 1541757426.249872446 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 243, "value": 0.0432}

:::MLPv0.5.0 ssd 1541757426.333923340 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 244, "value": 0.04337777777777778}

:::MLPv0.5.0 ssd 1541757426.418844223 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 245, "value": 0.043555555555555556}

:::MLPv0.5.0 ssd 1541757426.503096581 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 246, "value": 0.04373333333333333}

:::MLPv0.5.0 ssd 1541757426.588476181 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 247, "value": 0.04391111111111111}

:::MLPv0.5.0 ssd 1541757426.672361135 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 248, "value": 0.044088888888888886}

:::MLPv0.5.0 ssd 1541757426.756183147 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 249, "value": 0.04426666666666666}

:::MLPv0.5.0 ssd 1541757426.841016531 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 250, "value": 0.04444444444444444}

:::MLPv0.5.0 ssd 1541757426.925692081 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 251, "value": 0.044622222222222216}

:::MLPv0.5.0 ssd 1541757427.010112524 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 252, "value": 0.04479999999999999}

:::MLPv0.5.0 ssd 1541757427.097195148 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 253, "value": 0.04497777777777777}

:::MLPv0.5.0 ssd 1541757427.181561947 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 254, "value": 0.04515555555555556}

:::MLPv0.5.0 ssd 1541757427.266014576 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 255, "value": 0.04533333333333334}

:::MLPv0.5.0 ssd 1541757427.348982334 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 256, "value": 0.04551111111111111}

:::MLPv0.5.0 ssd 1541757427.432777405 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 257, "value": 0.04568888888888889}

:::MLPv0.5.0 ssd 1541757427.516804695 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 258, "value": 0.04586666666666667}

:::MLPv0.5.0 ssd 1541757427.600848198 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 259, "value": 0.04604444444444444}

:::MLPv0.5.0 ssd 1541757427.685304880 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 260, "value": 0.04622222222222222}
Iteration:    260, Loss function: 7.081, Average Loss: 2.395, avg. samples / sec: 24273.46
Iteration:    260, Loss function: 7.343, Average Loss: 2.394, avg. samples / sec: 24232.14
Iteration:    260, Loss function: 7.475, Average Loss: 2.395, avg. samples / sec: 24248.41
Iteration:    260, Loss function: 7.398, Average Loss: 2.390, avg. samples / sec: 24237.83
Iteration:    260, Loss function: 7.517, Average Loss: 2.395, avg. samples / sec: 24256.20
Iteration:    260, Loss function: 7.625, Average Loss: 2.391, avg. samples / sec: 24270.89
Iteration:    260, Loss function: 7.241, Average Loss: 2.394, avg. samples / sec: 24222.88
Iteration:    260, Loss function: 6.492, Average Loss: 2.395, avg. samples / sec: 24207.44

:::MLPv0.5.0 ssd 1541757427.769870281 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 261, "value": 0.0464}

:::MLPv0.5.0 ssd 1541757427.854175568 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 262, "value": 0.046577777777777774}

:::MLPv0.5.0 ssd 1541757427.938296795 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 263, "value": 0.04675555555555555}

:::MLPv0.5.0 ssd 1541757428.022675753 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 264, "value": 0.04693333333333333}

:::MLPv0.5.0 ssd 1541757428.107395649 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 265, "value": 0.047111111111111104}

:::MLPv0.5.0 ssd 1541757428.191201448 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 266, "value": 0.04728888888888888}

:::MLPv0.5.0 ssd 1541757428.275789022 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 267, "value": 0.04746666666666667}

:::MLPv0.5.0 ssd 1541757428.360079288 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 268, "value": 0.04764444444444445}

:::MLPv0.5.0 ssd 1541757428.444284439 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 269, "value": 0.047822222222222224}

:::MLPv0.5.0 ssd 1541757428.528198957 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 270, "value": 0.048}

:::MLPv0.5.0 ssd 1541757428.612149715 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 271, "value": 0.04817777777777778}

:::MLPv0.5.0 ssd 1541757428.698298216 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 272, "value": 0.048355555555555554}

:::MLPv0.5.0 ssd 1541757428.783073187 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 273, "value": 0.04853333333333333}

:::MLPv0.5.0 ssd 1541757428.867033482 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 274, "value": 0.04871111111111111}

:::MLPv0.5.0 ssd 1541757428.951484680 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 275, "value": 0.048888888888888885}

:::MLPv0.5.0 ssd 1541757429.035600185 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 276, "value": 0.04906666666666666}

:::MLPv0.5.0 ssd 1541757429.119823217 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 277, "value": 0.04924444444444444}

:::MLPv0.5.0 ssd 1541757429.203752756 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 278, "value": 0.049422222222222215}

:::MLPv0.5.0 ssd 1541757429.287842989 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 279, "value": 0.04959999999999999}

:::MLPv0.5.0 ssd 1541757429.371872187 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 280, "value": 0.04977777777777778}
Iteration:    280, Loss function: 7.232, Average Loss: 2.491, avg. samples / sec: 24282.94
Iteration:    280, Loss function: 6.850, Average Loss: 2.490, avg. samples / sec: 24296.65
Iteration:    280, Loss function: 6.971, Average Loss: 2.487, avg. samples / sec: 24289.44
Iteration:    280, Loss function: 7.084, Average Loss: 2.490, avg. samples / sec: 24259.67
Iteration:    280, Loss function: 6.851, Average Loss: 2.489, avg. samples / sec: 24325.09
Iteration:    280, Loss function: 7.352, Average Loss: 2.490, avg. samples / sec: 24261.94
Iteration:    280, Loss function: 7.384, Average Loss: 2.487, avg. samples / sec: 24255.48
Iteration:    280, Loss function: 6.930, Average Loss: 2.490, avg. samples / sec: 24240.57

:::MLPv0.5.0 ssd 1541757429.455658197 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 281, "value": 0.04995555555555556}

:::MLPv0.5.0 ssd 1541757429.539829254 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 282, "value": 0.050133333333333335}

:::MLPv0.5.0 ssd 1541757429.623518467 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 283, "value": 0.05031111111111111}

:::MLPv0.5.0 ssd 1541757429.707108498 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 284, "value": 0.05048888888888889}

:::MLPv0.5.0 ssd 1541757429.792105675 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 285, "value": 0.050666666666666665}

:::MLPv0.5.0 ssd 1541757429.876344681 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 286, "value": 0.05084444444444444}

:::MLPv0.5.0 ssd 1541757429.960150957 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 287, "value": 0.05102222222222222}

:::MLPv0.5.0 ssd 1541757430.044029474 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 288, "value": 0.051199999999999996}

:::MLPv0.5.0 ssd 1541757430.131940603 (train.py:553) train_epoch: 5

:::MLPv0.5.0 ssd 1541757430.136053801 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 289, "value": 0.05137777777777777}

:::MLPv0.5.0 ssd 1541757430.222371340 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 290, "value": 0.05155555555555555}

:::MLPv0.5.0 ssd 1541757430.306407213 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 291, "value": 0.051733333333333326}

:::MLPv0.5.0 ssd 1541757430.391018391 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 292, "value": 0.0519111111111111}

:::MLPv0.5.0 ssd 1541757430.474858046 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 293, "value": 0.05208888888888889}

:::MLPv0.5.0 ssd 1541757430.559247732 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 294, "value": 0.05226666666666667}

:::MLPv0.5.0 ssd 1541757430.642917871 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 295, "value": 0.052444444444444446}

:::MLPv0.5.0 ssd 1541757430.726856709 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 296, "value": 0.05262222222222222}

:::MLPv0.5.0 ssd 1541757430.810878277 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 297, "value": 0.0528}

:::MLPv0.5.0 ssd 1541757430.895272017 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 298, "value": 0.052977777777777776}

:::MLPv0.5.0 ssd 1541757430.979169846 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 299, "value": 0.05315555555555555}

:::MLPv0.5.0 ssd 1541757431.063016891 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 300, "value": 0.05333333333333333}
Iteration:    300, Loss function: 7.349, Average Loss: 2.577, avg. samples / sec: 24259.41
Iteration:    300, Loss function: 6.171, Average Loss: 2.581, avg. samples / sec: 24223.91
Iteration:    300, Loss function: 6.901, Average Loss: 2.579, avg. samples / sec: 24227.62
Iteration:    300, Loss function: 6.516, Average Loss: 2.576, avg. samples / sec: 24208.36
Iteration:    300, Loss function: 7.001, Average Loss: 2.579, avg. samples / sec: 24199.42
Iteration:    300, Loss function: 7.188, Average Loss: 2.581, avg. samples / sec: 24232.17
Iteration:    300, Loss function: 7.353, Average Loss: 2.578, avg. samples / sec: 24205.97
Iteration:    300, Loss function: 6.593, Average Loss: 2.578, avg. samples / sec: 24199.09

:::MLPv0.5.0 ssd 1541757431.146142960 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 301, "value": 0.053511111111111107}

:::MLPv0.5.0 ssd 1541757431.229903460 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 302, "value": 0.05368888888888888}

:::MLPv0.5.0 ssd 1541757431.314480066 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 303, "value": 0.05386666666666666}

:::MLPv0.5.0 ssd 1541757431.398792744 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 304, "value": 0.05404444444444444}

:::MLPv0.5.0 ssd 1541757431.483111620 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 305, "value": 0.05422222222222223}

:::MLPv0.5.0 ssd 1541757431.567178726 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 306, "value": 0.054400000000000004}

:::MLPv0.5.0 ssd 1541757431.650593519 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 307, "value": 0.05457777777777778}

:::MLPv0.5.0 ssd 1541757431.734021902 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 308, "value": 0.05475555555555556}

:::MLPv0.5.0 ssd 1541757431.818013191 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 309, "value": 0.054933333333333334}

:::MLPv0.5.0 ssd 1541757431.901762962 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 310, "value": 0.05511111111111111}

:::MLPv0.5.0 ssd 1541757431.986077785 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 311, "value": 0.05528888888888889}

:::MLPv0.5.0 ssd 1541757432.070910692 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 312, "value": 0.055466666666666664}

:::MLPv0.5.0 ssd 1541757432.154863596 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 313, "value": 0.05564444444444444}

:::MLPv0.5.0 ssd 1541757432.238770485 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 314, "value": 0.05582222222222222}

:::MLPv0.5.0 ssd 1541757432.322981119 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 315, "value": 0.055999999999999994}

:::MLPv0.5.0 ssd 1541757432.407237291 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 316, "value": 0.05617777777777777}

:::MLPv0.5.0 ssd 1541757432.490875959 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 317, "value": 0.05635555555555555}

:::MLPv0.5.0 ssd 1541757432.575280428 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 318, "value": 0.05653333333333334}

:::MLPv0.5.0 ssd 1541757432.659756184 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 319, "value": 0.056711111111111115}

:::MLPv0.5.0 ssd 1541757432.743038177 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 320, "value": 0.05688888888888889}
Iteration:    320, Loss function: 7.801, Average Loss: 2.671, avg. samples / sec: 24379.96
Iteration:    320, Loss function: 7.768, Average Loss: 2.675, avg. samples / sec: 24401.16
Iteration:    320, Loss function: 7.557, Average Loss: 2.678, avg. samples / sec: 24379.91
Iteration:    320, Loss function: 8.249, Average Loss: 2.673, avg. samples / sec: 24407.76
Iteration:    320, Loss function: 7.552, Average Loss: 2.673, avg. samples / sec: 24406.67
Iteration:    320, Loss function: 7.227, Average Loss: 2.673, avg. samples / sec: 24387.82
Iteration:    320, Loss function: 7.357, Average Loss: 2.672, avg. samples / sec: 24400.72
Iteration:    320, Loss function: 7.712, Average Loss: 2.676, avg. samples / sec: 24351.68

:::MLPv0.5.0 ssd 1541757432.827552319 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 321, "value": 0.05706666666666667}

:::MLPv0.5.0 ssd 1541757432.913448334 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 322, "value": 0.057244444444444445}

:::MLPv0.5.0 ssd 1541757432.997706413 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 323, "value": 0.05742222222222222}

:::MLPv0.5.0 ssd 1541757433.084237814 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 324, "value": 0.0576}

:::MLPv0.5.0 ssd 1541757433.168643236 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 325, "value": 0.057777777777777775}

:::MLPv0.5.0 ssd 1541757433.252396822 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 326, "value": 0.05795555555555555}

:::MLPv0.5.0 ssd 1541757433.336738586 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 327, "value": 0.05813333333333333}

:::MLPv0.5.0 ssd 1541757433.420506477 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 328, "value": 0.058311111111111105}

:::MLPv0.5.0 ssd 1541757433.504146099 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 329, "value": 0.05848888888888888}

:::MLPv0.5.0 ssd 1541757433.587899923 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 330, "value": 0.05866666666666666}

:::MLPv0.5.0 ssd 1541757433.672414064 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 331, "value": 0.05884444444444445}

:::MLPv0.5.0 ssd 1541757433.756319046 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 332, "value": 0.059022222222222226}

:::MLPv0.5.0 ssd 1541757433.840703487 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 333, "value": 0.0592}

:::MLPv0.5.0 ssd 1541757433.924647093 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 334, "value": 0.05937777777777778}

:::MLPv0.5.0 ssd 1541757434.008850336 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 335, "value": 0.059555555555555556}

:::MLPv0.5.0 ssd 1541757434.092616558 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 336, "value": 0.05973333333333333}

:::MLPv0.5.0 ssd 1541757434.177263021 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 337, "value": 0.05991111111111111}

:::MLPv0.5.0 ssd 1541757434.261219740 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 338, "value": 0.060088888888888886}

:::MLPv0.5.0 ssd 1541757434.345120907 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 339, "value": 0.06026666666666666}

:::MLPv0.5.0 ssd 1541757434.427993536 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 340, "value": 0.06044444444444444}
Iteration:    340, Loss function: 6.372, Average Loss: 2.762, avg. samples / sec: 24335.34
Iteration:    340, Loss function: 6.545, Average Loss: 2.769, avg. samples / sec: 24314.63
Iteration:    340, Loss function: 7.290, Average Loss: 2.764, avg. samples / sec: 24322.73
Iteration:    340, Loss function: 6.997, Average Loss: 2.765, avg. samples / sec: 24298.85
Iteration:    340, Loss function: 6.842, Average Loss: 2.763, avg. samples / sec: 24316.38
Iteration:    340, Loss function: 7.280, Average Loss: 2.761, avg. samples / sec: 24282.25
Iteration:    340, Loss function: 7.198, Average Loss: 2.764, avg. samples / sec: 24340.28
Iteration:    340, Loss function: 6.436, Average Loss: 2.763, avg. samples / sec: 24276.38

:::MLPv0.5.0 ssd 1541757434.512557507 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 341, "value": 0.060622222222222216}

:::MLPv0.5.0 ssd 1541757434.595934868 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 342, "value": 0.06079999999999999}

:::MLPv0.5.0 ssd 1541757434.680279255 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 343, "value": 0.06097777777777777}

:::MLPv0.5.0 ssd 1541757434.763616323 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 344, "value": 0.06115555555555556}

:::MLPv0.5.0 ssd 1541757434.847831011 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 345, "value": 0.06133333333333334}

:::MLPv0.5.0 ssd 1541757434.931471825 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 346, "value": 0.061511111111111114}

:::MLPv0.5.0 ssd 1541757435.012256861 (train.py:553) train_epoch: 6

:::MLPv0.5.0 ssd 1541757435.016378164 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 347, "value": 0.06168888888888889}

:::MLPv0.5.0 ssd 1541757435.099819422 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 348, "value": 0.06186666666666667}

:::MLPv0.5.0 ssd 1541757435.184473038 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 349, "value": 0.062044444444444444}

:::MLPv0.5.0 ssd 1541757435.267871380 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 350, "value": 0.06222222222222222}

:::MLPv0.5.0 ssd 1541757435.351835966 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 351, "value": 0.0624}

:::MLPv0.5.0 ssd 1541757435.435150385 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 352, "value": 0.06257777777777777}

:::MLPv0.5.0 ssd 1541757435.518810034 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 353, "value": 0.06275555555555555}

:::MLPv0.5.0 ssd 1541757435.602741241 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 354, "value": 0.06293333333333333}

:::MLPv0.5.0 ssd 1541757435.686644077 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 355, "value": 0.0631111111111111}

:::MLPv0.5.0 ssd 1541757435.770823479 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 356, "value": 0.0632888888888889}

:::MLPv0.5.0 ssd 1541757435.854899883 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 357, "value": 0.06346666666666667}

:::MLPv0.5.0 ssd 1541757435.938347101 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 358, "value": 0.06364444444444445}

:::MLPv0.5.0 ssd 1541757436.022360325 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 359, "value": 0.06382222222222222}

:::MLPv0.5.0 ssd 1541757436.106732368 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 360, "value": 0.064}
Iteration:    360, Loss function: 6.876, Average Loss: 2.842, avg. samples / sec: 24406.08
Iteration:    360, Loss function: 6.547, Average Loss: 2.843, avg. samples / sec: 24436.73
Iteration:    360, Loss function: 6.756, Average Loss: 2.839, avg. samples / sec: 24426.92
Iteration:    360, Loss function: 6.643, Average Loss: 2.847, avg. samples / sec: 24383.52
Iteration:    360, Loss function: 6.368, Average Loss: 2.842, avg. samples / sec: 24411.26
Iteration:    360, Loss function: 6.783, Average Loss: 2.843, avg. samples / sec: 24381.57
Iteration:    360, Loss function: 6.838, Average Loss: 2.845, avg. samples / sec: 24370.94
Iteration:    360, Loss function: 6.842, Average Loss: 2.839, avg. samples / sec: 24369.09

:::MLPv0.5.0 ssd 1541757436.190963268 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 361, "value": 0.06417777777777778}

:::MLPv0.5.0 ssd 1541757436.275107384 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 362, "value": 0.06435555555555555}

:::MLPv0.5.0 ssd 1541757436.358757973 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 363, "value": 0.06453333333333333}

:::MLPv0.5.0 ssd 1541757436.443172455 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 364, "value": 0.06471111111111111}

:::MLPv0.5.0 ssd 1541757436.527045727 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 365, "value": 0.06488888888888888}

:::MLPv0.5.0 ssd 1541757436.610746384 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 366, "value": 0.06506666666666666}

:::MLPv0.5.0 ssd 1541757436.695859432 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 367, "value": 0.06524444444444444}

:::MLPv0.5.0 ssd 1541757436.779949427 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 368, "value": 0.06542222222222221}

:::MLPv0.5.0 ssd 1541757436.863813639 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 369, "value": 0.0656}

:::MLPv0.5.0 ssd 1541757436.947897911 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 370, "value": 0.06577777777777778}

:::MLPv0.5.0 ssd 1541757437.031217337 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 371, "value": 0.06595555555555556}

:::MLPv0.5.0 ssd 1541757437.114856005 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 372, "value": 0.06613333333333334}

:::MLPv0.5.0 ssd 1541757437.199205637 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 373, "value": 0.06631111111111111}

:::MLPv0.5.0 ssd 1541757437.285641909 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 374, "value": 0.06648888888888889}

:::MLPv0.5.0 ssd 1541757437.369770527 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 375, "value": 0.06666666666666667}

:::MLPv0.5.0 ssd 1541757437.453872204 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 376, "value": 0.06684444444444444}

:::MLPv0.5.0 ssd 1541757437.537674904 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 377, "value": 0.06702222222222222}

:::MLPv0.5.0 ssd 1541757437.621637583 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 378, "value": 0.0672}

:::MLPv0.5.0 ssd 1541757437.705487728 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 379, "value": 0.06737777777777777}

:::MLPv0.5.0 ssd 1541757437.789645910 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 380, "value": 0.06755555555555555}
Iteration:    380, Loss function: 6.655, Average Loss: 2.921, avg. samples / sec: 24350.35
Iteration:    380, Loss function: 6.530, Average Loss: 2.915, avg. samples / sec: 24351.96
Iteration:    380, Loss function: 6.257, Average Loss: 2.914, avg. samples / sec: 24357.75
Iteration:    380, Loss function: 6.554, Average Loss: 2.915, avg. samples / sec: 24297.08
Iteration:    380, Loss function: 6.162, Average Loss: 2.918, avg. samples / sec: 24334.51
Iteration:    380, Loss function: 6.828, Average Loss: 2.913, avg. samples / sec: 24336.08
Iteration:    380, Loss function: 6.901, Average Loss: 2.911, avg. samples / sec: 24283.80
Iteration:    380, Loss function: 6.532, Average Loss: 2.914, avg. samples / sec: 24272.73

:::MLPv0.5.0 ssd 1541757437.873999596 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 381, "value": 0.06773333333333333}

:::MLPv0.5.0 ssd 1541757437.958817244 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 382, "value": 0.06791111111111112}

:::MLPv0.5.0 ssd 1541757438.043384314 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 383, "value": 0.0680888888888889}

:::MLPv0.5.0 ssd 1541757438.127654552 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 384, "value": 0.06826666666666667}

:::MLPv0.5.0 ssd 1541757438.211664438 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 385, "value": 0.06844444444444445}

:::MLPv0.5.0 ssd 1541757438.295605421 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 386, "value": 0.06862222222222222}

:::MLPv0.5.0 ssd 1541757438.379376650 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 387, "value": 0.0688}

:::MLPv0.5.0 ssd 1541757438.463088989 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 388, "value": 0.06897777777777778}

:::MLPv0.5.0 ssd 1541757438.547054052 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 389, "value": 0.06915555555555555}

:::MLPv0.5.0 ssd 1541757438.631353855 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 390, "value": 0.06933333333333333}

:::MLPv0.5.0 ssd 1541757438.715436459 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 391, "value": 0.0695111111111111}

:::MLPv0.5.0 ssd 1541757438.800647497 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 392, "value": 0.06968888888888888}

:::MLPv0.5.0 ssd 1541757438.884178400 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 393, "value": 0.06986666666666666}

:::MLPv0.5.0 ssd 1541757438.968358517 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 394, "value": 0.07004444444444444}

:::MLPv0.5.0 ssd 1541757439.052431107 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 395, "value": 0.07022222222222223}

:::MLPv0.5.0 ssd 1541757439.136241436 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 396, "value": 0.0704}

:::MLPv0.5.0 ssd 1541757439.220215797 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 397, "value": 0.07057777777777778}

:::MLPv0.5.0 ssd 1541757439.304491282 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 398, "value": 0.07075555555555556}

:::MLPv0.5.0 ssd 1541757439.388459921 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 399, "value": 0.07093333333333333}

:::MLPv0.5.0 ssd 1541757439.472416162 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 400, "value": 0.07111111111111111}
Iteration:    400, Loss function: 6.235, Average Loss: 2.993, avg. samples / sec: 24348.73
Iteration:    400, Loss function: 6.550, Average Loss: 2.986, avg. samples / sec: 24356.19
Iteration:    400, Loss function: 6.698, Average Loss: 2.982, avg. samples / sec: 24390.55
Iteration:    400, Loss function: 6.333, Average Loss: 2.986, avg. samples / sec: 24352.38
Iteration:    400, Loss function: 6.126, Average Loss: 2.987, avg. samples / sec: 24371.54
Iteration:    400, Loss function: 6.483, Average Loss: 2.988, avg. samples / sec: 24347.81
Iteration:    400, Loss function: 6.480, Average Loss: 2.985, avg. samples / sec: 24359.33
Iteration:    400, Loss function: 6.762, Average Loss: 2.986, avg. samples / sec: 24349.05

:::MLPv0.5.0 ssd 1541757439.556759596 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 401, "value": 0.07128888888888889}

:::MLPv0.5.0 ssd 1541757439.640953064 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 402, "value": 0.07146666666666666}

:::MLPv0.5.0 ssd 1541757439.724941730 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 403, "value": 0.07164444444444444}

:::MLPv0.5.0 ssd 1541757439.808965921 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 404, "value": 0.07182222222222222}

:::MLPv0.5.0 ssd 1541757439.890359402 (train.py:553) train_epoch: 7

:::MLPv0.5.0 ssd 1541757439.894675970 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 405, "value": 0.072}

:::MLPv0.5.0 ssd 1541757439.978850365 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 406, "value": 0.07217777777777777}

:::MLPv0.5.0 ssd 1541757440.063004971 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 407, "value": 0.07235555555555555}

:::MLPv0.5.0 ssd 1541757440.147484779 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 408, "value": 0.07253333333333334}

:::MLPv0.5.0 ssd 1541757440.230812311 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 409, "value": 0.07271111111111112}

:::MLPv0.5.0 ssd 1541757440.313993454 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 410, "value": 0.07288888888888889}

:::MLPv0.5.0 ssd 1541757440.398168325 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 411, "value": 0.07306666666666667}

:::MLPv0.5.0 ssd 1541757440.481790543 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 412, "value": 0.07324444444444445}

:::MLPv0.5.0 ssd 1541757440.566278219 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 413, "value": 0.07342222222222222}

:::MLPv0.5.0 ssd 1541757440.650412083 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 414, "value": 0.0736}

:::MLPv0.5.0 ssd 1541757440.734771252 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 415, "value": 0.07377777777777778}

:::MLPv0.5.0 ssd 1541757440.818856955 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 416, "value": 0.07395555555555555}

:::MLPv0.5.0 ssd 1541757440.903094530 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 417, "value": 0.07413333333333333}

:::MLPv0.5.0 ssd 1541757440.986919165 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 418, "value": 0.0743111111111111}

:::MLPv0.5.0 ssd 1541757441.070736885 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 419, "value": 0.07448888888888888}

:::MLPv0.5.0 ssd 1541757441.154153824 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 420, "value": 0.07466666666666666}
Iteration:    420, Loss function: 5.933, Average Loss: 3.062, avg. samples / sec: 24358.93
Iteration:    420, Loss function: 6.346, Average Loss: 3.059, avg. samples / sec: 24392.32
Iteration:    420, Loss function: 6.362, Average Loss: 3.056, avg. samples / sec: 24352.59
Iteration:    420, Loss function: 6.281, Average Loss: 3.057, avg. samples / sec: 24361.76
Iteration:    420, Loss function: 6.328, Average Loss: 3.056, avg. samples / sec: 24402.71
Iteration:    420, Loss function: 6.286, Average Loss: 3.055, avg. samples / sec: 24373.15
Iteration:    420, Loss function: 6.206, Average Loss: 3.052, avg. samples / sec: 24312.15
Iteration:    420, Loss function: 6.649, Average Loss: 3.057, avg. samples / sec: 24317.28

:::MLPv0.5.0 ssd 1541757441.237911701 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 421, "value": 0.07484444444444445}

:::MLPv0.5.0 ssd 1541757441.321839333 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 422, "value": 0.07502222222222223}

:::MLPv0.5.0 ssd 1541757441.406161308 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 423, "value": 0.0752}

:::MLPv0.5.0 ssd 1541757441.490234852 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 424, "value": 0.07537777777777778}

:::MLPv0.5.0 ssd 1541757441.573817253 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 425, "value": 0.07555555555555556}

:::MLPv0.5.0 ssd 1541757441.657861948 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 426, "value": 0.07573333333333333}

:::MLPv0.5.0 ssd 1541757441.742973328 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 427, "value": 0.07591111111111111}

:::MLPv0.5.0 ssd 1541757441.826760054 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 428, "value": 0.07608888888888889}

:::MLPv0.5.0 ssd 1541757441.910945177 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 429, "value": 0.07626666666666666}

:::MLPv0.5.0 ssd 1541757441.994330168 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 430, "value": 0.07644444444444444}

:::MLPv0.5.0 ssd 1541757442.078186750 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 431, "value": 0.07662222222222222}

:::MLPv0.5.0 ssd 1541757442.162024975 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 432, "value": 0.0768}

:::MLPv0.5.0 ssd 1541757442.252870321 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 433, "value": 0.07697777777777778}

:::MLPv0.5.0 ssd 1541757442.336442947 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 434, "value": 0.07715555555555556}

:::MLPv0.5.0 ssd 1541757442.420291901 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 435, "value": 0.07733333333333334}

:::MLPv0.5.0 ssd 1541757442.504909992 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 436, "value": 0.07751111111111111}

:::MLPv0.5.0 ssd 1541757442.589013815 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 437, "value": 0.07768888888888889}

:::MLPv0.5.0 ssd 1541757442.672151566 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 438, "value": 0.07786666666666667}

:::MLPv0.5.0 ssd 1541757442.757033825 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 439, "value": 0.07804444444444444}

:::MLPv0.5.0 ssd 1541757442.840627670 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 440, "value": 0.07822222222222222}
Iteration:    440, Loss function: 6.886, Average Loss: 3.116, avg. samples / sec: 24346.00
Iteration:    440, Loss function: 6.161, Average Loss: 3.122, avg. samples / sec: 24345.61
Iteration:    440, Loss function: 6.075, Average Loss: 3.120, avg. samples / sec: 24292.40
Iteration:    440, Loss function: 6.219, Average Loss: 3.125, avg. samples / sec: 24281.64
Iteration:    440, Loss function: 5.974, Average Loss: 3.123, avg. samples / sec: 24253.45
Iteration:    440, Loss function: 6.455, Average Loss: 3.120, avg. samples / sec: 24265.93
Iteration:    440, Loss function: 6.372, Average Loss: 3.124, avg. samples / sec: 24241.78
Iteration:    440, Loss function: 5.788, Average Loss: 3.122, avg. samples / sec: 24249.49

:::MLPv0.5.0 ssd 1541757442.924172640 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 441, "value": 0.0784}

:::MLPv0.5.0 ssd 1541757443.008641720 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 442, "value": 0.07857777777777777}

:::MLPv0.5.0 ssd 1541757443.095034361 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 443, "value": 0.07875555555555555}

:::MLPv0.5.0 ssd 1541757443.180260420 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 444, "value": 0.07893333333333333}

:::MLPv0.5.0 ssd 1541757443.263896227 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 445, "value": 0.0791111111111111}

:::MLPv0.5.0 ssd 1541757443.347842455 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 446, "value": 0.0792888888888889}

:::MLPv0.5.0 ssd 1541757443.431885004 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 447, "value": 0.07946666666666667}

:::MLPv0.5.0 ssd 1541757443.516478062 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 448, "value": 0.07964444444444445}

:::MLPv0.5.0 ssd 1541757443.600418091 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 449, "value": 0.07982222222222222}

:::MLPv0.5.0 ssd 1541757443.684565306 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 450, "value": 0.08}

:::MLPv0.5.0 ssd 1541757443.768043756 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 451, "value": 0.08017777777777778}

:::MLPv0.5.0 ssd 1541757443.851835489 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 452, "value": 0.08035555555555556}

:::MLPv0.5.0 ssd 1541757443.938313484 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 453, "value": 0.08053333333333333}

:::MLPv0.5.0 ssd 1541757444.022068977 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 454, "value": 0.08071111111111111}

:::MLPv0.5.0 ssd 1541757444.105635881 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 455, "value": 0.08088888888888889}

:::MLPv0.5.0 ssd 1541757444.189789772 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 456, "value": 0.08106666666666666}

:::MLPv0.5.0 ssd 1541757444.274274111 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 457, "value": 0.08124444444444444}

:::MLPv0.5.0 ssd 1541757444.358556509 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 458, "value": 0.08142222222222222}

:::MLPv0.5.0 ssd 1541757444.442417860 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 459, "value": 0.0816}

:::MLPv0.5.0 ssd 1541757444.526133776 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 460, "value": 0.08177777777777778}
Iteration:    460, Loss function: 6.197, Average Loss: 3.187, avg. samples / sec: 24303.91
Iteration:    460, Loss function: 6.506, Average Loss: 3.192, avg. samples / sec: 24305.05
Iteration:    460, Loss function: 6.501, Average Loss: 3.187, avg. samples / sec: 24338.68
Iteration:    460, Loss function: 6.102, Average Loss: 3.190, avg. samples / sec: 24342.63
Iteration:    460, Loss function: 6.320, Average Loss: 3.185, avg. samples / sec: 24295.53
Iteration:    460, Loss function: 6.045, Average Loss: 3.180, avg. samples / sec: 24252.72
Iteration:    460, Loss function: 6.262, Average Loss: 3.184, avg. samples / sec: 24299.15
Iteration:    460, Loss function: 6.065, Average Loss: 3.184, avg. samples / sec: 24293.59

:::MLPv0.5.0 ssd 1541757444.609940529 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 461, "value": 0.08195555555555556}

:::MLPv0.5.0 ssd 1541757444.694087744 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 462, "value": 0.08213333333333334}

:::MLPv0.5.0 ssd 1541757444.775081873 (train.py:553) train_epoch: 8

:::MLPv0.5.0 ssd 1541757444.779222727 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 463, "value": 0.08231111111111111}

:::MLPv0.5.0 ssd 1541757444.863010406 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 464, "value": 0.08248888888888889}

:::MLPv0.5.0 ssd 1541757444.947932005 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 465, "value": 0.08266666666666667}

:::MLPv0.5.0 ssd 1541757445.032288551 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 466, "value": 0.08284444444444444}

:::MLPv0.5.0 ssd 1541757445.115664721 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 467, "value": 0.08302222222222222}

:::MLPv0.5.0 ssd 1541757445.199553490 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 468, "value": 0.0832}

:::MLPv0.5.0 ssd 1541757445.282990932 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 469, "value": 0.08337777777777777}

:::MLPv0.5.0 ssd 1541757445.366897345 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 470, "value": 0.08355555555555555}

:::MLPv0.5.0 ssd 1541757445.451118231 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 471, "value": 0.08373333333333333}

:::MLPv0.5.0 ssd 1541757445.535495758 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 472, "value": 0.08391111111111112}

:::MLPv0.5.0 ssd 1541757445.619231939 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 473, "value": 0.0840888888888889}

:::MLPv0.5.0 ssd 1541757445.703097105 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 474, "value": 0.08426666666666667}

:::MLPv0.5.0 ssd 1541757445.787681341 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 475, "value": 0.08444444444444445}

:::MLPv0.5.0 ssd 1541757445.871556282 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 476, "value": 0.08462222222222222}

:::MLPv0.5.0 ssd 1541757445.955385923 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 477, "value": 0.0848}

:::MLPv0.5.0 ssd 1541757446.039402485 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 478, "value": 0.08497777777777778}

:::MLPv0.5.0 ssd 1541757446.123328686 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 479, "value": 0.08515555555555555}

:::MLPv0.5.0 ssd 1541757446.206844330 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 480, "value": 0.08533333333333333}
Iteration:    480, Loss function: 6.360, Average Loss: 3.241, avg. samples / sec: 24432.31
Iteration:    480, Loss function: 5.975, Average Loss: 3.243, avg. samples / sec: 24378.20
Iteration:    480, Loss function: 5.996, Average Loss: 3.242, avg. samples / sec: 24370.29
Iteration:    480, Loss function: 6.522, Average Loss: 3.248, avg. samples / sec: 24370.95
Iteration:    480, Loss function: 6.065, Average Loss: 3.243, avg. samples / sec: 24368.34
Iteration:    480, Loss function: 6.405, Average Loss: 3.248, avg. samples / sec: 24337.95
Iteration:    480, Loss function: 6.165, Average Loss: 3.238, avg. samples / sec: 24377.71
Iteration:    480, Loss function: 6.262, Average Loss: 3.234, avg. samples / sec: 24364.64

:::MLPv0.5.0 ssd 1541757446.290904284 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 481, "value": 0.08551111111111111}

:::MLPv0.5.0 ssd 1541757446.375273705 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 482, "value": 0.08568888888888888}

:::MLPv0.5.0 ssd 1541757446.460059404 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 483, "value": 0.08586666666666666}

:::MLPv0.5.0 ssd 1541757446.544136047 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 484, "value": 0.08604444444444445}

:::MLPv0.5.0 ssd 1541757446.627985001 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 485, "value": 0.08622222222222223}

:::MLPv0.5.0 ssd 1541757446.711067200 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 486, "value": 0.0864}

:::MLPv0.5.0 ssd 1541757446.795320511 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 487, "value": 0.08657777777777778}

:::MLPv0.5.0 ssd 1541757446.879566669 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 488, "value": 0.08675555555555556}

:::MLPv0.5.0 ssd 1541757446.963187695 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 489, "value": 0.08693333333333333}

:::MLPv0.5.0 ssd 1541757447.047465324 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 490, "value": 0.08711111111111111}

:::MLPv0.5.0 ssd 1541757447.131195545 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 491, "value": 0.08728888888888889}

:::MLPv0.5.0 ssd 1541757447.214740276 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 492, "value": 0.08746666666666666}

:::MLPv0.5.0 ssd 1541757447.298584461 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 493, "value": 0.08764444444444444}

:::MLPv0.5.0 ssd 1541757447.383130074 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 494, "value": 0.08782222222222222}

:::MLPv0.5.0 ssd 1541757447.467307568 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 495, "value": 0.088}

:::MLPv0.5.0 ssd 1541757447.551745176 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 496, "value": 0.08817777777777777}

:::MLPv0.5.0 ssd 1541757447.635720253 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 497, "value": 0.08835555555555556}

:::MLPv0.5.0 ssd 1541757447.719065428 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 498, "value": 0.08853333333333334}

:::MLPv0.5.0 ssd 1541757447.803603649 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 499, "value": 0.08871111111111112}

:::MLPv0.5.0 ssd 1541757447.888018370 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 500, "value": 0.08888888888888889}
Iteration:    500, Loss function: 5.867, Average Loss: 3.312, avg. samples / sec: 24367.73
Iteration:    500, Loss function: 6.187, Average Loss: 3.313, avg. samples / sec: 24366.28
Iteration:    500, Loss function: 6.576, Average Loss: 3.309, avg. samples / sec: 24378.93
Iteration:    500, Loss function: 6.501, Average Loss: 3.319, avg. samples / sec: 24355.86
Iteration:    500, Loss function: 5.966, Average Loss: 3.308, avg. samples / sec: 24376.04
Iteration:    500, Loss function: 6.603, Average Loss: 3.304, avg. samples / sec: 24388.40
Iteration:    500, Loss function: 6.401, Average Loss: 3.311, avg. samples / sec: 24320.43
Iteration:    500, Loss function: 6.316, Average Loss: 3.318, avg. samples / sec: 24355.70

:::MLPv0.5.0 ssd 1541757447.971956730 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 501, "value": 0.08906666666666667}

:::MLPv0.5.0 ssd 1541757448.056101322 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 502, "value": 0.08924444444444445}

:::MLPv0.5.0 ssd 1541757448.139725208 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 503, "value": 0.08942222222222222}

:::MLPv0.5.0 ssd 1541757448.224133968 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 504, "value": 0.0896}

:::MLPv0.5.0 ssd 1541757448.308387280 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 505, "value": 0.08977777777777778}

:::MLPv0.5.0 ssd 1541757448.392364740 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 506, "value": 0.08995555555555555}

:::MLPv0.5.0 ssd 1541757448.477062702 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 507, "value": 0.09013333333333333}

:::MLPv0.5.0 ssd 1541757448.562076569 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 508, "value": 0.0903111111111111}

:::MLPv0.5.0 ssd 1541757448.646365643 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 509, "value": 0.09048888888888888}

:::MLPv0.5.0 ssd 1541757448.730124950 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 510, "value": 0.09066666666666667}

:::MLPv0.5.0 ssd 1541757448.814057589 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 511, "value": 0.09084444444444445}

:::MLPv0.5.0 ssd 1541757448.898170471 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 512, "value": 0.09102222222222223}

:::MLPv0.5.0 ssd 1541757448.982942581 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 513, "value": 0.0912}

:::MLPv0.5.0 ssd 1541757449.066708565 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 514, "value": 0.09137777777777778}

:::MLPv0.5.0 ssd 1541757449.150528669 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 515, "value": 0.09155555555555556}

:::MLPv0.5.0 ssd 1541757449.234253883 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 516, "value": 0.09173333333333333}

:::MLPv0.5.0 ssd 1541757449.318256855 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 517, "value": 0.09191111111111111}

:::MLPv0.5.0 ssd 1541757449.402263403 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 518, "value": 0.09208888888888889}

:::MLPv0.5.0 ssd 1541757449.486818075 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 519, "value": 0.09226666666666666}

:::MLPv0.5.0 ssd 1541757449.568220615 (train.py:553) train_epoch: 9

:::MLPv0.5.0 ssd 1541757449.572367907 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 520, "value": 0.09244444444444444}
Iteration:    520, Loss function: 5.418, Average Loss: 3.366, avg. samples / sec: 24317.62
Iteration:    520, Loss function: 5.984, Average Loss: 3.369, avg. samples / sec: 24371.10
Iteration:    520, Loss function: 5.376, Average Loss: 3.363, avg. samples / sec: 24318.10
Iteration:    520, Loss function: 5.387, Average Loss: 3.371, avg. samples / sec: 24329.36
Iteration:    520, Loss function: 5.399, Average Loss: 3.364, avg. samples / sec: 24306.65
Iteration:    520, Loss function: 5.463, Average Loss: 3.363, avg. samples / sec: 24332.02
Iteration:    520, Loss function: 5.465, Average Loss: 3.358, avg. samples / sec: 24313.18
Iteration:    520, Loss function: 6.215, Average Loss: 3.362, avg. samples / sec: 24293.17

:::MLPv0.5.0 ssd 1541757449.656004667 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 521, "value": 0.09262222222222222}

:::MLPv0.5.0 ssd 1541757449.739822626 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 522, "value": 0.0928}

:::MLPv0.5.0 ssd 1541757449.824215889 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 523, "value": 0.09297777777777778}

:::MLPv0.5.0 ssd 1541757449.908185244 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 524, "value": 0.09315555555555556}

:::MLPv0.5.0 ssd 1541757449.992171288 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 525, "value": 0.09333333333333334}

:::MLPv0.5.0 ssd 1541757450.076382875 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 526, "value": 0.09351111111111111}

:::MLPv0.5.0 ssd 1541757450.160265207 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 527, "value": 0.09368888888888889}

:::MLPv0.5.0 ssd 1541757450.244513988 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 528, "value": 0.09386666666666667}

:::MLPv0.5.0 ssd 1541757450.328222990 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 529, "value": 0.09404444444444444}

:::MLPv0.5.0 ssd 1541757450.411800623 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 530, "value": 0.09422222222222222}

:::MLPv0.5.0 ssd 1541757450.495417595 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 531, "value": 0.0944}

:::MLPv0.5.0 ssd 1541757450.579658031 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 532, "value": 0.09457777777777777}

:::MLPv0.5.0 ssd 1541757450.663556099 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 533, "value": 0.09475555555555555}

:::MLPv0.5.0 ssd 1541757450.747119188 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 534, "value": 0.09493333333333333}

:::MLPv0.5.0 ssd 1541757450.831745863 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 535, "value": 0.0951111111111111}

:::MLPv0.5.0 ssd 1541757450.915928364 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 536, "value": 0.0952888888888889}

:::MLPv0.5.0 ssd 1541757450.999895334 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 537, "value": 0.09546666666666667}

:::MLPv0.5.0 ssd 1541757451.083374739 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 538, "value": 0.09564444444444445}

:::MLPv0.5.0 ssd 1541757451.167322159 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 539, "value": 0.09582222222222223}

:::MLPv0.5.0 ssd 1541757451.251481295 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 540, "value": 0.096}
Iteration:    540, Loss function: 5.384, Average Loss: 3.412, avg. samples / sec: 24398.66
Iteration:    540, Loss function: 5.660, Average Loss: 3.415, avg. samples / sec: 24394.61
Iteration:    540, Loss function: 5.511, Average Loss: 3.418, avg. samples / sec: 24396.30
Iteration:    540, Loss function: 5.468, Average Loss: 3.412, avg. samples / sec: 24398.86
Iteration:    540, Loss function: 5.587, Average Loss: 3.410, avg. samples / sec: 24387.31
Iteration:    540, Loss function: 5.764, Average Loss: 3.408, avg. samples / sec: 24407.11
Iteration:    540, Loss function: 5.412, Average Loss: 3.405, avg. samples / sec: 24405.31
Iteration:    540, Loss function: 5.742, Average Loss: 3.410, avg. samples / sec: 24407.85

:::MLPv0.5.0 ssd 1541757451.334856987 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 541, "value": 0.09617777777777778}

:::MLPv0.5.0 ssd 1541757451.418953419 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 542, "value": 0.09635555555555556}

:::MLPv0.5.0 ssd 1541757451.503227949 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 543, "value": 0.09653333333333333}

:::MLPv0.5.0 ssd 1541757451.589756489 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 544, "value": 0.09671111111111111}

:::MLPv0.5.0 ssd 1541757451.673164368 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 545, "value": 0.09688888888888889}

:::MLPv0.5.0 ssd 1541757451.757237196 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 546, "value": 0.09706666666666666}

:::MLPv0.5.0 ssd 1541757451.840999603 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 547, "value": 0.09724444444444444}

:::MLPv0.5.0 ssd 1541757451.924998283 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 548, "value": 0.09742222222222222}

:::MLPv0.5.0 ssd 1541757452.009089708 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 549, "value": 0.09759999999999999}

:::MLPv0.5.0 ssd 1541757452.093462467 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 550, "value": 0.09777777777777777}

:::MLPv0.5.0 ssd 1541757452.177373648 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 551, "value": 0.09795555555555555}

:::MLPv0.5.0 ssd 1541757452.261207581 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 552, "value": 0.09813333333333334}

:::MLPv0.5.0 ssd 1541757452.345242739 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 553, "value": 0.09831111111111111}

:::MLPv0.5.0 ssd 1541757452.428772449 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 554, "value": 0.09848888888888889}

:::MLPv0.5.0 ssd 1541757452.513129711 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 555, "value": 0.09866666666666667}

:::MLPv0.5.0 ssd 1541757452.596816540 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 556, "value": 0.09884444444444444}

:::MLPv0.5.0 ssd 1541757452.685437679 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 557, "value": 0.09902222222222222}

:::MLPv0.5.0 ssd 1541757452.769484282 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 558, "value": 0.09920000000000001}

:::MLPv0.5.0 ssd 1541757452.853913069 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 559, "value": 0.09937777777777779}

:::MLPv0.5.0 ssd 1541757452.937779665 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 560, "value": 0.09955555555555556}
Iteration:    560, Loss function: 6.239, Average Loss: 3.458, avg. samples / sec: 24299.17
Iteration:    560, Loss function: 5.621, Average Loss: 3.461, avg. samples / sec: 24286.63
Iteration:    560, Loss function: 6.349, Average Loss: 3.466, avg. samples / sec: 24290.88
Iteration:    560, Loss function: 6.038, Average Loss: 3.454, avg. samples / sec: 24308.64
Iteration:    560, Loss function: 6.223, Average Loss: 3.459, avg. samples / sec: 24287.80
Iteration:    560, Loss function: 6.286, Average Loss: 3.458, avg. samples / sec: 24319.64
Iteration:    560, Loss function: 5.804, Average Loss: 3.458, avg. samples / sec: 24272.06
Iteration:    560, Loss function: 6.026, Average Loss: 3.459, avg. samples / sec: 24270.84

:::MLPv0.5.0 ssd 1541757453.021841526 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 561, "value": 0.09973333333333334}

:::MLPv0.5.0 ssd 1541757453.106088400 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 562, "value": 0.09991111111111112}

:::MLPv0.5.0 ssd 1541757453.190191269 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 563, "value": 0.1000888888888889}

:::MLPv0.5.0 ssd 1541757453.274837494 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 564, "value": 0.10026666666666667}

:::MLPv0.5.0 ssd 1541757453.358637333 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 565, "value": 0.10044444444444445}

:::MLPv0.5.0 ssd 1541757453.442843676 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 566, "value": 0.10062222222222222}

:::MLPv0.5.0 ssd 1541757453.527627707 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 567, "value": 0.1008}

:::MLPv0.5.0 ssd 1541757453.611160994 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 568, "value": 0.10097777777777778}

:::MLPv0.5.0 ssd 1541757453.694924116 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 569, "value": 0.10115555555555555}

:::MLPv0.5.0 ssd 1541757453.778861284 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 570, "value": 0.10133333333333333}

:::MLPv0.5.0 ssd 1541757453.863385916 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 571, "value": 0.10151111111111111}

:::MLPv0.5.0 ssd 1541757453.947190523 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 572, "value": 0.10168888888888888}

:::MLPv0.5.0 ssd 1541757454.031550169 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 573, "value": 0.10186666666666666}

:::MLPv0.5.0 ssd 1541757454.115628481 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 574, "value": 0.10204444444444444}

:::MLPv0.5.0 ssd 1541757454.202393770 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 575, "value": 0.10222222222222221}

:::MLPv0.5.0 ssd 1541757454.286605120 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 576, "value": 0.10239999999999999}

:::MLPv0.5.0 ssd 1541757454.370540380 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 577, "value": 0.10257777777777778}

:::MLPv0.5.0 ssd 1541757454.451816082 (train.py:553) train_epoch: 10

:::MLPv0.5.0 ssd 1541757454.455974817 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 578, "value": 0.10275555555555556}

:::MLPv0.5.0 ssd 1541757454.540321827 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 579, "value": 0.10293333333333334}

:::MLPv0.5.0 ssd 1541757454.624406099 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 580, "value": 0.10311111111111111}
Iteration:    580, Loss function: 5.742, Average Loss: 3.514, avg. samples / sec: 24288.04
Iteration:    580, Loss function: 5.477, Average Loss: 3.507, avg. samples / sec: 24298.93
Iteration:    580, Loss function: 6.069, Average Loss: 3.507, avg. samples / sec: 24314.96
Iteration:    580, Loss function: 5.368, Average Loss: 3.507, avg. samples / sec: 24281.53
Iteration:    580, Loss function: 5.714, Average Loss: 3.501, avg. samples / sec: 24277.69
Iteration:    580, Loss function: 5.671, Average Loss: 3.508, avg. samples / sec: 24263.43
Iteration:    580, Loss function: 5.647, Average Loss: 3.506, avg. samples / sec: 24263.94
Iteration:    580, Loss function: 6.032, Average Loss: 3.506, avg. samples / sec: 24265.49

:::MLPv0.5.0 ssd 1541757454.708309412 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 581, "value": 0.10328888888888889}

:::MLPv0.5.0 ssd 1541757454.792093039 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 582, "value": 0.10346666666666667}

:::MLPv0.5.0 ssd 1541757454.876306772 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 583, "value": 0.10364444444444444}

:::MLPv0.5.0 ssd 1541757454.959883928 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 584, "value": 0.10382222222222223}

:::MLPv0.5.0 ssd 1541757455.043606520 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 585, "value": 0.10400000000000001}

:::MLPv0.5.0 ssd 1541757455.128088236 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 586, "value": 0.10417777777777779}

:::MLPv0.5.0 ssd 1541757455.212533712 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 587, "value": 0.10435555555555556}

:::MLPv0.5.0 ssd 1541757455.296163797 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 588, "value": 0.10453333333333334}

:::MLPv0.5.0 ssd 1541757455.380156994 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 589, "value": 0.10471111111111112}

:::MLPv0.5.0 ssd 1541757455.464485407 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 590, "value": 0.10488888888888889}

:::MLPv0.5.0 ssd 1541757455.548543692 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 591, "value": 0.10506666666666667}

:::MLPv0.5.0 ssd 1541757455.632974386 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 592, "value": 0.10524444444444445}

:::MLPv0.5.0 ssd 1541757455.717542887 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 593, "value": 0.10542222222222222}

:::MLPv0.5.0 ssd 1541757455.801425695 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 594, "value": 0.1056}

:::MLPv0.5.0 ssd 1541757455.885366440 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 595, "value": 0.10577777777777778}

:::MLPv0.5.0 ssd 1541757455.969672203 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 596, "value": 0.10595555555555555}

:::MLPv0.5.0 ssd 1541757456.053306341 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 597, "value": 0.10613333333333333}

:::MLPv0.5.0 ssd 1541757456.137805939 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 598, "value": 0.1063111111111111}

:::MLPv0.5.0 ssd 1541757456.222158909 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 599, "value": 0.10648888888888888}

:::MLPv0.5.0 ssd 1541757456.306463480 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 600, "value": 0.10666666666666666}
Iteration:    600, Loss function: 6.037, Average Loss: 3.545, avg. samples / sec: 24364.51
Iteration:    600, Loss function: 5.621, Average Loss: 3.546, avg. samples / sec: 24386.46
Iteration:    600, Loss function: 5.253, Average Loss: 3.553, avg. samples / sec: 24352.68
Iteration:    600, Loss function: 5.491, Average Loss: 3.546, avg. samples / sec: 24379.37
Iteration:    600, Loss function: 5.342, Average Loss: 3.546, avg. samples / sec: 24366.84
Iteration:    600, Loss function: 6.210, Average Loss: 3.549, avg. samples / sec: 24341.57
Iteration:    600, Loss function: 5.661, Average Loss: 3.547, avg. samples / sec: 24340.76
Iteration:    600, Loss function: 5.477, Average Loss: 3.544, avg. samples / sec: 24317.02

:::MLPv0.5.0 ssd 1541757456.390171289 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 601, "value": 0.10684444444444444}

:::MLPv0.5.0 ssd 1541757456.474470139 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 602, "value": 0.10702222222222221}

:::MLPv0.5.0 ssd 1541757456.558348894 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 603, "value": 0.1072}

:::MLPv0.5.0 ssd 1541757456.642923355 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 604, "value": 0.10737777777777778}

:::MLPv0.5.0 ssd 1541757456.726828098 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 605, "value": 0.10755555555555556}

:::MLPv0.5.0 ssd 1541757456.810890436 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 606, "value": 0.10773333333333333}

:::MLPv0.5.0 ssd 1541757456.894978046 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 607, "value": 0.10791111111111111}

:::MLPv0.5.0 ssd 1541757456.979031801 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 608, "value": 0.10808888888888889}

:::MLPv0.5.0 ssd 1541757457.062657595 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 609, "value": 0.10826666666666668}

:::MLPv0.5.0 ssd 1541757457.147123337 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 610, "value": 0.10844444444444445}

:::MLPv0.5.0 ssd 1541757457.231149197 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 611, "value": 0.10862222222222223}

:::MLPv0.5.0 ssd 1541757457.316215038 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 612, "value": 0.10880000000000001}

:::MLPv0.5.0 ssd 1541757457.399764299 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 613, "value": 0.10897777777777778}

:::MLPv0.5.0 ssd 1541757457.483565807 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 614, "value": 0.10915555555555556}

:::MLPv0.5.0 ssd 1541757457.567812204 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 615, "value": 0.10933333333333334}

:::MLPv0.5.0 ssd 1541757457.651616573 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 616, "value": 0.10951111111111111}

:::MLPv0.5.0 ssd 1541757457.735908270 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 617, "value": 0.10968888888888889}

:::MLPv0.5.0 ssd 1541757457.819833279 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 618, "value": 0.10986666666666667}

:::MLPv0.5.0 ssd 1541757457.903849363 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 619, "value": 0.11004444444444444}

:::MLPv0.5.0 ssd 1541757457.987999201 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 620, "value": 0.11022222222222222}
Iteration:    620, Loss function: 6.393, Average Loss: 3.589, avg. samples / sec: 24367.74
Iteration:    620, Loss function: 5.656, Average Loss: 3.586, avg. samples / sec: 24360.51
Iteration:    620, Loss function: 5.507, Average Loss: 3.587, avg. samples / sec: 24360.41
Iteration:    620, Loss function: 5.398, Average Loss: 3.594, avg. samples / sec: 24362.16
Iteration:    620, Loss function: 5.620, Average Loss: 3.589, avg. samples / sec: 24338.06
Iteration:    620, Loss function: 5.778, Average Loss: 3.589, avg. samples / sec: 24328.89
Iteration:    620, Loss function: 5.229, Average Loss: 3.589, avg. samples / sec: 24325.34
Iteration:    620, Loss function: 5.636, Average Loss: 3.588, avg. samples / sec: 24364.78

:::MLPv0.5.0 ssd 1541757458.072035313 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 621, "value": 0.1104}

:::MLPv0.5.0 ssd 1541757458.156155348 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 622, "value": 0.11057777777777777}

:::MLPv0.5.0 ssd 1541757458.240422964 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 623, "value": 0.11075555555555555}

:::MLPv0.5.0 ssd 1541757458.324244738 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 624, "value": 0.11093333333333333}

:::MLPv0.5.0 ssd 1541757458.408287525 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 625, "value": 0.1111111111111111}

:::MLPv0.5.0 ssd 1541757458.491972923 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 626, "value": 0.11128888888888888}

:::MLPv0.5.0 ssd 1541757458.576564074 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 627, "value": 0.11146666666666666}

:::MLPv0.5.0 ssd 1541757458.660534620 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 628, "value": 0.11164444444444445}

:::MLPv0.5.0 ssd 1541757458.744188309 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 629, "value": 0.11182222222222223}

:::MLPv0.5.0 ssd 1541757458.827942133 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 630, "value": 0.112}

:::MLPv0.5.0 ssd 1541757458.912092686 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 631, "value": 0.11217777777777778}

:::MLPv0.5.0 ssd 1541757458.996316195 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 632, "value": 0.11235555555555556}

:::MLPv0.5.0 ssd 1541757459.080292463 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 633, "value": 0.11253333333333333}

:::MLPv0.5.0 ssd 1541757459.164008141 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 634, "value": 0.11271111111111111}

:::MLPv0.5.0 ssd 1541757459.249418259 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 635, "value": 0.1128888888888889}

:::MLPv0.5.0 ssd 1541757459.331211805 (train.py:553) train_epoch: 11

:::MLPv0.5.0 ssd 1541757459.335403919 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 636, "value": 0.11306666666666668}

:::MLPv0.5.0 ssd 1541757459.419348001 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 637, "value": 0.11324444444444445}

:::MLPv0.5.0 ssd 1541757459.503801823 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 638, "value": 0.11342222222222223}

:::MLPv0.5.0 ssd 1541757459.588103056 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 639, "value": 0.1136}

:::MLPv0.5.0 ssd 1541757459.672216177 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 640, "value": 0.11377777777777778}
Iteration:    640, Loss function: 5.317, Average Loss: 3.637, avg. samples / sec: 24323.23
Iteration:    640, Loss function: 5.770, Average Loss: 3.628, avg. samples / sec: 24315.31
Iteration:    640, Loss function: 6.175, Average Loss: 3.631, avg. samples / sec: 24363.53
Iteration:    640, Loss function: 5.307, Average Loss: 3.631, avg. samples / sec: 24342.83
Iteration:    640, Loss function: 5.946, Average Loss: 3.632, avg. samples / sec: 24328.03
Iteration:    640, Loss function: 5.560, Average Loss: 3.628, avg. samples / sec: 24278.87
Iteration:    640, Loss function: 6.268, Average Loss: 3.629, avg. samples / sec: 24278.93
Iteration:    640, Loss function: 6.017, Average Loss: 3.632, avg. samples / sec: 24321.08

:::MLPv0.5.0 ssd 1541757459.756040096 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 641, "value": 0.11395555555555556}

:::MLPv0.5.0 ssd 1541757459.840022326 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 642, "value": 0.11413333333333334}

:::MLPv0.5.0 ssd 1541757459.924018621 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 643, "value": 0.11431111111111111}

:::MLPv0.5.0 ssd 1541757460.009922504 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 644, "value": 0.11448888888888889}

:::MLPv0.5.0 ssd 1541757460.093502522 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 645, "value": 0.11466666666666667}

:::MLPv0.5.0 ssd 1541757460.177857637 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 646, "value": 0.11484444444444444}

:::MLPv0.5.0 ssd 1541757460.262534857 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 647, "value": 0.11502222222222222}

:::MLPv0.5.0 ssd 1541757460.346431255 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 648, "value": 0.1152}

:::MLPv0.5.0 ssd 1541757460.431185246 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 649, "value": 0.11537777777777777}

:::MLPv0.5.0 ssd 1541757460.515357256 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 650, "value": 0.11555555555555555}

:::MLPv0.5.0 ssd 1541757460.600357771 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 651, "value": 0.11573333333333333}

:::MLPv0.5.0 ssd 1541757460.684072733 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 652, "value": 0.1159111111111111}

:::MLPv0.5.0 ssd 1541757460.767465353 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 653, "value": 0.11608888888888888}

:::MLPv0.5.0 ssd 1541757460.851169825 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 654, "value": 0.11626666666666667}

:::MLPv0.5.0 ssd 1541757460.935528278 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 655, "value": 0.11644444444444445}

:::MLPv0.5.0 ssd 1541757461.022711754 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 656, "value": 0.11662222222222222}

:::MLPv0.5.0 ssd 1541757461.106549025 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 657, "value": 0.1168}

:::MLPv0.5.0 ssd 1541757461.190516472 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 658, "value": 0.11697777777777778}

:::MLPv0.5.0 ssd 1541757461.274255037 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 659, "value": 0.11715555555555555}

:::MLPv0.5.0 ssd 1541757461.358610868 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 660, "value": 0.11733333333333333}
Iteration:    660, Loss function: 5.042, Average Loss: 3.662, avg. samples / sec: 24330.86
Iteration:    660, Loss function: 5.088, Average Loss: 3.671, avg. samples / sec: 24286.88
Iteration:    660, Loss function: 4.698, Average Loss: 3.662, avg. samples / sec: 24293.49
Iteration:    660, Loss function: 5.345, Average Loss: 3.665, avg. samples / sec: 24289.38
Iteration:    660, Loss function: 5.807, Average Loss: 3.665, avg. samples / sec: 24321.93
Iteration:    660, Loss function: 5.623, Average Loss: 3.668, avg. samples / sec: 24326.43
Iteration:    660, Loss function: 5.730, Average Loss: 3.663, avg. samples / sec: 24284.06
Iteration:    660, Loss function: 5.041, Average Loss: 3.668, avg. samples / sec: 24287.07

:::MLPv0.5.0 ssd 1541757461.442343473 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 661, "value": 0.11751111111111112}

:::MLPv0.5.0 ssd 1541757461.526096821 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 662, "value": 0.1176888888888889}

:::MLPv0.5.0 ssd 1541757461.610388041 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 663, "value": 0.11786666666666668}

:::MLPv0.5.0 ssd 1541757461.694816351 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 664, "value": 0.11804444444444445}

:::MLPv0.5.0 ssd 1541757461.779377699 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 665, "value": 0.11822222222222223}

:::MLPv0.5.0 ssd 1541757461.863668203 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 666, "value": 0.1184}

:::MLPv0.5.0 ssd 1541757461.947618246 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 667, "value": 0.11857777777777778}

:::MLPv0.5.0 ssd 1541757462.031641245 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 668, "value": 0.11875555555555556}

:::MLPv0.5.0 ssd 1541757462.115319014 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 669, "value": 0.11893333333333334}

:::MLPv0.5.0 ssd 1541757462.200371742 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 670, "value": 0.11911111111111111}

:::MLPv0.5.0 ssd 1541757462.284580946 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 671, "value": 0.11928888888888889}

:::MLPv0.5.0 ssd 1541757462.368613482 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 672, "value": 0.11946666666666667}

:::MLPv0.5.0 ssd 1541757462.453438520 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 673, "value": 0.11964444444444444}

:::MLPv0.5.0 ssd 1541757462.537718296 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 674, "value": 0.11982222222222222}

:::MLPv0.5.0 ssd 1541757462.622062445 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 675, "value": 0.12}

:::MLPv0.5.0 ssd 1541757462.707137823 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 676, "value": 0.12017777777777777}

:::MLPv0.5.0 ssd 1541757462.791317701 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 677, "value": 0.12035555555555555}

:::MLPv0.5.0 ssd 1541757462.875418425 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 678, "value": 0.12053333333333333}

:::MLPv0.5.0 ssd 1541757462.959675074 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 679, "value": 0.1207111111111111}

:::MLPv0.5.0 ssd 1541757463.043451309 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 680, "value": 0.12088888888888889}
Iteration:    680, Loss function: 5.948, Average Loss: 3.701, avg. samples / sec: 24316.61
Iteration:    680, Loss function: 5.789, Average Loss: 3.707, avg. samples / sec: 24343.67
Iteration:    680, Loss function: 5.805, Average Loss: 3.709, avg. samples / sec: 24312.14
Iteration:    680, Loss function: 5.291, Average Loss: 3.702, avg. samples / sec: 24317.28
Iteration:    680, Loss function: 5.466, Average Loss: 3.703, avg. samples / sec: 24298.61
Iteration:    680, Loss function: 5.574, Average Loss: 3.700, avg. samples / sec: 24277.23
Iteration:    680, Loss function: 5.591, Average Loss: 3.698, avg. samples / sec: 24299.80
Iteration:    680, Loss function: 6.159, Average Loss: 3.700, avg. samples / sec: 24254.31

:::MLPv0.5.0 ssd 1541757463.127282619 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 681, "value": 0.12106666666666667}

:::MLPv0.5.0 ssd 1541757463.211282730 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 682, "value": 0.12124444444444445}

:::MLPv0.5.0 ssd 1541757463.295128107 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 683, "value": 0.12142222222222222}

:::MLPv0.5.0 ssd 1541757463.379220009 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 684, "value": 0.1216}

:::MLPv0.5.0 ssd 1541757463.463329554 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 685, "value": 0.12177777777777778}

:::MLPv0.5.0 ssd 1541757463.547651768 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 686, "value": 0.12195555555555557}

:::MLPv0.5.0 ssd 1541757463.631741047 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 687, "value": 0.12213333333333334}

:::MLPv0.5.0 ssd 1541757463.716120481 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 688, "value": 0.12231111111111112}

:::MLPv0.5.0 ssd 1541757463.800263643 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 689, "value": 0.1224888888888889}

:::MLPv0.5.0 ssd 1541757463.884377956 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 690, "value": 0.12266666666666667}

:::MLPv0.5.0 ssd 1541757463.968115091 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 691, "value": 0.12284444444444445}

:::MLPv0.5.0 ssd 1541757464.053031921 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 692, "value": 0.12302222222222223}

:::MLPv0.5.0 ssd 1541757464.137972832 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 693, "value": 0.1232}

:::MLPv0.5.0 ssd 1541757464.219036579 (train.py:553) train_epoch: 12

:::MLPv0.5.0 ssd 1541757464.223253012 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 694, "value": 0.12337777777777778}

:::MLPv0.5.0 ssd 1541757464.309133768 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 695, "value": 0.12355555555555556}

:::MLPv0.5.0 ssd 1541757464.392906427 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 696, "value": 0.12373333333333333}

:::MLPv0.5.0 ssd 1541757464.477024555 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 697, "value": 0.12391111111111111}

:::MLPv0.5.0 ssd 1541757464.560346842 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 698, "value": 0.12408888888888889}

:::MLPv0.5.0 ssd 1541757464.643978357 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 699, "value": 0.12426666666666666}

:::MLPv0.5.0 ssd 1541757464.728014231 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 700, "value": 0.12444444444444444}
Iteration:    700, Loss function: 5.543, Average Loss: 3.738, avg. samples / sec: 24318.63
Iteration:    700, Loss function: 4.910, Average Loss: 3.735, avg. samples / sec: 24318.51
Iteration:    700, Loss function: 4.851, Average Loss: 3.731, avg. samples / sec: 24368.42
Iteration:    700, Loss function: 5.695, Average Loss: 3.729, avg. samples / sec: 24346.09
Iteration:    700, Loss function: 5.545, Average Loss: 3.733, avg. samples / sec: 24308.13
Iteration:    700, Loss function: 5.601, Average Loss: 3.733, avg. samples / sec: 24311.93
Iteration:    700, Loss function: 5.594, Average Loss: 3.733, avg. samples / sec: 24256.69
Iteration:    700, Loss function: 5.885, Average Loss: 3.732, avg. samples / sec: 24288.60

:::MLPv0.5.0 ssd 1541757464.812433720 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 701, "value": 0.12462222222222222}

:::MLPv0.5.0 ssd 1541757464.896791697 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 702, "value": 0.1248}

:::MLPv0.5.0 ssd 1541757464.981028318 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 703, "value": 0.12497777777777777}

:::MLPv0.5.0 ssd 1541757465.065605164 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 704, "value": 0.12515555555555555}

:::MLPv0.5.0 ssd 1541757465.149243593 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 705, "value": 0.12533333333333335}

:::MLPv0.5.0 ssd 1541757465.233922005 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 706, "value": 0.12551111111111113}

:::MLPv0.5.0 ssd 1541757465.318328857 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 707, "value": 0.1256888888888889}

:::MLPv0.5.0 ssd 1541757465.402506590 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 708, "value": 0.12586666666666668}

:::MLPv0.5.0 ssd 1541757465.486342430 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 709, "value": 0.12604444444444446}

:::MLPv0.5.0 ssd 1541757465.570459843 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 710, "value": 0.12622222222222224}

:::MLPv0.5.0 ssd 1541757465.654862642 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 711, "value": 0.1264}

:::MLPv0.5.0 ssd 1541757465.739005327 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 712, "value": 0.1265777777777778}

:::MLPv0.5.0 ssd 1541757465.823161840 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 713, "value": 0.12675555555555557}

:::MLPv0.5.0 ssd 1541757465.907155991 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 714, "value": 0.12693333333333334}

:::MLPv0.5.0 ssd 1541757465.991509914 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 715, "value": 0.12711111111111112}

:::MLPv0.5.0 ssd 1541757466.076275826 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 716, "value": 0.1272888888888889}

:::MLPv0.5.0 ssd 1541757466.160331011 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 717, "value": 0.12746666666666667}

:::MLPv0.5.0 ssd 1541757466.243881226 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 718, "value": 0.12764444444444445}

:::MLPv0.5.0 ssd 1541757466.327955723 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 719, "value": 0.12782222222222223}

:::MLPv0.5.0 ssd 1541757466.412102222 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 720, "value": 0.128}
Iteration:    720, Loss function: 4.706, Average Loss: 3.770, avg. samples / sec: 24320.08
Iteration:    720, Loss function: 4.968, Average Loss: 3.768, avg. samples / sec: 24335.39
Iteration:    720, Loss function: 5.578, Average Loss: 3.766, avg. samples / sec: 24312.58
Iteration:    720, Loss function: 5.338, Average Loss: 3.760, avg. samples / sec: 24302.96
Iteration:    720, Loss function: 4.622, Average Loss: 3.763, avg. samples / sec: 24282.15
Iteration:    720, Loss function: 5.181, Average Loss: 3.767, avg. samples / sec: 24273.10
Iteration:    720, Loss function: 5.289, Average Loss: 3.766, avg. samples / sec: 24328.37
Iteration:    720, Loss function: 5.621, Average Loss: 3.766, avg. samples / sec: 24322.76

:::MLPv0.5.0 ssd 1541757466.496083021 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 721, "value": 0.12817777777777778}

:::MLPv0.5.0 ssd 1541757466.579758644 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 722, "value": 0.12835555555555556}

:::MLPv0.5.0 ssd 1541757466.663880110 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 723, "value": 0.12853333333333333}

:::MLPv0.5.0 ssd 1541757466.747857094 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 724, "value": 0.1287111111111111}

:::MLPv0.5.0 ssd 1541757466.832926035 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 725, "value": 0.1288888888888889}

:::MLPv0.5.0 ssd 1541757466.916998625 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 726, "value": 0.12906666666666666}

:::MLPv0.5.0 ssd 1541757467.001016855 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 727, "value": 0.12924444444444444}

:::MLPv0.5.0 ssd 1541757467.085119486 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 728, "value": 0.12942222222222222}

:::MLPv0.5.0 ssd 1541757467.169191599 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 729, "value": 0.1296}

:::MLPv0.5.0 ssd 1541757467.253056049 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 730, "value": 0.12977777777777777}

:::MLPv0.5.0 ssd 1541757467.337002516 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 731, "value": 0.12995555555555555}

:::MLPv0.5.0 ssd 1541757467.420535564 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 732, "value": 0.13013333333333332}

:::MLPv0.5.0 ssd 1541757467.504759550 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 733, "value": 0.1303111111111111}

:::MLPv0.5.0 ssd 1541757467.588803530 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 734, "value": 0.13048888888888888}

:::MLPv0.5.0 ssd 1541757467.673208475 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 735, "value": 0.13066666666666665}

:::MLPv0.5.0 ssd 1541757467.757357121 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 736, "value": 0.13084444444444446}

:::MLPv0.5.0 ssd 1541757467.841372967 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 737, "value": 0.13102222222222223}

:::MLPv0.5.0 ssd 1541757467.925112247 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 738, "value": 0.1312}

:::MLPv0.5.0 ssd 1541757468.009479284 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 739, "value": 0.1313777777777778}

:::MLPv0.5.0 ssd 1541757468.093845606 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 740, "value": 0.13155555555555556}
Iteration:    740, Loss function: 5.213, Average Loss: 3.795, avg. samples / sec: 24408.55
Iteration:    740, Loss function: 4.503, Average Loss: 3.795, avg. samples / sec: 24373.34
Iteration:    740, Loss function: 5.040, Average Loss: 3.795, avg. samples / sec: 24356.40
Iteration:    740, Loss function: 5.305, Average Loss: 3.788, avg. samples / sec: 24369.52
Iteration:    740, Loss function: 5.015, Average Loss: 3.796, avg. samples / sec: 24360.00
Iteration:    740, Loss function: 4.549, Average Loss: 3.791, avg. samples / sec: 24372.24
Iteration:    740, Loss function: 5.626, Average Loss: 3.794, avg. samples / sec: 24374.17
Iteration:    740, Loss function: 5.043, Average Loss: 3.794, avg. samples / sec: 24345.58

:::MLPv0.5.0 ssd 1541757468.178023100 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 741, "value": 0.13173333333333334}

:::MLPv0.5.0 ssd 1541757468.262178421 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 742, "value": 0.13191111111111112}

:::MLPv0.5.0 ssd 1541757468.345726728 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 743, "value": 0.1320888888888889}

:::MLPv0.5.0 ssd 1541757468.429663897 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 744, "value": 0.13226666666666667}

:::MLPv0.5.0 ssd 1541757468.513672113 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 745, "value": 0.13244444444444445}

:::MLPv0.5.0 ssd 1541757468.598136663 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 746, "value": 0.13262222222222222}

:::MLPv0.5.0 ssd 1541757468.682174921 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 747, "value": 0.1328}

:::MLPv0.5.0 ssd 1541757468.766348839 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 748, "value": 0.13297777777777778}

:::MLPv0.5.0 ssd 1541757468.850057364 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 749, "value": 0.13315555555555555}

:::MLPv0.5.0 ssd 1541757468.934178352 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 750, "value": 0.13333333333333333}

:::MLPv0.5.0 ssd 1541757469.015751123 (train.py:553) train_epoch: 13

:::MLPv0.5.0 ssd 1541757469.019896984 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 751, "value": 0.1335111111111111}

:::MLPv0.5.0 ssd 1541757469.103975534 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 752, "value": 0.13368888888888888}

:::MLPv0.5.0 ssd 1541757469.187666178 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 753, "value": 0.13386666666666666}

:::MLPv0.5.0 ssd 1541757469.271846294 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 754, "value": 0.13404444444444444}

:::MLPv0.5.0 ssd 1541757469.355885983 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 755, "value": 0.13422222222222221}

:::MLPv0.5.0 ssd 1541757469.439485550 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 756, "value": 0.1344}

:::MLPv0.5.0 ssd 1541757469.523819208 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 757, "value": 0.13457777777777777}

:::MLPv0.5.0 ssd 1541757469.607698917 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 758, "value": 0.13475555555555557}

:::MLPv0.5.0 ssd 1541757469.691641331 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 759, "value": 0.13493333333333335}

:::MLPv0.5.0 ssd 1541757469.775597334 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 760, "value": 0.13511111111111113}
Iteration:    760, Loss function: 5.912, Average Loss: 3.831, avg. samples / sec: 24382.16
Iteration:    760, Loss function: 6.358, Average Loss: 3.827, avg. samples / sec: 24362.50
Iteration:    760, Loss function: 5.737, Average Loss: 3.831, avg. samples / sec: 24347.44
Iteration:    760, Loss function: 6.058, Average Loss: 3.835, avg. samples / sec: 24362.60
Iteration:    760, Loss function: 5.647, Average Loss: 3.832, avg. samples / sec: 24305.14
Iteration:    760, Loss function: 5.951, Average Loss: 3.832, avg. samples / sec: 24350.78
Iteration:    760, Loss function: 6.208, Average Loss: 3.834, avg. samples / sec: 24301.12
Iteration:    760, Loss function: 5.220, Average Loss: 3.832, avg. samples / sec: 24356.80

:::MLPv0.5.0 ssd 1541757469.859820604 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 761, "value": 0.1352888888888889}

:::MLPv0.5.0 ssd 1541757469.943577290 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 762, "value": 0.13546666666666668}

:::MLPv0.5.0 ssd 1541757470.027622700 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 763, "value": 0.13564444444444446}

:::MLPv0.5.0 ssd 1541757470.112041235 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 764, "value": 0.13582222222222223}

:::MLPv0.5.0 ssd 1541757470.195853233 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 765, "value": 0.136}

:::MLPv0.5.0 ssd 1541757470.279915571 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 766, "value": 0.1361777777777778}

:::MLPv0.5.0 ssd 1541757470.364240408 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 767, "value": 0.13635555555555556}

:::MLPv0.5.0 ssd 1541757470.448450327 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 768, "value": 0.13653333333333334}

:::MLPv0.5.0 ssd 1541757470.532804489 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 769, "value": 0.13671111111111112}

:::MLPv0.5.0 ssd 1541757470.617043972 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 770, "value": 0.1368888888888889}

:::MLPv0.5.0 ssd 1541757470.700942516 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 771, "value": 0.13706666666666667}

:::MLPv0.5.0 ssd 1541757470.785235405 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 772, "value": 0.13724444444444445}

:::MLPv0.5.0 ssd 1541757470.869498253 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 773, "value": 0.13742222222222222}

:::MLPv0.5.0 ssd 1541757470.953722954 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 774, "value": 0.1376}

:::MLPv0.5.0 ssd 1541757471.037668467 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 775, "value": 0.13777777777777778}

:::MLPv0.5.0 ssd 1541757471.122016191 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 776, "value": 0.13795555555555555}

:::MLPv0.5.0 ssd 1541757471.205905199 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 777, "value": 0.13813333333333333}

:::MLPv0.5.0 ssd 1541757471.289738178 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 778, "value": 0.1383111111111111}

:::MLPv0.5.0 ssd 1541757471.374409199 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 779, "value": 0.13848888888888888}

:::MLPv0.5.0 ssd 1541757471.458164454 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 780, "value": 0.13866666666666666}
Iteration:    780, Loss function: 4.932, Average Loss: 3.861, avg. samples / sec: 24354.73
Iteration:    780, Loss function: 4.587, Average Loss: 3.862, avg. samples / sec: 24343.33
Iteration:    780, Loss function: 4.709, Average Loss: 3.856, avg. samples / sec: 24339.06
Iteration:    780, Loss function: 5.131, Average Loss: 3.860, avg. samples / sec: 24391.58
Iteration:    780, Loss function: 5.167, Average Loss: 3.863, avg. samples / sec: 24373.88
Iteration:    780, Loss function: 5.300, Average Loss: 3.864, avg. samples / sec: 24335.02
Iteration:    780, Loss function: 4.880, Average Loss: 3.861, avg. samples / sec: 24353.14
Iteration:    780, Loss function: 5.007, Average Loss: 3.861, avg. samples / sec: 24352.38

:::MLPv0.5.0 ssd 1541757471.541898489 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 781, "value": 0.13884444444444444}

:::MLPv0.5.0 ssd 1541757471.626589298 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 782, "value": 0.1390222222222222}

:::MLPv0.5.0 ssd 1541757471.710370302 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 783, "value": 0.1392}

:::MLPv0.5.0 ssd 1541757471.794309616 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 784, "value": 0.13937777777777777}

:::MLPv0.5.0 ssd 1541757471.878704309 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 785, "value": 0.13955555555555554}

:::MLPv0.5.0 ssd 1541757471.962364197 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 786, "value": 0.13973333333333332}

:::MLPv0.5.0 ssd 1541757472.046204567 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 787, "value": 0.13991111111111112}

:::MLPv0.5.0 ssd 1541757472.131105185 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 788, "value": 0.1400888888888889}

:::MLPv0.5.0 ssd 1541757472.215251923 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 789, "value": 0.14026666666666668}

:::MLPv0.5.0 ssd 1541757472.299711227 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 790, "value": 0.14044444444444446}

:::MLPv0.5.0 ssd 1541757472.383474827 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 791, "value": 0.14062222222222223}

:::MLPv0.5.0 ssd 1541757472.467436552 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 792, "value": 0.1408}

:::MLPv0.5.0 ssd 1541757472.551799774 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 793, "value": 0.14097777777777779}

:::MLPv0.5.0 ssd 1541757472.635434389 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 794, "value": 0.14115555555555556}

:::MLPv0.5.0 ssd 1541757472.719249487 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 795, "value": 0.14133333333333334}

:::MLPv0.5.0 ssd 1541757472.802732468 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 796, "value": 0.14151111111111112}

:::MLPv0.5.0 ssd 1541757472.887468576 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 797, "value": 0.1416888888888889}

:::MLPv0.5.0 ssd 1541757472.971313715 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 798, "value": 0.14186666666666667}

:::MLPv0.5.0 ssd 1541757473.055051804 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 799, "value": 0.14204444444444445}

:::MLPv0.5.0 ssd 1541757473.139580488 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 800, "value": 0.14222222222222222}
Iteration:    800, Loss function: 4.769, Average Loss: 3.882, avg. samples / sec: 24364.16
Iteration:    800, Loss function: 5.380, Average Loss: 3.889, avg. samples / sec: 24382.28
Iteration:    800, Loss function: 5.617, Average Loss: 3.886, avg. samples / sec: 24357.08
Iteration:    800, Loss function: 5.356, Average Loss: 3.882, avg. samples / sec: 24371.76
Iteration:    800, Loss function: 5.357, Average Loss: 3.880, avg. samples / sec: 24333.39
Iteration:    800, Loss function: 5.224, Average Loss: 3.886, avg. samples / sec: 24361.49
Iteration:    800, Loss function: 5.182, Average Loss: 3.887, avg. samples / sec: 24332.41
Iteration:    800, Loss function: 5.677, Average Loss: 3.882, avg. samples / sec: 24320.00

:::MLPv0.5.0 ssd 1541757473.223468065 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 801, "value": 0.1424}

:::MLPv0.5.0 ssd 1541757473.307419538 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 802, "value": 0.14257777777777778}

:::MLPv0.5.0 ssd 1541757473.391328335 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 803, "value": 0.14275555555555555}

:::MLPv0.5.0 ssd 1541757473.475394011 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 804, "value": 0.14293333333333333}

:::MLPv0.5.0 ssd 1541757473.560646296 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 805, "value": 0.1431111111111111}

:::MLPv0.5.0 ssd 1541757473.645092010 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 806, "value": 0.14328888888888888}

:::MLPv0.5.0 ssd 1541757473.728884220 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 807, "value": 0.14346666666666666}

:::MLPv0.5.0 ssd 1541757473.813457489 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 808, "value": 0.14364444444444444}

:::MLPv0.5.0 ssd 1541757473.894523144 (train.py:553) train_epoch: 14

:::MLPv0.5.0 ssd 1541757473.898643494 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 809, "value": 0.14382222222222224}

:::MLPv0.5.0 ssd 1541757473.982448101 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 810, "value": 0.14400000000000002}

:::MLPv0.5.0 ssd 1541757474.066164255 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 811, "value": 0.1441777777777778}

:::MLPv0.5.0 ssd 1541757474.150284052 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 812, "value": 0.14435555555555557}

:::MLPv0.5.0 ssd 1541757474.234327793 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 813, "value": 0.14453333333333335}

:::MLPv0.5.0 ssd 1541757474.318497896 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 814, "value": 0.14471111111111112}

:::MLPv0.5.0 ssd 1541757474.402837276 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 815, "value": 0.1448888888888889}

:::MLPv0.5.0 ssd 1541757474.487034321 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 816, "value": 0.14506666666666668}

:::MLPv0.5.0 ssd 1541757474.570710897 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 817, "value": 0.14524444444444445}

:::MLPv0.5.0 ssd 1541757474.655284882 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 818, "value": 0.14542222222222223}

:::MLPv0.5.0 ssd 1541757474.739434242 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 819, "value": 0.1456}

:::MLPv0.5.0 ssd 1541757474.823950291 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 820, "value": 0.14577777777777778}
Iteration:    820, Loss function: 4.974, Average Loss: 3.909, avg. samples / sec: 24360.59
Iteration:    820, Loss function: 5.176, Average Loss: 3.906, avg. samples / sec: 24341.52
Iteration:    820, Loss function: 4.515, Average Loss: 3.907, avg. samples / sec: 24312.49
Iteration:    820, Loss function: 5.150, Average Loss: 3.915, avg. samples / sec: 24320.29
Iteration:    820, Loss function: 4.998, Average Loss: 3.903, avg. samples / sec: 24335.33
Iteration:    820, Loss function: 5.529, Average Loss: 3.909, avg. samples / sec: 24353.29
Iteration:    820, Loss function: 4.939, Average Loss: 3.915, avg. samples / sec: 24333.86
Iteration:    820, Loss function: 4.742, Average Loss: 3.913, avg. samples / sec: 24277.24

:::MLPv0.5.0 ssd 1541757474.907500744 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 821, "value": 0.14595555555555556}

:::MLPv0.5.0 ssd 1541757474.991596460 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 822, "value": 0.14613333333333334}

:::MLPv0.5.0 ssd 1541757475.076368093 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 823, "value": 0.14631111111111111}

:::MLPv0.5.0 ssd 1541757475.160333872 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 824, "value": 0.1464888888888889}

:::MLPv0.5.0 ssd 1541757475.244548321 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 825, "value": 0.14666666666666667}

:::MLPv0.5.0 ssd 1541757475.328468323 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 826, "value": 0.14684444444444444}

:::MLPv0.5.0 ssd 1541757475.412324190 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 827, "value": 0.14702222222222222}

:::MLPv0.5.0 ssd 1541757475.497269392 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 828, "value": 0.1472}

:::MLPv0.5.0 ssd 1541757475.581329107 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 829, "value": 0.14737777777777777}

:::MLPv0.5.0 ssd 1541757475.665270329 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 830, "value": 0.14755555555555555}

:::MLPv0.5.0 ssd 1541757475.749760628 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 831, "value": 0.14773333333333333}

:::MLPv0.5.0 ssd 1541757475.833832264 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 832, "value": 0.1479111111111111}

:::MLPv0.5.0 ssd 1541757475.918081284 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 833, "value": 0.14808888888888888}

:::MLPv0.5.0 ssd 1541757476.001901627 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 834, "value": 0.14826666666666666}

:::MLPv0.5.0 ssd 1541757476.085862637 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 835, "value": 0.14844444444444443}

:::MLPv0.5.0 ssd 1541757476.169420958 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 836, "value": 0.1486222222222222}

:::MLPv0.5.0 ssd 1541757476.253835917 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 837, "value": 0.14880000000000002}

:::MLPv0.5.0 ssd 1541757476.337294102 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 838, "value": 0.1489777777777778}

:::MLPv0.5.0 ssd 1541757476.422010422 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 839, "value": 0.14915555555555557}

:::MLPv0.5.0 ssd 1541757476.506426811 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 840, "value": 0.14933333333333335}
Iteration:    840, Loss function: 5.484, Average Loss: 3.929, avg. samples / sec: 24350.92
Iteration:    840, Loss function: 5.747, Average Loss: 3.932, avg. samples / sec: 24347.22
Iteration:    840, Loss function: 5.618, Average Loss: 3.932, avg. samples / sec: 24337.16
Iteration:    840, Loss function: 5.313, Average Loss: 3.927, avg. samples / sec: 24343.71
Iteration:    840, Loss function: 5.153, Average Loss: 3.938, avg. samples / sec: 24362.44
Iteration:    840, Loss function: 5.232, Average Loss: 3.938, avg. samples / sec: 24321.07
Iteration:    840, Loss function: 5.665, Average Loss: 3.935, avg. samples / sec: 24328.05
Iteration:    840, Loss function: 5.606, Average Loss: 3.936, avg. samples / sec: 24329.76

:::MLPv0.5.0 ssd 1541757476.590865374 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 841, "value": 0.14951111111111112}

:::MLPv0.5.0 ssd 1541757476.674991369 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 842, "value": 0.1496888888888889}

:::MLPv0.5.0 ssd 1541757476.758774042 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 843, "value": 0.14986666666666668}

:::MLPv0.5.0 ssd 1541757476.842699766 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 844, "value": 0.15004444444444445}

:::MLPv0.5.0 ssd 1541757476.926630259 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 845, "value": 0.15022222222222223}

:::MLPv0.5.0 ssd 1541757477.010643959 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 846, "value": 0.1504}

:::MLPv0.5.0 ssd 1541757477.094828844 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 847, "value": 0.15057777777777778}

:::MLPv0.5.0 ssd 1541757477.178743839 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 848, "value": 0.15075555555555556}

:::MLPv0.5.0 ssd 1541757477.263081789 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 849, "value": 0.15093333333333334}

:::MLPv0.5.0 ssd 1541757477.347225428 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 850, "value": 0.1511111111111111}

:::MLPv0.5.0 ssd 1541757477.431117296 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 851, "value": 0.1512888888888889}

:::MLPv0.5.0 ssd 1541757477.514886141 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 852, "value": 0.15146666666666667}

:::MLPv0.5.0 ssd 1541757477.598568201 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 853, "value": 0.15164444444444444}

:::MLPv0.5.0 ssd 1541757477.683666229 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 854, "value": 0.15182222222222222}

:::MLPv0.5.0 ssd 1541757477.767619371 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 855, "value": 0.152}

:::MLPv0.5.0 ssd 1541757477.851476669 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 856, "value": 0.15217777777777777}

:::MLPv0.5.0 ssd 1541757477.935371161 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 857, "value": 0.15235555555555555}

:::MLPv0.5.0 ssd 1541757478.020040512 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 858, "value": 0.15253333333333333}

:::MLPv0.5.0 ssd 1541757478.103770733 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 859, "value": 0.1527111111111111}

:::MLPv0.5.0 ssd 1541757478.187587023 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 860, "value": 0.15288888888888888}
Iteration:    860, Loss function: 5.409, Average Loss: 3.964, avg. samples / sec: 24394.09
Iteration:    860, Loss function: 5.644, Average Loss: 3.956, avg. samples / sec: 24364.21
Iteration:    860, Loss function: 5.202, Average Loss: 3.961, avg. samples / sec: 24400.76
Iteration:    860, Loss function: 6.179, Average Loss: 3.957, avg. samples / sec: 24365.49
Iteration:    860, Loss function: 6.480, Average Loss: 3.956, avg. samples / sec: 24371.05
Iteration:    860, Loss function: 5.786, Average Loss: 3.965, avg. samples / sec: 24364.42
Iteration:    860, Loss function: 5.559, Average Loss: 3.962, avg. samples / sec: 24398.89
Iteration:    860, Loss function: 5.188, Average Loss: 3.958, avg. samples / sec: 24334.04

:::MLPv0.5.0 ssd 1541757478.271058321 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 861, "value": 0.15306666666666666}

:::MLPv0.5.0 ssd 1541757478.355391502 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 862, "value": 0.15324444444444446}

:::MLPv0.5.0 ssd 1541757478.439561367 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 863, "value": 0.15342222222222224}

:::MLPv0.5.0 ssd 1541757478.523510933 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 864, "value": 0.15360000000000001}

:::MLPv0.5.0 ssd 1541757478.607600689 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 865, "value": 0.1537777777777778}

:::MLPv0.5.0 ssd 1541757478.691539526 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 866, "value": 0.15395555555555557}

:::MLPv0.5.0 ssd 1541757478.773601770 (train.py:553) train_epoch: 15

:::MLPv0.5.0 ssd 1541757478.777812243 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 867, "value": 0.15413333333333334}

:::MLPv0.5.0 ssd 1541757478.861805677 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 868, "value": 0.15431111111111112}

:::MLPv0.5.0 ssd 1541757478.945909500 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 869, "value": 0.1544888888888889}

:::MLPv0.5.0 ssd 1541757479.030303955 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 870, "value": 0.15466666666666667}

:::MLPv0.5.0 ssd 1541757479.114347935 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 871, "value": 0.15484444444444445}

:::MLPv0.5.0 ssd 1541757479.198613167 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 872, "value": 0.15502222222222223}

:::MLPv0.5.0 ssd 1541757479.282698393 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 873, "value": 0.1552}

:::MLPv0.5.0 ssd 1541757479.367192030 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 874, "value": 0.15537777777777778}

:::MLPv0.5.0 ssd 1541757479.451254845 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 875, "value": 0.15555555555555556}

:::MLPv0.5.0 ssd 1541757479.535012245 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 876, "value": 0.15573333333333333}

:::MLPv0.5.0 ssd 1541757479.618864059 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 877, "value": 0.1559111111111111}

:::MLPv0.5.0 ssd 1541757479.703423977 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 878, "value": 0.1560888888888889}

:::MLPv0.5.0 ssd 1541757479.787571907 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 879, "value": 0.15626666666666666}

:::MLPv0.5.0 ssd 1541757479.871957302 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 880, "value": 0.15644444444444444}
Iteration:    880, Loss function: 4.467, Average Loss: 3.988, avg. samples / sec: 24323.86
Iteration:    880, Loss function: 5.049, Average Loss: 3.983, avg. samples / sec: 24316.12
Iteration:    880, Loss function: 5.247, Average Loss: 3.983, avg. samples / sec: 24321.47
Iteration:    880, Loss function: 5.225, Average Loss: 3.989, avg. samples / sec: 24305.78
Iteration:    880, Loss function: 5.188, Average Loss: 3.993, avg. samples / sec: 24313.91
Iteration:    880, Loss function: 4.988, Average Loss: 3.987, avg. samples / sec: 24314.13
Iteration:    880, Loss function: 5.085, Average Loss: 3.983, avg. samples / sec: 24301.19
Iteration:    880, Loss function: 5.185, Average Loss: 3.988, avg. samples / sec: 24310.96

:::MLPv0.5.0 ssd 1541757479.956549406 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 881, "value": 0.15662222222222222}

:::MLPv0.5.0 ssd 1541757480.040185928 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 882, "value": 0.1568}

:::MLPv0.5.0 ssd 1541757480.124334335 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 883, "value": 0.15697777777777777}

:::MLPv0.5.0 ssd 1541757480.208137274 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 884, "value": 0.15715555555555555}

:::MLPv0.5.0 ssd 1541757480.292225122 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 885, "value": 0.15733333333333333}

:::MLPv0.5.0 ssd 1541757480.376425266 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 886, "value": 0.1575111111111111}

:::MLPv0.5.0 ssd 1541757480.460894108 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 887, "value": 0.15768888888888888}

:::MLPv0.5.0 ssd 1541757480.545124769 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 888, "value": 0.15786666666666668}

:::MLPv0.5.0 ssd 1541757480.629248142 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 889, "value": 0.15804444444444446}

:::MLPv0.5.0 ssd 1541757480.713359118 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 890, "value": 0.15822222222222224}

:::MLPv0.5.0 ssd 1541757480.797467470 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 891, "value": 0.1584}

:::MLPv0.5.0 ssd 1541757480.881555319 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 892, "value": 0.1585777777777778}

:::MLPv0.5.0 ssd 1541757480.965659142 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 893, "value": 0.15875555555555557}

:::MLPv0.5.0 ssd 1541757481.049695492 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 894, "value": 0.15893333333333334}

:::MLPv0.5.0 ssd 1541757481.134262800 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 895, "value": 0.15911111111111112}

:::MLPv0.5.0 ssd 1541757481.218372107 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 896, "value": 0.1592888888888889}

:::MLPv0.5.0 ssd 1541757481.302435160 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 897, "value": 0.15946666666666667}

:::MLPv0.5.0 ssd 1541757481.386306286 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 898, "value": 0.15964444444444445}

:::MLPv0.5.0 ssd 1541757481.470352650 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 899, "value": 0.15982222222222223}
Iteration:    900, Loss function: 4.466, Average Loss: 4.000, avg. samples / sec: 24372.77
Iteration:    900, Loss function: 4.786, Average Loss: 4.001, avg. samples / sec: 24367.18
Iteration:    900, Loss function: 5.035, Average Loss: 4.007, avg. samples / sec: 24360.12
Iteration:    900, Loss function: 4.948, Average Loss: 4.003, avg. samples / sec: 24391.60
Iteration:    900, Loss function: 4.687, Average Loss: 4.006, avg. samples / sec: 24406.32
Iteration:    900, Loss function: 5.547, Average Loss: 4.010, avg. samples / sec: 24361.06
Iteration:    900, Loss function: 5.069, Average Loss: 4.012, avg. samples / sec: 24344.47
Iteration:    900, Loss function: 4.665, Average Loss: 4.003, avg. samples / sec: 24341.80
Iteration:    920, Loss function: 5.085, Average Loss: 4.028, avg. samples / sec: 24527.45
Iteration:    920, Loss function: 4.875, Average Loss: 4.017, avg. samples / sec: 24520.18
Iteration:    920, Loss function: 5.311, Average Loss: 4.024, avg. samples / sec: 24531.93
Iteration:    920, Loss function: 5.493, Average Loss: 4.020, avg. samples / sec: 24522.09
Iteration:    920, Loss function: 4.384, Average Loss: 4.027, avg. samples / sec: 24528.25
Iteration:    920, Loss function: 5.870, Average Loss: 4.029, avg. samples / sec: 24546.09
Iteration:    920, Loss function: 5.455, Average Loss: 4.023, avg. samples / sec: 24547.06
Iteration:    920, Loss function: 5.622, Average Loss: 4.023, avg. samples / sec: 24476.57

:::MLPv0.5.0 ssd 1541757483.638066292 (train.py:553) train_epoch: 16
Iteration:    940, Loss function: 5.405, Average Loss: 4.048, avg. samples / sec: 24546.90
Iteration:    940, Loss function: 5.077, Average Loss: 4.040, avg. samples / sec: 24541.45
Iteration:    940, Loss function: 5.298, Average Loss: 4.041, avg. samples / sec: 24544.33
Iteration:    940, Loss function: 5.434, Average Loss: 4.053, avg. samples / sec: 24560.71
Iteration:    940, Loss function: 5.105, Average Loss: 4.047, avg. samples / sec: 24539.63
Iteration:    940, Loss function: 4.614, Average Loss: 4.044, avg. samples / sec: 24540.51
Iteration:    940, Loss function: 5.023, Average Loss: 4.044, avg. samples / sec: 24518.19
Iteration:    940, Loss function: 4.560, Average Loss: 4.044, avg. samples / sec: 24481.08
Iteration:    960, Loss function: 4.231, Average Loss: 4.059, avg. samples / sec: 24613.20
Iteration:    960, Loss function: 4.759, Average Loss: 4.062, avg. samples / sec: 24570.13
Iteration:    960, Loss function: 4.712, Average Loss: 4.063, avg. samples / sec: 24542.13
Iteration:    960, Loss function: 4.883, Average Loss: 4.056, avg. samples / sec: 24543.25
Iteration:    960, Loss function: 4.927, Average Loss: 4.058, avg. samples / sec: 24513.14
Iteration:    960, Loss function: 4.750, Average Loss: 4.068, avg. samples / sec: 24515.08
Iteration:    960, Loss function: 4.784, Average Loss: 4.062, avg. samples / sec: 24557.85
Iteration:    960, Loss function: 4.381, Average Loss: 4.059, avg. samples / sec: 24541.08
Iteration:    980, Loss function: 4.465, Average Loss: 4.077, avg. samples / sec: 24590.14
Iteration:    980, Loss function: 4.688, Average Loss: 4.073, avg. samples / sec: 24625.59
Iteration:    980, Loss function: 5.102, Average Loss: 4.076, avg. samples / sec: 24586.94
Iteration:    980, Loss function: 4.791, Average Loss: 4.074, avg. samples / sec: 24581.24
Iteration:    980, Loss function: 5.138, Average Loss: 4.072, avg. samples / sec: 24589.44
Iteration:    980, Loss function: 4.706, Average Loss: 4.082, avg. samples / sec: 24626.46
Iteration:    980, Loss function: 6.079, Average Loss: 4.076, avg. samples / sec: 24609.83
Iteration:    980, Loss function: 5.031, Average Loss: 4.076, avg. samples / sec: 24590.08

:::MLPv0.5.0 ssd 1541757488.392099619 (train.py:553) train_epoch: 17
Iteration:   1000, Loss function: 5.007, Average Loss: 4.095, avg. samples / sec: 24518.15
Iteration:   1000, Loss function: 4.517, Average Loss: 4.093, avg. samples / sec: 24514.36
Iteration:   1000, Loss function: 5.137, Average Loss: 4.097, avg. samples / sec: 24506.78
Iteration:   1000, Loss function: 5.161, Average Loss: 4.101, avg. samples / sec: 24514.88
Iteration:   1000, Loss function: 4.341, Average Loss: 4.095, avg. samples / sec: 24539.20
Iteration:   1000, Loss function: 4.768, Average Loss: 4.089, avg. samples / sec: 24502.23
Iteration:   1000, Loss function: 4.784, Average Loss: 4.092, avg. samples / sec: 24472.09
Iteration:   1000, Loss function: 4.697, Average Loss: 4.094, avg. samples / sec: 24512.39
Iteration:   1020, Loss function: 4.718, Average Loss: 4.103, avg. samples / sec: 24553.42
Iteration:   1020, Loss function: 4.592, Average Loss: 4.107, avg. samples / sec: 24586.46
Iteration:   1020, Loss function: 5.205, Average Loss: 4.107, avg. samples / sec: 24570.82
Iteration:   1020, Loss function: 4.590, Average Loss: 4.108, avg. samples / sec: 24536.73
Iteration:   1020, Loss function: 4.270, Average Loss: 4.108, avg. samples / sec: 24528.08
Iteration:   1020, Loss function: 4.876, Average Loss: 4.107, avg. samples / sec: 24525.25
Iteration:   1020, Loss function: 4.961, Average Loss: 4.114, avg. samples / sec: 24527.31
Iteration:   1020, Loss function: 4.479, Average Loss: 4.109, avg. samples / sec: 24511.66

:::MLPv0.5.0 ssd 1541757493.234029531 (train.py:553) train_epoch: 18
Iteration:   1040, Loss function: 4.848, Average Loss: 4.122, avg. samples / sec: 24539.89
Iteration:   1040, Loss function: 4.317, Average Loss: 4.127, avg. samples / sec: 24544.94
Iteration:   1040, Loss function: 4.583, Average Loss: 4.119, avg. samples / sec: 24524.29
Iteration:   1040, Loss function: 4.767, Average Loss: 4.117, avg. samples / sec: 24504.59
Iteration:   1040, Loss function: 4.998, Average Loss: 4.121, avg. samples / sec: 24506.94
Iteration:   1040, Loss function: 4.287, Average Loss: 4.121, avg. samples / sec: 24495.15
Iteration:   1040, Loss function: 4.948, Average Loss: 4.118, avg. samples / sec: 24488.67
Iteration:   1040, Loss function: 4.688, Average Loss: 4.122, avg. samples / sec: 24510.75
Iteration:   1060, Loss function: 4.630, Average Loss: 4.135, avg. samples / sec: 24526.53
Iteration:   1060, Loss function: 4.661, Average Loss: 4.134, avg. samples / sec: 24534.08
Iteration:   1060, Loss function: 5.036, Average Loss: 4.130, avg. samples / sec: 24574.28
Iteration:   1060, Loss function: 4.661, Average Loss: 4.134, avg. samples / sec: 24550.82
Iteration:   1060, Loss function: 4.934, Average Loss: 4.140, avg. samples / sec: 24518.26
Iteration:   1060, Loss function: 4.490, Average Loss: 4.135, avg. samples / sec: 24548.46
Iteration:   1060, Loss function: 4.835, Average Loss: 4.131, avg. samples / sec: 24505.90
Iteration:   1060, Loss function: 4.412, Average Loss: 4.133, avg. samples / sec: 24505.80
Iteration:   1080, Loss function: 4.789, Average Loss: 4.145, avg. samples / sec: 24541.60
Iteration:   1080, Loss function: 5.054, Average Loss: 4.146, avg. samples / sec: 24545.85
Iteration:   1080, Loss function: 4.899, Average Loss: 4.143, avg. samples / sec: 24590.88
Iteration:   1080, Loss function: 4.197, Average Loss: 4.146, avg. samples / sec: 24544.46
Iteration:   1080, Loss function: 4.391, Average Loss: 4.144, avg. samples / sec: 24601.06
Iteration:   1080, Loss function: 4.160, Average Loss: 4.152, avg. samples / sec: 24546.47
Iteration:   1080, Loss function: 4.394, Average Loss: 4.140, avg. samples / sec: 24502.41
Iteration:   1080, Loss function: 4.729, Average Loss: 4.150, avg. samples / sec: 24529.46

:::MLPv0.5.0 ssd 1541757498.078246832 (train.py:553) train_epoch: 19
Iteration:   1100, Loss function: 4.602, Average Loss: 4.154, avg. samples / sec: 24501.68
Iteration:   1100, Loss function: 4.161, Average Loss: 4.154, avg. samples / sec: 24495.45
Iteration:   1100, Loss function: 4.781, Average Loss: 4.152, avg. samples / sec: 24495.09
Iteration:   1100, Loss function: 4.308, Average Loss: 4.153, avg. samples / sec: 24489.95
Iteration:   1100, Loss function: 5.128, Average Loss: 4.149, avg. samples / sec: 24534.57
Iteration:   1100, Loss function: 4.881, Average Loss: 4.161, avg. samples / sec: 24493.13
Iteration:   1100, Loss function: 5.275, Average Loss: 4.153, avg. samples / sec: 24463.83
Iteration:   1100, Loss function: 4.650, Average Loss: 4.157, avg. samples / sec: 24498.34
Iteration:   1120, Loss function: 4.222, Average Loss: 4.163, avg. samples / sec: 24574.62
Iteration:   1120, Loss function: 4.995, Average Loss: 4.167, avg. samples / sec: 24570.27
Iteration:   1120, Loss function: 4.418, Average Loss: 4.171, avg. samples / sec: 24581.09
Iteration:   1120, Loss function: 4.902, Average Loss: 4.164, avg. samples / sec: 24605.63
Iteration:   1120, Loss function: 4.922, Average Loss: 4.166, avg. samples / sec: 24569.46
Iteration:   1120, Loss function: 4.867, Average Loss: 4.167, avg. samples / sec: 24556.40
Iteration:   1120, Loss function: 4.785, Average Loss: 4.159, avg. samples / sec: 24559.60
Iteration:   1120, Loss function: 4.741, Average Loss: 4.169, avg. samples / sec: 24560.88
Iteration:   1140, Loss function: 4.257, Average Loss: 4.175, avg. samples / sec: 24561.10
Iteration:   1140, Loss function: 4.505, Average Loss: 4.179, avg. samples / sec: 24574.05
Iteration:   1140, Loss function: 4.494, Average Loss: 4.169, avg. samples / sec: 24555.55
Iteration:   1140, Loss function: 4.866, Average Loss: 4.176, avg. samples / sec: 24562.67
Iteration:   1140, Loss function: 4.412, Average Loss: 4.170, avg. samples / sec: 24571.06
Iteration:   1140, Loss function: 4.419, Average Loss: 4.175, avg. samples / sec: 24584.53
Iteration:   1140, Loss function: 4.423, Average Loss: 4.174, avg. samples / sec: 24526.88
Iteration:   1140, Loss function: 4.342, Average Loss: 4.179, avg. samples / sec: 24516.57

:::MLPv0.5.0 ssd 1541757502.918817043 (train.py:553) train_epoch: 20
Iteration:   1160, Loss function: 4.823, Average Loss: 4.177, avg. samples / sec: 24486.77
Iteration:   1160, Loss function: 5.966, Average Loss: 4.185, avg. samples / sec: 24483.15
Iteration:   1160, Loss function: 4.576, Average Loss: 4.183, avg. samples / sec: 24487.77
Iteration:   1160, Loss function: 4.730, Average Loss: 4.180, avg. samples / sec: 24481.08
Iteration:   1160, Loss function: 4.554, Average Loss: 4.184, avg. samples / sec: 24467.79
Iteration:   1160, Loss function: 5.047, Average Loss: 4.183, avg. samples / sec: 24480.70
Iteration:   1160, Loss function: 4.819, Average Loss: 4.181, avg. samples / sec: 24474.34
Iteration:   1160, Loss function: 4.627, Average Loss: 4.186, avg. samples / sec: 24482.94
Iteration:   1180, Loss function: 5.288, Average Loss: 4.190, avg. samples / sec: 24540.38
Iteration:   1180, Loss function: 4.704, Average Loss: 4.196, avg. samples / sec: 24539.82
Iteration:   1180, Loss function: 4.511, Average Loss: 4.195, avg. samples / sec: 24540.72
Iteration:   1180, Loss function: 5.096, Average Loss: 4.198, avg. samples / sec: 24582.40
Iteration:   1180, Loss function: 5.070, Average Loss: 4.199, avg. samples / sec: 24564.71
Iteration:   1180, Loss function: 4.638, Average Loss: 4.196, avg. samples / sec: 24530.83
Iteration:   1180, Loss function: 4.214, Average Loss: 4.194, avg. samples / sec: 24522.75
Iteration:   1180, Loss function: 4.950, Average Loss: 4.193, avg. samples / sec: 24536.60
Iteration:   1200, Loss function: 4.964, Average Loss: 4.207, avg. samples / sec: 24528.76
Iteration:   1200, Loss function: 4.401, Average Loss: 4.201, avg. samples / sec: 24548.48
Iteration:   1200, Loss function: 4.562, Average Loss: 4.201, avg. samples / sec: 24520.98
Iteration:   1200, Loss function: 4.707, Average Loss: 4.204, avg. samples / sec: 24542.88
Iteration:   1200, Loss function: 5.170, Average Loss: 4.206, avg. samples / sec: 24526.87
Iteration:   1200, Loss function: 4.304, Average Loss: 4.207, avg. samples / sec: 24513.11
Iteration:   1200, Loss function: 4.220, Average Loss: 4.204, avg. samples / sec: 24545.57
Iteration:   1200, Loss function: 4.700, Average Loss: 4.197, avg. samples / sec: 24490.37

:::MLPv0.5.0 ssd 1541757507.677946091 (train.py:553) train_epoch: 21
Iteration:   1220, Loss function: 4.630, Average Loss: 4.207, avg. samples / sec: 24512.22
Iteration:   1220, Loss function: 4.497, Average Loss: 4.211, avg. samples / sec: 24510.75
Iteration:   1220, Loss function: 4.255, Average Loss: 4.216, avg. samples / sec: 24513.65
Iteration:   1220, Loss function: 3.781, Average Loss: 4.204, avg. samples / sec: 24502.98
Iteration:   1220, Loss function: 3.461, Average Loss: 4.212, avg. samples / sec: 24500.92
Iteration:   1220, Loss function: 4.478, Average Loss: 4.204, avg. samples / sec: 24523.24
Iteration:   1220, Loss function: 4.443, Average Loss: 4.211, avg. samples / sec: 24496.72
Iteration:   1220, Loss function: 4.201, Average Loss: 4.212, avg. samples / sec: 24474.52
Iteration:   1240, Loss function: 4.533, Average Loss: 4.212, avg. samples / sec: 24560.36
Iteration:   1240, Loss function: 4.492, Average Loss: 4.218, avg. samples / sec: 24554.93
Iteration:   1240, Loss function: 4.428, Average Loss: 4.223, avg. samples / sec: 24553.68
Iteration:   1240, Loss function: 4.511, Average Loss: 4.218, avg. samples / sec: 24565.75
Iteration:   1240, Loss function: 4.436, Average Loss: 4.212, avg. samples / sec: 24547.30
Iteration:   1240, Loss function: 4.532, Average Loss: 4.220, avg. samples / sec: 24587.49
Iteration:   1240, Loss function: 4.577, Average Loss: 4.212, avg. samples / sec: 24542.33
Iteration:   1240, Loss function: 5.120, Average Loss: 4.221, avg. samples / sec: 24503.92
Iteration:   1260, Loss function: 5.154, Average Loss: 4.228, avg. samples / sec: 24554.49
Iteration:   1260, Loss function: 5.380, Average Loss: 4.223, avg. samples / sec: 24529.67
Iteration:   1260, Loss function: 3.746, Average Loss: 4.216, avg. samples / sec: 24531.75
Iteration:   1260, Loss function: 5.021, Average Loss: 4.230, avg. samples / sec: 24582.86
Iteration:   1260, Loss function: 4.015, Average Loss: 4.219, avg. samples / sec: 24525.91
Iteration:   1260, Loss function: 4.394, Average Loss: 4.218, avg. samples / sec: 24549.85
Iteration:   1260, Loss function: 4.335, Average Loss: 4.230, avg. samples / sec: 24523.37
Iteration:   1260, Loss function: 4.368, Average Loss: 4.224, avg. samples / sec: 24521.07

:::MLPv0.5.0 ssd 1541757512.520674706 (train.py:553) train_epoch: 22
Iteration:   1280, Loss function: 4.527, Average Loss: 4.236, avg. samples / sec: 24572.99
Iteration:   1280, Loss function: 4.870, Average Loss: 4.224, avg. samples / sec: 24575.82
Iteration:   1280, Loss function: 4.412, Average Loss: 4.224, avg. samples / sec: 24566.65
Iteration:   1280, Loss function: 4.533, Average Loss: 4.231, avg. samples / sec: 24563.37
Iteration:   1280, Loss function: 4.783, Average Loss: 4.237, avg. samples / sec: 24571.52
Iteration:   1280, Loss function: 4.423, Average Loss: 4.223, avg. samples / sec: 24560.44
Iteration:   1280, Loss function: 4.430, Average Loss: 4.235, avg. samples / sec: 24541.79
Iteration:   1280, Loss function: 4.570, Average Loss: 4.232, avg. samples / sec: 24524.89
Iteration:   1300, Loss function: 4.293, Average Loss: 4.238, avg. samples / sec: 24570.78
Iteration:   1300, Loss function: 4.737, Average Loss: 4.237, avg. samples / sec: 24603.05
Iteration:   1300, Loss function: 4.011, Average Loss: 4.239, avg. samples / sec: 24578.07
Iteration:   1300, Loss function: 4.261, Average Loss: 4.230, avg. samples / sec: 24556.64
Iteration:   1300, Loss function: 4.198, Average Loss: 4.225, avg. samples / sec: 24563.80
Iteration:   1300, Loss function: 4.016, Average Loss: 4.236, avg. samples / sec: 24550.01
Iteration:   1300, Loss function: 4.756, Average Loss: 4.234, avg. samples / sec: 24599.20
Iteration:   1300, Loss function: 4.227, Average Loss: 4.228, avg. samples / sec: 24524.73
Iteration:   1320, Loss function: 4.086, Average Loss: 4.240, avg. samples / sec: 24583.91
Iteration:   1320, Loss function: 4.917, Average Loss: 4.232, avg. samples / sec: 24598.05
Iteration:   1320, Loss function: 4.666, Average Loss: 4.243, avg. samples / sec: 24609.80
Iteration:   1320, Loss function: 5.008, Average Loss: 4.245, avg. samples / sec: 24575.88
Iteration:   1320, Loss function: 3.976, Average Loss: 4.239, avg. samples / sec: 24583.97
Iteration:   1320, Loss function: 4.603, Average Loss: 4.235, avg. samples / sec: 24567.32
Iteration:   1320, Loss function: 4.340, Average Loss: 4.236, avg. samples / sec: 24594.35
Iteration:   1320, Loss function: 4.601, Average Loss: 4.241, avg. samples / sec: 24523.88

:::MLPv0.5.0 ssd 1541757517.354316711 (train.py:553) train_epoch: 23
Iteration:   1340, Loss function: 4.130, Average Loss: 4.245, avg. samples / sec: 24551.05
Iteration:   1340, Loss function: 4.325, Average Loss: 4.234, avg. samples / sec: 24550.65
Iteration:   1340, Loss function: 4.485, Average Loss: 4.241, avg. samples / sec: 24584.24
Iteration:   1340, Loss function: 4.553, Average Loss: 4.243, avg. samples / sec: 24570.66
Iteration:   1340, Loss function: 3.915, Average Loss: 4.239, avg. samples / sec: 24544.94
Iteration:   1340, Loss function: 4.450, Average Loss: 4.245, avg. samples / sec: 24504.09
Iteration:   1340, Loss function: 4.335, Average Loss: 4.244, avg. samples / sec: 24549.34
Iteration:   1340, Loss function: 4.297, Average Loss: 4.242, avg. samples / sec: 24480.48
Iteration:   1360, Loss function: 5.178, Average Loss: 4.252, avg. samples / sec: 24482.51
Iteration:   1360, Loss function: 4.729, Average Loss: 4.247, avg. samples / sec: 24541.76
Iteration:   1360, Loss function: 5.243, Average Loss: 4.249, avg. samples / sec: 24510.26
Iteration:   1360, Loss function: 4.494, Average Loss: 4.249, avg. samples / sec: 24482.38
Iteration:   1360, Loss function: 4.732, Average Loss: 4.249, avg. samples / sec: 24464.33
Iteration:   1360, Loss function: 4.834, Average Loss: 4.250, avg. samples / sec: 24514.30
Iteration:   1360, Loss function: 5.255, Average Loss: 4.240, avg. samples / sec: 24459.01
Iteration:   1360, Loss function: 4.637, Average Loss: 4.250, avg. samples / sec: 24484.19
Iteration:   1380, Loss function: 4.792, Average Loss: 4.255, avg. samples / sec: 24627.54
Iteration:   1380, Loss function: 4.073, Average Loss: 4.256, avg. samples / sec: 24572.11
Iteration:   1380, Loss function: 4.320, Average Loss: 4.254, avg. samples / sec: 24594.33
Iteration:   1380, Loss function: 4.309, Average Loss: 4.254, avg. samples / sec: 24581.37
Iteration:   1380, Loss function: 4.461, Average Loss: 4.255, avg. samples / sec: 24555.70
Iteration:   1380, Loss function: 4.308, Average Loss: 4.247, avg. samples / sec: 24575.33
Iteration:   1380, Loss function: 4.167, Average Loss: 4.250, avg. samples / sec: 24552.84
Iteration:   1380, Loss function: 4.046, Average Loss: 4.250, avg. samples / sec: 24546.39

:::MLPv0.5.0 ssd 1541757522.196069002 (train.py:553) train_epoch: 24
Iteration:   1400, Loss function: 4.550, Average Loss: 4.255, avg. samples / sec: 24535.27
Iteration:   1400, Loss function: 4.686, Average Loss: 4.258, avg. samples / sec: 24500.53
Iteration:   1400, Loss function: 4.330, Average Loss: 4.259, avg. samples / sec: 24509.66
Iteration:   1400, Loss function: 5.035, Average Loss: 4.260, avg. samples / sec: 24485.16
Iteration:   1400, Loss function: 4.682, Average Loss: 4.250, avg. samples / sec: 24498.67
Iteration:   1400, Loss function: 4.072, Average Loss: 4.252, avg. samples / sec: 24496.97
Iteration:   1400, Loss function: 4.348, Average Loss: 4.259, avg. samples / sec: 24458.50
Iteration:   1400, Loss function: 3.733, Average Loss: 4.255, avg. samples / sec: 24459.98
Iteration:   1420, Loss function: 4.393, Average Loss: 4.257, avg. samples / sec: 24615.48
Iteration:   1420, Loss function: 4.084, Average Loss: 4.261, avg. samples / sec: 24555.47
Iteration:   1420, Loss function: 3.634, Average Loss: 4.261, avg. samples / sec: 24595.15
Iteration:   1420, Loss function: 4.501, Average Loss: 4.261, avg. samples / sec: 24565.70
Iteration:   1420, Loss function: 4.148, Average Loss: 4.252, avg. samples / sec: 24570.05
Iteration:   1420, Loss function: 4.415, Average Loss: 4.256, avg. samples / sec: 24572.04
Iteration:   1420, Loss function: 4.154, Average Loss: 4.256, avg. samples / sec: 24511.28
Iteration:   1420, Loss function: 4.131, Average Loss: 4.262, avg. samples / sec: 24534.79
Iteration:   1440, Loss function: 4.453, Average Loss: 4.262, avg. samples / sec: 24551.55
Iteration:   1440, Loss function: 4.791, Average Loss: 4.263, avg. samples / sec: 24541.25
Iteration:   1440, Loss function: 4.425, Average Loss: 4.263, avg. samples / sec: 24545.65
Iteration:   1440, Loss function: 4.101, Average Loss: 4.253, avg. samples / sec: 24549.67
Iteration:   1440, Loss function: 4.595, Average Loss: 4.264, avg. samples / sec: 24572.04
Iteration:   1440, Loss function: 4.286, Average Loss: 4.258, avg. samples / sec: 24540.80
Iteration:   1440, Loss function: 4.384, Average Loss: 4.260, avg. samples / sec: 24502.24
Iteration:   1440, Loss function: 4.366, Average Loss: 4.259, avg. samples / sec: 24526.75

:::MLPv0.5.0 ssd 1541757526.954356909 (train.py:553) train_epoch: 25
Iteration:   1460, Loss function: 4.638, Average Loss: 4.269, avg. samples / sec: 24574.04
Iteration:   1460, Loss function: 3.913, Average Loss: 4.266, avg. samples / sec: 24611.73
Iteration:   1460, Loss function: 4.239, Average Loss: 4.269, avg. samples / sec: 24568.37
Iteration:   1460, Loss function: 4.824, Average Loss: 4.270, avg. samples / sec: 24564.13
Iteration:   1460, Loss function: 4.624, Average Loss: 4.261, avg. samples / sec: 24570.81
Iteration:   1460, Loss function: 4.049, Average Loss: 4.268, avg. samples / sec: 24573.87
Iteration:   1460, Loss function: 4.526, Average Loss: 4.267, avg. samples / sec: 24571.68
Iteration:   1460, Loss function: 4.118, Average Loss: 4.266, avg. samples / sec: 24598.84
Iteration:   1480, Loss function: 4.438, Average Loss: 4.269, avg. samples / sec: 24570.07
Iteration:   1480, Loss function: 4.276, Average Loss: 4.267, avg. samples / sec: 24577.44
Iteration:   1480, Loss function: 4.445, Average Loss: 4.262, avg. samples / sec: 24575.40
Iteration:   1480, Loss function: 4.097, Average Loss: 4.270, avg. samples / sec: 24570.02
Iteration:   1480, Loss function: 3.979, Average Loss: 4.272, avg. samples / sec: 24569.91
Iteration:   1480, Loss function: 3.709, Average Loss: 4.266, avg. samples / sec: 24573.29
Iteration:   1480, Loss function: 4.273, Average Loss: 4.269, avg. samples / sec: 24574.83
Iteration:   1480, Loss function: 4.214, Average Loss: 4.268, avg. samples / sec: 24520.77
Iteration:   1500, Loss function: 4.747, Average Loss: 4.273, avg. samples / sec: 24585.86
Iteration:   1500, Loss function: 4.532, Average Loss: 4.274, avg. samples / sec: 24611.50
Iteration:   1500, Loss function: 4.240, Average Loss: 4.268, avg. samples / sec: 24564.39
Iteration:   1500, Loss function: 4.620, Average Loss: 4.276, avg. samples / sec: 24566.13
Iteration:   1500, Loss function: 4.250, Average Loss: 4.276, avg. samples / sec: 24579.57
Iteration:   1500, Loss function: 4.154, Average Loss: 4.279, avg. samples / sec: 24561.57
Iteration:   1500, Loss function: 4.384, Average Loss: 4.272, avg. samples / sec: 24548.52
Iteration:   1500, Loss function: 3.925, Average Loss: 4.273, avg. samples / sec: 24500.77

:::MLPv0.5.0 ssd 1541757531.789699316 (train.py:553) train_epoch: 26
Iteration:   1520, Loss function: 4.675, Average Loss: 4.274, avg. samples / sec: 24540.20
Iteration:   1520, Loss function: 4.795, Average Loss: 4.275, avg. samples / sec: 24547.30
Iteration:   1520, Loss function: 4.564, Average Loss: 4.272, avg. samples / sec: 24539.83
Iteration:   1520, Loss function: 4.138, Average Loss: 4.276, avg. samples / sec: 24599.77
Iteration:   1520, Loss function: 4.453, Average Loss: 4.272, avg. samples / sec: 24551.73
Iteration:   1520, Loss function: 4.115, Average Loss: 4.276, avg. samples / sec: 24514.72
Iteration:   1520, Loss function: 4.083, Average Loss: 4.281, avg. samples / sec: 24513.67
Iteration:   1520, Loss function: 4.135, Average Loss: 4.276, avg. samples / sec: 24491.87
Iteration:   1540, Loss function: 4.700, Average Loss: 4.278, avg. samples / sec: 24589.93
Iteration:   1540, Loss function: 4.234, Average Loss: 4.277, avg. samples / sec: 24647.84
Iteration:   1540, Loss function: 4.683, Average Loss: 4.277, avg. samples / sec: 24594.54
Iteration:   1540, Loss function: 4.411, Average Loss: 4.282, avg. samples / sec: 24621.53
Iteration:   1540, Loss function: 4.412, Average Loss: 4.273, avg. samples / sec: 24587.86
Iteration:   1540, Loss function: 4.692, Average Loss: 4.275, avg. samples / sec: 24583.96
Iteration:   1540, Loss function: 4.921, Average Loss: 4.279, avg. samples / sec: 24574.40
Iteration:   1540, Loss function: 4.513, Average Loss: 4.274, avg. samples / sec: 24533.13

:::MLPv0.5.0 ssd 1541757536.625030756 (train.py:553) train_epoch: 27
Iteration:   1560, Loss function: 4.088, Average Loss: 4.283, avg. samples / sec: 24558.67
Iteration:   1560, Loss function: 4.615, Average Loss: 4.286, avg. samples / sec: 24560.00
Iteration:   1560, Loss function: 4.565, Average Loss: 4.280, avg. samples / sec: 24558.67
Iteration:   1560, Loss function: 4.084, Average Loss: 4.275, avg. samples / sec: 24608.87
Iteration:   1560, Loss function: 3.939, Average Loss: 4.277, avg. samples / sec: 24558.05
Iteration:   1560, Loss function: 4.892, Average Loss: 4.280, avg. samples / sec: 24541.02
Iteration:   1560, Loss function: 4.083, Average Loss: 4.283, avg. samples / sec: 24554.32
Iteration:   1560, Loss function: 4.573, Average Loss: 4.275, avg. samples / sec: 24500.45
Iteration:   1580, Loss function: 4.413, Average Loss: 4.281, avg. samples / sec: 24539.13
Iteration:   1580, Loss function: 4.496, Average Loss: 4.275, avg. samples / sec: 24527.54
Iteration:   1580, Loss function: 4.671, Average Loss: 4.280, avg. samples / sec: 24524.21
Iteration:   1580, Loss function: 5.155, Average Loss: 4.286, avg. samples / sec: 24509.35
Iteration:   1580, Loss function: 4.750, Average Loss: 4.277, avg. samples / sec: 24575.34
Iteration:   1580, Loss function: 3.813, Average Loss: 4.276, avg. samples / sec: 24516.46
Iteration:   1580, Loss function: 4.403, Average Loss: 4.285, avg. samples / sec: 24540.10
Iteration:   1580, Loss function: 4.262, Average Loss: 4.287, avg. samples / sec: 24481.00
Iteration:   1600, Loss function: 4.262, Average Loss: 4.282, avg. samples / sec: 24525.68
Iteration:   1600, Loss function: 4.949, Average Loss: 4.286, avg. samples / sec: 24565.20
Iteration:   1600, Loss function: 4.366, Average Loss: 4.279, avg. samples / sec: 24523.85
Iteration:   1600, Loss function: 3.302, Average Loss: 4.286, avg. samples / sec: 24521.60
Iteration:   1600, Loss function: 4.596, Average Loss: 4.276, avg. samples / sec: 24498.59
Iteration:   1600, Loss function: 3.928, Average Loss: 4.276, avg. samples / sec: 24513.52
Iteration:   1600, Loss function: 4.534, Average Loss: 4.285, avg. samples / sec: 24521.84
Iteration:   1600, Loss function: 4.231, Average Loss: 4.277, avg. samples / sec: 24500.44

:::MLPv0.5.0 ssd 1541757541.468714237 (train.py:553) train_epoch: 28
Iteration:   1620, Loss function: 4.018, Average Loss: 4.278, avg. samples / sec: 24549.47
Iteration:   1620, Loss function: 4.291, Average Loss: 4.285, avg. samples / sec: 24548.90
Iteration:   1620, Loss function: 3.602, Average Loss: 4.278, avg. samples / sec: 24516.75
Iteration:   1620, Loss function: 3.991, Average Loss: 4.287, avg. samples / sec: 24513.39
Iteration:   1620, Loss function: 4.172, Average Loss: 4.285, avg. samples / sec: 24492.43
Iteration:   1620, Loss function: 4.469, Average Loss: 4.278, avg. samples / sec: 24495.20
Iteration:   1620, Loss function: 3.994, Average Loss: 4.285, avg. samples / sec: 24465.28
Iteration:   1620, Loss function: 4.330, Average Loss: 4.276, avg. samples / sec: 24497.21
Iteration:   1640, Loss function: 4.654, Average Loss: 4.287, avg. samples / sec: 24540.75
Iteration:   1640, Loss function: 4.286, Average Loss: 4.287, avg. samples / sec: 24590.88
Iteration:   1640, Loss function: 4.346, Average Loss: 4.277, avg. samples / sec: 24542.89
Iteration:   1640, Loss function: 4.051, Average Loss: 4.279, avg. samples / sec: 24581.94
Iteration:   1640, Loss function: 4.514, Average Loss: 4.285, avg. samples / sec: 24538.88
Iteration:   1640, Loss function: 4.737, Average Loss: 4.290, avg. samples / sec: 24527.11
Iteration:   1640, Loss function: 4.222, Average Loss: 4.279, avg. samples / sec: 24504.95
Iteration:   1640, Loss function: 4.224, Average Loss: 4.274, avg. samples / sec: 24544.94
Iteration:   1660, Loss function: 4.334, Average Loss: 4.283, avg. samples / sec: 24560.01
Iteration:   1660, Loss function: 4.515, Average Loss: 4.280, avg. samples / sec: 24562.04
Iteration:   1660, Loss function: 4.130, Average Loss: 4.280, avg. samples / sec: 24535.61
Iteration:   1660, Loss function: 4.813, Average Loss: 4.286, avg. samples / sec: 24505.15
Iteration:   1660, Loss function: 4.248, Average Loss: 4.290, avg. samples / sec: 24519.53
Iteration:   1660, Loss function: 4.176, Average Loss: 4.276, avg. samples / sec: 24486.34
Iteration:   1660, Loss function: 4.284, Average Loss: 4.273, avg. samples / sec: 24528.77
Iteration:   1660, Loss function: 3.969, Average Loss: 4.287, avg. samples / sec: 24481.94

:::MLPv0.5.0 ssd 1541757546.225882530 (train.py:553) train_epoch: 29
Iteration:   1680, Loss function: 4.383, Average Loss: 4.272, avg. samples / sec: 24606.64
Iteration:   1680, Loss function: 3.922, Average Loss: 4.285, avg. samples / sec: 24584.65
Iteration:   1680, Loss function: 4.626, Average Loss: 4.281, avg. samples / sec: 24552.40
Iteration:   1680, Loss function: 4.539, Average Loss: 4.278, avg. samples / sec: 24553.29
Iteration:   1680, Loss function: 4.855, Average Loss: 4.279, avg. samples / sec: 24531.42
Iteration:   1680, Loss function: 3.982, Average Loss: 4.287, avg. samples / sec: 24550.90
Iteration:   1680, Loss function: 4.885, Average Loss: 4.286, avg. samples / sec: 24557.79
Iteration:   1680, Loss function: 5.328, Average Loss: 4.274, avg. samples / sec: 24552.71
Iteration:   1700, Loss function: 4.401, Average Loss: 4.287, avg. samples / sec: 24571.72
Iteration:   1700, Loss function: 4.178, Average Loss: 4.284, avg. samples / sec: 24620.99
Iteration:   1700, Loss function: 3.954, Average Loss: 4.277, avg. samples / sec: 24571.97
Iteration:   1700, Loss function: 4.874, Average Loss: 4.277, avg. samples / sec: 24595.65
Iteration:   1700, Loss function: 4.422, Average Loss: 4.271, avg. samples / sec: 24559.22
Iteration:   1700, Loss function: 4.409, Average Loss: 4.287, avg. samples / sec: 24580.61
Iteration:   1700, Loss function: 4.365, Average Loss: 4.281, avg. samples / sec: 24535.91
Iteration:   1700, Loss function: 4.536, Average Loss: 4.275, avg. samples / sec: 24562.37
Iteration:   1720, Loss function: 3.738, Average Loss: 4.279, avg. samples / sec: 24582.03
Iteration:   1720, Loss function: 3.853, Average Loss: 4.284, avg. samples / sec: 24543.53
Iteration:   1720, Loss function: 4.440, Average Loss: 4.277, avg. samples / sec: 24549.62
Iteration:   1720, Loss function: 4.012, Average Loss: 4.274, avg. samples / sec: 24544.91
Iteration:   1720, Loss function: 3.723, Average Loss: 4.269, avg. samples / sec: 24543.91
Iteration:   1720, Loss function: 4.556, Average Loss: 4.283, avg. samples / sec: 24512.17
Iteration:   1720, Loss function: 3.852, Average Loss: 4.285, avg. samples / sec: 24543.95
Iteration:   1720, Loss function: 3.826, Average Loss: 4.272, avg. samples / sec: 24559.50

:::MLPv0.5.0 ssd 1541757551.063471317 (train.py:553) train_epoch: 30
Iteration:   1740, Loss function: 4.375, Average Loss: 4.279, avg. samples / sec: 24566.85
Iteration:   1740, Loss function: 4.256, Average Loss: 4.275, avg. samples / sec: 24566.56
Iteration:   1740, Loss function: 4.669, Average Loss: 4.283, avg. samples / sec: 24588.94
Iteration:   1740, Loss function: 4.643, Average Loss: 4.271, avg. samples / sec: 24591.25
Iteration:   1740, Loss function: 4.210, Average Loss: 4.284, avg. samples / sec: 24534.75
Iteration:   1740, Loss function: 4.435, Average Loss: 4.287, avg. samples / sec: 24550.61
Iteration:   1740, Loss function: 4.380, Average Loss: 4.269, avg. samples / sec: 24515.33
Iteration:   1740, Loss function: 4.561, Average Loss: 4.279, avg. samples / sec: 24482.70
Iteration:   1760, Loss function: 3.940, Average Loss: 4.280, avg. samples / sec: 24647.06
Iteration:   1760, Loss function: 4.589, Average Loss: 4.285, avg. samples / sec: 24590.45
Iteration:   1760, Loss function: 3.885, Average Loss: 4.283, avg. samples / sec: 24563.03
Iteration:   1760, Loss function: 4.079, Average Loss: 4.269, avg. samples / sec: 24613.42
Iteration:   1760, Loss function: 4.037, Average Loss: 4.271, avg. samples / sec: 24578.26
Iteration:   1760, Loss function: 4.263, Average Loss: 4.287, avg. samples / sec: 24598.63
Iteration:   1760, Loss function: 4.408, Average Loss: 4.282, avg. samples / sec: 24544.35
Iteration:   1760, Loss function: 3.762, Average Loss: 4.277, avg. samples / sec: 24545.39
Iteration:   1780, Loss function: 3.813, Average Loss: 4.279, avg. samples / sec: 24517.89
Iteration:   1780, Loss function: 4.467, Average Loss: 4.278, avg. samples / sec: 24528.24
Iteration:   1780, Loss function: 4.091, Average Loss: 4.278, avg. samples / sec: 24524.26
Iteration:   1780, Loss function: 3.733, Average Loss: 4.267, avg. samples / sec: 24501.79
Iteration:   1780, Loss function: 3.732, Average Loss: 4.279, avg. samples / sec: 24482.51
Iteration:   1780, Loss function: 4.187, Average Loss: 4.285, avg. samples / sec: 24499.97
Iteration:   1780, Loss function: 4.124, Average Loss: 4.283, avg. samples / sec: 24462.01
Iteration:   1780, Loss function: 4.116, Average Loss: 4.271, avg. samples / sec: 24459.90

:::MLPv0.5.0 ssd 1541757555.903736115 (train.py:553) train_epoch: 31
Iteration:   1800, Loss function: 4.330, Average Loss: 4.267, avg. samples / sec: 24534.08
Iteration:   1800, Loss function: 3.644, Average Loss: 4.277, avg. samples / sec: 24515.94
Iteration:   1800, Loss function: 4.009, Average Loss: 4.279, avg. samples / sec: 24508.24
Iteration:   1800, Loss function: 4.048, Average Loss: 4.277, avg. samples / sec: 24529.61
Iteration:   1800, Loss function: 3.969, Average Loss: 4.288, avg. samples / sec: 24529.37
Iteration:   1800, Loss function: 4.290, Average Loss: 4.283, avg. samples / sec: 24551.06
Iteration:   1800, Loss function: 4.296, Average Loss: 4.273, avg. samples / sec: 24539.74
Iteration:   1800, Loss function: 4.968, Average Loss: 4.279, avg. samples / sec: 24474.68
Iteration:   1820, Loss function: 4.388, Average Loss: 4.275, avg. samples / sec: 24579.44
Iteration:   1820, Loss function: 4.559, Average Loss: 4.279, avg. samples / sec: 24575.85
Iteration:   1820, Loss function: 4.696, Average Loss: 4.283, avg. samples / sec: 24577.86
Iteration:   1820, Loss function: 4.064, Average Loss: 4.287, avg. samples / sec: 24576.91
Iteration:   1820, Loss function: 4.439, Average Loss: 4.269, avg. samples / sec: 24559.66
Iteration:   1820, Loss function: 3.685, Average Loss: 4.277, avg. samples / sec: 24584.16
Iteration:   1820, Loss function: 4.072, Average Loss: 4.274, avg. samples / sec: 24546.49
Iteration:   1820, Loss function: 4.240, Average Loss: 4.276, avg. samples / sec: 24512.47
Iteration:   1840, Loss function: 4.110, Average Loss: 4.277, avg. samples / sec: 24577.68
Iteration:   1840, Loss function: 4.813, Average Loss: 4.270, avg. samples / sec: 24524.08
Iteration:   1840, Loss function: 4.878, Average Loss: 4.278, avg. samples / sec: 24537.81
Iteration:   1840, Loss function: 4.187, Average Loss: 4.280, avg. samples / sec: 24504.49
Iteration:   1840, Loss function: 4.404, Average Loss: 4.275, avg. samples / sec: 24500.76
Iteration:   1840, Loss function: 4.315, Average Loss: 4.276, avg. samples / sec: 24548.45
Iteration:   1840, Loss function: 4.151, Average Loss: 4.282, avg. samples / sec: 24488.25
Iteration:   1840, Loss function: 3.860, Average Loss: 4.288, avg. samples / sec: 24483.41

:::MLPv0.5.0 ssd 1541757560.745030403 (train.py:553) train_epoch: 32
Iteration:   1860, Loss function: 4.157, Average Loss: 4.278, avg. samples / sec: 24578.25
Iteration:   1860, Loss function: 4.154, Average Loss: 4.277, avg. samples / sec: 24552.15
Iteration:   1860, Loss function: 3.990, Average Loss: 4.275, avg. samples / sec: 24554.43
Iteration:   1860, Loss function: 3.220, Average Loss: 4.267, avg. samples / sec: 24509.35
Iteration:   1860, Loss function: 4.057, Average Loss: 4.283, avg. samples / sec: 24533.19
Iteration:   1860, Loss function: 3.969, Average Loss: 4.271, avg. samples / sec: 24519.95
Iteration:   1860, Loss function: 5.073, Average Loss: 4.278, avg. samples / sec: 24484.99
Iteration:   1860, Loss function: 4.054, Average Loss: 4.276, avg. samples / sec: 24476.86

































































:::MLPv0.5.0 ssd 1541757563.001009226 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757563.001540422 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757563.001988411 (train.py:220) eval_start: 32
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 30
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.14 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.14 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.14 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.14 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.14 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.14 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.14 s
Predicting Ended, total time: 6.14 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(343725, 7)
(343725, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/343725
(343725, 7)
(343725, 7)
Loading and preparing results...
(343725, 7)
Loading and preparing results...
Loading and preparing results...
0/343725
0/343725
0/343725
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(343725, 7)
Loading and preparing results...
(343725, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(343725, 7)
Converting ndarray to lists...
(343725, 7)
Converting ndarray to lists...
(343725, 7)
Loading and preparing results...
(343725, 7)
0/343725
0/343725
0/343725
0/343725
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(343725, 7)
Loading and preparing results...
(343725, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(343725, 7)
(343725, 7)
(343725, 7)
(343725, 7)
Loading and preparing results...
(343725, 7)
(343725, 7)
0/343725
0/343725
Loading and preparing results...
0/343725
Loading and preparing results...
0/343725
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/343725
Converting ndarray to lists...
Loading and preparing results...
0/343725
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(343725, 7)
(343725, 7)
(343725, 7)
0/343725
Converting ndarray to lists...
(343725, 7)
Converting ndarray to lists...
(343725, 7)
0/343725
(343725, 7)
Converting ndarray to lists...
(343725, 7)
(343725, 7)
0/343725
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/343725
Loading and preparing results...
(343725, 7)
(343725, 7)
0/343725
0/343725
Loading and preparing results...
(343725, 7)
(343725, 7)
0/343725
Converting ndarray to lists...
Loading and preparing results...
0/343725
0/343725
Loading and preparing results...
0/343725
(343725, 7)
Loading and preparing results...
0/343725
(343725, 7)
0/343725
Loading and preparing results...
Loading and preparing results...
0/343725
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(343725, 7)
(343725, 7)
0/343725
(343725, 7)
0/343725
Loading and preparing results...
(343725, 7)
Converting ndarray to lists...
0/343725
Converting ndarray to lists...
0/343725
Converting ndarray to lists...
0/343725
Loading and preparing results...
Loading and preparing results...
0/343725
Loading and preparing results...
Loading and preparing results...
0/343725
(343725, 7)
0/343725
(343725, 7)
0/343725
Loading and preparing results...
0/343725
Converting ndarray to lists...
(343725, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(343725, 7)
(343725, 7)
Loading and preparing results...
0/343725
(343725, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/343725
Converting ndarray to lists...
0/343725
(343725, 7)
(343725, 7)
Converting ndarray to lists...
0/343725
(343725, 7)
Loading and preparing results...
Converting ndarray to lists...
(343725, 7)
(343725, 7)
Loading and preparing results...
0/343725
0/343725
(343725, 7)
(343725, 7)
0/343725
0/343725
0/343725
0/343725
0/343725
Converting ndarray to lists...
(343725, 7)
Loading and preparing results...
(343725, 7)
(343725, 7)
0/343725
0/343725
0/343725
Converting ndarray to lists...
Converting ndarray to lists...
0/343725
0/343725
(343725, 7)
(343725, 7)
0/343725
0/343725
(343725, 7)
0/343725
Converting ndarray to lists...
Loading and preparing results...
(343725, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(343725, 7)
0/343725
(343725, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/343725
0/343725
Converting ndarray to lists...
(343725, 7)
(343725, 7)
Loading and preparing results...
Loading and preparing results...
0/343725
0/343725
(343725, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/343725
(343725, 7)
(343725, 7)
0/343725
0/343725
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.30s)
creating index...
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
DONE (t=3.40s).
Accumulating evaluation results...
DONE (t=3.41s).
Accumulating evaluation results...
DONE (t=3.41s).
Accumulating evaluation results...
DONE (t=3.43s).
Accumulating evaluation results...
DONE (t=3.42s).
Accumulating evaluation results...
DONE (t=3.45s).
Accumulating evaluation results...
DONE (t=3.44s).
Accumulating evaluation results...
DONE (t=3.46s).
Accumulating evaluation results...
DONE (t=1.17s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
DONE (t=1.18s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
DONE (t=1.18s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.250
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.062
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.14527 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
DONE (t=1.18s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
DONE (t=1.19s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.250
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.062
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.14527 AP goal: 0.21200
DONE (t=1.21s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
DONE (t=1.18s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.250
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.062
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.14527 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
DONE (t=1.20s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.250
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.062
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.14527 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.250
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.062
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.14527 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.250
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.062
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.14527 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.250
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.062
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.14527 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.250
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.062
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.14527 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757576.303249359 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757576.303839922 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.14527077741597014}

:::MLPv0.5.0 ssd 1541757576.304308653 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.14527077741597014}

:::MLPv0.5.0 ssd 1541757576.304746866 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757576.305178165 (train.py:338) eval_stop: 32
Iteration:   1880, Loss function: 3.992, Average Loss: 4.265, avg. samples / sec: 2619.91
Iteration:   1880, Loss function: 3.688, Average Loss: 4.270, avg. samples / sec: 2619.54
Iteration:   1880, Loss function: 3.884, Average Loss: 4.275, avg. samples / sec: 2620.00
Iteration:   1880, Loss function: 3.734, Average Loss: 4.274, avg. samples / sec: 2619.20
Iteration:   1880, Loss function: 3.950, Average Loss: 4.272, avg. samples / sec: 2619.84
Iteration:   1880, Loss function: 4.134, Average Loss: 4.269, avg. samples / sec: 2619.54
Iteration:   1880, Loss function: 4.078, Average Loss: 4.277, avg. samples / sec: 2619.41
Iteration:   1880, Loss function: 4.075, Average Loss: 4.273, avg. samples / sec: 2618.73
Iteration:   1900, Loss function: 4.722, Average Loss: 4.268, avg. samples / sec: 24684.78
Iteration:   1900, Loss function: 4.003, Average Loss: 4.278, avg. samples / sec: 24689.55
Iteration:   1900, Loss function: 4.173, Average Loss: 4.276, avg. samples / sec: 24648.48
Iteration:   1900, Loss function: 4.496, Average Loss: 4.271, avg. samples / sec: 24627.11
Iteration:   1900, Loss function: 4.387, Average Loss: 4.274, avg. samples / sec: 24652.56
Iteration:   1900, Loss function: 5.080, Average Loss: 4.273, avg. samples / sec: 24648.61
Iteration:   1900, Loss function: 4.613, Average Loss: 4.265, avg. samples / sec: 24608.38
Iteration:   1900, Loss function: 4.678, Average Loss: 4.273, avg. samples / sec: 24646.72

:::MLPv0.5.0 ssd 1541757579.463156462 (train.py:553) train_epoch: 33
Iteration:   1920, Loss function: 4.238, Average Loss: 4.263, avg. samples / sec: 24595.41
Iteration:   1920, Loss function: 4.033, Average Loss: 4.272, avg. samples / sec: 24584.65
Iteration:   1920, Loss function: 4.309, Average Loss: 4.270, avg. samples / sec: 24563.32
Iteration:   1920, Loss function: 4.119, Average Loss: 4.273, avg. samples / sec: 24617.00
Iteration:   1920, Loss function: 3.307, Average Loss: 4.267, avg. samples / sec: 24574.94
Iteration:   1920, Loss function: 4.871, Average Loss: 4.277, avg. samples / sec: 24563.90
Iteration:   1920, Loss function: 4.114, Average Loss: 4.273, avg. samples / sec: 24560.35
Iteration:   1920, Loss function: 4.212, Average Loss: 4.275, avg. samples / sec: 24543.22
Iteration:   1940, Loss function: 4.591, Average Loss: 4.270, avg. samples / sec: 24576.28
Iteration:   1940, Loss function: 4.359, Average Loss: 4.265, avg. samples / sec: 24573.51
Iteration:   1940, Loss function: 4.055, Average Loss: 4.267, avg. samples / sec: 24559.77
Iteration:   1940, Loss function: 3.910, Average Loss: 4.272, avg. samples / sec: 24566.17
Iteration:   1940, Loss function: 3.928, Average Loss: 4.269, avg. samples / sec: 24558.39
Iteration:   1940, Loss function: 4.003, Average Loss: 4.266, avg. samples / sec: 24571.44
Iteration:   1940, Loss function: 4.118, Average Loss: 4.259, avg. samples / sec: 24542.78
Iteration:   1940, Loss function: 4.185, Average Loss: 4.270, avg. samples / sec: 24528.18
Iteration:   1960, Loss function: 3.885, Average Loss: 4.264, avg. samples / sec: 24600.27
Iteration:   1960, Loss function: 4.281, Average Loss: 4.258, avg. samples / sec: 24611.83
Iteration:   1960, Loss function: 4.224, Average Loss: 4.269, avg. samples / sec: 24578.98
Iteration:   1960, Loss function: 3.940, Average Loss: 4.270, avg. samples / sec: 24587.48
Iteration:   1960, Loss function: 3.967, Average Loss: 4.267, avg. samples / sec: 24640.20
Iteration:   1960, Loss function: 4.250, Average Loss: 4.264, avg. samples / sec: 24586.88
Iteration:   1960, Loss function: 3.858, Average Loss: 4.267, avg. samples / sec: 24548.39
Iteration:   1960, Loss function: 3.853, Average Loss: 4.265, avg. samples / sec: 24531.51

:::MLPv0.5.0 ssd 1541757584.299855947 (train.py:553) train_epoch: 34
Iteration:   1980, Loss function: 4.322, Average Loss: 4.256, avg. samples / sec: 24481.89
Iteration:   1980, Loss function: 4.246, Average Loss: 4.267, avg. samples / sec: 24473.38
Iteration:   1980, Loss function: 4.402, Average Loss: 4.270, avg. samples / sec: 24480.30
Iteration:   1980, Loss function: 4.311, Average Loss: 4.265, avg. samples / sec: 24472.70
Iteration:   1980, Loss function: 3.920, Average Loss: 4.263, avg. samples / sec: 24471.73
Iteration:   1980, Loss function: 4.385, Average Loss: 4.266, avg. samples / sec: 24496.35
Iteration:   1980, Loss function: 3.976, Average Loss: 4.266, avg. samples / sec: 24479.44
Iteration:   1980, Loss function: 3.919, Average Loss: 4.271, avg. samples / sec: 24423.48
Iteration:   2000, Loss function: 4.271, Average Loss: 4.267, avg. samples / sec: 24608.99
Iteration:   2000, Loss function: 3.951, Average Loss: 4.265, avg. samples / sec: 24640.63
Iteration:   2000, Loss function: 3.684, Average Loss: 4.262, avg. samples / sec: 24625.57
Iteration:   2000, Loss function: 4.078, Average Loss: 4.264, avg. samples / sec: 24604.18
Iteration:   2000, Loss function: 4.217, Average Loss: 4.263, avg. samples / sec: 24620.18
Iteration:   2000, Loss function: 4.237, Average Loss: 4.254, avg. samples / sec: 24562.40
Iteration:   2000, Loss function: 3.709, Average Loss: 4.259, avg. samples / sec: 24592.20
Iteration:   2000, Loss function: 4.239, Average Loss: 4.270, avg. samples / sec: 24608.68
Iteration:   2020, Loss function: 4.154, Average Loss: 4.251, avg. samples / sec: 24597.88
Iteration:   2020, Loss function: 3.941, Average Loss: 4.253, avg. samples / sec: 24597.81
Iteration:   2020, Loss function: 4.685, Average Loss: 4.262, avg. samples / sec: 24558.15
Iteration:   2020, Loss function: 4.260, Average Loss: 4.262, avg. samples / sec: 24531.70
Iteration:   2020, Loss function: 4.395, Average Loss: 4.264, avg. samples / sec: 24528.29
Iteration:   2020, Loss function: 3.871, Average Loss: 4.268, avg. samples / sec: 24583.67
Iteration:   2020, Loss function: 4.396, Average Loss: 4.259, avg. samples / sec: 24545.12
Iteration:   2020, Loss function: 3.893, Average Loss: 4.257, avg. samples / sec: 24510.13

:::MLPv0.5.0 ssd 1541757589.137880802 (train.py:553) train_epoch: 35
Iteration:   2040, Loss function: 3.860, Average Loss: 4.248, avg. samples / sec: 24503.47
Iteration:   2040, Loss function: 4.262, Average Loss: 4.262, avg. samples / sec: 24538.37
Iteration:   2040, Loss function: 4.372, Average Loss: 4.256, avg. samples / sec: 24548.79
Iteration:   2040, Loss function: 4.233, Average Loss: 4.261, avg. samples / sec: 24500.67
Iteration:   2040, Loss function: 4.005, Average Loss: 4.261, avg. samples / sec: 24520.61
Iteration:   2040, Loss function: 3.948, Average Loss: 4.255, avg. samples / sec: 24523.66
Iteration:   2040, Loss function: 4.732, Average Loss: 4.261, avg. samples / sec: 24483.08
Iteration:   2040, Loss function: 3.998, Average Loss: 4.252, avg. samples / sec: 24456.96
Iteration:   2060, Loss function: 3.876, Average Loss: 4.258, avg. samples / sec: 24550.31
Iteration:   2060, Loss function: 4.043, Average Loss: 4.260, avg. samples / sec: 24538.49
Iteration:   2060, Loss function: 4.471, Average Loss: 4.245, avg. samples / sec: 24535.78
Iteration:   2060, Loss function: 4.585, Average Loss: 4.254, avg. samples / sec: 24507.27
Iteration:   2060, Loss function: 4.705, Average Loss: 4.247, avg. samples / sec: 24554.44
Iteration:   2060, Loss function: 3.896, Average Loss: 4.253, avg. samples / sec: 24520.36
Iteration:   2060, Loss function: 3.551, Average Loss: 4.260, avg. samples / sec: 24503.49
Iteration:   2060, Loss function: 4.529, Average Loss: 4.258, avg. samples / sec: 24534.28

:::MLPv0.5.0 ssd 1541757593.980656147 (train.py:553) train_epoch: 36
Iteration:   2080, Loss function: 3.828, Average Loss: 4.243, avg. samples / sec: 24528.40
Iteration:   2080, Loss function: 3.875, Average Loss: 4.259, avg. samples / sec: 24527.21
Iteration:   2080, Loss function: 4.331, Average Loss: 4.257, avg. samples / sec: 24519.54
Iteration:   2080, Loss function: 3.936, Average Loss: 4.247, avg. samples / sec: 24555.20
Iteration:   2080, Loss function: 4.166, Average Loss: 4.257, avg. samples / sec: 24569.44
Iteration:   2080, Loss function: 3.834, Average Loss: 4.250, avg. samples / sec: 24551.94
Iteration:   2080, Loss function: 4.470, Average Loss: 4.259, avg. samples / sec: 24524.76
Iteration:   2080, Loss function: 3.996, Average Loss: 4.253, avg. samples / sec: 24512.59
Iteration:   2100, Loss function: 4.103, Average Loss: 4.256, avg. samples / sec: 24498.89
Iteration:   2100, Loss function: 3.738, Average Loss: 4.239, avg. samples / sec: 24498.88
Iteration:   2100, Loss function: 4.237, Average Loss: 4.246, avg. samples / sec: 24485.96
Iteration:   2100, Loss function: 4.019, Average Loss: 4.250, avg. samples / sec: 24524.00
Iteration:   2100, Loss function: 3.690, Average Loss: 4.254, avg. samples / sec: 24465.82
Iteration:   2100, Loss function: 3.912, Average Loss: 4.246, avg. samples / sec: 24479.52
Iteration:   2100, Loss function: 4.103, Average Loss: 4.254, avg. samples / sec: 24498.27
Iteration:   2100, Loss function: 3.710, Average Loss: 4.255, avg. samples / sec: 24456.72
Iteration:   2120, Loss function: 3.841, Average Loss: 4.249, avg. samples / sec: 24539.54
Iteration:   2120, Loss function: 4.111, Average Loss: 4.243, avg. samples / sec: 24507.35
Iteration:   2120, Loss function: 4.173, Average Loss: 4.242, avg. samples / sec: 24523.94
Iteration:   2120, Loss function: 3.738, Average Loss: 4.235, avg. samples / sec: 24474.71
Iteration:   2120, Loss function: 4.107, Average Loss: 4.252, avg. samples / sec: 24473.43
Iteration:   2120, Loss function: 3.609, Average Loss: 4.244, avg. samples / sec: 24489.44
Iteration:   2120, Loss function: 3.881, Average Loss: 4.252, avg. samples / sec: 24511.06
Iteration:   2120, Loss function: 4.089, Average Loss: 4.252, avg. samples / sec: 24498.22

:::MLPv0.5.0 ssd 1541757598.828187943 (train.py:553) train_epoch: 37
Iteration:   2140, Loss function: 4.384, Average Loss: 4.230, avg. samples / sec: 24537.79
Iteration:   2140, Loss function: 4.337, Average Loss: 4.245, avg. samples / sec: 24484.35
Iteration:   2140, Loss function: 3.794, Average Loss: 4.248, avg. samples / sec: 24528.57
Iteration:   2140, Loss function: 3.609, Average Loss: 4.237, avg. samples / sec: 24501.21
Iteration:   2140, Loss function: 3.632, Average Loss: 4.247, avg. samples / sec: 24510.18
Iteration:   2140, Loss function: 3.953, Average Loss: 4.244, avg. samples / sec: 24505.88
Iteration:   2140, Loss function: 3.992, Average Loss: 4.247, avg. samples / sec: 24514.97
Iteration:   2140, Loss function: 3.868, Average Loss: 4.238, avg. samples / sec: 24471.61
Iteration:   2160, Loss function: 3.886, Average Loss: 4.242, avg. samples / sec: 24602.33
Iteration:   2160, Loss function: 4.820, Average Loss: 4.229, avg. samples / sec: 24554.69
Iteration:   2160, Loss function: 4.389, Average Loss: 4.234, avg. samples / sec: 24585.16
Iteration:   2160, Loss function: 3.492, Average Loss: 4.233, avg. samples / sec: 24554.84
Iteration:   2160, Loss function: 5.045, Average Loss: 4.248, avg. samples / sec: 24542.65
Iteration:   2160, Loss function: 4.261, Average Loss: 4.245, avg. samples / sec: 24533.35
Iteration:   2160, Loss function: 4.773, Average Loss: 4.243, avg. samples / sec: 24520.97
Iteration:   2160, Loss function: 4.394, Average Loss: 4.243, avg. samples / sec: 24541.88
Iteration:   2180, Loss function: 4.306, Average Loss: 4.248, avg. samples / sec: 24588.84
Iteration:   2180, Loss function: 4.134, Average Loss: 4.234, avg. samples / sec: 24552.99
Iteration:   2180, Loss function: 4.276, Average Loss: 4.232, avg. samples / sec: 24523.20
Iteration:   2180, Loss function: 3.833, Average Loss: 4.252, avg. samples / sec: 24558.24
Iteration:   2180, Loss function: 4.491, Average Loss: 4.246, avg. samples / sec: 24564.02
Iteration:   2180, Loss function: 3.913, Average Loss: 4.237, avg. samples / sec: 24522.79
Iteration:   2180, Loss function: 4.651, Average Loss: 4.247, avg. samples / sec: 24472.58
Iteration:   2180, Loss function: 3.798, Average Loss: 4.247, avg. samples / sec: 24520.64

:::MLPv0.5.0 ssd 1541757603.588291645 (train.py:553) train_epoch: 38
Iteration:   2200, Loss function: 3.520, Average Loss: 4.242, avg. samples / sec: 24515.78
Iteration:   2200, Loss function: 3.496, Average Loss: 4.245, avg. samples / sec: 24568.26
Iteration:   2200, Loss function: 4.141, Average Loss: 4.243, avg. samples / sec: 24526.71
Iteration:   2200, Loss function: 4.262, Average Loss: 4.237, avg. samples / sec: 24531.21
Iteration:   2200, Loss function: 3.759, Average Loss: 4.227, avg. samples / sec: 24509.63
Iteration:   2200, Loss function: 4.186, Average Loss: 4.229, avg. samples / sec: 24490.17
Iteration:   2200, Loss function: 4.162, Average Loss: 4.246, avg. samples / sec: 24461.29
Iteration:   2200, Loss function: 4.032, Average Loss: 4.244, avg. samples / sec: 24500.52
Iteration:   2220, Loss function: 4.248, Average Loss: 4.225, avg. samples / sec: 24509.24
Iteration:   2220, Loss function: 3.801, Average Loss: 4.242, avg. samples / sec: 24551.81
Iteration:   2220, Loss function: 3.968, Average Loss: 4.242, avg. samples / sec: 24545.61
Iteration:   2220, Loss function: 4.085, Average Loss: 4.238, avg. samples / sec: 24493.76
Iteration:   2220, Loss function: 4.264, Average Loss: 4.226, avg. samples / sec: 24511.89
Iteration:   2220, Loss function: 3.782, Average Loss: 4.236, avg. samples / sec: 24469.14
Iteration:   2220, Loss function: 4.239, Average Loss: 4.233, avg. samples / sec: 24467.68
Iteration:   2220, Loss function: 4.497, Average Loss: 4.241, avg. samples / sec: 24450.53
Iteration:   2240, Loss function: 3.973, Average Loss: 4.237, avg. samples / sec: 24537.85
Iteration:   2240, Loss function: 3.930, Average Loss: 4.223, avg. samples / sec: 24520.31
Iteration:   2240, Loss function: 3.667, Average Loss: 4.223, avg. samples / sec: 24530.93
Iteration:   2240, Loss function: 3.716, Average Loss: 4.231, avg. samples / sec: 24533.44
Iteration:   2240, Loss function: 3.999, Average Loss: 4.234, avg. samples / sec: 24520.63
Iteration:   2240, Loss function: 4.209, Average Loss: 4.238, avg. samples / sec: 24522.31
Iteration:   2240, Loss function: 4.178, Average Loss: 4.235, avg. samples / sec: 24471.64
Iteration:   2240, Loss function: 4.026, Average Loss: 4.237, avg. samples / sec: 24474.41

:::MLPv0.5.0 ssd 1541757608.433194637 (train.py:553) train_epoch: 39
Iteration:   2260, Loss function: 4.186, Average Loss: 4.223, avg. samples / sec: 24508.64
Iteration:   2260, Loss function: 4.202, Average Loss: 4.235, avg. samples / sec: 24571.22
Iteration:   2260, Loss function: 4.569, Average Loss: 4.221, avg. samples / sec: 24510.12
Iteration:   2260, Loss function: 4.176, Average Loss: 4.232, avg. samples / sec: 24521.03
Iteration:   2260, Loss function: 4.242, Average Loss: 4.231, avg. samples / sec: 24511.52
Iteration:   2260, Loss function: 4.653, Average Loss: 4.239, avg. samples / sec: 24526.93
Iteration:   2260, Loss function: 4.181, Average Loss: 4.235, avg. samples / sec: 24454.16
Iteration:   2260, Loss function: 4.029, Average Loss: 4.237, avg. samples / sec: 24513.87
Iteration:   2280, Loss function: 3.953, Average Loss: 4.232, avg. samples / sec: 24533.52
Iteration:   2280, Loss function: 3.524, Average Loss: 4.219, avg. samples / sec: 24515.75
Iteration:   2280, Loss function: 3.214, Average Loss: 4.227, avg. samples / sec: 24527.46
Iteration:   2280, Loss function: 3.934, Average Loss: 4.222, avg. samples / sec: 24497.35
Iteration:   2280, Loss function: 4.434, Average Loss: 4.228, avg. samples / sec: 24508.35
Iteration:   2280, Loss function: 3.589, Average Loss: 4.231, avg. samples / sec: 24537.12
Iteration:   2280, Loss function: 4.210, Average Loss: 4.236, avg. samples / sec: 24532.44
Iteration:   2280, Loss function: 4.002, Average Loss: 4.236, avg. samples / sec: 24514.50
Iteration:   2300, Loss function: 3.954, Average Loss: 4.224, avg. samples / sec: 24604.28
Iteration:   2300, Loss function: 3.975, Average Loss: 4.220, avg. samples / sec: 24591.15
Iteration:   2300, Loss function: 3.998, Average Loss: 4.228, avg. samples / sec: 24550.44
Iteration:   2300, Loss function: 3.519, Average Loss: 4.225, avg. samples / sec: 24575.87
Iteration:   2300, Loss function: 4.012, Average Loss: 4.215, avg. samples / sec: 24545.64
Iteration:   2300, Loss function: 4.390, Average Loss: 4.226, avg. samples / sec: 24568.37
Iteration:   2300, Loss function: 3.974, Average Loss: 4.230, avg. samples / sec: 24580.11
Iteration:   2300, Loss function: 4.170, Average Loss: 4.229, avg. samples / sec: 24554.27

:::MLPv0.5.0 ssd 1541757613.275714874 (train.py:553) train_epoch: 40
Iteration:   2320, Loss function: 3.807, Average Loss: 4.222, avg. samples / sec: 24506.50
Iteration:   2320, Loss function: 4.149, Average Loss: 4.226, avg. samples / sec: 24550.48
Iteration:   2320, Loss function: 4.264, Average Loss: 4.222, avg. samples / sec: 24521.29
Iteration:   2320, Loss function: 4.129, Average Loss: 4.227, avg. samples / sec: 24512.26
Iteration:   2320, Loss function: 4.254, Average Loss: 4.219, avg. samples / sec: 24507.68
Iteration:   2320, Loss function: 4.085, Average Loss: 4.225, avg. samples / sec: 24555.43
Iteration:   2320, Loss function: 3.673, Average Loss: 4.225, avg. samples / sec: 24537.65
Iteration:   2320, Loss function: 3.544, Average Loss: 4.212, avg. samples / sec: 24505.49
Iteration:   2340, Loss function: 4.669, Average Loss: 4.220, avg. samples / sec: 24543.45
Iteration:   2340, Loss function: 3.836, Average Loss: 4.216, avg. samples / sec: 24549.07
Iteration:   2340, Loss function: 4.233, Average Loss: 4.221, avg. samples / sec: 24550.12
Iteration:   2340, Loss function: 3.985, Average Loss: 4.221, avg. samples / sec: 24548.31
Iteration:   2340, Loss function: 3.870, Average Loss: 4.220, avg. samples / sec: 24541.84
Iteration:   2340, Loss function: 3.835, Average Loss: 4.218, avg. samples / sec: 24514.90
Iteration:   2340, Loss function: 4.012, Average Loss: 4.207, avg. samples / sec: 24539.43
Iteration:   2340, Loss function: 4.197, Average Loss: 4.216, avg. samples / sec: 24483.72
Iteration:   2360, Loss function: 4.076, Average Loss: 4.218, avg. samples / sec: 24632.72
Iteration:   2360, Loss function: 3.964, Average Loss: 4.218, avg. samples / sec: 24636.54
Iteration:   2360, Loss function: 4.942, Average Loss: 4.214, avg. samples / sec: 24627.82
Iteration:   2360, Loss function: 4.762, Average Loss: 4.216, avg. samples / sec: 24651.97
Iteration:   2360, Loss function: 4.504, Average Loss: 4.219, avg. samples / sec: 24595.67
Iteration:   2360, Loss function: 4.331, Average Loss: 4.204, avg. samples / sec: 24634.85
Iteration:   2360, Loss function: 4.411, Average Loss: 4.219, avg. samples / sec: 24592.66
Iteration:   2360, Loss function: 4.592, Average Loss: 4.215, avg. samples / sec: 24631.68

:::MLPv0.5.0 ssd 1541757618.112086773 (train.py:553) train_epoch: 41
Iteration:   2380, Loss function: 4.404, Average Loss: 4.215, avg. samples / sec: 24516.60
Iteration:   2380, Loss function: 4.048, Average Loss: 4.215, avg. samples / sec: 24507.02
Iteration:   2380, Loss function: 4.305, Average Loss: 4.219, avg. samples / sec: 24539.37
Iteration:   2380, Loss function: 4.376, Average Loss: 4.216, avg. samples / sec: 24539.89
Iteration:   2380, Loss function: 4.543, Average Loss: 4.218, avg. samples / sec: 24474.07
Iteration:   2380, Loss function: 4.302, Average Loss: 4.205, avg. samples / sec: 24511.25
Iteration:   2380, Loss function: 3.864, Average Loss: 4.214, avg. samples / sec: 24467.77
Iteration:   2380, Loss function: 4.219, Average Loss: 4.220, avg. samples / sec: 24486.71
Iteration:   2400, Loss function: 3.568, Average Loss: 4.214, avg. samples / sec: 24612.44
Iteration:   2400, Loss function: 3.991, Average Loss: 4.209, avg. samples / sec: 24552.08
Iteration:   2400, Loss function: 3.298, Average Loss: 4.201, avg. samples / sec: 24583.50
Iteration:   2400, Loss function: 4.050, Average Loss: 4.214, avg. samples / sec: 24571.00
Iteration:   2400, Loss function: 4.019, Average Loss: 4.216, avg. samples / sec: 24547.45
Iteration:   2400, Loss function: 4.034, Average Loss: 4.211, avg. samples / sec: 24564.97
Iteration:   2400, Loss function: 3.653, Average Loss: 4.209, avg. samples / sec: 24580.23
Iteration:   2400, Loss function: 4.232, Average Loss: 4.212, avg. samples / sec: 24533.65
Iteration:   2420, Loss function: 4.182, Average Loss: 4.198, avg. samples / sec: 24587.28
Iteration:   2420, Loss function: 3.720, Average Loss: 4.210, avg. samples / sec: 24601.23
Iteration:   2420, Loss function: 3.706, Average Loss: 4.205, avg. samples / sec: 24576.12
Iteration:   2420, Loss function: 5.267, Average Loss: 4.217, avg. samples / sec: 24573.04
Iteration:   2420, Loss function: 4.509, Average Loss: 4.212, avg. samples / sec: 24549.05
Iteration:   2420, Loss function: 4.009, Average Loss: 4.211, avg. samples / sec: 24551.91
Iteration:   2420, Loss function: 4.184, Average Loss: 4.206, avg. samples / sec: 24544.79
Iteration:   2420, Loss function: 4.172, Average Loss: 4.208, avg. samples / sec: 24532.67

:::MLPv0.5.0 ssd 1541757622.866198063 (train.py:553) train_epoch: 42
Iteration:   2440, Loss function: 4.156, Average Loss: 4.206, avg. samples / sec: 24573.40
Iteration:   2440, Loss function: 3.576, Average Loss: 4.193, avg. samples / sec: 24564.61
Iteration:   2440, Loss function: 4.544, Average Loss: 4.206, avg. samples / sec: 24609.13
Iteration:   2440, Loss function: 3.497, Average Loss: 4.206, avg. samples / sec: 24578.34
Iteration:   2440, Loss function: 4.397, Average Loss: 4.201, avg. samples / sec: 24547.66
Iteration:   2440, Loss function: 3.858, Average Loss: 4.215, avg. samples / sec: 24555.05
Iteration:   2440, Loss function: 4.318, Average Loss: 4.203, avg. samples / sec: 24584.51
Iteration:   2440, Loss function: 3.766, Average Loss: 4.201, avg. samples / sec: 24572.69
Iteration:   2460, Loss function: 3.805, Average Loss: 4.194, avg. samples / sec: 24638.31
Iteration:   2460, Loss function: 4.156, Average Loss: 4.211, avg. samples / sec: 24615.76
Iteration:   2460, Loss function: 3.922, Average Loss: 4.191, avg. samples / sec: 24580.17
Iteration:   2460, Loss function: 4.105, Average Loss: 4.203, avg. samples / sec: 24574.81
Iteration:   2460, Loss function: 3.903, Average Loss: 4.202, avg. samples / sec: 24581.30
Iteration:   2460, Loss function: 3.974, Average Loss: 4.200, avg. samples / sec: 24598.73
Iteration:   2460, Loss function: 3.927, Average Loss: 4.203, avg. samples / sec: 24564.28
Iteration:   2460, Loss function: 4.368, Average Loss: 4.198, avg. samples / sec: 24564.93
Iteration:   2480, Loss function: 3.913, Average Loss: 4.198, avg. samples / sec: 24726.48
Iteration:   2480, Loss function: 3.985, Average Loss: 4.187, avg. samples / sec: 24691.81
Iteration:   2480, Loss function: 4.105, Average Loss: 4.187, avg. samples / sec: 24670.01
Iteration:   2480, Loss function: 3.470, Average Loss: 4.192, avg. samples / sec: 24720.91
Iteration:   2480, Loss function: 3.961, Average Loss: 4.206, avg. samples / sec: 24670.26
Iteration:   2480, Loss function: 3.770, Average Loss: 4.199, avg. samples / sec: 24671.61
Iteration:   2480, Loss function: 3.609, Average Loss: 4.201, avg. samples / sec: 24698.26
Iteration:   2480, Loss function: 3.634, Average Loss: 4.198, avg. samples / sec: 24632.03

:::MLPv0.5.0 ssd 1541757627.690698862 (train.py:553) train_epoch: 43
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1

:::MLPv0.5.0 ssd 1541757629.023683548 (train.py:578) opt_learning_rate: 0.016
Iteration:   2500, Loss function: 4.469, Average Loss: 4.183, avg. samples / sec: 24644.13
Iteration:   2500, Loss function: 4.031, Average Loss: 4.196, avg. samples / sec: 24660.05
Iteration:   2500, Loss function: 3.621, Average Loss: 4.194, avg. samples / sec: 24677.00
Iteration:   2500, Loss function: 4.549, Average Loss: 4.201, avg. samples / sec: 24631.39
Iteration:   2500, Loss function: 3.901, Average Loss: 4.192, avg. samples / sec: 24633.99
Iteration:   2500, Loss function: 4.005, Average Loss: 4.191, avg. samples / sec: 24594.85
Iteration:   2500, Loss function: 3.745, Average Loss: 4.180, avg. samples / sec: 24614.24
Iteration:   2500, Loss function: 3.750, Average Loss: 4.186, avg. samples / sec: 24607.19

































































:::MLPv0.5.0 ssd 1541757629.106461763 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757629.107016087 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757629.107478380 (train.py:220) eval_start: 43
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.49 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.49 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.49 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.49 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.49 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.49 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.49 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.49 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(331253, 7)
Loading and preparing results...
(331253, 7)
Converting ndarray to lists...
(331253, 7)
Loading and preparing results...
(331253, 7)
(331253, 7)
Loading and preparing results...
Loading and preparing results...
(331253, 7)
Converting ndarray to lists...
0/331253
0/331253
Converting ndarray to lists...
0/331253
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(331253, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(331253, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/331253
Loading and preparing results...
0/331253
Converting ndarray to lists...
0/331253
Converting ndarray to lists...
0/331253
Converting ndarray to lists...
(331253, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/331253
(331253, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(331253, 7)
Converting ndarray to lists...
(331253, 7)
Loading and preparing results...
Loading and preparing results...
(331253, 7)
(331253, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(331253, 7)
(331253, 7)
Converting ndarray to lists...
0/331253
Converting ndarray to lists...
Converting ndarray to lists...
(331253, 7)
(331253, 7)
0/331253
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/331253
Loading and preparing results...
Loading and preparing results...
(331253, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/331253
(331253, 7)
Loading and preparing results...
(331253, 7)
(331253, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/331253
Converting ndarray to lists...
Converting ndarray to lists...
(331253, 7)
Loading and preparing results...
0/331253
Converting ndarray to lists...
0/331253
Loading and preparing results...
(331253, 7)
0/331253
Converting ndarray to lists...
0/331253
(331253, 7)
(331253, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/331253
Loading and preparing results...
0/331253
Converting ndarray to lists...
(331253, 7)
(331253, 7)
0/331253
(331253, 7)
Converting ndarray to lists...
0/331253
Converting ndarray to lists...
0/331253
0/331253
Converting ndarray to lists...
(331253, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(331253, 7)
(331253, 7)
(331253, 7)
(331253, 7)
Converting ndarray to lists...
(331253, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(331253, 7)
(331253, 7)
0/331253
(331253, 7)
0/331253
Converting ndarray to lists...
0/331253
Converting ndarray to lists...
(331253, 7)
(331253, 7)
0/331253
(331253, 7)
(331253, 7)
Loading and preparing results...
Loading and preparing results...
(331253, 7)
0/331253
0/331253
0/331253
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/331253
(331253, 7)
Converting ndarray to lists...
0/331253
0/331253
Converting ndarray to lists...
(331253, 7)
Converting ndarray to lists...
(331253, 7)
0/331253
0/331253
0/331253
0/331253
Converting ndarray to lists...
(331253, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/331253
Converting ndarray to lists...
Loading and preparing results...
(331253, 7)
0/331253
Loading and preparing results...
0/331253
Converting ndarray to lists...
(331253, 7)
0/331253
(331253, 7)
0/331253
0/331253
(331253, 7)
0/331253
0/331253
Converting ndarray to lists...
0/331253
Converting ndarray to lists...
0/331253
(331253, 7)
(331253, 7)
Converting ndarray to lists...
(331253, 7)
0/331253
0/331253
0/331253
(331253, 7)
0/331253
(331253, 7)
0/331253
Converting ndarray to lists...
0/331253
Converting ndarray to lists...
(331253, 7)
Converting ndarray to lists...
0/331253
(331253, 7)
0/331253
(331253, 7)
Loading and preparing results...
(331253, 7)
(331253, 7)
0/331253
(331253, 7)
(331253, 7)
0/331253
0/331253
0/331253
Converting ndarray to lists...
0/331253
0/331253
0/331253
0/331253
(331253, 7)
0/331253
DONE (t=1.74s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.79s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=1.99s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.09s)
creating index...
DONE (t=2.09s)
creating index...
DONE (t=2.11s)
creating index...
index created!
DONE (t=2.11s)
creating index...
DONE (t=2.12s)
creating index...
DONE (t=2.13s)
creating index...
index created!
DONE (t=2.14s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.15s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=3.56s).
Accumulating evaluation results...
DONE (t=3.56s).
Accumulating evaluation results...
DONE (t=3.53s).
Accumulating evaluation results...
DONE (t=3.57s).
Accumulating evaluation results...
DONE (t=3.58s).
Accumulating evaluation results...
DONE (t=3.57s).
Accumulating evaluation results...
DONE (t=3.60s).
Accumulating evaluation results...
DONE (t=3.60s).
Accumulating evaluation results...
DONE (t=1.17s).
DONE (t=1.18s).
DONE (t=1.17s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.149
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
DONE (t=1.17s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.149
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.149
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.170
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.170
DONE (t=1.18s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.170
DONE (t=1.18s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.252
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.288
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399
Current AP: 0.15402 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.149
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.154
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.288
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399
Current AP: 0.15402 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.288
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399
Current AP: 0.15402 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
DONE (t=1.19s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.170
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.149
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.149
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.252
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.288
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399
Current AP: 0.15402 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.170
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.170
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.149
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.288
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399
Current AP: 0.15402 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.288
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399
Current AP: 0.15402 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.170
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.288
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399
Current AP: 0.15402 AP goal: 0.21200
DONE (t=1.20s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.294
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.149
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.170
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.264
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.288
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399
Current AP: 0.15402 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757640.750735283 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757640.751337528 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.1540249697483867}

:::MLPv0.5.0 ssd 1541757640.751807451 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.1540249697483867}

:::MLPv0.5.0 ssd 1541757640.752269506 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757640.752721548 (train.py:338) eval_stop: 43
Iteration:   2520, Loss function: 3.474, Average Loss: 4.172, avg. samples / sec: 2978.61
Iteration:   2520, Loss function: 3.881, Average Loss: 4.181, avg. samples / sec: 2978.98
Iteration:   2520, Loss function: 3.422, Average Loss: 4.177, avg. samples / sec: 2979.28
Iteration:   2520, Loss function: 3.216, Average Loss: 4.190, avg. samples / sec: 2978.82
Iteration:   2520, Loss function: 3.895, Average Loss: 4.187, avg. samples / sec: 2978.62
Iteration:   2520, Loss function: 3.336, Average Loss: 4.181, avg. samples / sec: 2978.55
Iteration:   2520, Loss function: 3.862, Average Loss: 4.181, avg. samples / sec: 2978.71
Iteration:   2520, Loss function: 3.863, Average Loss: 4.171, avg. samples / sec: 2978.68
Iteration:   2540, Loss function: 3.307, Average Loss: 4.157, avg. samples / sec: 24603.90
Iteration:   2540, Loss function: 3.479, Average Loss: 4.160, avg. samples / sec: 24638.05
Iteration:   2540, Loss function: 3.635, Average Loss: 4.177, avg. samples / sec: 24612.86
Iteration:   2540, Loss function: 3.505, Average Loss: 4.164, avg. samples / sec: 24600.17
Iteration:   2540, Loss function: 3.358, Average Loss: 4.169, avg. samples / sec: 24623.76
Iteration:   2540, Loss function: 3.659, Average Loss: 4.168, avg. samples / sec: 24575.54
Iteration:   2540, Loss function: 3.300, Average Loss: 4.165, avg. samples / sec: 24582.08
Iteration:   2540, Loss function: 4.037, Average Loss: 4.175, avg. samples / sec: 24551.53

:::MLPv0.5.0 ssd 1541757644.604019403 (train.py:553) train_epoch: 44
Iteration:   2560, Loss function: 4.155, Average Loss: 4.145, avg. samples / sec: 24627.06
Iteration:   2560, Loss function: 3.212, Average Loss: 4.150, avg. samples / sec: 24676.25
Iteration:   2560, Loss function: 4.048, Average Loss: 4.163, avg. samples / sec: 24616.40
Iteration:   2560, Loss function: 3.600, Average Loss: 4.143, avg. samples / sec: 24601.67
Iteration:   2560, Loss function: 3.966, Average Loss: 4.154, avg. samples / sec: 24611.92
Iteration:   2560, Loss function: 3.260, Average Loss: 4.153, avg. samples / sec: 24617.79
Iteration:   2560, Loss function: 3.269, Average Loss: 4.151, avg. samples / sec: 24594.49
Iteration:   2560, Loss function: 2.715, Average Loss: 4.159, avg. samples / sec: 24642.01
Iteration:   2580, Loss function: 3.035, Average Loss: 4.128, avg. samples / sec: 24671.37
Iteration:   2580, Loss function: 3.225, Average Loss: 4.137, avg. samples / sec: 24699.51
Iteration:   2580, Loss function: 2.873, Average Loss: 4.145, avg. samples / sec: 24673.02
Iteration:   2580, Loss function: 3.772, Average Loss: 4.136, avg. samples / sec: 24697.02
Iteration:   2580, Loss function: 3.350, Average Loss: 4.145, avg. samples / sec: 24694.62
Iteration:   2580, Loss function: 3.163, Average Loss: 4.138, avg. samples / sec: 24656.87
Iteration:   2580, Loss function: 3.244, Average Loss: 4.135, avg. samples / sec: 24628.67
Iteration:   2580, Loss function: 3.158, Average Loss: 4.126, avg. samples / sec: 24640.18

:::MLPv0.5.0 ssd 1541757649.423227072 (train.py:553) train_epoch: 45
Iteration:   2600, Loss function: 3.132, Average Loss: 4.113, avg. samples / sec: 24640.10
Iteration:   2600, Loss function: 3.312, Average Loss: 4.119, avg. samples / sec: 24680.32
Iteration:   2600, Loss function: 3.384, Average Loss: 4.122, avg. samples / sec: 24645.55
Iteration:   2600, Loss function: 3.101, Average Loss: 4.122, avg. samples / sec: 24640.30
Iteration:   2600, Loss function: 3.360, Average Loss: 4.112, avg. samples / sec: 24655.37
Iteration:   2600, Loss function: 2.947, Average Loss: 4.128, avg. samples / sec: 24614.48
Iteration:   2600, Loss function: 3.933, Average Loss: 4.131, avg. samples / sec: 24628.56
Iteration:   2600, Loss function: 3.405, Average Loss: 4.122, avg. samples / sec: 24625.25
Iteration:   2620, Loss function: 3.143, Average Loss: 4.112, avg. samples / sec: 24664.01
Iteration:   2620, Loss function: 3.376, Average Loss: 4.106, avg. samples / sec: 24631.05
Iteration:   2620, Loss function: 3.823, Average Loss: 4.095, avg. samples / sec: 24635.60
Iteration:   2620, Loss function: 3.152, Average Loss: 4.108, avg. samples / sec: 24593.39
Iteration:   2620, Loss function: 3.515, Average Loss: 4.110, avg. samples / sec: 24636.75
Iteration:   2620, Loss function: 2.988, Average Loss: 4.101, avg. samples / sec: 24579.15
Iteration:   2620, Loss function: 4.016, Average Loss: 4.118, avg. samples / sec: 24614.47
Iteration:   2620, Loss function: 3.123, Average Loss: 4.097, avg. samples / sec: 24572.59
Iteration:   2640, Loss function: 3.252, Average Loss: 4.085, avg. samples / sec: 24674.75
Iteration:   2640, Loss function: 3.843, Average Loss: 4.099, avg. samples / sec: 24622.32
Iteration:   2640, Loss function: 3.229, Average Loss: 4.091, avg. samples / sec: 24625.51
Iteration:   2640, Loss function: 3.190, Average Loss: 4.094, avg. samples / sec: 24653.03
Iteration:   2640, Loss function: 4.156, Average Loss: 4.095, avg. samples / sec: 24641.98
Iteration:   2640, Loss function: 3.689, Average Loss: 4.082, avg. samples / sec: 24640.77
Iteration:   2640, Loss function: 3.893, Average Loss: 4.102, avg. samples / sec: 24632.31
Iteration:   2640, Loss function: 3.483, Average Loss: 4.081, avg. samples / sec: 24599.18

:::MLPv0.5.0 ssd 1541757654.166430950 (train.py:553) train_epoch: 46
Iteration:   2660, Loss function: 3.487, Average Loss: 4.083, avg. samples / sec: 24597.30
Iteration:   2660, Loss function: 2.806, Average Loss: 4.069, avg. samples / sec: 24595.46
Iteration:   2660, Loss function: 3.792, Average Loss: 4.068, avg. samples / sec: 24625.89
Iteration:   2660, Loss function: 3.211, Average Loss: 4.086, avg. samples / sec: 24613.87
Iteration:   2660, Loss function: 3.652, Average Loss: 4.081, avg. samples / sec: 24589.26
Iteration:   2660, Loss function: 3.056, Average Loss: 4.081, avg. samples / sec: 24570.12
Iteration:   2660, Loss function: 3.835, Average Loss: 4.068, avg. samples / sec: 24597.78
Iteration:   2660, Loss function: 3.585, Average Loss: 4.076, avg. samples / sec: 24536.54
Iteration:   2680, Loss function: 3.210, Average Loss: 4.067, avg. samples / sec: 24672.57
Iteration:   2680, Loss function: 3.556, Average Loss: 4.054, avg. samples / sec: 24670.54
Iteration:   2680, Loss function: 3.538, Average Loss: 4.066, avg. samples / sec: 24687.80
Iteration:   2680, Loss function: 3.854, Average Loss: 4.054, avg. samples / sec: 24650.79
Iteration:   2680, Loss function: 3.117, Average Loss: 4.064, avg. samples / sec: 24666.95
Iteration:   2680, Loss function: 2.977, Average Loss: 4.061, avg. samples / sec: 24688.08
Iteration:   2680, Loss function: 3.006, Average Loss: 4.051, avg. samples / sec: 24665.41
Iteration:   2680, Loss function: 3.353, Average Loss: 4.071, avg. samples / sec: 24634.36
Iteration:   2700, Loss function: 2.965, Average Loss: 4.053, avg. samples / sec: 24640.34
Iteration:   2700, Loss function: 3.476, Average Loss: 4.039, avg. samples / sec: 24639.99
Iteration:   2700, Loss function: 3.431, Average Loss: 4.057, avg. samples / sec: 24693.83
Iteration:   2700, Loss function: 3.189, Average Loss: 4.050, avg. samples / sec: 24650.16
Iteration:   2700, Loss function: 3.314, Average Loss: 4.047, avg. samples / sec: 24655.38
Iteration:   2700, Loss function: 3.554, Average Loss: 4.038, avg. samples / sec: 24652.74
Iteration:   2700, Loss function: 3.477, Average Loss: 4.040, avg. samples / sec: 24621.79
Iteration:   2700, Loss function: 3.205, Average Loss: 4.049, avg. samples / sec: 24616.53

:::MLPv0.5.0 ssd 1541757658.987118006 (train.py:553) train_epoch: 47
Iteration:   2720, Loss function: 3.239, Average Loss: 4.034, avg. samples / sec: 24673.86
Iteration:   2720, Loss function: 2.876, Average Loss: 4.039, avg. samples / sec: 24624.16
Iteration:   2720, Loss function: 3.057, Average Loss: 4.026, avg. samples / sec: 24673.19
Iteration:   2720, Loss function: 3.091, Average Loss: 4.041, avg. samples / sec: 24618.40
Iteration:   2720, Loss function: 3.477, Average Loss: 4.032, avg. samples / sec: 24627.95
Iteration:   2720, Loss function: 3.125, Average Loss: 4.026, avg. samples / sec: 24635.16
Iteration:   2720, Loss function: 3.047, Average Loss: 4.033, avg. samples / sec: 24612.44
Iteration:   2720, Loss function: 3.117, Average Loss: 4.024, avg. samples / sec: 24565.28
Iteration:   2740, Loss function: 3.102, Average Loss: 4.022, avg. samples / sec: 24552.01
Iteration:   2740, Loss function: 3.330, Average Loss: 4.026, avg. samples / sec: 24555.41
Iteration:   2740, Loss function: 2.983, Average Loss: 4.020, avg. samples / sec: 24519.04
Iteration:   2740, Loss function: 3.454, Average Loss: 4.017, avg. samples / sec: 24546.36
Iteration:   2740, Loss function: 3.433, Average Loss: 4.019, avg. samples / sec: 24555.34
Iteration:   2740, Loss function: 3.270, Average Loss: 4.013, avg. samples / sec: 24506.15
Iteration:   2740, Loss function: 3.597, Average Loss: 4.010, avg. samples / sec: 24566.36
Iteration:   2740, Loss function: 3.472, Average Loss: 4.013, avg. samples / sec: 24532.48
Iteration:   2760, Loss function: 3.013, Average Loss: 4.009, avg. samples / sec: 24555.26
Iteration:   2760, Loss function: 3.734, Average Loss: 4.011, avg. samples / sec: 24559.97
Iteration:   2760, Loss function: 3.429, Average Loss: 3.995, avg. samples / sec: 24573.31
Iteration:   2760, Loss function: 3.043, Average Loss: 4.007, avg. samples / sec: 24555.65
Iteration:   2760, Loss function: 3.490, Average Loss: 4.002, avg. samples / sec: 24555.46
Iteration:   2760, Loss function: 3.305, Average Loss: 4.005, avg. samples / sec: 24555.79
Iteration:   2760, Loss function: 3.281, Average Loss: 3.999, avg. samples / sec: 24565.33
Iteration:   2760, Loss function: 3.684, Average Loss: 4.000, avg. samples / sec: 24539.48

:::MLPv0.5.0 ssd 1541757663.823440552 (train.py:553) train_epoch: 48
Iteration:   2780, Loss function: 3.093, Average Loss: 3.994, avg. samples / sec: 24550.81
Iteration:   2780, Loss function: 3.232, Average Loss: 3.996, avg. samples / sec: 24554.02
Iteration:   2780, Loss function: 3.557, Average Loss: 3.993, avg. samples / sec: 24560.38
Iteration:   2780, Loss function: 3.642, Average Loss: 3.987, avg. samples / sec: 24551.69
Iteration:   2780, Loss function: 3.130, Average Loss: 3.981, avg. samples / sec: 24542.04
Iteration:   2780, Loss function: 2.975, Average Loss: 3.990, avg. samples / sec: 24547.84
Iteration:   2780, Loss function: 3.464, Average Loss: 3.988, avg. samples / sec: 24534.03
Iteration:   2780, Loss function: 3.093, Average Loss: 3.985, avg. samples / sec: 24549.57
Iteration:   2800, Loss function: 3.166, Average Loss: 3.980, avg. samples / sec: 24593.46
Iteration:   2800, Loss function: 3.296, Average Loss: 3.972, avg. samples / sec: 24620.99
Iteration:   2800, Loss function: 3.043, Average Loss: 3.966, avg. samples / sec: 24622.19
Iteration:   2800, Loss function: 3.833, Average Loss: 3.981, avg. samples / sec: 24583.52
Iteration:   2800, Loss function: 3.182, Average Loss: 3.973, avg. samples / sec: 24590.23
Iteration:   2800, Loss function: 3.405, Average Loss: 3.975, avg. samples / sec: 24602.23
Iteration:   2800, Loss function: 3.504, Average Loss: 3.978, avg. samples / sec: 24554.75
Iteration:   2800, Loss function: 2.749, Average Loss: 3.969, avg. samples / sec: 24596.34

































































:::MLPv0.5.0 ssd 1541757667.159017801 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757667.159561634 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757667.160038471 (train.py:220) eval_start: 48
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.95 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.95 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.95 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.95 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.95 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.95 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.95 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.95 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(311196, 7)
(311196, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(311196, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/311196
(311196, 7)
Loading and preparing results...
0/311196
Converting ndarray to lists...
Converting ndarray to lists...
(311196, 7)
Loading and preparing results...
Loading and preparing results...
0/311196
0/311196
Loading and preparing results...
(311196, 7)
Loading and preparing results...
0/311196
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(311196, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(311196, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(311196, 7)
0/311196
Loading and preparing results...
0/311196
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(311196, 7)
Converting ndarray to lists...
(311196, 7)
Loading and preparing results...
Converting ndarray to lists...
(311196, 7)
Loading and preparing results...
0/311196
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/311196
Converting ndarray to lists...
Loading and preparing results...
(311196, 7)
Converting ndarray to lists...
Loading and preparing results...
(311196, 7)
(311196, 7)
(311196, 7)
(311196, 7)
0/311196
0/311196
0/311196
Loading and preparing results...
(311196, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(311196, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(311196, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/311196
0/311196
(311196, 7)
0/311196
(311196, 7)
(311196, 7)
(311196, 7)
0/311196
0/311196
(311196, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(311196, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/311196
0/311196
Converting ndarray to lists...
(311196, 7)
0/311196
(311196, 7)
0/311196
0/311196
Converting ndarray to lists...
Loading and preparing results...
0/311196
(311196, 7)
Converting ndarray to lists...
(311196, 7)
Converting ndarray to lists...
0/311196
Converting ndarray to lists...
Converting ndarray to lists...
(311196, 7)
(311196, 7)
0/311196
0/311196
Converting ndarray to lists...
0/311196
(311196, 7)
Loading and preparing results...
(311196, 7)
0/311196
Converting ndarray to lists...
0/311196
Converting ndarray to lists...
0/311196
0/311196
(311196, 7)
Converting ndarray to lists...
Loading and preparing results...
(311196, 7)
(311196, 7)
0/311196
Loading and preparing results...
(311196, 7)
(311196, 7)
Converting ndarray to lists...
0/311196
(311196, 7)
(311196, 7)
Converting ndarray to lists...
0/311196
0/311196
0/311196
0/311196
Converting ndarray to lists...
(311196, 7)
Converting ndarray to lists...
(311196, 7)
0/311196
0/311196
(311196, 7)
0/311196
0/311196
(311196, 7)
(311196, 7)
0/311196
(311196, 7)
0/311196
0/311196
0/311196
0/311196
0/311196
Loading and preparing results...
Converting ndarray to lists...
(311196, 7)
0/311196
Loading and preparing results...
Converting ndarray to lists...
(311196, 7)
Loading and preparing results...
0/311196
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(311196, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/311196
Converting ndarray to lists...
(311196, 7)
(311196, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(311196, 7)
0/311196
0/311196
(311196, 7)
Converting ndarray to lists...
(311196, 7)
0/311196
0/311196
(311196, 7)
0/311196
0/311196
Loading and preparing results...
Converting ndarray to lists...
(311196, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/311196
Loading and preparing results...
(311196, 7)
(311196, 7)
Converting ndarray to lists...
(311196, 7)
Loading and preparing results...
0/311196
0/311196
(311196, 7)
Converting ndarray to lists...
0/311196
0/311196
(311196, 7)
Converting ndarray to lists...
0/311196
(311196, 7)
Loading and preparing results...
0/311196
Converting ndarray to lists...
(311196, 7)
0/311196
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.78s)
creating index...
index created!
index created!
DONE (t=1.80s)
creating index...
index created!
index created!
index created!
index created!
index created!
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.83s)
creating index...
index created!
DONE (t=1.83s)
creating index...
index created!
DONE (t=1.83s)
creating index...
index created!
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=1.85s)
creating index...
DONE (t=1.86s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=1.87s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
DONE (t=1.95s)
creating index...
index created!
DONE (t=1.96s)
creating index...
index created!
DONE (t=1.96s)
creating index...
index created!
DONE (t=1.97s)
creating index...
index created!
DONE (t=1.97s)
creating index...
index created!
DONE (t=1.97s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=2.00s)
creating index...
index created!
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.05s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
DONE (t=3.23s).
Accumulating evaluation results...
DONE (t=3.29s).
Accumulating evaluation results...
DONE (t=3.31s).
Accumulating evaluation results...
DONE (t=3.30s).
Accumulating evaluation results...
DONE (t=3.30s).
Accumulating evaluation results...
DONE (t=3.30s).
Accumulating evaluation results...
DONE (t=3.32s).
Accumulating evaluation results...
DONE (t=3.32s).
Accumulating evaluation results...
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.308
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.093
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21350 AP goal: 0.21200
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.308
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.093
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21350 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.308
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.093
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21350 AP goal: 0.21200
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.308
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.093
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21350 AP goal: 0.21200
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.308
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.093
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21350 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.337
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.308
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.093
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21350 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.337
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.227
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.308
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.093
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21350 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.308
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.322
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.093
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21350 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757677.778559685 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757677.779256105 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.21350459093510618}

:::MLPv0.5.0 ssd 1541757677.779754639 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.21350459093510618}

:::MLPv0.5.0 ssd 1541757677.780225277 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757677.780738592 (train.py:338) eval_stop: 48

:::MLPv0.5.0 ssd 1541757678.891267776 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1541757678.891824245 (train.py:707) run_final
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2018-11-09 10:01:20 AM
RESULT,OBJECT_DETECTION,,328,nvidia,2018-11-09 09:55:52 AM
ENDING TIMING RUN AT 2018-11-09 10:01:26 AM
RESULT,OBJECT_DETECTION,,329,nvidia,2018-11-09 09:55:57 AM
ENDING TIMING RUN AT 2018-11-09 10:01:29 AM
RESULT,OBJECT_DETECTION,,328,nvidia,2018-11-09 09:56:01 AM
ENDING TIMING RUN AT 2018-11-09 10:01:24 AM
RESULT,OBJECT_DETECTION,,329,nvidia,2018-11-09 09:55:55 AM
ENDING TIMING RUN AT 2018-11-09 10:01:25 AM
RESULT,OBJECT_DETECTION,,328,nvidia,2018-11-09 09:55:57 AM
ENDING TIMING RUN AT 2018-11-09 10:01:20 AM
RESULT,OBJECT_DETECTION,,329,nvidia,2018-11-09 09:55:51 AM
ENDING TIMING RUN AT 2018-11-09 10:01:28 AM
RESULT,OBJECT_DETECTION,,328,nvidia,2018-11-09 09:56:00 AM
ENDING TIMING RUN AT 2018-11-09 10:01:25 AM
RESULT,OBJECT_DETECTION,,329,nvidia,2018-11-09 09:55:56 AM
