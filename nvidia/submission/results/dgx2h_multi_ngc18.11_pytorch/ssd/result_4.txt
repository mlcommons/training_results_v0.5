Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3

:::MLPv0.5.0 ssd 1541757380.482051611 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757389.055430174 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757395.746758223 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757385.902977943 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757391.065186501 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757386.208086014 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757379.661208391 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541757388.829266787 (<string>:1) run_clear_caches
Launching on node circe-n001
+ pids+=($!)
+ set +x
Launching on node circe-n002
+ pids+=($!)
+ set +x
Launching on node circe-n003
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n001
+ pids+=($!)
+ set +x
Launching on node circe-n004
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n002
+ srun -N 1 -n 1 -w circe-n001 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=0 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35124 -e SLURM_NTASKS_PER_NODE=8 cont_35124 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n007
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n003
+ srun -N 1 -n 1 -w circe-n002 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=1 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35124 -e SLURM_NTASKS_PER_NODE=8 cont_35124 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n010
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n004
+ srun -N 1 -n 1 -w circe-n003 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=2 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35124 -e SLURM_NTASKS_PER_NODE=8 cont_35124 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n031
+ srun -N 1 -n 1 -w circe-n004 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=3 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35124 -e SLURM_NTASKS_PER_NODE=8 cont_35124 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n007
+ pids+=($!)
+ set +x
Launching on node circe-n032
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n010
+ srun -N 1 -n 1 -w circe-n007 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=4 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35124 -e SLURM_NTASKS_PER_NODE=8 cont_35124 ./run_and_time.sh
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n031
+ srun -N 1 -n 1 -w circe-n010 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=5 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35124 -e SLURM_NTASKS_PER_NODE=8 cont_35124 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n032
+ srun -N 1 -n 1 -w circe-n031 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=6 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35124 -e SLURM_NTASKS_PER_NODE=8 cont_35124 ./run_and_time.sh
+ srun -N 1 -n 1 -w circe-n032 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=7 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35124 -e SLURM_NTASKS_PER_NODE=8 cont_35124 ./run_and_time.sh
Run vars: id 35124 gpus 8 mparams  --nnodes=8 --node_rank=4 --master_addr=10.0.1.1 --master_port=4242
Run vars: id 35124 gpus 8 mparams  --nnodes=8 --node_rank=1 --master_addr=10.0.1.1 --master_port=4242
Run vars: id 35124 gpus 8 mparams  --nnodes=8 --node_rank=5 --master_addr=10.0.1.1 --master_port=4242
Run vars: id 35124 gpus 8 mparams  --nnodes=8 --node_rank=6 --master_addr=10.0.1.1 --master_port=4242
Run vars: id 35124 gpus 8 mparams  --nnodes=8 --node_rank=0 --master_addr=10.0.1.1 --master_port=4242
Run vars: id 35124 gpus 8 mparams  --nnodes=8 --node_rank=2 --master_addr=10.0.1.1 --master_port=4242
Run vars: id 35124 gpus 8 mparams  --nnodes=8 --node_rank=3 --master_addr=10.0.1.1 --master_port=4242
STARTING TIMING RUN AT 2018-11-09 09:56:26 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=4 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:56:29 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=1 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:56:36 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=5 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
Run vars: id 35124 gpus 8 mparams  --nnodes=8 --node_rank=7 --master_addr=10.0.1.1 --master_port=4242
STARTING TIMING RUN AT 2018-11-09 09:56:29 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=6 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:56:19 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=0 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:56:26 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=3 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:56:21 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=2 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:56:31 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=7 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
0 Using seed = 4145125564
1 Using seed = 4145125565
2 Using seed = 4145125566
4 Using seed = 4145125568
3 Using seed = 4145125567
14 Using seed = 4145125578
13 Using seed = 4145125577
15 Using seed = 4145125579
12 Using seed = 4145125576
10 Using seed = 4145125574
8 Using seed = 4145125572
11 Using seed = 4145125575
9 Using seed = 4145125573
22 Using seed = 4145125586
23 Using seed = 4145125587
20 Using seed = 4145125584
21 Using seed = 4145125585
16 Using seed = 4145125580
19 Using seed = 4145125583
17 Using seed = 4145125581
18 Using seed = 4145125582
25 Using seed = 4145125589
27 Using seed = 4145125591
29 Using seed = 4145125593
26 Using seed = 4145125590
30 Using seed = 4145125594
24 Using seed = 4145125588
31 Using seed = 4145125595
28 Using seed = 4145125592
38 Using seed = 4145125602
39 Using seed = 4145125603
36 Using seed = 4145125600
37 Using seed = 4145125601
34 Using seed = 4145125598
32 Using seed = 4145125596
35 Using seed = 4145125599
33 Using seed = 4145125597
40 Using seed = 4145125604
41 Using seed = 4145125605
47 Using seed = 4145125611
42 Using seed = 4145125606
46 Using seed = 4145125610
45 Using seed = 4145125609
43 Using seed = 4145125607
44 Using seed = 4145125608
53 Using seed = 4145125617
55 Using seed = 4145125619
51 Using seed = 4145125615
49 Using seed = 4145125613
50 Using seed = 4145125614
48 Using seed = 4145125612
54 Using seed = 4145125618
52 Using seed = 4145125616
56 Using seed = 4145125620
59 Using seed = 4145125623
57 Using seed = 4145125621
63 Using seed = 4145125627
62 Using seed = 4145125626
60 Using seed = 4145125624
58 Using seed = 4145125622
61 Using seed = 4145125625
6 Using seed = 4145125570
7 Using seed = 4145125571
5 Using seed = 4145125569

:::MLPv0.5.0 ssd 1541757393.735394001 (train.py:371) run_start

:::MLPv0.5.0 ssd 1541757393.738774300 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1541757393.739315987 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1541757393.761207819 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1541757393.761705160 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1541757393.810567379 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1541757393.823105574 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1541757393.837640524 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1541757393.857523203 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1541757393.863429070 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1541757393.881600857 (train.py:382) input_size: 300
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.42s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
index created!
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
time_check a: 1541757401.331536531
time_check a: 1541757396.309182405
time_check a: 1541757401.046300888
time_check a: 1541757410.941462517
time_check a: 1541757406.210007906
time_check a: 1541757404.263380051
time_check a: 1541757394.800158501
time_check a: 1541757403.967137098
time_check b: 1541757428.142872572
time_check b: 1541757423.380409718
time_check b: 1541757418.355280638
time_check b: 1541757423.091471195
time_check b: 1541757416.910128355
time_check b: 1541757426.388811588
time_check b: 1541757426.098074198
time_check b: 1541757433.525833845

:::MLPv0.5.0 ssd 1541757417.882910252 (train.py:413) input_order

:::MLPv0.5.0 ssd 1541757417.895609379 (train.py:414) input_batch_size: 32

:::MLPv0.5.0 ssd 1541757419.148400545 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541757419.148995161 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541757419.175259113 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1541757420.947380781 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1541757420.947958946 (train.py:477) opt_learning_rate: 0.16

:::MLPv0.5.0 ssd 1541757420.948410273 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1541757420.948828936 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1541757420.949241877 (train.py:483) opt_learning_rate_warmup_steps: 900

:::MLPv0.5.0 ssd 1541757422.288687706 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541757422.289286613 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541757422.315594912 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss

:::MLPv0.5.0 ssd 1541757425.573855162 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1541757425.574437380 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1541757425.577358007 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 22.189, Average Loss: 0.022, avg. samples / sec: 22949.09
Iteration:      0, Loss function: 23.012, Average Loss: 0.023, avg. samples / sec: 24475.82
Iteration:      0, Loss function: 23.204, Average Loss: 0.023, avg. samples / sec: 18869.69
Iteration:      0, Loss function: 23.443, Average Loss: 0.023, avg. samples / sec: 20040.96
Iteration:      0, Loss function: 23.445, Average Loss: 0.023, avg. samples / sec: 17934.07
Iteration:      0, Loss function: 23.138, Average Loss: 0.023, avg. samples / sec: 11494.53
Iteration:      0, Loss function: 23.268, Average Loss: 0.023, avg. samples / sec: 20160.00
Iteration:      0, Loss function: 23.636, Average Loss: 0.024, avg. samples / sec: 24851.04

:::MLPv0.5.0 ssd 1541757428.068437338 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.0001777777777777767}

:::MLPv0.5.0 ssd 1541757428.435881615 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0003555555555555534}

:::MLPv0.5.0 ssd 1541757428.536649704 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0005333333333333301}

:::MLPv0.5.0 ssd 1541757428.639711857 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0007111111111111068}

:::MLPv0.5.0 ssd 1541757428.749551773 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0008888888888888835}

:::MLPv0.5.0 ssd 1541757428.853507519 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0010666666666666602}

:::MLPv0.5.0 ssd 1541757428.960337162 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.001244444444444437}

:::MLPv0.5.0 ssd 1541757429.062262535 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0014222222222222136}

:::MLPv0.5.0 ssd 1541757429.154622555 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0015999999999999903}

:::MLPv0.5.0 ssd 1541757429.251233339 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.001777777777777767}

:::MLPv0.5.0 ssd 1541757429.346812487 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0019555555555555437}

:::MLPv0.5.0 ssd 1541757429.451472998 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.0021333333333333204}

:::MLPv0.5.0 ssd 1541757429.542665482 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.002311111111111097}

:::MLPv0.5.0 ssd 1541757429.639929056 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.002488888888888874}

:::MLPv0.5.0 ssd 1541757429.729226589 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.0026666666666666505}

:::MLPv0.5.0 ssd 1541757429.833069324 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.0028444444444444272}

:::MLPv0.5.0 ssd 1541757429.920749903 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.0030222222222222317}

:::MLPv0.5.0 ssd 1541757430.014274120 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.0032000000000000084}

:::MLPv0.5.0 ssd 1541757430.103931427 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.003377777777777785}

:::MLPv0.5.0 ssd 1541757430.197067976 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.003555555555555562}
Iteration:     20, Loss function: 20.371, Average Loss: 0.442, avg. samples / sec: 8881.64
Iteration:     20, Loss function: 20.305, Average Loss: 0.443, avg. samples / sec: 8882.18
Iteration:     20, Loss function: 20.156, Average Loss: 0.441, avg. samples / sec: 8880.67
Iteration:     20, Loss function: 20.185, Average Loss: 0.445, avg. samples / sec: 8878.64
Iteration:     20, Loss function: 20.388, Average Loss: 0.440, avg. samples / sec: 8876.61
Iteration:     20, Loss function: 19.513, Average Loss: 0.443, avg. samples / sec: 8876.14
Iteration:     20, Loss function: 20.822, Average Loss: 0.439, avg. samples / sec: 8875.53
Iteration:     20, Loss function: 20.416, Average Loss: 0.442, avg. samples / sec: 8873.59

:::MLPv0.5.0 ssd 1541757430.285623312 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.0037333333333333385}

:::MLPv0.5.0 ssd 1541757430.374883175 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.003911111111111115}

:::MLPv0.5.0 ssd 1541757430.464551449 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.004088888888888892}

:::MLPv0.5.0 ssd 1541757430.552881718 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.004266666666666669}

:::MLPv0.5.0 ssd 1541757430.643221855 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.004444444444444445}

:::MLPv0.5.0 ssd 1541757430.737106085 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.004622222222222222}

:::MLPv0.5.0 ssd 1541757430.825285196 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.004799999999999999}

:::MLPv0.5.0 ssd 1541757430.915879726 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.004977777777777775}

:::MLPv0.5.0 ssd 1541757431.008164644 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.005155555555555552}

:::MLPv0.5.0 ssd 1541757431.095672607 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.005333333333333329}

:::MLPv0.5.0 ssd 1541757431.183430672 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.0055111111111111055}

:::MLPv0.5.0 ssd 1541757431.271188498 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.005688888888888882}

:::MLPv0.5.0 ssd 1541757431.360320568 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.005866666666666659}

:::MLPv0.5.0 ssd 1541757431.449389219 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.006044444444444436}

:::MLPv0.5.0 ssd 1541757431.536553383 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.006222222222222212}

:::MLPv0.5.0 ssd 1541757431.624524832 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.006399999999999989}

:::MLPv0.5.0 ssd 1541757431.711456060 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.006577777777777766}

:::MLPv0.5.0 ssd 1541757431.810971737 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.0067555555555555424}

:::MLPv0.5.0 ssd 1541757431.897696495 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.006933333333333319}

:::MLPv0.5.0 ssd 1541757431.986296177 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.007111111111111096}
Iteration:     40, Loss function: 16.061, Average Loss: 0.816, avg. samples / sec: 22889.73
Iteration:     40, Loss function: 15.490, Average Loss: 0.812, avg. samples / sec: 22921.82
Iteration:     40, Loss function: 14.719, Average Loss: 0.811, avg. samples / sec: 22887.48
Iteration:     40, Loss function: 15.904, Average Loss: 0.814, avg. samples / sec: 22907.56
Iteration:     40, Loss function: 15.488, Average Loss: 0.816, avg. samples / sec: 22940.01
Iteration:     40, Loss function: 16.316, Average Loss: 0.808, avg. samples / sec: 22904.97
Iteration:     40, Loss function: 15.787, Average Loss: 0.816, avg. samples / sec: 22890.69
Iteration:     40, Loss function: 15.511, Average Loss: 0.811, avg. samples / sec: 22809.95

:::MLPv0.5.0 ssd 1541757432.077996016 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.0072888888888888725}

:::MLPv0.5.0 ssd 1541757432.167814732 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.007466666666666649}

:::MLPv0.5.0 ssd 1541757432.256369114 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.007644444444444454}

:::MLPv0.5.0 ssd 1541757432.346931696 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.00782222222222223}

:::MLPv0.5.0 ssd 1541757432.439830065 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.008000000000000007}

:::MLPv0.5.0 ssd 1541757432.525547981 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.008177777777777784}

:::MLPv0.5.0 ssd 1541757432.613418818 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.00835555555555556}

:::MLPv0.5.0 ssd 1541757432.703674316 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.008533333333333337}

:::MLPv0.5.0 ssd 1541757432.794418335 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.008711111111111114}

:::MLPv0.5.0 ssd 1541757432.881774664 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.00888888888888889}

:::MLPv0.5.0 ssd 1541757432.968424797 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.009066666666666667}

:::MLPv0.5.0 ssd 1541757433.063267231 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.009244444444444444}

:::MLPv0.5.0 ssd 1541757433.149723291 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.00942222222222222}

:::MLPv0.5.0 ssd 1541757433.237256765 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.009599999999999997}

:::MLPv0.5.0 ssd 1541757433.323136806 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.009777777777777774}

:::MLPv0.5.0 ssd 1541757433.411307335 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.00995555555555555}

:::MLPv0.5.0 ssd 1541757433.500129461 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.010133333333333328}

:::MLPv0.5.0 ssd 1541757433.589847565 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1541757433.594120264 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 58, "value": 0.010311111111111104}

:::MLPv0.5.0 ssd 1541757433.686612368 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 59, "value": 0.010488888888888881}

:::MLPv0.5.0 ssd 1541757433.775976658 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 60, "value": 0.010666666666666658}
Iteration:     60, Loss function: 12.200, Average Loss: 1.045, avg. samples / sec: 22892.32
Iteration:     60, Loss function: 11.891, Average Loss: 1.051, avg. samples / sec: 22889.74
Iteration:     60, Loss function: 11.793, Average Loss: 1.053, avg. samples / sec: 22892.00
Iteration:     60, Loss function: 11.626, Average Loss: 1.045, avg. samples / sec: 22969.20
Iteration:     60, Loss function: 11.430, Average Loss: 1.052, avg. samples / sec: 22914.88
Iteration:     60, Loss function: 12.221, Average Loss: 1.051, avg. samples / sec: 22881.69
Iteration:     60, Loss function: 11.851, Average Loss: 1.045, avg. samples / sec: 22881.90
Iteration:     60, Loss function: 12.194, Average Loss: 1.046, avg. samples / sec: 22847.79

:::MLPv0.5.0 ssd 1541757433.861365795 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 61, "value": 0.010844444444444434}

:::MLPv0.5.0 ssd 1541757433.950836897 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 62, "value": 0.011022222222222211}

:::MLPv0.5.0 ssd 1541757434.035884619 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 63, "value": 0.011199999999999988}

:::MLPv0.5.0 ssd 1541757434.128121853 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 64, "value": 0.011377777777777764}

:::MLPv0.5.0 ssd 1541757434.215029240 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 65, "value": 0.011555555555555541}

:::MLPv0.5.0 ssd 1541757434.301482916 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 66, "value": 0.011733333333333318}

:::MLPv0.5.0 ssd 1541757434.395573854 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 67, "value": 0.011911111111111095}

:::MLPv0.5.0 ssd 1541757434.482521534 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 68, "value": 0.012088888888888899}

:::MLPv0.5.0 ssd 1541757434.568898201 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 69, "value": 0.012266666666666676}

:::MLPv0.5.0 ssd 1541757434.659249306 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 70, "value": 0.012444444444444452}

:::MLPv0.5.0 ssd 1541757434.745187998 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 71, "value": 0.012622222222222229}

:::MLPv0.5.0 ssd 1541757434.833793163 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 72, "value": 0.012800000000000006}

:::MLPv0.5.0 ssd 1541757434.927167416 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 73, "value": 0.012977777777777783}

:::MLPv0.5.0 ssd 1541757435.014625311 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 74, "value": 0.01315555555555556}

:::MLPv0.5.0 ssd 1541757435.099779129 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 75, "value": 0.013333333333333336}

:::MLPv0.5.0 ssd 1541757435.191880465 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 76, "value": 0.013511111111111113}

:::MLPv0.5.0 ssd 1541757435.279547453 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 77, "value": 0.01368888888888889}

:::MLPv0.5.0 ssd 1541757435.371709108 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 78, "value": 0.013866666666666666}

:::MLPv0.5.0 ssd 1541757435.457376957 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 79, "value": 0.014044444444444443}

:::MLPv0.5.0 ssd 1541757435.545012712 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 80, "value": 0.01422222222222222}
Iteration:     80, Loss function: 9.469, Average Loss: 1.235, avg. samples / sec: 23154.37
Iteration:     80, Loss function: 10.043, Average Loss: 1.244, avg. samples / sec: 23165.52
Iteration:     80, Loss function: 9.620, Average Loss: 1.241, avg. samples / sec: 23153.27
Iteration:     80, Loss function: 9.498, Average Loss: 1.241, avg. samples / sec: 23155.33
Iteration:     80, Loss function: 10.032, Average Loss: 1.241, avg. samples / sec: 23155.07
Iteration:     80, Loss function: 10.185, Average Loss: 1.235, avg. samples / sec: 23154.41
Iteration:     80, Loss function: 9.864, Average Loss: 1.237, avg. samples / sec: 23152.70
Iteration:     80, Loss function: 9.623, Average Loss: 1.237, avg. samples / sec: 23111.47

:::MLPv0.5.0 ssd 1541757435.631968498 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 81, "value": 0.014399999999999996}

:::MLPv0.5.0 ssd 1541757435.718388319 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 82, "value": 0.014577777777777773}

:::MLPv0.5.0 ssd 1541757435.810620308 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 83, "value": 0.01475555555555555}

:::MLPv0.5.0 ssd 1541757435.899466276 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 84, "value": 0.014933333333333326}

:::MLPv0.5.0 ssd 1541757435.990957737 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 85, "value": 0.015111111111111103}

:::MLPv0.5.0 ssd 1541757436.076875687 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 86, "value": 0.01528888888888888}

:::MLPv0.5.0 ssd 1541757436.161594868 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 87, "value": 0.015466666666666656}

:::MLPv0.5.0 ssd 1541757436.249739408 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 88, "value": 0.015644444444444433}

:::MLPv0.5.0 ssd 1541757436.336926699 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 89, "value": 0.01582222222222221}

:::MLPv0.5.0 ssd 1541757436.422614574 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 90, "value": 0.015999999999999986}

:::MLPv0.5.0 ssd 1541757436.508280993 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 91, "value": 0.016177777777777763}

:::MLPv0.5.0 ssd 1541757436.593363523 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 92, "value": 0.01635555555555554}

:::MLPv0.5.0 ssd 1541757436.682293415 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 93, "value": 0.016533333333333317}

:::MLPv0.5.0 ssd 1541757436.770796776 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 94, "value": 0.01671111111111112}

:::MLPv0.5.0 ssd 1541757436.858211756 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 95, "value": 0.016888888888888898}

:::MLPv0.5.0 ssd 1541757436.944496632 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 96, "value": 0.017066666666666674}

:::MLPv0.5.0 ssd 1541757437.029020309 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 97, "value": 0.01724444444444445}

:::MLPv0.5.0 ssd 1541757437.115966320 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 98, "value": 0.017422222222222228}

:::MLPv0.5.0 ssd 1541757437.201896429 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 99, "value": 0.017600000000000005}

:::MLPv0.5.0 ssd 1541757437.288563728 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 100, "value": 0.01777777777777778}
Iteration:    100, Loss function: 9.088, Average Loss: 1.401, avg. samples / sec: 23496.75
Iteration:    100, Loss function: 9.425, Average Loss: 1.398, avg. samples / sec: 23543.98
Iteration:    100, Loss function: 8.810, Average Loss: 1.403, avg. samples / sec: 23494.22
Iteration:    100, Loss function: 9.113, Average Loss: 1.396, avg. samples / sec: 23541.09
Iteration:    100, Loss function: 8.907, Average Loss: 1.393, avg. samples / sec: 23485.18
Iteration:    100, Loss function: 8.961, Average Loss: 1.400, avg. samples / sec: 23486.86
Iteration:    100, Loss function: 8.963, Average Loss: 1.404, avg. samples / sec: 23464.68
Iteration:    100, Loss function: 9.119, Average Loss: 1.395, avg. samples / sec: 23469.71

:::MLPv0.5.0 ssd 1541757437.375082970 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 101, "value": 0.017955555555555558}

:::MLPv0.5.0 ssd 1541757437.461670876 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 102, "value": 0.018133333333333335}

:::MLPv0.5.0 ssd 1541757437.548386097 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 103, "value": 0.01831111111111111}

:::MLPv0.5.0 ssd 1541757437.632603645 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 104, "value": 0.018488888888888888}

:::MLPv0.5.0 ssd 1541757437.719651937 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 105, "value": 0.018666666666666665}

:::MLPv0.5.0 ssd 1541757437.807112932 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 106, "value": 0.01884444444444444}

:::MLPv0.5.0 ssd 1541757437.893444777 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 107, "value": 0.019022222222222218}

:::MLPv0.5.0 ssd 1541757437.980307341 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 108, "value": 0.019199999999999995}

:::MLPv0.5.0 ssd 1541757438.065705299 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 109, "value": 0.01937777777777777}

:::MLPv0.5.0 ssd 1541757438.155240774 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 110, "value": 0.019555555555555548}

:::MLPv0.5.0 ssd 1541757438.240630627 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 111, "value": 0.019733333333333325}

:::MLPv0.5.0 ssd 1541757438.325576544 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 112, "value": 0.0199111111111111}

:::MLPv0.5.0 ssd 1541757438.412425280 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 113, "value": 0.02008888888888888}

:::MLPv0.5.0 ssd 1541757438.498403311 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 114, "value": 0.020266666666666655}

:::MLPv0.5.0 ssd 1541757438.583431482 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 115, "value": 0.020444444444444432}

:::MLPv0.5.0 ssd 1541757438.667412043 (train.py:553) train_epoch: 2

:::MLPv0.5.0 ssd 1541757438.671574593 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 116, "value": 0.02062222222222221}

:::MLPv0.5.0 ssd 1541757438.762522697 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 117, "value": 0.020799999999999985}

:::MLPv0.5.0 ssd 1541757438.848710299 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 118, "value": 0.020977777777777762}

:::MLPv0.5.0 ssd 1541757438.934578896 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 119, "value": 0.02115555555555554}

:::MLPv0.5.0 ssd 1541757439.019247770 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 120, "value": 0.021333333333333343}
Iteration:    120, Loss function: 9.115, Average Loss: 1.547, avg. samples / sec: 23676.19
Iteration:    120, Loss function: 9.026, Average Loss: 1.546, avg. samples / sec: 23672.54
Iteration:    120, Loss function: 9.009, Average Loss: 1.556, avg. samples / sec: 23661.00
Iteration:    120, Loss function: 8.815, Average Loss: 1.550, avg. samples / sec: 23656.16
Iteration:    120, Loss function: 8.770, Average Loss: 1.552, avg. samples / sec: 23650.80
Iteration:    120, Loss function: 8.940, Average Loss: 1.555, avg. samples / sec: 23692.48
Iteration:    120, Loss function: 9.369, Average Loss: 1.551, avg. samples / sec: 23654.45
Iteration:    120, Loss function: 9.360, Average Loss: 1.549, avg. samples / sec: 23678.70

:::MLPv0.5.0 ssd 1541757439.104718924 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 121, "value": 0.02151111111111112}

:::MLPv0.5.0 ssd 1541757439.188725471 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 122, "value": 0.021688888888888896}

:::MLPv0.5.0 ssd 1541757439.273097038 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 123, "value": 0.021866666666666673}

:::MLPv0.5.0 ssd 1541757439.358498812 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 124, "value": 0.02204444444444445}

:::MLPv0.5.0 ssd 1541757439.442661524 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 125, "value": 0.022222222222222227}

:::MLPv0.5.0 ssd 1541757439.526785851 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 126, "value": 0.022400000000000003}

:::MLPv0.5.0 ssd 1541757439.611331224 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 127, "value": 0.02257777777777778}

:::MLPv0.5.0 ssd 1541757439.695678473 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 128, "value": 0.022755555555555557}

:::MLPv0.5.0 ssd 1541757439.781531811 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 129, "value": 0.022933333333333333}

:::MLPv0.5.0 ssd 1541757439.866496086 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 130, "value": 0.02311111111111111}

:::MLPv0.5.0 ssd 1541757439.952609777 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 131, "value": 0.023288888888888887}

:::MLPv0.5.0 ssd 1541757440.038035393 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 132, "value": 0.023466666666666663}

:::MLPv0.5.0 ssd 1541757440.123087645 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 133, "value": 0.02364444444444444}

:::MLPv0.5.0 ssd 1541757440.209329605 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 134, "value": 0.023822222222222217}

:::MLPv0.5.0 ssd 1541757440.293612003 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 135, "value": 0.023999999999999994}

:::MLPv0.5.0 ssd 1541757440.379597425 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 136, "value": 0.02417777777777777}

:::MLPv0.5.0 ssd 1541757440.465244770 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 137, "value": 0.024355555555555547}

:::MLPv0.5.0 ssd 1541757440.554348230 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 138, "value": 0.024533333333333324}

:::MLPv0.5.0 ssd 1541757440.639245510 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 139, "value": 0.0247111111111111}

:::MLPv0.5.0 ssd 1541757440.725493193 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 140, "value": 0.024888888888888877}
Iteration:    140, Loss function: 8.473, Average Loss: 1.698, avg. samples / sec: 24026.48
Iteration:    140, Loss function: 8.643, Average Loss: 1.690, avg. samples / sec: 24001.61
Iteration:    140, Loss function: 8.361, Average Loss: 1.701, avg. samples / sec: 24017.04
Iteration:    140, Loss function: 8.316, Average Loss: 1.701, avg. samples / sec: 24000.40
Iteration:    140, Loss function: 8.468, Average Loss: 1.696, avg. samples / sec: 24020.93
Iteration:    140, Loss function: 8.940, Average Loss: 1.693, avg. samples / sec: 23982.40
Iteration:    140, Loss function: 8.948, Average Loss: 1.695, avg. samples / sec: 23994.58
Iteration:    140, Loss function: 8.652, Average Loss: 1.694, avg. samples / sec: 24028.62

:::MLPv0.5.0 ssd 1541757440.810812712 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 141, "value": 0.025066666666666654}

:::MLPv0.5.0 ssd 1541757440.894759417 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 142, "value": 0.02524444444444443}

:::MLPv0.5.0 ssd 1541757440.990436316 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 143, "value": 0.025422222222222207}

:::MLPv0.5.0 ssd 1541757441.075535297 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 144, "value": 0.025599999999999984}

:::MLPv0.5.0 ssd 1541757441.161696672 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 145, "value": 0.02577777777777779}

:::MLPv0.5.0 ssd 1541757441.247653723 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 146, "value": 0.025955555555555565}

:::MLPv0.5.0 ssd 1541757441.332556963 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 147, "value": 0.026133333333333342}

:::MLPv0.5.0 ssd 1541757441.416913271 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 148, "value": 0.02631111111111112}

:::MLPv0.5.0 ssd 1541757441.502032757 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 149, "value": 0.026488888888888895}

:::MLPv0.5.0 ssd 1541757441.588694334 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 150, "value": 0.026666666666666672}

:::MLPv0.5.0 ssd 1541757441.674131155 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 151, "value": 0.02684444444444445}

:::MLPv0.5.0 ssd 1541757441.766964197 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 152, "value": 0.027022222222222225}

:::MLPv0.5.0 ssd 1541757441.851944208 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 153, "value": 0.027200000000000002}

:::MLPv0.5.0 ssd 1541757441.938897133 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 154, "value": 0.02737777777777778}

:::MLPv0.5.0 ssd 1541757442.032850504 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 155, "value": 0.027555555555555555}

:::MLPv0.5.0 ssd 1541757442.117866278 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 156, "value": 0.027733333333333332}

:::MLPv0.5.0 ssd 1541757442.202419758 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 157, "value": 0.02791111111111111}

:::MLPv0.5.0 ssd 1541757442.287748098 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 158, "value": 0.028088888888888885}

:::MLPv0.5.0 ssd 1541757442.372830153 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 159, "value": 0.028266666666666662}

:::MLPv0.5.0 ssd 1541757442.456518888 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 160, "value": 0.02844444444444444}
Iteration:    160, Loss function: 8.802, Average Loss: 1.834, avg. samples / sec: 23670.55
Iteration:    160, Loss function: 8.494, Average Loss: 1.827, avg. samples / sec: 23666.79
Iteration:    160, Loss function: 8.239, Average Loss: 1.828, avg. samples / sec: 23677.17
Iteration:    160, Loss function: 8.625, Average Loss: 1.833, avg. samples / sec: 23646.53
Iteration:    160, Loss function: 8.546, Average Loss: 1.824, avg. samples / sec: 23641.60
Iteration:    160, Loss function: 8.798, Average Loss: 1.829, avg. samples / sec: 23652.85
Iteration:    160, Loss function: 8.653, Average Loss: 1.827, avg. samples / sec: 23647.42
Iteration:    160, Loss function: 8.579, Average Loss: 1.833, avg. samples / sec: 23635.19

:::MLPv0.5.0 ssd 1541757442.542940140 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 161, "value": 0.028622222222222216}

:::MLPv0.5.0 ssd 1541757442.627503395 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 162, "value": 0.028799999999999992}

:::MLPv0.5.0 ssd 1541757442.714489937 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 163, "value": 0.02897777777777777}

:::MLPv0.5.0 ssd 1541757442.799419641 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 164, "value": 0.029155555555555546}

:::MLPv0.5.0 ssd 1541757442.883879900 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 165, "value": 0.029333333333333322}

:::MLPv0.5.0 ssd 1541757442.969255209 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 166, "value": 0.0295111111111111}

:::MLPv0.5.0 ssd 1541757443.054455519 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 167, "value": 0.029688888888888876}

:::MLPv0.5.0 ssd 1541757443.138414383 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 168, "value": 0.029866666666666652}

:::MLPv0.5.0 ssd 1541757443.223591089 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 169, "value": 0.03004444444444443}

:::MLPv0.5.0 ssd 1541757443.309104204 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 170, "value": 0.030222222222222206}

:::MLPv0.5.0 ssd 1541757443.394655943 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 171, "value": 0.03040000000000001}

:::MLPv0.5.0 ssd 1541757443.481904745 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 172, "value": 0.030577777777777787}

:::MLPv0.5.0 ssd 1541757443.567860126 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 173, "value": 0.030755555555555564}

:::MLPv0.5.0 ssd 1541757443.654751062 (train.py:553) train_epoch: 3

:::MLPv0.5.0 ssd 1541757443.658883572 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 174, "value": 0.03093333333333334}

:::MLPv0.5.0 ssd 1541757443.743072510 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 175, "value": 0.031111111111111117}

:::MLPv0.5.0 ssd 1541757443.830069304 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 176, "value": 0.031288888888888894}

:::MLPv0.5.0 ssd 1541757443.915971041 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 177, "value": 0.03146666666666667}

:::MLPv0.5.0 ssd 1541757444.002297401 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 178, "value": 0.03164444444444445}

:::MLPv0.5.0 ssd 1541757444.086575508 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 179, "value": 0.031822222222222224}

:::MLPv0.5.0 ssd 1541757444.171983957 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 180, "value": 0.032}
Iteration:    180, Loss function: 7.565, Average Loss: 1.960, avg. samples / sec: 23920.14
Iteration:    180, Loss function: 7.813, Average Loss: 1.953, avg. samples / sec: 23908.09
Iteration:    180, Loss function: 7.937, Average Loss: 1.955, avg. samples / sec: 23882.39
Iteration:    180, Loss function: 7.904, Average Loss: 1.957, avg. samples / sec: 23880.33
Iteration:    180, Loss function: 8.180, Average Loss: 1.961, avg. samples / sec: 23862.26
Iteration:    180, Loss function: 8.059, Average Loss: 1.955, avg. samples / sec: 23885.80
Iteration:    180, Loss function: 8.240, Average Loss: 1.963, avg. samples / sec: 23837.41
Iteration:    180, Loss function: 8.453, Average Loss: 1.956, avg. samples / sec: 23850.96

:::MLPv0.5.0 ssd 1541757444.256731033 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 181, "value": 0.03217777777777778}

:::MLPv0.5.0 ssd 1541757444.342875957 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 182, "value": 0.032355555555555554}

:::MLPv0.5.0 ssd 1541757444.427806377 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 183, "value": 0.03253333333333333}

:::MLPv0.5.0 ssd 1541757444.512885332 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 184, "value": 0.03271111111111111}

:::MLPv0.5.0 ssd 1541757444.596968651 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 185, "value": 0.032888888888888884}

:::MLPv0.5.0 ssd 1541757444.681026459 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 186, "value": 0.03306666666666666}

:::MLPv0.5.0 ssd 1541757444.766324282 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 187, "value": 0.03324444444444444}

:::MLPv0.5.0 ssd 1541757444.851081610 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 188, "value": 0.033422222222222214}

:::MLPv0.5.0 ssd 1541757444.936378956 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 189, "value": 0.03359999999999999}

:::MLPv0.5.0 ssd 1541757445.021304846 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 190, "value": 0.03377777777777777}

:::MLPv0.5.0 ssd 1541757445.105803728 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 191, "value": 0.033955555555555544}

:::MLPv0.5.0 ssd 1541757445.191518784 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 192, "value": 0.03413333333333332}

:::MLPv0.5.0 ssd 1541757445.276194811 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 193, "value": 0.0343111111111111}

:::MLPv0.5.0 ssd 1541757445.368419647 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 194, "value": 0.034488888888888874}

:::MLPv0.5.0 ssd 1541757445.455977917 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 195, "value": 0.03466666666666665}

:::MLPv0.5.0 ssd 1541757445.543119192 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 196, "value": 0.03484444444444443}

:::MLPv0.5.0 ssd 1541757445.628145456 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 197, "value": 0.03502222222222222}

:::MLPv0.5.0 ssd 1541757445.712815046 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 198, "value": 0.035199999999999995}

:::MLPv0.5.0 ssd 1541757445.797694445 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 199, "value": 0.03537777777777777}

:::MLPv0.5.0 ssd 1541757445.883382082 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 200, "value": 0.03555555555555555}
Iteration:    200, Loss function: 8.297, Average Loss: 2.075, avg. samples / sec: 23933.83
Iteration:    200, Loss function: 7.211, Average Loss: 2.076, avg. samples / sec: 23933.62
Iteration:    200, Loss function: 8.476, Average Loss: 2.076, avg. samples / sec: 23924.01
Iteration:    200, Loss function: 7.869, Average Loss: 2.076, avg. samples / sec: 23976.55
Iteration:    200, Loss function: 7.743, Average Loss: 2.075, avg. samples / sec: 23931.71
Iteration:    200, Loss function: 8.022, Average Loss: 2.083, avg. samples / sec: 23899.45
Iteration:    200, Loss function: 8.205, Average Loss: 2.083, avg. samples / sec: 23912.66
Iteration:    200, Loss function: 7.694, Average Loss: 2.082, avg. samples / sec: 23918.35

:::MLPv0.5.0 ssd 1541757445.968187332 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 201, "value": 0.035733333333333325}

:::MLPv0.5.0 ssd 1541757446.054432631 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 202, "value": 0.0359111111111111}

:::MLPv0.5.0 ssd 1541757446.138207912 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 203, "value": 0.03608888888888889}

:::MLPv0.5.0 ssd 1541757446.222370148 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 204, "value": 0.03626666666666667}

:::MLPv0.5.0 ssd 1541757446.306729794 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 205, "value": 0.036444444444444446}

:::MLPv0.5.0 ssd 1541757446.390947104 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 206, "value": 0.03662222222222222}

:::MLPv0.5.0 ssd 1541757446.477450848 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 207, "value": 0.0368}

:::MLPv0.5.0 ssd 1541757446.561815262 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 208, "value": 0.036977777777777776}

:::MLPv0.5.0 ssd 1541757446.645889044 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 209, "value": 0.03715555555555555}

:::MLPv0.5.0 ssd 1541757446.731702089 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 210, "value": 0.03733333333333333}

:::MLPv0.5.0 ssd 1541757446.816947937 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 211, "value": 0.037511111111111106}

:::MLPv0.5.0 ssd 1541757446.901164293 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 212, "value": 0.03768888888888888}

:::MLPv0.5.0 ssd 1541757446.985898018 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 213, "value": 0.03786666666666666}

:::MLPv0.5.0 ssd 1541757447.070844412 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 214, "value": 0.038044444444444436}

:::MLPv0.5.0 ssd 1541757447.155193567 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 215, "value": 0.03822222222222221}

:::MLPv0.5.0 ssd 1541757447.239032030 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 216, "value": 0.038400000000000004}

:::MLPv0.5.0 ssd 1541757447.324520588 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 217, "value": 0.03857777777777778}

:::MLPv0.5.0 ssd 1541757447.408526421 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 218, "value": 0.03875555555555556}

:::MLPv0.5.0 ssd 1541757447.493464947 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 219, "value": 0.038933333333333334}

:::MLPv0.5.0 ssd 1541757447.579313755 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 220, "value": 0.03911111111111111}
Iteration:    220, Loss function: 7.848, Average Loss: 2.188, avg. samples / sec: 24155.46
Iteration:    220, Loss function: 7.713, Average Loss: 2.191, avg. samples / sec: 24175.00
Iteration:    220, Loss function: 8.126, Average Loss: 2.189, avg. samples / sec: 24152.14
Iteration:    220, Loss function: 7.753, Average Loss: 2.197, avg. samples / sec: 24202.06
Iteration:    220, Loss function: 7.707, Average Loss: 2.195, avg. samples / sec: 24177.39
Iteration:    220, Loss function: 8.322, Average Loss: 2.196, avg. samples / sec: 24167.72
Iteration:    220, Loss function: 7.607, Average Loss: 2.189, avg. samples / sec: 24151.21
Iteration:    220, Loss function: 7.922, Average Loss: 2.190, avg. samples / sec: 24128.84

:::MLPv0.5.0 ssd 1541757447.662973881 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 221, "value": 0.03928888888888889}

:::MLPv0.5.0 ssd 1541757447.747732878 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 222, "value": 0.039466666666666664}

:::MLPv0.5.0 ssd 1541757447.832427979 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 223, "value": 0.03964444444444444}

:::MLPv0.5.0 ssd 1541757447.916905880 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 224, "value": 0.03982222222222222}

:::MLPv0.5.0 ssd 1541757448.000360966 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 225, "value": 0.039999999999999994}

:::MLPv0.5.0 ssd 1541757448.086761236 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 226, "value": 0.04017777777777777}

:::MLPv0.5.0 ssd 1541757448.170962334 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 227, "value": 0.04035555555555555}

:::MLPv0.5.0 ssd 1541757448.255706549 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 228, "value": 0.04053333333333334}

:::MLPv0.5.0 ssd 1541757448.339853764 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 229, "value": 0.040711111111111115}

:::MLPv0.5.0 ssd 1541757448.424651861 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 230, "value": 0.04088888888888889}

:::MLPv0.5.0 ssd 1541757448.509026051 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 231, "value": 0.04106666666666667}

:::MLPv0.5.0 ssd 1541757448.590117693 (train.py:553) train_epoch: 4

:::MLPv0.5.0 ssd 1541757448.594342947 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 232, "value": 0.041244444444444445}

:::MLPv0.5.0 ssd 1541757448.678921223 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 233, "value": 0.04142222222222222}

:::MLPv0.5.0 ssd 1541757448.763318300 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 234, "value": 0.0416}

:::MLPv0.5.0 ssd 1541757448.851098299 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 235, "value": 0.041777777777777775}

:::MLPv0.5.0 ssd 1541757448.936310291 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 236, "value": 0.04195555555555555}

:::MLPv0.5.0 ssd 1541757449.020609617 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 237, "value": 0.04213333333333333}

:::MLPv0.5.0 ssd 1541757449.104574680 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 238, "value": 0.042311111111111105}

:::MLPv0.5.0 ssd 1541757449.188213825 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 239, "value": 0.04248888888888888}

:::MLPv0.5.0 ssd 1541757449.271812201 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 240, "value": 0.04266666666666666}
Iteration:    240, Loss function: 7.143, Average Loss: 2.303, avg. samples / sec: 24241.22
Iteration:    240, Loss function: 7.641, Average Loss: 2.299, avg. samples / sec: 24198.59
Iteration:    240, Loss function: 7.252, Average Loss: 2.299, avg. samples / sec: 24237.94
Iteration:    240, Loss function: 7.608, Average Loss: 2.297, avg. samples / sec: 24229.94
Iteration:    240, Loss function: 7.591, Average Loss: 2.300, avg. samples / sec: 24196.85
Iteration:    240, Loss function: 7.230, Average Loss: 2.305, avg. samples / sec: 24197.59
Iteration:    240, Loss function: 7.506, Average Loss: 2.304, avg. samples / sec: 24172.51
Iteration:    240, Loss function: 7.670, Average Loss: 2.303, avg. samples / sec: 24169.40

:::MLPv0.5.0 ssd 1541757449.356482506 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 241, "value": 0.04284444444444445}

:::MLPv0.5.0 ssd 1541757449.442441463 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 242, "value": 0.043022222222222226}

:::MLPv0.5.0 ssd 1541757449.526917219 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 243, "value": 0.0432}

:::MLPv0.5.0 ssd 1541757449.610542774 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 244, "value": 0.04337777777777778}

:::MLPv0.5.0 ssd 1541757449.694949389 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 245, "value": 0.043555555555555556}

:::MLPv0.5.0 ssd 1541757449.787518740 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 246, "value": 0.04373333333333333}

:::MLPv0.5.0 ssd 1541757449.875583410 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 247, "value": 0.04391111111111111}

:::MLPv0.5.0 ssd 1541757449.961513758 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 248, "value": 0.044088888888888886}

:::MLPv0.5.0 ssd 1541757450.045405865 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 249, "value": 0.04426666666666666}

:::MLPv0.5.0 ssd 1541757450.129121065 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 250, "value": 0.04444444444444444}

:::MLPv0.5.0 ssd 1541757450.213107347 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 251, "value": 0.044622222222222216}

:::MLPv0.5.0 ssd 1541757450.298531055 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 252, "value": 0.04479999999999999}

:::MLPv0.5.0 ssd 1541757450.385544538 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 253, "value": 0.04497777777777777}

:::MLPv0.5.0 ssd 1541757450.469640970 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 254, "value": 0.04515555555555556}

:::MLPv0.5.0 ssd 1541757450.553597689 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 255, "value": 0.04533333333333334}

:::MLPv0.5.0 ssd 1541757450.638099909 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 256, "value": 0.04551111111111111}

:::MLPv0.5.0 ssd 1541757450.722002983 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 257, "value": 0.04568888888888889}

:::MLPv0.5.0 ssd 1541757450.805701733 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 258, "value": 0.04586666666666667}

:::MLPv0.5.0 ssd 1541757450.889642477 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 259, "value": 0.04604444444444444}

:::MLPv0.5.0 ssd 1541757450.973636627 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 260, "value": 0.04622222222222222}
Iteration:    260, Loss function: 7.793, Average Loss: 2.406, avg. samples / sec: 24081.69
Iteration:    260, Loss function: 7.183, Average Loss: 2.401, avg. samples / sec: 24065.78
Iteration:    260, Loss function: 7.315, Average Loss: 2.402, avg. samples / sec: 24074.63
Iteration:    260, Loss function: 7.232, Average Loss: 2.405, avg. samples / sec: 24082.69
Iteration:    260, Loss function: 7.403, Average Loss: 2.403, avg. samples / sec: 24084.51
Iteration:    260, Loss function: 6.662, Average Loss: 2.405, avg. samples / sec: 24029.74
Iteration:    260, Loss function: 6.998, Average Loss: 2.398, avg. samples / sec: 24035.66
Iteration:    260, Loss function: 7.581, Average Loss: 2.403, avg. samples / sec: 24018.91

:::MLPv0.5.0 ssd 1541757451.057415247 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 261, "value": 0.0464}

:::MLPv0.5.0 ssd 1541757451.141810894 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 262, "value": 0.046577777777777774}

:::MLPv0.5.0 ssd 1541757451.226459980 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 263, "value": 0.04675555555555555}

:::MLPv0.5.0 ssd 1541757451.310411692 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 264, "value": 0.04693333333333333}

:::MLPv0.5.0 ssd 1541757451.394506454 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 265, "value": 0.047111111111111104}

:::MLPv0.5.0 ssd 1541757451.478925467 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 266, "value": 0.04728888888888888}

:::MLPv0.5.0 ssd 1541757451.563716888 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 267, "value": 0.04746666666666667}

:::MLPv0.5.0 ssd 1541757451.647465706 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 268, "value": 0.04764444444444445}

:::MLPv0.5.0 ssd 1541757451.731676102 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 269, "value": 0.047822222222222224}

:::MLPv0.5.0 ssd 1541757451.816241741 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 270, "value": 0.048}

:::MLPv0.5.0 ssd 1541757451.901983023 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 271, "value": 0.04817777777777778}

:::MLPv0.5.0 ssd 1541757451.986509800 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 272, "value": 0.048355555555555554}

:::MLPv0.5.0 ssd 1541757452.071390390 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 273, "value": 0.04853333333333333}

:::MLPv0.5.0 ssd 1541757452.155582190 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 274, "value": 0.04871111111111111}

:::MLPv0.5.0 ssd 1541757452.239707232 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 275, "value": 0.048888888888888885}

:::MLPv0.5.0 ssd 1541757452.326558352 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 276, "value": 0.04906666666666666}

:::MLPv0.5.0 ssd 1541757452.410517931 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 277, "value": 0.04924444444444444}

:::MLPv0.5.0 ssd 1541757452.495152235 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 278, "value": 0.049422222222222215}

:::MLPv0.5.0 ssd 1541757452.579779387 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 279, "value": 0.04959999999999999}

:::MLPv0.5.0 ssd 1541757452.663748264 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 280, "value": 0.04977777777777778}
Iteration:    280, Loss function: 7.732, Average Loss: 2.500, avg. samples / sec: 24271.90
Iteration:    280, Loss function: 7.477, Average Loss: 2.492, avg. samples / sec: 24270.58
Iteration:    280, Loss function: 7.409, Average Loss: 2.497, avg. samples / sec: 24257.34
Iteration:    280, Loss function: 6.943, Average Loss: 2.500, avg. samples / sec: 24282.90
Iteration:    280, Loss function: 7.795, Average Loss: 2.498, avg. samples / sec: 24238.45
Iteration:    280, Loss function: 7.336, Average Loss: 2.498, avg. samples / sec: 24223.88
Iteration:    280, Loss function: 7.887, Average Loss: 2.497, avg. samples / sec: 24213.25
Iteration:    280, Loss function: 7.505, Average Loss: 2.501, avg. samples / sec: 24184.99

:::MLPv0.5.0 ssd 1541757452.747725487 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 281, "value": 0.04995555555555556}

:::MLPv0.5.0 ssd 1541757452.832823753 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 282, "value": 0.050133333333333335}

:::MLPv0.5.0 ssd 1541757452.916716337 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 283, "value": 0.05031111111111111}

:::MLPv0.5.0 ssd 1541757453.000780344 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 284, "value": 0.05048888888888889}

:::MLPv0.5.0 ssd 1541757453.084573269 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 285, "value": 0.050666666666666665}

:::MLPv0.5.0 ssd 1541757453.168663979 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 286, "value": 0.05084444444444444}

:::MLPv0.5.0 ssd 1541757453.253018856 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 287, "value": 0.05102222222222222}

:::MLPv0.5.0 ssd 1541757453.337647915 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 288, "value": 0.051199999999999996}

:::MLPv0.5.0 ssd 1541757453.419014454 (train.py:553) train_epoch: 5

:::MLPv0.5.0 ssd 1541757453.423181057 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 289, "value": 0.05137777777777777}

:::MLPv0.5.0 ssd 1541757453.507407188 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 290, "value": 0.05155555555555555}

:::MLPv0.5.0 ssd 1541757453.591259956 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 291, "value": 0.051733333333333326}

:::MLPv0.5.0 ssd 1541757453.674909353 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 292, "value": 0.0519111111111111}

:::MLPv0.5.0 ssd 1541757453.760456562 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 293, "value": 0.05208888888888889}

:::MLPv0.5.0 ssd 1541757453.844306707 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 294, "value": 0.05226666666666667}

:::MLPv0.5.0 ssd 1541757453.927996159 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 295, "value": 0.052444444444444446}

:::MLPv0.5.0 ssd 1541757454.012207508 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 296, "value": 0.05262222222222222}

:::MLPv0.5.0 ssd 1541757454.098164320 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 297, "value": 0.0528}

:::MLPv0.5.0 ssd 1541757454.182573318 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 298, "value": 0.052977777777777776}

:::MLPv0.5.0 ssd 1541757454.267817736 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 299, "value": 0.05315555555555555}

:::MLPv0.5.0 ssd 1541757454.352098465 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 300, "value": 0.05333333333333333}
Iteration:    300, Loss function: 7.281, Average Loss: 2.596, avg. samples / sec: 24261.98
Iteration:    300, Loss function: 6.928, Average Loss: 2.589, avg. samples / sec: 24265.55
Iteration:    300, Loss function: 6.364, Average Loss: 2.595, avg. samples / sec: 24268.95
Iteration:    300, Loss function: 7.252, Average Loss: 2.595, avg. samples / sec: 24285.48
Iteration:    300, Loss function: 7.390, Average Loss: 2.592, avg. samples / sec: 24262.46
Iteration:    300, Loss function: 6.883, Average Loss: 2.595, avg. samples / sec: 24243.69
Iteration:    300, Loss function: 7.156, Average Loss: 2.597, avg. samples / sec: 24271.38
Iteration:    300, Loss function: 7.132, Average Loss: 2.596, avg. samples / sec: 24233.27

:::MLPv0.5.0 ssd 1541757454.436170816 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 301, "value": 0.053511111111111107}

:::MLPv0.5.0 ssd 1541757454.519965172 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 302, "value": 0.05368888888888888}

:::MLPv0.5.0 ssd 1541757454.604068518 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 303, "value": 0.05386666666666666}

:::MLPv0.5.0 ssd 1541757454.687965631 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 304, "value": 0.05404444444444444}

:::MLPv0.5.0 ssd 1541757454.772521496 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 305, "value": 0.05422222222222223}

:::MLPv0.5.0 ssd 1541757454.857404470 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 306, "value": 0.054400000000000004}

:::MLPv0.5.0 ssd 1541757454.941448450 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 307, "value": 0.05457777777777778}

:::MLPv0.5.0 ssd 1541757455.025556326 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 308, "value": 0.05475555555555556}

:::MLPv0.5.0 ssd 1541757455.109459400 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 309, "value": 0.054933333333333334}

:::MLPv0.5.0 ssd 1541757455.193593025 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 310, "value": 0.05511111111111111}

:::MLPv0.5.0 ssd 1541757455.279242992 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 311, "value": 0.05528888888888889}

:::MLPv0.5.0 ssd 1541757455.363324642 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 312, "value": 0.055466666666666664}

:::MLPv0.5.0 ssd 1541757455.447422981 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 313, "value": 0.05564444444444444}

:::MLPv0.5.0 ssd 1541757455.532017469 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 314, "value": 0.05582222222222222}

:::MLPv0.5.0 ssd 1541757455.616811037 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 315, "value": 0.055999999999999994}

:::MLPv0.5.0 ssd 1541757455.701472044 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 316, "value": 0.05617777777777777}

:::MLPv0.5.0 ssd 1541757455.785905838 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 317, "value": 0.05635555555555555}

:::MLPv0.5.0 ssd 1541757455.870205879 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 318, "value": 0.05653333333333334}

:::MLPv0.5.0 ssd 1541757455.954404116 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 319, "value": 0.056711111111111115}

:::MLPv0.5.0 ssd 1541757456.038699865 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 320, "value": 0.05688888888888889}
Iteration:    320, Loss function: 6.743, Average Loss: 2.684, avg. samples / sec: 24290.70
Iteration:    320, Loss function: 6.962, Average Loss: 2.678, avg. samples / sec: 24286.64
Iteration:    320, Loss function: 7.062, Average Loss: 2.682, avg. samples / sec: 24303.90
Iteration:    320, Loss function: 6.882, Average Loss: 2.682, avg. samples / sec: 24328.04
Iteration:    320, Loss function: 6.750, Average Loss: 2.684, avg. samples / sec: 24309.85
Iteration:    320, Loss function: 6.530, Average Loss: 2.675, avg. samples / sec: 24259.28
Iteration:    320, Loss function: 7.167, Average Loss: 2.681, avg. samples / sec: 24261.18
Iteration:    320, Loss function: 7.072, Average Loss: 2.682, avg. samples / sec: 24252.17

:::MLPv0.5.0 ssd 1541757456.124772549 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 321, "value": 0.05706666666666667}

:::MLPv0.5.0 ssd 1541757456.208394289 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 322, "value": 0.057244444444444445}

:::MLPv0.5.0 ssd 1541757456.293770313 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 323, "value": 0.05742222222222222}

:::MLPv0.5.0 ssd 1541757456.378377914 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 324, "value": 0.0576}

:::MLPv0.5.0 ssd 1541757456.462217331 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 325, "value": 0.057777777777777775}

:::MLPv0.5.0 ssd 1541757456.546146154 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 326, "value": 0.05795555555555555}

:::MLPv0.5.0 ssd 1541757456.629806042 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 327, "value": 0.05813333333333333}

:::MLPv0.5.0 ssd 1541757456.714465618 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 328, "value": 0.058311111111111105}

:::MLPv0.5.0 ssd 1541757456.798320055 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 329, "value": 0.05848888888888888}

:::MLPv0.5.0 ssd 1541757456.882347822 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 330, "value": 0.05866666666666666}

:::MLPv0.5.0 ssd 1541757456.967461586 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 331, "value": 0.05884444444444445}

:::MLPv0.5.0 ssd 1541757457.052294731 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 332, "value": 0.059022222222222226}

:::MLPv0.5.0 ssd 1541757457.136555433 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 333, "value": 0.0592}

:::MLPv0.5.0 ssd 1541757457.220940113 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 334, "value": 0.05937777777777778}

:::MLPv0.5.0 ssd 1541757457.306572914 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 335, "value": 0.059555555555555556}

:::MLPv0.5.0 ssd 1541757457.391071558 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 336, "value": 0.05973333333333333}

:::MLPv0.5.0 ssd 1541757457.475095987 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 337, "value": 0.05991111111111111}

:::MLPv0.5.0 ssd 1541757457.559147120 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 338, "value": 0.060088888888888886}

:::MLPv0.5.0 ssd 1541757457.643978596 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 339, "value": 0.06026666666666666}

:::MLPv0.5.0 ssd 1541757457.728515387 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 340, "value": 0.06044444444444444}
Iteration:    340, Loss function: 7.221, Average Loss: 2.771, avg. samples / sec: 24236.63
Iteration:    340, Loss function: 7.377, Average Loss: 2.766, avg. samples / sec: 24265.92
Iteration:    340, Loss function: 6.811, Average Loss: 2.764, avg. samples / sec: 24229.13
Iteration:    340, Loss function: 6.764, Average Loss: 2.760, avg. samples / sec: 24232.99
Iteration:    340, Loss function: 6.965, Average Loss: 2.765, avg. samples / sec: 24199.61
Iteration:    340, Loss function: 6.766, Average Loss: 2.769, avg. samples / sec: 24218.68
Iteration:    340, Loss function: 6.992, Average Loss: 2.769, avg. samples / sec: 24202.19
Iteration:    340, Loss function: 6.355, Average Loss: 2.766, avg. samples / sec: 24195.80

:::MLPv0.5.0 ssd 1541757457.813048363 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 341, "value": 0.060622222222222216}

:::MLPv0.5.0 ssd 1541757457.897500515 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 342, "value": 0.06079999999999999}

:::MLPv0.5.0 ssd 1541757457.983242273 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 343, "value": 0.06097777777777777}

:::MLPv0.5.0 ssd 1541757458.067451239 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 344, "value": 0.06115555555555556}

:::MLPv0.5.0 ssd 1541757458.151322603 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 345, "value": 0.06133333333333334}

:::MLPv0.5.0 ssd 1541757458.235188723 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 346, "value": 0.061511111111111114}

:::MLPv0.5.0 ssd 1541757458.316606522 (train.py:553) train_epoch: 6

:::MLPv0.5.0 ssd 1541757458.320725203 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 347, "value": 0.06168888888888889}

:::MLPv0.5.0 ssd 1541757458.405884981 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 348, "value": 0.06186666666666667}

:::MLPv0.5.0 ssd 1541757458.490113974 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 349, "value": 0.062044444444444444}

:::MLPv0.5.0 ssd 1541757458.574440241 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 350, "value": 0.06222222222222222}

:::MLPv0.5.0 ssd 1541757458.658017874 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 351, "value": 0.0624}

:::MLPv0.5.0 ssd 1541757458.742203951 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 352, "value": 0.06257777777777777}

:::MLPv0.5.0 ssd 1541757458.826495647 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 353, "value": 0.06275555555555555}

:::MLPv0.5.0 ssd 1541757458.910024643 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 354, "value": 0.06293333333333333}

:::MLPv0.5.0 ssd 1541757458.993569374 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 355, "value": 0.0631111111111111}

:::MLPv0.5.0 ssd 1541757459.077411413 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 356, "value": 0.0632888888888889}

:::MLPv0.5.0 ssd 1541757459.161394596 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 357, "value": 0.06346666666666667}

:::MLPv0.5.0 ssd 1541757459.245588064 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 358, "value": 0.06364444444444445}

:::MLPv0.5.0 ssd 1541757459.330480576 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 359, "value": 0.06382222222222222}

:::MLPv0.5.0 ssd 1541757459.414453268 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 360, "value": 0.064}
Iteration:    360, Loss function: 6.181, Average Loss: 2.850, avg. samples / sec: 24293.29
Iteration:    360, Loss function: 6.371, Average Loss: 2.850, avg. samples / sec: 24348.46
Iteration:    360, Loss function: 6.834, Average Loss: 2.845, avg. samples / sec: 24349.54
Iteration:    360, Loss function: 7.117, Average Loss: 2.847, avg. samples / sec: 24276.28
Iteration:    360, Loss function: 6.787, Average Loss: 2.844, avg. samples / sec: 24320.02
Iteration:    360, Loss function: 6.703, Average Loss: 2.841, avg. samples / sec: 24305.38
Iteration:    360, Loss function: 7.055, Average Loss: 2.842, avg. samples / sec: 24277.11
Iteration:    360, Loss function: 6.553, Average Loss: 2.845, avg. samples / sec: 24316.83

:::MLPv0.5.0 ssd 1541757459.497742891 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 361, "value": 0.06417777777777778}

:::MLPv0.5.0 ssd 1541757459.582613468 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 362, "value": 0.06435555555555555}

:::MLPv0.5.0 ssd 1541757459.667129993 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 363, "value": 0.06453333333333333}

:::MLPv0.5.0 ssd 1541757459.751858711 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 364, "value": 0.06471111111111111}

:::MLPv0.5.0 ssd 1541757459.835512638 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 365, "value": 0.06488888888888888}

:::MLPv0.5.0 ssd 1541757459.919310093 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 366, "value": 0.06506666666666666}

:::MLPv0.5.0 ssd 1541757460.003332138 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 367, "value": 0.06524444444444444}

:::MLPv0.5.0 ssd 1541757460.087723017 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 368, "value": 0.06542222222222221}

:::MLPv0.5.0 ssd 1541757460.171229839 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 369, "value": 0.0656}

:::MLPv0.5.0 ssd 1541757460.255697012 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 370, "value": 0.06577777777777778}

:::MLPv0.5.0 ssd 1541757460.339781046 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 371, "value": 0.06595555555555556}

:::MLPv0.5.0 ssd 1541757460.423698902 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 372, "value": 0.06613333333333334}

:::MLPv0.5.0 ssd 1541757460.507492065 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 373, "value": 0.06631111111111111}

:::MLPv0.5.0 ssd 1541757460.591747046 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 374, "value": 0.06648888888888889}

:::MLPv0.5.0 ssd 1541757460.676047087 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 375, "value": 0.06666666666666667}

:::MLPv0.5.0 ssd 1541757460.760837078 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 376, "value": 0.06684444444444444}

:::MLPv0.5.0 ssd 1541757460.844866037 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 377, "value": 0.06702222222222222}

:::MLPv0.5.0 ssd 1541757460.928373337 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 378, "value": 0.0672}

:::MLPv0.5.0 ssd 1541757461.012810707 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 379, "value": 0.06737777777777777}

:::MLPv0.5.0 ssd 1541757461.096908092 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 380, "value": 0.06755555555555555}
Iteration:    380, Loss function: 7.021, Average Loss: 2.928, avg. samples / sec: 24352.27
Iteration:    380, Loss function: 7.129, Average Loss: 2.924, avg. samples / sec: 24350.17
Iteration:    380, Loss function: 6.701, Average Loss: 2.923, avg. samples / sec: 24376.64
Iteration:    380, Loss function: 6.859, Average Loss: 2.926, avg. samples / sec: 24347.09
Iteration:    380, Loss function: 7.046, Average Loss: 2.924, avg. samples / sec: 24345.93
Iteration:    380, Loss function: 6.403, Average Loss: 2.930, avg. samples / sec: 24319.66
Iteration:    380, Loss function: 6.619, Average Loss: 2.919, avg. samples / sec: 24333.14
Iteration:    380, Loss function: 7.039, Average Loss: 2.921, avg. samples / sec: 24333.50

:::MLPv0.5.0 ssd 1541757461.180451393 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 381, "value": 0.06773333333333333}

:::MLPv0.5.0 ssd 1541757461.264663458 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 382, "value": 0.06791111111111112}

:::MLPv0.5.0 ssd 1541757461.348273993 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 383, "value": 0.0680888888888889}

:::MLPv0.5.0 ssd 1541757461.432578087 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 384, "value": 0.06826666666666667}

:::MLPv0.5.0 ssd 1541757461.517090082 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 385, "value": 0.06844444444444445}

:::MLPv0.5.0 ssd 1541757461.602286339 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 386, "value": 0.06862222222222222}

:::MLPv0.5.0 ssd 1541757461.686841488 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 387, "value": 0.0688}

:::MLPv0.5.0 ssd 1541757461.771369219 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 388, "value": 0.06897777777777778}

:::MLPv0.5.0 ssd 1541757461.855024099 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 389, "value": 0.06915555555555555}

:::MLPv0.5.0 ssd 1541757461.939992905 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 390, "value": 0.06933333333333333}

:::MLPv0.5.0 ssd 1541757462.024886131 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 391, "value": 0.0695111111111111}

:::MLPv0.5.0 ssd 1541757462.109424353 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 392, "value": 0.06968888888888888}

:::MLPv0.5.0 ssd 1541757462.193947792 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 393, "value": 0.06986666666666666}

:::MLPv0.5.0 ssd 1541757462.277568102 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 394, "value": 0.07004444444444444}

:::MLPv0.5.0 ssd 1541757462.361594200 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 395, "value": 0.07022222222222223}

:::MLPv0.5.0 ssd 1541757462.445907354 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 396, "value": 0.0704}

:::MLPv0.5.0 ssd 1541757462.530030489 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 397, "value": 0.07057777777777778}

:::MLPv0.5.0 ssd 1541757462.614230633 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 398, "value": 0.07075555555555556}

:::MLPv0.5.0 ssd 1541757462.697829008 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 399, "value": 0.07093333333333333}

:::MLPv0.5.0 ssd 1541757462.782433033 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 400, "value": 0.07111111111111111}
Iteration:    400, Loss function: 6.345, Average Loss: 2.998, avg. samples / sec: 24297.61
Iteration:    400, Loss function: 5.648, Average Loss: 2.993, avg. samples / sec: 24298.34
Iteration:    400, Loss function: 6.082, Average Loss: 2.997, avg. samples / sec: 24309.58
Iteration:    400, Loss function: 6.827, Average Loss: 2.997, avg. samples / sec: 24304.32
Iteration:    400, Loss function: 6.475, Average Loss: 2.994, avg. samples / sec: 24323.99
Iteration:    400, Loss function: 6.638, Average Loss: 3.000, avg. samples / sec: 24300.85
Iteration:    400, Loss function: 6.121, Average Loss: 2.990, avg. samples / sec: 24310.40
Iteration:    400, Loss function: 6.073, Average Loss: 2.996, avg. samples / sec: 24242.22

:::MLPv0.5.0 ssd 1541757462.866823673 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 401, "value": 0.07128888888888889}

:::MLPv0.5.0 ssd 1541757462.951203585 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 402, "value": 0.07146666666666666}

:::MLPv0.5.0 ssd 1541757463.035343170 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 403, "value": 0.07164444444444444}

:::MLPv0.5.0 ssd 1541757463.119900703 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 404, "value": 0.07182222222222222}

:::MLPv0.5.0 ssd 1541757463.202990294 (train.py:553) train_epoch: 7

:::MLPv0.5.0 ssd 1541757463.207167625 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 405, "value": 0.072}

:::MLPv0.5.0 ssd 1541757463.291134357 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 406, "value": 0.07217777777777777}

:::MLPv0.5.0 ssd 1541757463.375820398 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 407, "value": 0.07235555555555555}

:::MLPv0.5.0 ssd 1541757463.459936619 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 408, "value": 0.07253333333333334}

:::MLPv0.5.0 ssd 1541757463.543648720 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 409, "value": 0.07271111111111112}

:::MLPv0.5.0 ssd 1541757463.627663374 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 410, "value": 0.07288888888888889}

:::MLPv0.5.0 ssd 1541757463.711394787 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 411, "value": 0.07306666666666667}

:::MLPv0.5.0 ssd 1541757463.795187712 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 412, "value": 0.07324444444444445}

:::MLPv0.5.0 ssd 1541757463.879682779 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 413, "value": 0.07342222222222222}

:::MLPv0.5.0 ssd 1541757463.964076042 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 414, "value": 0.0736}

:::MLPv0.5.0 ssd 1541757464.048071384 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 415, "value": 0.07377777777777778}

:::MLPv0.5.0 ssd 1541757464.132087708 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 416, "value": 0.07395555555555555}

:::MLPv0.5.0 ssd 1541757464.216449022 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 417, "value": 0.07413333333333333}

:::MLPv0.5.0 ssd 1541757464.300920010 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 418, "value": 0.0743111111111111}

:::MLPv0.5.0 ssd 1541757464.384955406 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 419, "value": 0.07448888888888888}

:::MLPv0.5.0 ssd 1541757464.470372915 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 420, "value": 0.07466666666666666}
Iteration:    420, Loss function: 7.522, Average Loss: 3.069, avg. samples / sec: 24272.01
Iteration:    420, Loss function: 7.464, Average Loss: 3.066, avg. samples / sec: 24324.67
Iteration:    420, Loss function: 7.817, Average Loss: 3.070, avg. samples / sec: 24282.04
Iteration:    420, Loss function: 7.584, Average Loss: 3.067, avg. samples / sec: 24279.70
Iteration:    420, Loss function: 6.737, Average Loss: 3.066, avg. samples / sec: 24273.17
Iteration:    420, Loss function: 7.587, Average Loss: 3.062, avg. samples / sec: 24270.55
Iteration:    420, Loss function: 7.503, Average Loss: 3.072, avg. samples / sec: 24249.22
Iteration:    420, Loss function: 7.207, Average Loss: 3.064, avg. samples / sec: 24215.90

:::MLPv0.5.0 ssd 1541757464.554957867 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 421, "value": 0.07484444444444445}

:::MLPv0.5.0 ssd 1541757464.638661623 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 422, "value": 0.07502222222222223}

:::MLPv0.5.0 ssd 1541757464.723033428 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 423, "value": 0.0752}

:::MLPv0.5.0 ssd 1541757464.807100058 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 424, "value": 0.07537777777777778}

:::MLPv0.5.0 ssd 1541757464.891156435 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 425, "value": 0.07555555555555556}

:::MLPv0.5.0 ssd 1541757464.975349903 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 426, "value": 0.07573333333333333}

:::MLPv0.5.0 ssd 1541757465.059195995 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 427, "value": 0.07591111111111111}

:::MLPv0.5.0 ssd 1541757465.143490791 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 428, "value": 0.07608888888888889}

:::MLPv0.5.0 ssd 1541757465.227360725 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 429, "value": 0.07626666666666666}

:::MLPv0.5.0 ssd 1541757465.311977148 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 430, "value": 0.07644444444444444}

:::MLPv0.5.0 ssd 1541757465.396436930 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 431, "value": 0.07662222222222222}

:::MLPv0.5.0 ssd 1541757465.480450153 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 432, "value": 0.0768}

:::MLPv0.5.0 ssd 1541757465.564147949 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 433, "value": 0.07697777777777778}

:::MLPv0.5.0 ssd 1541757465.649041653 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 434, "value": 0.07715555555555556}

:::MLPv0.5.0 ssd 1541757465.733324528 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 435, "value": 0.07733333333333334}

:::MLPv0.5.0 ssd 1541757465.817886114 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 436, "value": 0.07751111111111111}

:::MLPv0.5.0 ssd 1541757465.902336836 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 437, "value": 0.07768888888888889}

:::MLPv0.5.0 ssd 1541757465.986179590 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 438, "value": 0.07786666666666667}

:::MLPv0.5.0 ssd 1541757466.070893288 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 439, "value": 0.07804444444444444}

:::MLPv0.5.0 ssd 1541757466.155525446 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 440, "value": 0.07822222222222222}
Iteration:    440, Loss function: 6.486, Average Loss: 3.144, avg. samples / sec: 24304.68
Iteration:    440, Loss function: 6.018, Average Loss: 3.139, avg. samples / sec: 24308.57
Iteration:    440, Loss function: 6.154, Average Loss: 3.134, avg. samples / sec: 24331.54
Iteration:    440, Loss function: 6.772, Average Loss: 3.144, avg. samples / sec: 24349.61
Iteration:    440, Loss function: 6.619, Average Loss: 3.147, avg. samples / sec: 24290.11
Iteration:    440, Loss function: 5.767, Average Loss: 3.139, avg. samples / sec: 24293.84
Iteration:    440, Loss function: 6.354, Average Loss: 3.141, avg. samples / sec: 24291.11
Iteration:    440, Loss function: 6.809, Average Loss: 3.135, avg. samples / sec: 24331.36

:::MLPv0.5.0 ssd 1541757466.239613533 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 441, "value": 0.0784}

:::MLPv0.5.0 ssd 1541757466.324450970 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 442, "value": 0.07857777777777777}

:::MLPv0.5.0 ssd 1541757466.409694433 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 443, "value": 0.07875555555555555}

:::MLPv0.5.0 ssd 1541757466.494020462 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 444, "value": 0.07893333333333333}

:::MLPv0.5.0 ssd 1541757466.578017712 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 445, "value": 0.0791111111111111}

:::MLPv0.5.0 ssd 1541757466.662856102 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 446, "value": 0.0792888888888889}

:::MLPv0.5.0 ssd 1541757466.746921539 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 447, "value": 0.07946666666666667}

:::MLPv0.5.0 ssd 1541757466.830865622 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 448, "value": 0.07964444444444445}

:::MLPv0.5.0 ssd 1541757466.915271282 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 449, "value": 0.07982222222222222}

:::MLPv0.5.0 ssd 1541757466.999512672 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 450, "value": 0.08}

:::MLPv0.5.0 ssd 1541757467.083746672 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 451, "value": 0.08017777777777778}

:::MLPv0.5.0 ssd 1541757467.168471098 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 452, "value": 0.08035555555555556}

:::MLPv0.5.0 ssd 1541757467.253852129 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 453, "value": 0.08053333333333333}

:::MLPv0.5.0 ssd 1541757467.338482141 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 454, "value": 0.08071111111111111}

:::MLPv0.5.0 ssd 1541757467.422914743 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 455, "value": 0.08088888888888889}

:::MLPv0.5.0 ssd 1541757467.506374359 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 456, "value": 0.08106666666666666}

:::MLPv0.5.0 ssd 1541757467.590808630 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 457, "value": 0.08124444444444444}

:::MLPv0.5.0 ssd 1541757467.674898624 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 458, "value": 0.08142222222222222}

:::MLPv0.5.0 ssd 1541757467.758502483 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 459, "value": 0.0816}

:::MLPv0.5.0 ssd 1541757467.842965603 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 460, "value": 0.08177777777777778}
Iteration:    460, Loss function: 6.385, Average Loss: 3.207, avg. samples / sec: 24276.94
Iteration:    460, Loss function: 5.754, Average Loss: 3.200, avg. samples / sec: 24296.30
Iteration:    460, Loss function: 6.239, Average Loss: 3.201, avg. samples / sec: 24289.69
Iteration:    460, Loss function: 5.966, Average Loss: 3.201, avg. samples / sec: 24263.07
Iteration:    460, Loss function: 6.356, Average Loss: 3.197, avg. samples / sec: 24257.68
Iteration:    460, Loss function: 5.882, Average Loss: 3.210, avg. samples / sec: 24274.59
Iteration:    460, Loss function: 5.939, Average Loss: 3.195, avg. samples / sec: 24277.01
Iteration:    460, Loss function: 6.640, Average Loss: 3.205, avg. samples / sec: 24234.20

:::MLPv0.5.0 ssd 1541757467.927269936 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 461, "value": 0.08195555555555556}

:::MLPv0.5.0 ssd 1541757468.011240959 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 462, "value": 0.08213333333333334}

:::MLPv0.5.0 ssd 1541757468.093066454 (train.py:553) train_epoch: 8

:::MLPv0.5.0 ssd 1541757468.097292185 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 463, "value": 0.08231111111111111}

:::MLPv0.5.0 ssd 1541757468.181447983 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 464, "value": 0.08248888888888889}

:::MLPv0.5.0 ssd 1541757468.265687227 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 465, "value": 0.08266666666666667}

:::MLPv0.5.0 ssd 1541757468.349897623 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 466, "value": 0.08284444444444444}

:::MLPv0.5.0 ssd 1541757468.433887482 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 467, "value": 0.08302222222222222}

:::MLPv0.5.0 ssd 1541757468.518202543 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 468, "value": 0.0832}

:::MLPv0.5.0 ssd 1541757468.602389812 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 469, "value": 0.08337777777777777}

:::MLPv0.5.0 ssd 1541757468.686835051 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 470, "value": 0.08355555555555555}

:::MLPv0.5.0 ssd 1541757468.771377802 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 471, "value": 0.08373333333333333}

:::MLPv0.5.0 ssd 1541757468.855175257 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 472, "value": 0.08391111111111112}

:::MLPv0.5.0 ssd 1541757468.939317226 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 473, "value": 0.0840888888888889}

:::MLPv0.5.0 ssd 1541757469.023438692 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 474, "value": 0.08426666666666667}

:::MLPv0.5.0 ssd 1541757469.107669592 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 475, "value": 0.08444444444444445}

:::MLPv0.5.0 ssd 1541757469.192124844 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 476, "value": 0.08462222222222222}

:::MLPv0.5.0 ssd 1541757469.276456356 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 477, "value": 0.0848}

:::MLPv0.5.0 ssd 1541757469.361219883 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 478, "value": 0.08497777777777778}

:::MLPv0.5.0 ssd 1541757469.445168972 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 479, "value": 0.08515555555555555}

:::MLPv0.5.0 ssd 1541757469.529138088 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 480, "value": 0.08533333333333333}
Iteration:    480, Loss function: 5.964, Average Loss: 3.263, avg. samples / sec: 24345.26
Iteration:    480, Loss function: 5.842, Average Loss: 3.264, avg. samples / sec: 24290.76
Iteration:    480, Loss function: 6.056, Average Loss: 3.265, avg. samples / sec: 24312.20
Iteration:    480, Loss function: 5.801, Average Loss: 3.255, avg. samples / sec: 24310.46
Iteration:    480, Loss function: 6.181, Average Loss: 3.256, avg. samples / sec: 24277.24
Iteration:    480, Loss function: 6.016, Average Loss: 3.258, avg. samples / sec: 24267.64
Iteration:    480, Loss function: 5.729, Average Loss: 3.260, avg. samples / sec: 24266.42
Iteration:    480, Loss function: 6.520, Average Loss: 3.255, avg. samples / sec: 24278.46

:::MLPv0.5.0 ssd 1541757469.616034985 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 481, "value": 0.08551111111111111}

:::MLPv0.5.0 ssd 1541757469.702159882 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 482, "value": 0.08568888888888888}

:::MLPv0.5.0 ssd 1541757469.786485434 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 483, "value": 0.08586666666666666}

:::MLPv0.5.0 ssd 1541757469.870739460 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 484, "value": 0.08604444444444445}

:::MLPv0.5.0 ssd 1541757469.955107689 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 485, "value": 0.08622222222222223}

:::MLPv0.5.0 ssd 1541757470.039263248 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 486, "value": 0.0864}

:::MLPv0.5.0 ssd 1541757470.123388767 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 487, "value": 0.08657777777777778}

:::MLPv0.5.0 ssd 1541757470.207344294 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 488, "value": 0.08675555555555556}

:::MLPv0.5.0 ssd 1541757470.291786671 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 489, "value": 0.08693333333333333}

:::MLPv0.5.0 ssd 1541757470.375760555 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 490, "value": 0.08711111111111111}

:::MLPv0.5.0 ssd 1541757470.459411621 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 491, "value": 0.08728888888888889}

:::MLPv0.5.0 ssd 1541757470.543696404 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 492, "value": 0.08746666666666666}

:::MLPv0.5.0 ssd 1541757470.628220081 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 493, "value": 0.08764444444444444}

:::MLPv0.5.0 ssd 1541757470.712734699 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 494, "value": 0.08782222222222222}

:::MLPv0.5.0 ssd 1541757470.796465397 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 495, "value": 0.088}

:::MLPv0.5.0 ssd 1541757470.881391287 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 496, "value": 0.08817777777777777}

:::MLPv0.5.0 ssd 1541757470.965018034 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 497, "value": 0.08835555555555556}

:::MLPv0.5.0 ssd 1541757471.049476862 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 498, "value": 0.08853333333333334}

:::MLPv0.5.0 ssd 1541757471.133437157 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 499, "value": 0.08871111111111112}

:::MLPv0.5.0 ssd 1541757471.217495203 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 500, "value": 0.08888888888888889}
Iteration:    500, Loss function: 5.925, Average Loss: 3.320, avg. samples / sec: 24264.15
Iteration:    500, Loss function: 6.244, Average Loss: 3.315, avg. samples / sec: 24296.45
Iteration:    500, Loss function: 6.549, Average Loss: 3.321, avg. samples / sec: 24259.96
Iteration:    500, Loss function: 5.956, Average Loss: 3.318, avg. samples / sec: 24293.53
Iteration:    500, Loss function: 6.635, Average Loss: 3.315, avg. samples / sec: 24242.84
Iteration:    500, Loss function: 6.712, Average Loss: 3.314, avg. samples / sec: 24242.64
Iteration:    500, Loss function: 6.072, Average Loss: 3.321, avg. samples / sec: 24212.04
Iteration:    500, Loss function: 6.347, Average Loss: 3.311, avg. samples / sec: 24250.18

:::MLPv0.5.0 ssd 1541757471.301973820 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 501, "value": 0.08906666666666667}

:::MLPv0.5.0 ssd 1541757471.385735273 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 502, "value": 0.08924444444444445}

:::MLPv0.5.0 ssd 1541757471.469643831 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 503, "value": 0.08942222222222222}

:::MLPv0.5.0 ssd 1541757471.554218292 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 504, "value": 0.0896}

:::MLPv0.5.0 ssd 1541757471.638465166 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 505, "value": 0.08977777777777778}

:::MLPv0.5.0 ssd 1541757471.722699404 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 506, "value": 0.08995555555555555}

:::MLPv0.5.0 ssd 1541757471.806507111 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 507, "value": 0.09013333333333333}

:::MLPv0.5.0 ssd 1541757471.890492439 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 508, "value": 0.0903111111111111}

:::MLPv0.5.0 ssd 1541757471.974822998 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 509, "value": 0.09048888888888888}

:::MLPv0.5.0 ssd 1541757472.059005260 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 510, "value": 0.09066666666666667}

:::MLPv0.5.0 ssd 1541757472.143563032 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 511, "value": 0.09084444444444445}

:::MLPv0.5.0 ssd 1541757472.227663755 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 512, "value": 0.09102222222222223}

:::MLPv0.5.0 ssd 1541757472.311769009 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 513, "value": 0.0912}

:::MLPv0.5.0 ssd 1541757472.396386385 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 514, "value": 0.09137777777777778}

:::MLPv0.5.0 ssd 1541757472.480929136 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 515, "value": 0.09155555555555556}

:::MLPv0.5.0 ssd 1541757472.565470934 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 516, "value": 0.09173333333333333}

:::MLPv0.5.0 ssd 1541757472.649851322 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 517, "value": 0.09191111111111111}

:::MLPv0.5.0 ssd 1541757472.733334541 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 518, "value": 0.09208888888888889}

:::MLPv0.5.0 ssd 1541757472.817360163 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 519, "value": 0.09226666666666666}

:::MLPv0.5.0 ssd 1541757472.898539305 (train.py:553) train_epoch: 9

:::MLPv0.5.0 ssd 1541757472.902744770 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 520, "value": 0.09244444444444444}
Iteration:    520, Loss function: 5.738, Average Loss: 3.367, avg. samples / sec: 24317.26
Iteration:    520, Loss function: 5.742, Average Loss: 3.371, avg. samples / sec: 24299.30
Iteration:    520, Loss function: 6.063, Average Loss: 3.374, avg. samples / sec: 24310.92
Iteration:    520, Loss function: 5.447, Average Loss: 3.361, avg. samples / sec: 24351.59
Iteration:    520, Loss function: 6.274, Average Loss: 3.365, avg. samples / sec: 24302.46
Iteration:    520, Loss function: 5.440, Average Loss: 3.368, avg. samples / sec: 24279.54
Iteration:    520, Loss function: 5.801, Average Loss: 3.372, avg. samples / sec: 24316.38
Iteration:    520, Loss function: 5.626, Average Loss: 3.367, avg. samples / sec: 24301.21

:::MLPv0.5.0 ssd 1541757472.986849070 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 521, "value": 0.09262222222222222}

:::MLPv0.5.0 ssd 1541757473.071249247 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 522, "value": 0.0928}

:::MLPv0.5.0 ssd 1541757473.155373335 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 523, "value": 0.09297777777777778}

:::MLPv0.5.0 ssd 1541757473.239347935 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 524, "value": 0.09315555555555556}

:::MLPv0.5.0 ssd 1541757473.324527979 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 525, "value": 0.09333333333333334}

:::MLPv0.5.0 ssd 1541757473.409440756 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 526, "value": 0.09351111111111111}

:::MLPv0.5.0 ssd 1541757473.495504856 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 527, "value": 0.09368888888888889}

:::MLPv0.5.0 ssd 1541757473.580262184 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 528, "value": 0.09386666666666667}

:::MLPv0.5.0 ssd 1541757473.664529324 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 529, "value": 0.09404444444444444}

:::MLPv0.5.0 ssd 1541757473.749057293 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 530, "value": 0.09422222222222222}

:::MLPv0.5.0 ssd 1541757473.834069490 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 531, "value": 0.0944}

:::MLPv0.5.0 ssd 1541757473.918538094 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 532, "value": 0.09457777777777777}

:::MLPv0.5.0 ssd 1541757474.003287315 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 533, "value": 0.09475555555555555}

:::MLPv0.5.0 ssd 1541757474.092000008 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 534, "value": 0.09493333333333333}

:::MLPv0.5.0 ssd 1541757474.176707745 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 535, "value": 0.0951111111111111}

:::MLPv0.5.0 ssd 1541757474.261270761 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 536, "value": 0.0952888888888889}

:::MLPv0.5.0 ssd 1541757474.345361471 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 537, "value": 0.09546666666666667}

:::MLPv0.5.0 ssd 1541757474.429470301 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 538, "value": 0.09564444444444445}

:::MLPv0.5.0 ssd 1541757474.513718605 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 539, "value": 0.09582222222222223}

:::MLPv0.5.0 ssd 1541757474.597931623 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 540, "value": 0.096}
Iteration:    540, Loss function: 6.030, Average Loss: 3.417, avg. samples / sec: 24167.67
Iteration:    540, Loss function: 6.528, Average Loss: 3.421, avg. samples / sec: 24169.11
Iteration:    540, Loss function: 6.188, Average Loss: 3.422, avg. samples / sec: 24164.47
Iteration:    540, Loss function: 6.416, Average Loss: 3.411, avg. samples / sec: 24160.21
Iteration:    540, Loss function: 6.167, Average Loss: 3.414, avg. samples / sec: 24161.64
Iteration:    540, Loss function: 6.330, Average Loss: 3.424, avg. samples / sec: 24164.93
Iteration:    540, Loss function: 5.911, Average Loss: 3.417, avg. samples / sec: 24160.43
Iteration:    540, Loss function: 6.101, Average Loss: 3.416, avg. samples / sec: 24141.57

:::MLPv0.5.0 ssd 1541757474.682427168 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 541, "value": 0.09617777777777778}

:::MLPv0.5.0 ssd 1541757474.766529799 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 542, "value": 0.09635555555555556}

:::MLPv0.5.0 ssd 1541757474.851223707 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 543, "value": 0.09653333333333333}

:::MLPv0.5.0 ssd 1541757474.935598850 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 544, "value": 0.09671111111111111}

:::MLPv0.5.0 ssd 1541757475.020313501 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 545, "value": 0.09688888888888889}

:::MLPv0.5.0 ssd 1541757475.104320765 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 546, "value": 0.09706666666666666}

:::MLPv0.5.0 ssd 1541757475.189139366 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 547, "value": 0.09724444444444444}

:::MLPv0.5.0 ssd 1541757475.273325682 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 548, "value": 0.09742222222222222}

:::MLPv0.5.0 ssd 1541757475.358019590 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 549, "value": 0.09759999999999999}

:::MLPv0.5.0 ssd 1541757475.442238808 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 550, "value": 0.09777777777777777}

:::MLPv0.5.0 ssd 1541757475.526243925 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 551, "value": 0.09795555555555555}

:::MLPv0.5.0 ssd 1541757475.610276461 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 552, "value": 0.09813333333333334}

:::MLPv0.5.0 ssd 1541757475.694874763 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 553, "value": 0.09831111111111111}

:::MLPv0.5.0 ssd 1541757475.779655933 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 554, "value": 0.09848888888888889}

:::MLPv0.5.0 ssd 1541757475.864084005 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 555, "value": 0.09866666666666667}

:::MLPv0.5.0 ssd 1541757475.948234797 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 556, "value": 0.09884444444444444}

:::MLPv0.5.0 ssd 1541757476.033046007 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 557, "value": 0.09902222222222222}

:::MLPv0.5.0 ssd 1541757476.117517471 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 558, "value": 0.09920000000000001}

:::MLPv0.5.0 ssd 1541757476.201874971 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 559, "value": 0.09937777777777779}

:::MLPv0.5.0 ssd 1541757476.285698652 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 560, "value": 0.09955555555555556}
Iteration:    560, Loss function: 5.463, Average Loss: 3.464, avg. samples / sec: 24298.63
Iteration:    560, Loss function: 5.690, Average Loss: 3.468, avg. samples / sec: 24251.36
Iteration:    560, Loss function: 5.826, Average Loss: 3.466, avg. samples / sec: 24309.64
Iteration:    560, Loss function: 5.694, Average Loss: 3.463, avg. samples / sec: 24272.53
Iteration:    560, Loss function: 5.942, Average Loss: 3.472, avg. samples / sec: 24266.88
Iteration:    560, Loss function: 5.338, Average Loss: 3.465, avg. samples / sec: 24222.25
Iteration:    560, Loss function: 5.613, Average Loss: 3.470, avg. samples / sec: 24226.48
Iteration:    560, Loss function: 5.260, Average Loss: 3.460, avg. samples / sec: 24236.06

:::MLPv0.5.0 ssd 1541757476.369861841 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 561, "value": 0.09973333333333334}

:::MLPv0.5.0 ssd 1541757476.454897404 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 562, "value": 0.09991111111111112}

:::MLPv0.5.0 ssd 1541757476.538968563 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 563, "value": 0.1000888888888889}

:::MLPv0.5.0 ssd 1541757476.623219967 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 564, "value": 0.10026666666666667}

:::MLPv0.5.0 ssd 1541757476.707098484 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 565, "value": 0.10044444444444445}

:::MLPv0.5.0 ssd 1541757476.791183710 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 566, "value": 0.10062222222222222}

:::MLPv0.5.0 ssd 1541757476.875286341 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 567, "value": 0.1008}

:::MLPv0.5.0 ssd 1541757476.959522486 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 568, "value": 0.10097777777777778}

:::MLPv0.5.0 ssd 1541757477.044073582 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 569, "value": 0.10115555555555555}

:::MLPv0.5.0 ssd 1541757477.128305197 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 570, "value": 0.10133333333333333}

:::MLPv0.5.0 ssd 1541757477.212164879 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 571, "value": 0.10151111111111111}

:::MLPv0.5.0 ssd 1541757477.296569109 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 572, "value": 0.10168888888888888}

:::MLPv0.5.0 ssd 1541757477.380874395 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 573, "value": 0.10186666666666666}

:::MLPv0.5.0 ssd 1541757477.465460539 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 574, "value": 0.10204444444444444}

:::MLPv0.5.0 ssd 1541757477.549003839 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 575, "value": 0.10222222222222221}

:::MLPv0.5.0 ssd 1541757477.633295536 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 576, "value": 0.10239999999999999}

:::MLPv0.5.0 ssd 1541757477.717934847 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 577, "value": 0.10257777777777778}

:::MLPv0.5.0 ssd 1541757477.799622774 (train.py:553) train_epoch: 10

:::MLPv0.5.0 ssd 1541757477.803846121 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 578, "value": 0.10275555555555556}

:::MLPv0.5.0 ssd 1541757477.888051271 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 579, "value": 0.10293333333333334}

:::MLPv0.5.0 ssd 1541757477.972559452 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 580, "value": 0.10311111111111111}
Iteration:    580, Loss function: 5.244, Average Loss: 3.507, avg. samples / sec: 24328.84
Iteration:    580, Loss function: 5.762, Average Loss: 3.513, avg. samples / sec: 24299.43
Iteration:    580, Loss function: 5.003, Average Loss: 3.507, avg. samples / sec: 24285.34
Iteration:    580, Loss function: 5.945, Average Loss: 3.512, avg. samples / sec: 24322.67
Iteration:    580, Loss function: 5.079, Average Loss: 3.516, avg. samples / sec: 24295.58
Iteration:    580, Loss function: 5.909, Average Loss: 3.506, avg. samples / sec: 24285.83
Iteration:    580, Loss function: 6.569, Average Loss: 3.503, avg. samples / sec: 24289.89
Iteration:    580, Loss function: 5.497, Average Loss: 3.507, avg. samples / sec: 24247.39

:::MLPv0.5.0 ssd 1541757478.057038307 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 581, "value": 0.10328888888888889}

:::MLPv0.5.0 ssd 1541757478.141438961 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 582, "value": 0.10346666666666667}

:::MLPv0.5.0 ssd 1541757478.225703716 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 583, "value": 0.10364444444444444}

:::MLPv0.5.0 ssd 1541757478.309957027 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 584, "value": 0.10382222222222223}

:::MLPv0.5.0 ssd 1541757478.394321442 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 585, "value": 0.10400000000000001}

:::MLPv0.5.0 ssd 1541757478.479117870 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 586, "value": 0.10417777777777779}

:::MLPv0.5.0 ssd 1541757478.563561916 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 587, "value": 0.10435555555555556}

:::MLPv0.5.0 ssd 1541757478.648403406 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 588, "value": 0.10453333333333334}

:::MLPv0.5.0 ssd 1541757478.732602835 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 589, "value": 0.10471111111111112}

:::MLPv0.5.0 ssd 1541757478.816763163 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 590, "value": 0.10488888888888889}

:::MLPv0.5.0 ssd 1541757478.903031588 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 591, "value": 0.10506666666666667}

:::MLPv0.5.0 ssd 1541757478.987393141 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 592, "value": 0.10524444444444445}

:::MLPv0.5.0 ssd 1541757479.071489334 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 593, "value": 0.10542222222222222}

:::MLPv0.5.0 ssd 1541757479.155954599 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 594, "value": 0.1056}

:::MLPv0.5.0 ssd 1541757479.240319252 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 595, "value": 0.10577777777777778}

:::MLPv0.5.0 ssd 1541757479.324527264 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 596, "value": 0.10595555555555555}

:::MLPv0.5.0 ssd 1541757479.408697367 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 597, "value": 0.10613333333333333}

:::MLPv0.5.0 ssd 1541757479.492838144 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 598, "value": 0.1063111111111111}

:::MLPv0.5.0 ssd 1541757479.577119827 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 599, "value": 0.10648888888888888}

:::MLPv0.5.0 ssd 1541757479.661328077 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 600, "value": 0.10666666666666666}
Iteration:    600, Loss function: 5.395, Average Loss: 3.553, avg. samples / sec: 24257.03
Iteration:    600, Loss function: 5.738, Average Loss: 3.556, avg. samples / sec: 24256.73
Iteration:    600, Loss function: 4.824, Average Loss: 3.553, avg. samples / sec: 24254.94
Iteration:    600, Loss function: 5.640, Average Loss: 3.547, avg. samples / sec: 24298.87
Iteration:    600, Loss function: 5.475, Average Loss: 3.550, avg. samples / sec: 24292.35
Iteration:    600, Loss function: 5.622, Average Loss: 3.553, avg. samples / sec: 24252.59
Iteration:    600, Loss function: 5.743, Average Loss: 3.559, avg. samples / sec: 24230.72
Iteration:    600, Loss function: 5.589, Average Loss: 3.554, avg. samples / sec: 24199.44

:::MLPv0.5.0 ssd 1541757479.745464087 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 601, "value": 0.10684444444444444}

:::MLPv0.5.0 ssd 1541757479.829345942 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 602, "value": 0.10702222222222221}

:::MLPv0.5.0 ssd 1541757479.913419724 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 603, "value": 0.1072}

:::MLPv0.5.0 ssd 1541757479.997322559 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 604, "value": 0.10737777777777778}

:::MLPv0.5.0 ssd 1541757480.081120014 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 605, "value": 0.10755555555555556}

:::MLPv0.5.0 ssd 1541757480.165443182 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 606, "value": 0.10773333333333333}

:::MLPv0.5.0 ssd 1541757480.249261141 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 607, "value": 0.10791111111111111}

:::MLPv0.5.0 ssd 1541757480.333938360 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 608, "value": 0.10808888888888889}

:::MLPv0.5.0 ssd 1541757480.418160677 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 609, "value": 0.10826666666666668}

:::MLPv0.5.0 ssd 1541757480.502127409 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 610, "value": 0.10844444444444445}

:::MLPv0.5.0 ssd 1541757480.586389542 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 611, "value": 0.10862222222222223}

:::MLPv0.5.0 ssd 1541757480.671249390 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 612, "value": 0.10880000000000001}

:::MLPv0.5.0 ssd 1541757480.755676746 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 613, "value": 0.10897777777777778}

:::MLPv0.5.0 ssd 1541757480.840065479 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 614, "value": 0.10915555555555556}

:::MLPv0.5.0 ssd 1541757480.924037457 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 615, "value": 0.10933333333333334}

:::MLPv0.5.0 ssd 1541757481.009342670 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 616, "value": 0.10951111111111111}

:::MLPv0.5.0 ssd 1541757481.093232870 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 617, "value": 0.10968888888888889}

:::MLPv0.5.0 ssd 1541757481.177053928 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 618, "value": 0.10986666666666667}

:::MLPv0.5.0 ssd 1541757481.261382341 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 619, "value": 0.11004444444444444}

:::MLPv0.5.0 ssd 1541757481.346359968 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 620, "value": 0.11022222222222222}
Iteration:    620, Loss function: 5.688, Average Loss: 3.599, avg. samples / sec: 24307.29
Iteration:    620, Loss function: 5.422, Average Loss: 3.597, avg. samples / sec: 24303.73
Iteration:    620, Loss function: 5.828, Average Loss: 3.597, avg. samples / sec: 24362.71
Iteration:    620, Loss function: 5.594, Average Loss: 3.599, avg. samples / sec: 24325.07
Iteration:    620, Loss function: 5.748, Average Loss: 3.596, avg. samples / sec: 24301.42
Iteration:    620, Loss function: 5.946, Average Loss: 3.592, avg. samples / sec: 24282.55
Iteration:    620, Loss function: 5.359, Average Loss: 3.596, avg. samples / sec: 24260.39
Iteration:    620, Loss function: 5.123, Average Loss: 3.603, avg. samples / sec: 24296.22

:::MLPv0.5.0 ssd 1541757481.430715799 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 621, "value": 0.1104}

:::MLPv0.5.0 ssd 1541757481.515220404 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 622, "value": 0.11057777777777777}

:::MLPv0.5.0 ssd 1541757481.600653887 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 623, "value": 0.11075555555555555}

:::MLPv0.5.0 ssd 1541757481.684736252 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 624, "value": 0.11093333333333333}

:::MLPv0.5.0 ssd 1541757481.769141436 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 625, "value": 0.1111111111111111}

:::MLPv0.5.0 ssd 1541757481.853523016 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 626, "value": 0.11128888888888888}

:::MLPv0.5.0 ssd 1541757481.937906504 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 627, "value": 0.11146666666666666}

:::MLPv0.5.0 ssd 1541757482.021486044 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 628, "value": 0.11164444444444445}

:::MLPv0.5.0 ssd 1541757482.106012344 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 629, "value": 0.11182222222222223}

:::MLPv0.5.0 ssd 1541757482.190483570 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 630, "value": 0.112}

:::MLPv0.5.0 ssd 1541757482.274810791 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 631, "value": 0.11217777777777778}

:::MLPv0.5.0 ssd 1541757482.359030485 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 632, "value": 0.11235555555555556}

:::MLPv0.5.0 ssd 1541757482.443858624 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 633, "value": 0.11253333333333333}

:::MLPv0.5.0 ssd 1541757482.527967930 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 634, "value": 0.11271111111111111}

:::MLPv0.5.0 ssd 1541757482.612320662 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 635, "value": 0.1128888888888889}

:::MLPv0.5.0 ssd 1541757482.693794250 (train.py:553) train_epoch: 11

:::MLPv0.5.0 ssd 1541757482.698022604 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 636, "value": 0.11306666666666668}

:::MLPv0.5.0 ssd 1541757482.782567024 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 637, "value": 0.11324444444444445}

:::MLPv0.5.0 ssd 1541757482.866662741 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 638, "value": 0.11342222222222223}

:::MLPv0.5.0 ssd 1541757482.950763226 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 639, "value": 0.1136}

:::MLPv0.5.0 ssd 1541757483.034852028 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 640, "value": 0.11377777777777778}
Iteration:    640, Loss function: 5.462, Average Loss: 3.633, avg. samples / sec: 24263.24
Iteration:    640, Loss function: 5.635, Average Loss: 3.634, avg. samples / sec: 24261.65
Iteration:    640, Loss function: 5.265, Average Loss: 3.631, avg. samples / sec: 24265.75
Iteration:    640, Loss function: 4.714, Average Loss: 3.632, avg. samples / sec: 24266.41
Iteration:    640, Loss function: 5.425, Average Loss: 3.640, avg. samples / sec: 24313.68
Iteration:    640, Loss function: 5.272, Average Loss: 3.628, avg. samples / sec: 24273.11
Iteration:    640, Loss function: 5.110, Average Loss: 3.629, avg. samples / sec: 24257.12
Iteration:    640, Loss function: 5.821, Average Loss: 3.632, avg. samples / sec: 24269.47

:::MLPv0.5.0 ssd 1541757483.122470617 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 641, "value": 0.11395555555555556}

:::MLPv0.5.0 ssd 1541757483.207360506 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 642, "value": 0.11413333333333334}

:::MLPv0.5.0 ssd 1541757483.293588161 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 643, "value": 0.11431111111111111}

:::MLPv0.5.0 ssd 1541757483.378071308 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 644, "value": 0.11448888888888889}

:::MLPv0.5.0 ssd 1541757483.461927176 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 645, "value": 0.11466666666666667}

:::MLPv0.5.0 ssd 1541757483.546453476 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 646, "value": 0.11484444444444444}

:::MLPv0.5.0 ssd 1541757483.630741835 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 647, "value": 0.11502222222222222}

:::MLPv0.5.0 ssd 1541757483.717005968 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 648, "value": 0.1152}

:::MLPv0.5.0 ssd 1541757483.801312923 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 649, "value": 0.11537777777777777}

:::MLPv0.5.0 ssd 1541757483.885445833 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 650, "value": 0.11555555555555555}

:::MLPv0.5.0 ssd 1541757483.969912529 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 651, "value": 0.11573333333333333}

:::MLPv0.5.0 ssd 1541757484.055197954 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 652, "value": 0.1159111111111111}

:::MLPv0.5.0 ssd 1541757484.139884949 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 653, "value": 0.11608888888888888}

:::MLPv0.5.0 ssd 1541757484.224399090 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 654, "value": 0.11626666666666667}

:::MLPv0.5.0 ssd 1541757484.308629513 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 655, "value": 0.11644444444444445}

:::MLPv0.5.0 ssd 1541757484.392588854 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 656, "value": 0.11662222222222222}

:::MLPv0.5.0 ssd 1541757484.476582766 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 657, "value": 0.1168}

:::MLPv0.5.0 ssd 1541757484.561574459 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 658, "value": 0.11697777777777778}

:::MLPv0.5.0 ssd 1541757484.645810127 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 659, "value": 0.11715555555555555}

:::MLPv0.5.0 ssd 1541757484.730217218 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 660, "value": 0.11733333333333333}
Iteration:    660, Loss function: 5.273, Average Loss: 3.666, avg. samples / sec: 24161.79
Iteration:    660, Loss function: 5.149, Average Loss: 3.669, avg. samples / sec: 24157.31
Iteration:    660, Loss function: 5.590, Average Loss: 3.667, avg. samples / sec: 24143.38
Iteration:    660, Loss function: 5.408, Average Loss: 3.667, avg. samples / sec: 24192.57
Iteration:    660, Loss function: 5.779, Average Loss: 3.676, avg. samples / sec: 24157.80
Iteration:    660, Loss function: 5.585, Average Loss: 3.667, avg. samples / sec: 24150.69
Iteration:    660, Loss function: 5.725, Average Loss: 3.664, avg. samples / sec: 24157.63
Iteration:    660, Loss function: 5.253, Average Loss: 3.662, avg. samples / sec: 24160.85

:::MLPv0.5.0 ssd 1541757484.814685106 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 661, "value": 0.11751111111111112}

:::MLPv0.5.0 ssd 1541757484.899605513 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 662, "value": 0.1176888888888889}

:::MLPv0.5.0 ssd 1541757484.983809471 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 663, "value": 0.11786666666666668}

:::MLPv0.5.0 ssd 1541757485.068097830 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 664, "value": 0.11804444444444445}

:::MLPv0.5.0 ssd 1541757485.152135134 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 665, "value": 0.11822222222222223}

:::MLPv0.5.0 ssd 1541757485.236906528 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 666, "value": 0.1184}

:::MLPv0.5.0 ssd 1541757485.321403027 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 667, "value": 0.11857777777777778}

:::MLPv0.5.0 ssd 1541757485.405438423 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 668, "value": 0.11875555555555556}

:::MLPv0.5.0 ssd 1541757485.490411043 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 669, "value": 0.11893333333333334}

:::MLPv0.5.0 ssd 1541757485.575511217 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 670, "value": 0.11911111111111111}

:::MLPv0.5.0 ssd 1541757485.659847498 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 671, "value": 0.11928888888888889}

:::MLPv0.5.0 ssd 1541757485.743957758 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 672, "value": 0.11946666666666667}

:::MLPv0.5.0 ssd 1541757485.827619076 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 673, "value": 0.11964444444444444}

:::MLPv0.5.0 ssd 1541757485.911245346 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 674, "value": 0.11982222222222222}

:::MLPv0.5.0 ssd 1541757485.995558262 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 675, "value": 0.12}

:::MLPv0.5.0 ssd 1541757486.080138683 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 676, "value": 0.12017777777777777}

:::MLPv0.5.0 ssd 1541757486.164652109 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 677, "value": 0.12035555555555555}

:::MLPv0.5.0 ssd 1541757486.248848677 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 678, "value": 0.12053333333333333}

:::MLPv0.5.0 ssd 1541757486.333479881 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 679, "value": 0.1207111111111111}

:::MLPv0.5.0 ssd 1541757486.417613745 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 680, "value": 0.12088888888888889}
Iteration:    680, Loss function: 5.431, Average Loss: 3.706, avg. samples / sec: 24279.11
Iteration:    680, Loss function: 5.897, Average Loss: 3.702, avg. samples / sec: 24285.33
Iteration:    680, Loss function: 5.356, Average Loss: 3.702, avg. samples / sec: 24284.89
Iteration:    680, Loss function: 5.463, Average Loss: 3.711, avg. samples / sec: 24279.00
Iteration:    680, Loss function: 5.700, Average Loss: 3.704, avg. samples / sec: 24274.90
Iteration:    680, Loss function: 5.939, Average Loss: 3.701, avg. samples / sec: 24257.54
Iteration:    680, Loss function: 5.809, Average Loss: 3.700, avg. samples / sec: 24216.30
Iteration:    680, Loss function: 5.690, Average Loss: 3.697, avg. samples / sec: 24247.17

:::MLPv0.5.0 ssd 1541757486.501761198 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 681, "value": 0.12106666666666667}

:::MLPv0.5.0 ssd 1541757486.586519003 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 682, "value": 0.12124444444444445}

:::MLPv0.5.0 ssd 1541757486.670813322 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 683, "value": 0.12142222222222222}

:::MLPv0.5.0 ssd 1541757486.754927874 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 684, "value": 0.1216}

:::MLPv0.5.0 ssd 1541757486.838852882 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 685, "value": 0.12177777777777778}

:::MLPv0.5.0 ssd 1541757486.923007727 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 686, "value": 0.12195555555555557}

:::MLPv0.5.0 ssd 1541757487.007234335 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 687, "value": 0.12213333333333334}

:::MLPv0.5.0 ssd 1541757487.091780424 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 688, "value": 0.12231111111111112}

:::MLPv0.5.0 ssd 1541757487.175834417 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 689, "value": 0.1224888888888889}

:::MLPv0.5.0 ssd 1541757487.259660482 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 690, "value": 0.12266666666666667}

:::MLPv0.5.0 ssd 1541757487.343071222 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 691, "value": 0.12284444444444445}

:::MLPv0.5.0 ssd 1541757487.427478790 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 692, "value": 0.12302222222222223}

:::MLPv0.5.0 ssd 1541757487.511393070 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 693, "value": 0.1232}

:::MLPv0.5.0 ssd 1541757487.592825413 (train.py:553) train_epoch: 12

:::MLPv0.5.0 ssd 1541757487.597009420 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 694, "value": 0.12337777777777778}

:::MLPv0.5.0 ssd 1541757487.681701422 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 695, "value": 0.12355555555555556}

:::MLPv0.5.0 ssd 1541757487.765680552 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 696, "value": 0.12373333333333333}

:::MLPv0.5.0 ssd 1541757487.850371122 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 697, "value": 0.12391111111111111}

:::MLPv0.5.0 ssd 1541757487.934705257 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 698, "value": 0.12408888888888889}

:::MLPv0.5.0 ssd 1541757488.018642902 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 699, "value": 0.12426666666666666}

:::MLPv0.5.0 ssd 1541757488.102966547 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 700, "value": 0.12444444444444444}
Iteration:    700, Loss function: 5.660, Average Loss: 3.748, avg. samples / sec: 24318.41
Iteration:    700, Loss function: 5.060, Average Loss: 3.748, avg. samples / sec: 24302.97
Iteration:    700, Loss function: 5.795, Average Loss: 3.747, avg. samples / sec: 24305.54
Iteration:    700, Loss function: 5.684, Average Loss: 3.747, avg. samples / sec: 24299.53
Iteration:    700, Loss function: 5.855, Average Loss: 3.748, avg. samples / sec: 24352.40
Iteration:    700, Loss function: 5.286, Average Loss: 3.745, avg. samples / sec: 24357.15
Iteration:    700, Loss function: 5.238, Average Loss: 3.758, avg. samples / sec: 24292.89
Iteration:    700, Loss function: 5.774, Average Loss: 3.748, avg. samples / sec: 24316.37

:::MLPv0.5.0 ssd 1541757488.190448046 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 701, "value": 0.12462222222222222}

:::MLPv0.5.0 ssd 1541757488.274420023 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 702, "value": 0.1248}

:::MLPv0.5.0 ssd 1541757488.358794451 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 703, "value": 0.12497777777777777}

:::MLPv0.5.0 ssd 1541757488.443173170 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 704, "value": 0.12515555555555555}

:::MLPv0.5.0 ssd 1541757488.527348757 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 705, "value": 0.12533333333333335}

:::MLPv0.5.0 ssd 1541757488.611154318 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 706, "value": 0.12551111111111113}

:::MLPv0.5.0 ssd 1541757488.695448875 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 707, "value": 0.1256888888888889}

:::MLPv0.5.0 ssd 1541757488.779242277 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 708, "value": 0.12586666666666668}

:::MLPv0.5.0 ssd 1541757488.863450766 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 709, "value": 0.12604444444444446}

:::MLPv0.5.0 ssd 1541757488.947905302 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 710, "value": 0.12622222222222224}

:::MLPv0.5.0 ssd 1541757489.031895876 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 711, "value": 0.1264}

:::MLPv0.5.0 ssd 1541757489.116665125 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 712, "value": 0.1265777777777778}

:::MLPv0.5.0 ssd 1541757489.201206446 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 713, "value": 0.12675555555555557}

:::MLPv0.5.0 ssd 1541757489.285810709 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 714, "value": 0.12693333333333334}

:::MLPv0.5.0 ssd 1541757489.370493650 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 715, "value": 0.12711111111111112}

:::MLPv0.5.0 ssd 1541757489.454777718 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 716, "value": 0.1272888888888889}

:::MLPv0.5.0 ssd 1541757489.539346457 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 717, "value": 0.12746666666666667}

:::MLPv0.5.0 ssd 1541757489.623482227 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 718, "value": 0.12764444444444445}

:::MLPv0.5.0 ssd 1541757489.707282782 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 719, "value": 0.12782222222222223}

:::MLPv0.5.0 ssd 1541757489.791693449 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 720, "value": 0.128}
Iteration:    720, Loss function: 5.455, Average Loss: 3.778, avg. samples / sec: 24266.11
Iteration:    720, Loss function: 4.746, Average Loss: 3.780, avg. samples / sec: 24260.58
Iteration:    720, Loss function: 4.682, Average Loss: 3.780, avg. samples / sec: 24259.56
Iteration:    720, Loss function: 4.490, Average Loss: 3.777, avg. samples / sec: 24248.36
Iteration:    720, Loss function: 5.716, Average Loss: 3.777, avg. samples / sec: 24257.54
Iteration:    720, Loss function: 5.621, Average Loss: 3.778, avg. samples / sec: 24229.21
Iteration:    720, Loss function: 4.929, Average Loss: 3.790, avg. samples / sec: 24237.55
Iteration:    720, Loss function: 4.999, Average Loss: 3.779, avg. samples / sec: 24194.18

:::MLPv0.5.0 ssd 1541757489.876021147 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 721, "value": 0.12817777777777778}

:::MLPv0.5.0 ssd 1541757489.960889101 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 722, "value": 0.12835555555555556}

:::MLPv0.5.0 ssd 1541757490.044579029 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 723, "value": 0.12853333333333333}

:::MLPv0.5.0 ssd 1541757490.129153013 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 724, "value": 0.1287111111111111}

:::MLPv0.5.0 ssd 1541757490.213391781 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 725, "value": 0.1288888888888889}

:::MLPv0.5.0 ssd 1541757490.297421932 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 726, "value": 0.12906666666666666}

:::MLPv0.5.0 ssd 1541757490.381502390 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 727, "value": 0.12924444444444444}

:::MLPv0.5.0 ssd 1541757490.465276241 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 728, "value": 0.12942222222222222}

:::MLPv0.5.0 ssd 1541757490.549584150 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 729, "value": 0.1296}

:::MLPv0.5.0 ssd 1541757490.634014130 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 730, "value": 0.12977777777777777}

:::MLPv0.5.0 ssd 1541757490.718195438 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 731, "value": 0.12995555555555555}

:::MLPv0.5.0 ssd 1541757490.802670002 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 732, "value": 0.13013333333333332}

:::MLPv0.5.0 ssd 1541757490.887372971 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 733, "value": 0.1303111111111111}

:::MLPv0.5.0 ssd 1541757490.972338676 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 734, "value": 0.13048888888888888}

:::MLPv0.5.0 ssd 1541757491.056851387 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 735, "value": 0.13066666666666665}

:::MLPv0.5.0 ssd 1541757491.141607523 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 736, "value": 0.13084444444444446}

:::MLPv0.5.0 ssd 1541757491.226094007 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 737, "value": 0.13102222222222223}

:::MLPv0.5.0 ssd 1541757491.310678482 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 738, "value": 0.1312}

:::MLPv0.5.0 ssd 1541757491.394783020 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 739, "value": 0.1313777777777778}

:::MLPv0.5.0 ssd 1541757491.479170322 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 740, "value": 0.13155555555555556}
Iteration:    740, Loss function: 5.405, Average Loss: 3.807, avg. samples / sec: 24275.60
Iteration:    740, Loss function: 5.737, Average Loss: 3.811, avg. samples / sec: 24272.25
Iteration:    740, Loss function: 5.333, Average Loss: 3.810, avg. samples / sec: 24269.65
Iteration:    740, Loss function: 5.648, Average Loss: 3.808, avg. samples / sec: 24280.37
Iteration:    740, Loss function: 5.161, Average Loss: 3.820, avg. samples / sec: 24282.17
Iteration:    740, Loss function: 5.654, Average Loss: 3.807, avg. samples / sec: 24291.08
Iteration:    740, Loss function: 4.950, Average Loss: 3.806, avg. samples / sec: 24247.57
Iteration:    740, Loss function: 5.512, Average Loss: 3.808, avg. samples / sec: 24244.63

:::MLPv0.5.0 ssd 1541757491.563378096 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 741, "value": 0.13173333333333334}

:::MLPv0.5.0 ssd 1541757491.647419930 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 742, "value": 0.13191111111111112}

:::MLPv0.5.0 ssd 1541757491.731758595 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 743, "value": 0.1320888888888889}

:::MLPv0.5.0 ssd 1541757491.815365553 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 744, "value": 0.13226666666666667}

:::MLPv0.5.0 ssd 1541757491.899488211 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 745, "value": 0.13244444444444445}

:::MLPv0.5.0 ssd 1541757491.984402657 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 746, "value": 0.13262222222222222}

:::MLPv0.5.0 ssd 1541757492.068785429 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 747, "value": 0.1328}

:::MLPv0.5.0 ssd 1541757492.153121471 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 748, "value": 0.13297777777777778}

:::MLPv0.5.0 ssd 1541757492.237489939 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 749, "value": 0.13315555555555555}

:::MLPv0.5.0 ssd 1541757492.321615458 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 750, "value": 0.13333333333333333}

:::MLPv0.5.0 ssd 1541757492.402891636 (train.py:553) train_epoch: 13

:::MLPv0.5.0 ssd 1541757492.407100201 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 751, "value": 0.1335111111111111}

:::MLPv0.5.0 ssd 1541757492.491562128 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 752, "value": 0.13368888888888888}

:::MLPv0.5.0 ssd 1541757492.576583385 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 753, "value": 0.13386666666666666}

:::MLPv0.5.0 ssd 1541757492.660607338 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 754, "value": 0.13404444444444444}

:::MLPv0.5.0 ssd 1541757492.744774342 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 755, "value": 0.13422222222222221}

:::MLPv0.5.0 ssd 1541757492.829308271 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 756, "value": 0.1344}

:::MLPv0.5.0 ssd 1541757492.913720846 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 757, "value": 0.13457777777777777}

:::MLPv0.5.0 ssd 1541757492.997847080 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 758, "value": 0.13475555555555557}

:::MLPv0.5.0 ssd 1541757493.081782341 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 759, "value": 0.13493333333333335}

:::MLPv0.5.0 ssd 1541757493.166611195 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 760, "value": 0.13511111111111113}
Iteration:    760, Loss function: 4.453, Average Loss: 3.838, avg. samples / sec: 24280.06
Iteration:    760, Loss function: 5.533, Average Loss: 3.838, avg. samples / sec: 24276.72
Iteration:    760, Loss function: 5.018, Average Loss: 3.846, avg. samples / sec: 24293.83
Iteration:    760, Loss function: 5.095, Average Loss: 3.832, avg. samples / sec: 24301.11
Iteration:    760, Loss function: 4.982, Average Loss: 3.833, avg. samples / sec: 24272.50
Iteration:    760, Loss function: 4.341, Average Loss: 3.832, avg. samples / sec: 24239.78
Iteration:    760, Loss function: 5.059, Average Loss: 3.833, avg. samples / sec: 24289.97
Iteration:    760, Loss function: 5.014, Average Loss: 3.831, avg. samples / sec: 24263.71

:::MLPv0.5.0 ssd 1541757493.250930786 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 761, "value": 0.1352888888888889}

:::MLPv0.5.0 ssd 1541757493.334770918 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 762, "value": 0.13546666666666668}

:::MLPv0.5.0 ssd 1541757493.418951511 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 763, "value": 0.13564444444444446}

:::MLPv0.5.0 ssd 1541757493.503652573 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 764, "value": 0.13582222222222223}

:::MLPv0.5.0 ssd 1541757493.587630987 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 765, "value": 0.136}

:::MLPv0.5.0 ssd 1541757493.672457695 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 766, "value": 0.1361777777777778}

:::MLPv0.5.0 ssd 1541757493.756199598 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 767, "value": 0.13635555555555556}

:::MLPv0.5.0 ssd 1541757493.840285778 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 768, "value": 0.13653333333333334}

:::MLPv0.5.0 ssd 1541757493.924365282 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 769, "value": 0.13671111111111112}

:::MLPv0.5.0 ssd 1541757494.009288788 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 770, "value": 0.1368888888888889}

:::MLPv0.5.0 ssd 1541757494.093236208 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 771, "value": 0.13706666666666667}

:::MLPv0.5.0 ssd 1541757494.177664280 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 772, "value": 0.13724444444444445}

:::MLPv0.5.0 ssd 1541757494.261950016 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 773, "value": 0.13742222222222222}

:::MLPv0.5.0 ssd 1541757494.345840454 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 774, "value": 0.1376}

:::MLPv0.5.0 ssd 1541757494.430061340 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 775, "value": 0.13777777777777778}

:::MLPv0.5.0 ssd 1541757494.514772654 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 776, "value": 0.13795555555555555}

:::MLPv0.5.0 ssd 1541757494.598726034 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 777, "value": 0.13813333333333333}

:::MLPv0.5.0 ssd 1541757494.683490276 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 778, "value": 0.1383111111111111}

:::MLPv0.5.0 ssd 1541757494.767489433 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 779, "value": 0.13848888888888888}

:::MLPv0.5.0 ssd 1541757494.851691008 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 780, "value": 0.13866666666666666}
Iteration:    780, Loss function: 5.402, Average Loss: 3.861, avg. samples / sec: 24343.23
Iteration:    780, Loss function: 5.289, Average Loss: 3.863, avg. samples / sec: 24346.96
Iteration:    780, Loss function: 5.550, Average Loss: 3.864, avg. samples / sec: 24332.58
Iteration:    780, Loss function: 5.187, Average Loss: 3.869, avg. samples / sec: 24301.53
Iteration:    780, Loss function: 5.124, Average Loss: 3.874, avg. samples / sec: 24275.72
Iteration:    780, Loss function: 5.359, Average Loss: 3.860, avg. samples / sec: 24319.17
Iteration:    780, Loss function: 5.283, Average Loss: 3.870, avg. samples / sec: 24263.76
Iteration:    780, Loss function: 5.303, Average Loss: 3.862, avg. samples / sec: 24271.69

:::MLPv0.5.0 ssd 1541757494.935655594 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 781, "value": 0.13884444444444444}

:::MLPv0.5.0 ssd 1541757495.019740582 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 782, "value": 0.1390222222222222}

:::MLPv0.5.0 ssd 1541757495.103869438 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 783, "value": 0.1392}

:::MLPv0.5.0 ssd 1541757495.188054085 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 784, "value": 0.13937777777777777}

:::MLPv0.5.0 ssd 1541757495.272491693 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 785, "value": 0.13955555555555554}

:::MLPv0.5.0 ssd 1541757495.356818438 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 786, "value": 0.13973333333333332}

:::MLPv0.5.0 ssd 1541757495.440570831 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 787, "value": 0.13991111111111112}

:::MLPv0.5.0 ssd 1541757495.524335861 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 788, "value": 0.1400888888888889}

:::MLPv0.5.0 ssd 1541757495.609424114 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 789, "value": 0.14026666666666668}

:::MLPv0.5.0 ssd 1541757495.693754673 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 790, "value": 0.14044444444444446}

:::MLPv0.5.0 ssd 1541757495.778688908 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 791, "value": 0.14062222222222223}

:::MLPv0.5.0 ssd 1541757495.863109589 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 792, "value": 0.1408}

:::MLPv0.5.0 ssd 1541757495.947470188 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 793, "value": 0.14097777777777779}

:::MLPv0.5.0 ssd 1541757496.031433105 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 794, "value": 0.14115555555555556}

:::MLPv0.5.0 ssd 1541757496.115980625 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 795, "value": 0.14133333333333334}

:::MLPv0.5.0 ssd 1541757496.199964046 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 796, "value": 0.14151111111111112}

:::MLPv0.5.0 ssd 1541757496.284619093 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 797, "value": 0.1416888888888889}

:::MLPv0.5.0 ssd 1541757496.369007826 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 798, "value": 0.14186666666666667}

:::MLPv0.5.0 ssd 1541757496.453049898 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 799, "value": 0.14204444444444445}

:::MLPv0.5.0 ssd 1541757496.537929535 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 800, "value": 0.14222222222222222}
Iteration:    800, Loss function: 4.924, Average Loss: 3.886, avg. samples / sec: 24295.92
Iteration:    800, Loss function: 5.355, Average Loss: 3.892, avg. samples / sec: 24293.17
Iteration:    800, Loss function: 4.611, Average Loss: 3.884, avg. samples / sec: 24339.39
Iteration:    800, Loss function: 4.815, Average Loss: 3.884, avg. samples / sec: 24274.64
Iteration:    800, Loss function: 5.151, Average Loss: 3.886, avg. samples / sec: 24295.98
Iteration:    800, Loss function: 5.286, Average Loss: 3.900, avg. samples / sec: 24294.69
Iteration:    800, Loss function: 5.032, Average Loss: 3.889, avg. samples / sec: 24248.61
Iteration:    800, Loss function: 5.299, Average Loss: 3.894, avg. samples / sec: 24292.80

:::MLPv0.5.0 ssd 1541757496.621800900 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 801, "value": 0.1424}

:::MLPv0.5.0 ssd 1541757496.706831932 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 802, "value": 0.14257777777777778}

:::MLPv0.5.0 ssd 1541757496.793578386 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 803, "value": 0.14275555555555555}

:::MLPv0.5.0 ssd 1541757496.878753424 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 804, "value": 0.14293333333333333}

:::MLPv0.5.0 ssd 1541757496.962884903 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 805, "value": 0.1431111111111111}

:::MLPv0.5.0 ssd 1541757497.047204494 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 806, "value": 0.14328888888888888}

:::MLPv0.5.0 ssd 1541757497.131577015 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 807, "value": 0.14346666666666666}

:::MLPv0.5.0 ssd 1541757497.215529680 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 808, "value": 0.14364444444444444}

:::MLPv0.5.0 ssd 1541757497.297496319 (train.py:553) train_epoch: 14

:::MLPv0.5.0 ssd 1541757497.301649570 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 809, "value": 0.14382222222222224}

:::MLPv0.5.0 ssd 1541757497.386288643 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 810, "value": 0.14400000000000002}

:::MLPv0.5.0 ssd 1541757497.470774412 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 811, "value": 0.1441777777777778}

:::MLPv0.5.0 ssd 1541757497.555438757 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 812, "value": 0.14435555555555557}

:::MLPv0.5.0 ssd 1541757497.639751673 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 813, "value": 0.14453333333333335}

:::MLPv0.5.0 ssd 1541757497.723948002 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 814, "value": 0.14471111111111112}

:::MLPv0.5.0 ssd 1541757497.809390306 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 815, "value": 0.1448888888888889}

:::MLPv0.5.0 ssd 1541757497.894282103 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 816, "value": 0.14506666666666668}

:::MLPv0.5.0 ssd 1541757497.978689432 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 817, "value": 0.14524444444444445}

:::MLPv0.5.0 ssd 1541757498.062965155 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 818, "value": 0.14542222222222223}

:::MLPv0.5.0 ssd 1541757498.147768497 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 819, "value": 0.1456}

:::MLPv0.5.0 ssd 1541757498.232027292 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 820, "value": 0.14577777777777778}
Iteration:    820, Loss function: 5.784, Average Loss: 3.926, avg. samples / sec: 24222.56
Iteration:    820, Loss function: 5.063, Average Loss: 3.918, avg. samples / sec: 24179.43
Iteration:    820, Loss function: 5.585, Average Loss: 3.918, avg. samples / sec: 24166.05
Iteration:    820, Loss function: 6.041, Average Loss: 3.920, avg. samples / sec: 24206.06
Iteration:    820, Loss function: 5.943, Average Loss: 3.931, avg. samples / sec: 24192.73
Iteration:    820, Loss function: 5.618, Average Loss: 3.915, avg. samples / sec: 24187.24
Iteration:    820, Loss function: 5.855, Average Loss: 3.913, avg. samples / sec: 24156.80
Iteration:    820, Loss function: 5.304, Average Loss: 3.914, avg. samples / sec: 24135.16

:::MLPv0.5.0 ssd 1541757498.316295147 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 821, "value": 0.14595555555555556}

:::MLPv0.5.0 ssd 1541757498.400105238 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 822, "value": 0.14613333333333334}

:::MLPv0.5.0 ssd 1541757498.483943462 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 823, "value": 0.14631111111111111}

:::MLPv0.5.0 ssd 1541757498.568002701 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 824, "value": 0.1464888888888889}

:::MLPv0.5.0 ssd 1541757498.651973963 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 825, "value": 0.14666666666666667}

:::MLPv0.5.0 ssd 1541757498.736095905 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 826, "value": 0.14684444444444444}

:::MLPv0.5.0 ssd 1541757498.820027828 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 827, "value": 0.14702222222222222}

:::MLPv0.5.0 ssd 1541757498.903486252 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 828, "value": 0.1472}

:::MLPv0.5.0 ssd 1541757498.987277031 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 829, "value": 0.14737777777777777}

:::MLPv0.5.0 ssd 1541757499.072051287 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 830, "value": 0.14755555555555555}

:::MLPv0.5.0 ssd 1541757499.156691551 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 831, "value": 0.14773333333333333}

:::MLPv0.5.0 ssd 1541757499.241546869 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 832, "value": 0.1479111111111111}

:::MLPv0.5.0 ssd 1541757499.326018572 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 833, "value": 0.14808888888888888}

:::MLPv0.5.0 ssd 1541757499.410763502 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 834, "value": 0.14826666666666666}

:::MLPv0.5.0 ssd 1541757499.494966030 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 835, "value": 0.14844444444444443}

:::MLPv0.5.0 ssd 1541757499.579766512 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 836, "value": 0.1486222222222222}

:::MLPv0.5.0 ssd 1541757499.663983822 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 837, "value": 0.14880000000000002}

:::MLPv0.5.0 ssd 1541757499.748025656 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 838, "value": 0.1489777777777778}

:::MLPv0.5.0 ssd 1541757499.832250357 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 839, "value": 0.14915555555555557}

:::MLPv0.5.0 ssd 1541757499.916606903 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 840, "value": 0.14933333333333335}
Iteration:    840, Loss function: 4.952, Average Loss: 3.951, avg. samples / sec: 24313.35
Iteration:    840, Loss function: 5.168, Average Loss: 3.939, avg. samples / sec: 24362.74
Iteration:    840, Loss function: 4.954, Average Loss: 3.940, avg. samples / sec: 24323.85
Iteration:    840, Loss function: 4.281, Average Loss: 3.954, avg. samples / sec: 24326.48
Iteration:    840, Loss function: 4.632, Average Loss: 3.946, avg. samples / sec: 24314.05
Iteration:    840, Loss function: 5.263, Average Loss: 3.942, avg. samples / sec: 24300.65
Iteration:    840, Loss function: 4.924, Average Loss: 3.938, avg. samples / sec: 24300.62
Iteration:    840, Loss function: 5.091, Average Loss: 3.945, avg. samples / sec: 24296.19

:::MLPv0.5.0 ssd 1541757500.000754595 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 841, "value": 0.14951111111111112}

:::MLPv0.5.0 ssd 1541757500.084612370 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 842, "value": 0.1496888888888889}

:::MLPv0.5.0 ssd 1541757500.169423103 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 843, "value": 0.14986666666666668}

:::MLPv0.5.0 ssd 1541757500.254152775 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 844, "value": 0.15004444444444445}

:::MLPv0.5.0 ssd 1541757500.339119911 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 845, "value": 0.15022222222222223}

:::MLPv0.5.0 ssd 1541757500.423310757 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 846, "value": 0.1504}

:::MLPv0.5.0 ssd 1541757500.507713795 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 847, "value": 0.15057777777777778}

:::MLPv0.5.0 ssd 1541757500.591514111 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 848, "value": 0.15075555555555556}

:::MLPv0.5.0 ssd 1541757500.675848484 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 849, "value": 0.15093333333333334}

:::MLPv0.5.0 ssd 1541757500.760675669 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 850, "value": 0.1511111111111111}

:::MLPv0.5.0 ssd 1541757500.845296144 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 851, "value": 0.1512888888888889}

:::MLPv0.5.0 ssd 1541757500.929606438 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 852, "value": 0.15146666666666667}

:::MLPv0.5.0 ssd 1541757501.013750076 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 853, "value": 0.15164444444444444}

:::MLPv0.5.0 ssd 1541757501.097968817 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 854, "value": 0.15182222222222222}

:::MLPv0.5.0 ssd 1541757501.182582617 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 855, "value": 0.152}

:::MLPv0.5.0 ssd 1541757501.266740084 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 856, "value": 0.15217777777777777}

:::MLPv0.5.0 ssd 1541757501.350975513 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 857, "value": 0.15235555555555555}

:::MLPv0.5.0 ssd 1541757501.435233355 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 858, "value": 0.15253333333333333}

:::MLPv0.5.0 ssd 1541757501.519246340 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 859, "value": 0.1527111111111111}

:::MLPv0.5.0 ssd 1541757501.603937626 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 860, "value": 0.15288888888888888}
Iteration:    860, Loss function: 4.602, Average Loss: 3.961, avg. samples / sec: 24277.71
Iteration:    860, Loss function: 4.971, Average Loss: 3.958, avg. samples / sec: 24323.26
Iteration:    860, Loss function: 5.109, Average Loss: 3.965, avg. samples / sec: 24288.59
Iteration:    860, Loss function: 5.820, Average Loss: 3.976, avg. samples / sec: 24280.99
Iteration:    860, Loss function: 4.965, Average Loss: 3.970, avg. samples / sec: 24269.35
Iteration:    860, Loss function: 4.937, Average Loss: 3.968, avg. samples / sec: 24304.67
Iteration:    860, Loss function: 4.730, Average Loss: 3.962, avg. samples / sec: 24247.31
Iteration:    860, Loss function: 5.336, Average Loss: 3.967, avg. samples / sec: 24233.94

:::MLPv0.5.0 ssd 1541757501.688491583 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 861, "value": 0.15306666666666666}

:::MLPv0.5.0 ssd 1541757501.772671700 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 862, "value": 0.15324444444444446}

:::MLPv0.5.0 ssd 1541757501.857174158 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 863, "value": 0.15342222222222224}

:::MLPv0.5.0 ssd 1541757501.941183805 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 864, "value": 0.15360000000000001}

:::MLPv0.5.0 ssd 1541757502.025091887 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 865, "value": 0.1537777777777778}

:::MLPv0.5.0 ssd 1541757502.109565020 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 866, "value": 0.15395555555555557}

:::MLPv0.5.0 ssd 1541757502.191538095 (train.py:553) train_epoch: 15

:::MLPv0.5.0 ssd 1541757502.195722818 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 867, "value": 0.15413333333333334}

:::MLPv0.5.0 ssd 1541757502.280700922 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 868, "value": 0.15431111111111112}

:::MLPv0.5.0 ssd 1541757502.364951849 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 869, "value": 0.1544888888888889}

:::MLPv0.5.0 ssd 1541757502.449320078 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 870, "value": 0.15466666666666667}

:::MLPv0.5.0 ssd 1541757502.532911301 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 871, "value": 0.15484444444444445}

:::MLPv0.5.0 ssd 1541757502.617249727 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 872, "value": 0.15502222222222223}

:::MLPv0.5.0 ssd 1541757502.701580524 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 873, "value": 0.1552}

:::MLPv0.5.0 ssd 1541757502.785493851 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 874, "value": 0.15537777777777778}

:::MLPv0.5.0 ssd 1541757502.869656086 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 875, "value": 0.15555555555555556}

:::MLPv0.5.0 ssd 1541757502.954209566 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 876, "value": 0.15573333333333333}

:::MLPv0.5.0 ssd 1541757503.038790226 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 877, "value": 0.1559111111111111}

:::MLPv0.5.0 ssd 1541757503.123996019 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 878, "value": 0.1560888888888889}

:::MLPv0.5.0 ssd 1541757503.208181858 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 879, "value": 0.15626666666666666}

:::MLPv0.5.0 ssd 1541757503.292428017 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 880, "value": 0.15644444444444444}
Iteration:    880, Loss function: 5.575, Average Loss: 3.985, avg. samples / sec: 24257.54
Iteration:    880, Loss function: 5.160, Average Loss: 3.992, avg. samples / sec: 24260.70
Iteration:    880, Loss function: 5.040, Average Loss: 3.990, avg. samples / sec: 24308.61
Iteration:    880, Loss function: 4.844, Average Loss: 3.997, avg. samples / sec: 24248.66
Iteration:    880, Loss function: 5.286, Average Loss: 3.990, avg. samples / sec: 24248.00
Iteration:    880, Loss function: 4.477, Average Loss: 3.986, avg. samples / sec: 24229.99
Iteration:    880, Loss function: 5.020, Average Loss: 3.983, avg. samples / sec: 24208.28
Iteration:    880, Loss function: 5.404, Average Loss: 3.984, avg. samples / sec: 24231.56

:::MLPv0.5.0 ssd 1541757503.376986027 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 881, "value": 0.15662222222222222}

:::MLPv0.5.0 ssd 1541757503.461276054 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 882, "value": 0.1568}

:::MLPv0.5.0 ssd 1541757503.546147346 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 883, "value": 0.15697777777777777}

:::MLPv0.5.0 ssd 1541757503.630592346 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 884, "value": 0.15715555555555555}

:::MLPv0.5.0 ssd 1541757503.714964151 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 885, "value": 0.15733333333333333}

:::MLPv0.5.0 ssd 1541757503.799711227 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 886, "value": 0.1575111111111111}

:::MLPv0.5.0 ssd 1541757503.883972168 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 887, "value": 0.15768888888888888}

:::MLPv0.5.0 ssd 1541757503.968621492 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 888, "value": 0.15786666666666668}

:::MLPv0.5.0 ssd 1541757504.053194523 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 889, "value": 0.15804444444444446}

:::MLPv0.5.0 ssd 1541757504.137616396 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 890, "value": 0.15822222222222224}

:::MLPv0.5.0 ssd 1541757504.221470118 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 891, "value": 0.1584}

:::MLPv0.5.0 ssd 1541757504.305882454 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 892, "value": 0.1585777777777778}

:::MLPv0.5.0 ssd 1541757504.390667200 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 893, "value": 0.15875555555555557}

:::MLPv0.5.0 ssd 1541757504.475472689 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 894, "value": 0.15893333333333334}

:::MLPv0.5.0 ssd 1541757504.559942007 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 895, "value": 0.15911111111111112}

:::MLPv0.5.0 ssd 1541757504.644061089 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 896, "value": 0.1592888888888889}

:::MLPv0.5.0 ssd 1541757504.728739262 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 897, "value": 0.15946666666666667}

:::MLPv0.5.0 ssd 1541757504.812826395 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 898, "value": 0.15964444444444445}

:::MLPv0.5.0 ssd 1541757504.897186279 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 899, "value": 0.15982222222222223}
Iteration:    900, Loss function: 4.698, Average Loss: 4.009, avg. samples / sec: 24289.43
Iteration:    900, Loss function: 4.532, Average Loss: 4.010, avg. samples / sec: 24289.84
Iteration:    900, Loss function: 5.336, Average Loss: 4.003, avg. samples / sec: 24328.44
Iteration:    900, Loss function: 4.560, Average Loss: 4.003, avg. samples / sec: 24333.05
Iteration:    900, Loss function: 4.857, Average Loss: 4.007, avg. samples / sec: 24272.13
Iteration:    900, Loss function: 5.259, Average Loss: 4.007, avg. samples / sec: 24293.28
Iteration:    900, Loss function: 5.595, Average Loss: 4.019, avg. samples / sec: 24251.39
Iteration:    900, Loss function: 4.423, Average Loss: 4.010, avg. samples / sec: 24261.53
Iteration:    920, Loss function: 4.876, Average Loss: 4.027, avg. samples / sec: 24536.45
Iteration:    920, Loss function: 5.182, Average Loss: 4.031, avg. samples / sec: 24513.42
Iteration:    920, Loss function: 5.501, Average Loss: 4.031, avg. samples / sec: 24522.61
Iteration:    920, Loss function: 5.229, Average Loss: 4.024, avg. samples / sec: 24515.91
Iteration:    920, Loss function: 4.663, Average Loss: 4.033, avg. samples / sec: 24540.46
Iteration:    920, Loss function: 5.110, Average Loss: 4.039, avg. samples / sec: 24536.82
Iteration:    920, Loss function: 4.921, Average Loss: 4.025, avg. samples / sec: 24496.04
Iteration:    920, Loss function: 5.211, Average Loss: 4.031, avg. samples / sec: 24450.36

:::MLPv0.5.0 ssd 1541757507.065911531 (train.py:553) train_epoch: 16
Iteration:    940, Loss function: 5.337, Average Loss: 4.054, avg. samples / sec: 24496.82
Iteration:    940, Loss function: 4.556, Average Loss: 4.052, avg. samples / sec: 24518.38
Iteration:    940, Loss function: 4.682, Average Loss: 4.044, avg. samples / sec: 24492.29
Iteration:    940, Loss function: 4.824, Average Loss: 4.055, avg. samples / sec: 24513.47
Iteration:    940, Loss function: 4.673, Average Loss: 4.044, avg. samples / sec: 24485.83
Iteration:    940, Loss function: 5.000, Average Loss: 4.047, avg. samples / sec: 24448.85
Iteration:    940, Loss function: 4.748, Average Loss: 4.050, avg. samples / sec: 24451.94
Iteration:    940, Loss function: 5.037, Average Loss: 4.050, avg. samples / sec: 24505.54
Iteration:    960, Loss function: 4.983, Average Loss: 4.067, avg. samples / sec: 24447.13
Iteration:    960, Loss function: 4.534, Average Loss: 4.067, avg. samples / sec: 24449.33
Iteration:    960, Loss function: 4.648, Average Loss: 4.061, avg. samples / sec: 24454.20
Iteration:    960, Loss function: 5.309, Average Loss: 4.061, avg. samples / sec: 24484.59
Iteration:    960, Loss function: 4.597, Average Loss: 4.070, avg. samples / sec: 24434.15
Iteration:    960, Loss function: 5.029, Average Loss: 4.063, avg. samples / sec: 24440.50
Iteration:    960, Loss function: 5.449, Average Loss: 4.068, avg. samples / sec: 24442.46
Iteration:    960, Loss function: 5.265, Average Loss: 4.059, avg. samples / sec: 24420.88
Iteration:    980, Loss function: 4.660, Average Loss: 4.081, avg. samples / sec: 24480.75
Iteration:    980, Loss function: 5.500, Average Loss: 4.087, avg. samples / sec: 24477.03
Iteration:    980, Loss function: 4.443, Average Loss: 4.078, avg. samples / sec: 24505.02
Iteration:    980, Loss function: 4.920, Average Loss: 4.081, avg. samples / sec: 24500.07
Iteration:    980, Loss function: 4.977, Average Loss: 4.087, avg. samples / sec: 24467.84
Iteration:    980, Loss function: 5.023, Average Loss: 4.075, avg. samples / sec: 24507.51
Iteration:    980, Loss function: 5.030, Average Loss: 4.077, avg. samples / sec: 24442.77
Iteration:    980, Loss function: 5.251, Average Loss: 4.078, avg. samples / sec: 24422.32

:::MLPv0.5.0 ssd 1541757511.836466551 (train.py:553) train_epoch: 17
Iteration:   1000, Loss function: 4.754, Average Loss: 4.094, avg. samples / sec: 24482.06
Iteration:   1000, Loss function: 4.785, Average Loss: 4.094, avg. samples / sec: 24446.26
Iteration:   1000, Loss function: 4.844, Average Loss: 4.099, avg. samples / sec: 24423.88
Iteration:   1000, Loss function: 4.493, Average Loss: 4.099, avg. samples / sec: 24450.34
Iteration:   1000, Loss function: 4.936, Average Loss: 4.091, avg. samples / sec: 24451.25
Iteration:   1000, Loss function: 4.802, Average Loss: 4.094, avg. samples / sec: 24432.07
Iteration:   1000, Loss function: 5.128, Average Loss: 4.089, avg. samples / sec: 24413.55
Iteration:   1000, Loss function: 4.232, Average Loss: 4.088, avg. samples / sec: 24436.21
Iteration:   1020, Loss function: 4.749, Average Loss: 4.108, avg. samples / sec: 24522.82
Iteration:   1020, Loss function: 4.690, Average Loss: 4.103, avg. samples / sec: 24591.84
Iteration:   1020, Loss function: 4.594, Average Loss: 4.108, avg. samples / sec: 24522.20
Iteration:   1020, Loss function: 5.257, Average Loss: 4.104, avg. samples / sec: 24570.94
Iteration:   1020, Loss function: 5.253, Average Loss: 4.116, avg. samples / sec: 24531.82
Iteration:   1020, Loss function: 5.211, Average Loss: 4.110, avg. samples / sec: 24517.53
Iteration:   1020, Loss function: 4.784, Average Loss: 4.108, avg. samples / sec: 24520.86
Iteration:   1020, Loss function: 4.922, Average Loss: 4.103, avg. samples / sec: 24512.86

:::MLPv0.5.0 ssd 1541757516.684739351 (train.py:553) train_epoch: 18
Iteration:   1040, Loss function: 5.050, Average Loss: 4.120, avg. samples / sec: 24520.99
Iteration:   1040, Loss function: 4.635, Average Loss: 4.118, avg. samples / sec: 24518.11
Iteration:   1040, Loss function: 4.723, Average Loss: 4.122, avg. samples / sec: 24492.78
Iteration:   1040, Loss function: 4.561, Average Loss: 4.132, avg. samples / sec: 24506.67
Iteration:   1040, Loss function: 4.206, Average Loss: 4.123, avg. samples / sec: 24513.36
Iteration:   1040, Loss function: 4.390, Average Loss: 4.118, avg. samples / sec: 24503.72
Iteration:   1040, Loss function: 4.330, Average Loss: 4.118, avg. samples / sec: 24520.19
Iteration:   1040, Loss function: 4.580, Average Loss: 4.120, avg. samples / sec: 24512.65
Iteration:   1060, Loss function: 4.567, Average Loss: 4.135, avg. samples / sec: 24570.14
Iteration:   1060, Loss function: 4.732, Average Loss: 4.145, avg. samples / sec: 24569.88
Iteration:   1060, Loss function: 4.525, Average Loss: 4.132, avg. samples / sec: 24527.90
Iteration:   1060, Loss function: 4.593, Average Loss: 4.131, avg. samples / sec: 24577.09
Iteration:   1060, Loss function: 5.363, Average Loss: 4.133, avg. samples / sec: 24553.86
Iteration:   1060, Loss function: 4.386, Average Loss: 4.134, avg. samples / sec: 24532.11
Iteration:   1060, Loss function: 4.803, Average Loss: 4.133, avg. samples / sec: 24546.60
Iteration:   1060, Loss function: 4.941, Average Loss: 4.130, avg. samples / sec: 24463.75
Iteration:   1080, Loss function: 4.714, Average Loss: 4.147, avg. samples / sec: 24465.15
Iteration:   1080, Loss function: 5.161, Average Loss: 4.143, avg. samples / sec: 24455.12
Iteration:   1080, Loss function: 4.480, Average Loss: 4.147, avg. samples / sec: 24448.72
Iteration:   1080, Loss function: 4.788, Average Loss: 4.140, avg. samples / sec: 24520.41
Iteration:   1080, Loss function: 5.489, Average Loss: 4.160, avg. samples / sec: 24433.14
Iteration:   1080, Loss function: 4.948, Average Loss: 4.146, avg. samples / sec: 24449.95
Iteration:   1080, Loss function: 4.757, Average Loss: 4.144, avg. samples / sec: 24424.15
Iteration:   1080, Loss function: 4.131, Average Loss: 4.145, avg. samples / sec: 24448.94

:::MLPv0.5.0 ssd 1541757521.536360025 (train.py:553) train_epoch: 19
Iteration:   1100, Loss function: 4.555, Average Loss: 4.168, avg. samples / sec: 24494.21
Iteration:   1100, Loss function: 4.134, Average Loss: 4.151, avg. samples / sec: 24475.32
Iteration:   1100, Loss function: 4.453, Average Loss: 4.158, avg. samples / sec: 24498.65
Iteration:   1100, Loss function: 5.130, Average Loss: 4.152, avg. samples / sec: 24467.34
Iteration:   1100, Loss function: 4.413, Average Loss: 4.157, avg. samples / sec: 24433.03
Iteration:   1100, Loss function: 5.005, Average Loss: 4.159, avg. samples / sec: 24432.56
Iteration:   1100, Loss function: 4.927, Average Loss: 4.157, avg. samples / sec: 24449.53
Iteration:   1100, Loss function: 4.745, Average Loss: 4.156, avg. samples / sec: 24439.97
Iteration:   1120, Loss function: 4.669, Average Loss: 4.165, avg. samples / sec: 24539.24
Iteration:   1120, Loss function: 4.777, Average Loss: 4.162, avg. samples / sec: 24476.10
Iteration:   1120, Loss function: 4.593, Average Loss: 4.170, avg. samples / sec: 24513.70
Iteration:   1120, Loss function: 4.369, Average Loss: 4.163, avg. samples / sec: 24479.63
Iteration:   1120, Loss function: 5.172, Average Loss: 4.168, avg. samples / sec: 24512.72
Iteration:   1120, Loss function: 4.221, Average Loss: 4.179, avg. samples / sec: 24434.58
Iteration:   1120, Loss function: 4.924, Average Loss: 4.171, avg. samples / sec: 24432.60
Iteration:   1120, Loss function: 4.787, Average Loss: 4.164, avg. samples / sec: 24481.42
Iteration:   1140, Loss function: 4.671, Average Loss: 4.178, avg. samples / sec: 24455.38
Iteration:   1140, Loss function: 4.660, Average Loss: 4.187, avg. samples / sec: 24486.46
Iteration:   1140, Loss function: 4.184, Average Loss: 4.171, avg. samples / sec: 24452.48
Iteration:   1140, Loss function: 4.546, Average Loss: 4.179, avg. samples / sec: 24496.66
Iteration:   1140, Loss function: 4.531, Average Loss: 4.171, avg. samples / sec: 24450.69
Iteration:   1140, Loss function: 4.383, Average Loss: 4.174, avg. samples / sec: 24500.18
Iteration:   1140, Loss function: 4.187, Average Loss: 4.176, avg. samples / sec: 24447.24
Iteration:   1140, Loss function: 4.939, Average Loss: 4.173, avg. samples / sec: 24396.65

:::MLPv0.5.0 ssd 1541757526.391376495 (train.py:553) train_epoch: 20
Iteration:   1160, Loss function: 5.255, Average Loss: 4.200, avg. samples / sec: 24477.37
Iteration:   1160, Loss function: 5.519, Average Loss: 4.188, avg. samples / sec: 24473.91
Iteration:   1160, Loss function: 4.302, Average Loss: 4.182, avg. samples / sec: 24436.95
Iteration:   1160, Loss function: 4.472, Average Loss: 4.183, avg. samples / sec: 24432.18
Iteration:   1160, Loss function: 4.791, Average Loss: 4.189, avg. samples / sec: 24422.38
Iteration:   1160, Loss function: 4.967, Average Loss: 4.186, avg. samples / sec: 24425.34
Iteration:   1160, Loss function: 4.229, Average Loss: 4.185, avg. samples / sec: 24475.70
Iteration:   1160, Loss function: 4.848, Average Loss: 4.193, avg. samples / sec: 24397.38
Iteration:   1180, Loss function: 4.646, Average Loss: 4.209, avg. samples / sec: 24393.80
Iteration:   1180, Loss function: 4.007, Average Loss: 4.201, avg. samples / sec: 24470.00
Iteration:   1180, Loss function: 4.258, Average Loss: 4.193, avg. samples / sec: 24434.39
Iteration:   1180, Loss function: 4.395, Average Loss: 4.191, avg. samples / sec: 24439.50
Iteration:   1180, Loss function: 4.451, Average Loss: 4.196, avg. samples / sec: 24391.30
Iteration:   1180, Loss function: 4.049, Average Loss: 4.189, avg. samples / sec: 24400.22
Iteration:   1180, Loss function: 4.182, Average Loss: 4.194, avg. samples / sec: 24398.70
Iteration:   1180, Loss function: 4.614, Average Loss: 4.197, avg. samples / sec: 24392.56
Iteration:   1200, Loss function: 4.400, Average Loss: 4.195, avg. samples / sec: 24476.17
Iteration:   1200, Loss function: 4.450, Average Loss: 4.204, avg. samples / sec: 24522.09
Iteration:   1200, Loss function: 4.977, Average Loss: 4.199, avg. samples / sec: 24466.50
Iteration:   1200, Loss function: 4.167, Average Loss: 4.201, avg. samples / sec: 24513.76
Iteration:   1200, Loss function: 4.597, Average Loss: 4.201, avg. samples / sec: 24465.14
Iteration:   1200, Loss function: 4.410, Average Loss: 4.216, avg. samples / sec: 24439.45
Iteration:   1200, Loss function: 4.315, Average Loss: 4.193, avg. samples / sec: 24436.93
Iteration:   1200, Loss function: 4.216, Average Loss: 4.208, avg. samples / sec: 24403.28

:::MLPv0.5.0 ssd 1541757531.495388508 (train.py:553) train_epoch: 21
Iteration:   1220, Loss function: 4.807, Average Loss: 4.212, avg. samples / sec: 20470.75
Iteration:   1220, Loss function: 4.556, Average Loss: 4.206, avg. samples / sec: 20467.33
Iteration:   1220, Loss function: 4.223, Average Loss: 4.208, avg. samples / sec: 20466.70
Iteration:   1220, Loss function: 3.889, Average Loss: 4.203, avg. samples / sec: 20446.47
Iteration:   1220, Loss function: 4.030, Average Loss: 4.222, avg. samples / sec: 20458.95
Iteration:   1220, Loss function: 4.591, Average Loss: 4.215, avg. samples / sec: 20483.62
Iteration:   1220, Loss function: 4.871, Average Loss: 4.201, avg. samples / sec: 20479.12
Iteration:   1220, Loss function: 4.752, Average Loss: 4.201, avg. samples / sec: 20431.86
Iteration:   1240, Loss function: 4.054, Average Loss: 4.220, avg. samples / sec: 24484.33
Iteration:   1240, Loss function: 4.565, Average Loss: 4.212, avg. samples / sec: 24490.13
Iteration:   1240, Loss function: 4.358, Average Loss: 4.208, avg. samples / sec: 24497.77
Iteration:   1240, Loss function: 4.583, Average Loss: 4.230, avg. samples / sec: 24490.22
Iteration:   1240, Loss function: 5.285, Average Loss: 4.217, avg. samples / sec: 24436.74
Iteration:   1240, Loss function: 4.473, Average Loss: 4.218, avg. samples / sec: 24450.92
Iteration:   1240, Loss function: 4.074, Average Loss: 4.208, avg. samples / sec: 24482.32
Iteration:   1240, Loss function: 4.613, Average Loss: 4.223, avg. samples / sec: 24470.17
Iteration:   1260, Loss function: 4.399, Average Loss: 4.227, avg. samples / sec: 24531.54
Iteration:   1260, Loss function: 4.335, Average Loss: 4.216, avg. samples / sec: 24570.16
Iteration:   1260, Loss function: 4.702, Average Loss: 4.220, avg. samples / sec: 24561.28
Iteration:   1260, Loss function: 4.712, Average Loss: 4.225, avg. samples / sec: 24581.59
Iteration:   1260, Loss function: 4.703, Average Loss: 4.213, avg. samples / sec: 24561.65
Iteration:   1260, Loss function: 4.363, Average Loss: 4.234, avg. samples / sec: 24525.90
Iteration:   1260, Loss function: 4.321, Average Loss: 4.228, avg. samples / sec: 24546.05
Iteration:   1260, Loss function: 4.242, Average Loss: 4.223, avg. samples / sec: 24529.54

:::MLPv0.5.0 ssd 1541757536.344858646 (train.py:553) train_epoch: 22
Iteration:   1280, Loss function: 4.722, Average Loss: 4.234, avg. samples / sec: 24449.44
Iteration:   1280, Loss function: 4.501, Average Loss: 4.226, avg. samples / sec: 24450.76
Iteration:   1280, Loss function: 4.435, Average Loss: 4.233, avg. samples / sec: 24445.76
Iteration:   1280, Loss function: 4.476, Average Loss: 4.222, avg. samples / sec: 24441.33
Iteration:   1280, Loss function: 5.029, Average Loss: 4.240, avg. samples / sec: 24464.41
Iteration:   1280, Loss function: 4.579, Average Loss: 4.231, avg. samples / sec: 24471.08
Iteration:   1280, Loss function: 4.463, Average Loss: 4.219, avg. samples / sec: 24435.23
Iteration:   1280, Loss function: 4.591, Average Loss: 4.235, avg. samples / sec: 24434.68
Iteration:   1300, Loss function: 4.354, Average Loss: 4.240, avg. samples / sec: 24606.63
Iteration:   1300, Loss function: 3.998, Average Loss: 4.238, avg. samples / sec: 24543.09
Iteration:   1300, Loss function: 4.532, Average Loss: 4.245, avg. samples / sec: 24563.56
Iteration:   1300, Loss function: 4.229, Average Loss: 4.237, avg. samples / sec: 24566.16
Iteration:   1300, Loss function: 4.411, Average Loss: 4.241, avg. samples / sec: 24533.34
Iteration:   1300, Loss function: 4.229, Average Loss: 4.233, avg. samples / sec: 24509.59
Iteration:   1300, Loss function: 4.207, Average Loss: 4.228, avg. samples / sec: 24502.82
Iteration:   1300, Loss function: 4.141, Average Loss: 4.224, avg. samples / sec: 24527.31
Iteration:   1320, Loss function: 4.234, Average Loss: 4.241, avg. samples / sec: 24515.22
Iteration:   1320, Loss function: 4.263, Average Loss: 4.226, avg. samples / sec: 24567.88
Iteration:   1320, Loss function: 4.706, Average Loss: 4.242, avg. samples / sec: 24510.23
Iteration:   1320, Loss function: 4.126, Average Loss: 4.232, avg. samples / sec: 24557.36
Iteration:   1320, Loss function: 3.866, Average Loss: 4.238, avg. samples / sec: 24550.59
Iteration:   1320, Loss function: 4.307, Average Loss: 4.245, avg. samples / sec: 24522.40
Iteration:   1320, Loss function: 4.356, Average Loss: 4.239, avg. samples / sec: 24504.16
Iteration:   1320, Loss function: 4.660, Average Loss: 4.248, avg. samples / sec: 24490.78

:::MLPv0.5.0 ssd 1541757541.193068743 (train.py:553) train_epoch: 23
Iteration:   1340, Loss function: 4.435, Average Loss: 4.242, avg. samples / sec: 24422.10
Iteration:   1340, Loss function: 4.541, Average Loss: 4.244, avg. samples / sec: 24414.86
Iteration:   1340, Loss function: 4.769, Average Loss: 4.232, avg. samples / sec: 24414.77
Iteration:   1340, Loss function: 4.668, Average Loss: 4.247, avg. samples / sec: 24409.73
Iteration:   1340, Loss function: 4.565, Average Loss: 4.236, avg. samples / sec: 24412.15
Iteration:   1340, Loss function: 4.722, Average Loss: 4.253, avg. samples / sec: 24413.07
Iteration:   1340, Loss function: 3.860, Average Loss: 4.244, avg. samples / sec: 24383.94
Iteration:   1340, Loss function: 4.334, Average Loss: 4.241, avg. samples / sec: 24385.03
Iteration:   1360, Loss function: 5.336, Average Loss: 4.245, avg. samples / sec: 24486.74
Iteration:   1360, Loss function: 5.048, Average Loss: 4.260, avg. samples / sec: 24491.08
Iteration:   1360, Loss function: 5.358, Average Loss: 4.257, avg. samples / sec: 24519.67
Iteration:   1360, Loss function: 4.845, Average Loss: 4.264, avg. samples / sec: 24513.21
Iteration:   1360, Loss function: 5.306, Average Loss: 4.251, avg. samples / sec: 24486.89
Iteration:   1360, Loss function: 4.894, Average Loss: 4.257, avg. samples / sec: 24470.51
Iteration:   1360, Loss function: 5.160, Average Loss: 4.254, avg. samples / sec: 24523.90
Iteration:   1360, Loss function: 4.777, Average Loss: 4.259, avg. samples / sec: 24420.66
Iteration:   1380, Loss function: 4.677, Average Loss: 4.270, avg. samples / sec: 24480.17
Iteration:   1380, Loss function: 4.802, Average Loss: 4.265, avg. samples / sec: 24490.15
Iteration:   1380, Loss function: 3.655, Average Loss: 4.264, avg. samples / sec: 24474.21
Iteration:   1380, Loss function: 4.690, Average Loss: 4.263, avg. samples / sec: 24433.42
Iteration:   1380, Loss function: 4.364, Average Loss: 4.268, avg. samples / sec: 24480.92
Iteration:   1380, Loss function: 4.886, Average Loss: 4.255, avg. samples / sec: 24417.53
Iteration:   1380, Loss function: 4.228, Average Loss: 4.271, avg. samples / sec: 24422.34
Iteration:   1380, Loss function: 4.318, Average Loss: 4.262, avg. samples / sec: 24427.06

:::MLPv0.5.0 ssd 1541757546.049340248 (train.py:553) train_epoch: 24
Iteration:   1400, Loss function: 3.706, Average Loss: 4.255, avg. samples / sec: 24513.28
Iteration:   1400, Loss function: 4.446, Average Loss: 4.267, avg. samples / sec: 24507.82
Iteration:   1400, Loss function: 3.856, Average Loss: 4.269, avg. samples / sec: 24448.09
Iteration:   1400, Loss function: 4.252, Average Loss: 4.266, avg. samples / sec: 24452.41
Iteration:   1400, Loss function: 4.501, Average Loss: 4.267, avg. samples / sec: 24431.02
Iteration:   1400, Loss function: 4.484, Average Loss: 4.266, avg. samples / sec: 24469.09
Iteration:   1400, Loss function: 3.525, Average Loss: 4.265, avg. samples / sec: 24481.59
Iteration:   1400, Loss function: 4.089, Average Loss: 4.272, avg. samples / sec: 24466.29
Iteration:   1420, Loss function: 4.334, Average Loss: 4.257, avg. samples / sec: 24490.41
Iteration:   1420, Loss function: 4.176, Average Loss: 4.271, avg. samples / sec: 24491.80
Iteration:   1420, Loss function: 4.349, Average Loss: 4.267, avg. samples / sec: 24493.20
Iteration:   1420, Loss function: 4.046, Average Loss: 4.267, avg. samples / sec: 24514.45
Iteration:   1420, Loss function: 4.803, Average Loss: 4.267, avg. samples / sec: 24499.35
Iteration:   1420, Loss function: 4.172, Average Loss: 4.266, avg. samples / sec: 24513.07
Iteration:   1420, Loss function: 4.131, Average Loss: 4.273, avg. samples / sec: 24519.69
Iteration:   1420, Loss function: 3.740, Average Loss: 4.267, avg. samples / sec: 24456.49
Iteration:   1440, Loss function: 4.543, Average Loss: 4.275, avg. samples / sec: 24504.95
Iteration:   1440, Loss function: 4.182, Average Loss: 4.269, avg. samples / sec: 24512.55
Iteration:   1440, Loss function: 4.399, Average Loss: 4.268, avg. samples / sec: 24509.05
Iteration:   1440, Loss function: 4.211, Average Loss: 4.276, avg. samples / sec: 24509.70
Iteration:   1440, Loss function: 4.751, Average Loss: 4.260, avg. samples / sec: 24473.88
Iteration:   1440, Loss function: 4.519, Average Loss: 4.270, avg. samples / sec: 24490.82
Iteration:   1440, Loss function: 4.559, Average Loss: 4.270, avg. samples / sec: 24456.32
Iteration:   1440, Loss function: 4.424, Average Loss: 4.267, avg. samples / sec: 24454.85

:::MLPv0.5.0 ssd 1541757550.820089817 (train.py:553) train_epoch: 25
Iteration:   1460, Loss function: 4.063, Average Loss: 4.270, avg. samples / sec: 24455.60
Iteration:   1460, Loss function: 4.424, Average Loss: 4.270, avg. samples / sec: 24408.83
Iteration:   1460, Loss function: 4.045, Average Loss: 4.274, avg. samples / sec: 24392.88
Iteration:   1460, Loss function: 4.589, Average Loss: 4.260, avg. samples / sec: 24415.48
Iteration:   1460, Loss function: 4.215, Average Loss: 4.273, avg. samples / sec: 24371.66
Iteration:   1460, Loss function: 3.779, Average Loss: 4.268, avg. samples / sec: 24427.08
Iteration:   1460, Loss function: 4.434, Average Loss: 4.272, avg. samples / sec: 24399.39
Iteration:   1460, Loss function: 4.825, Average Loss: 4.279, avg. samples / sec: 24354.72
Iteration:   1480, Loss function: 4.166, Average Loss: 4.271, avg. samples / sec: 24454.34
Iteration:   1480, Loss function: 4.060, Average Loss: 4.262, avg. samples / sec: 24444.62
Iteration:   1480, Loss function: 4.192, Average Loss: 4.274, avg. samples / sec: 24458.16
Iteration:   1480, Loss function: 4.338, Average Loss: 4.275, avg. samples / sec: 24437.34
Iteration:   1480, Loss function: 4.398, Average Loss: 4.279, avg. samples / sec: 24482.82
Iteration:   1480, Loss function: 4.720, Average Loss: 4.276, avg. samples / sec: 24465.16
Iteration:   1480, Loss function: 4.096, Average Loss: 4.268, avg. samples / sec: 24437.65
Iteration:   1480, Loss function: 4.511, Average Loss: 4.270, avg. samples / sec: 24401.47
Iteration:   1500, Loss function: 4.263, Average Loss: 4.270, avg. samples / sec: 24573.92
Iteration:   1500, Loss function: 4.838, Average Loss: 4.274, avg. samples / sec: 24519.56
Iteration:   1500, Loss function: 4.160, Average Loss: 4.270, avg. samples / sec: 24570.77
Iteration:   1500, Loss function: 4.036, Average Loss: 4.273, avg. samples / sec: 24543.11
Iteration:   1500, Loss function: 4.572, Average Loss: 4.277, avg. samples / sec: 24526.63
Iteration:   1500, Loss function: 4.164, Average Loss: 4.280, avg. samples / sec: 24512.42
Iteration:   1500, Loss function: 3.961, Average Loss: 4.262, avg. samples / sec: 24503.86
Iteration:   1500, Loss function: 4.013, Average Loss: 4.277, avg. samples / sec: 24487.71

:::MLPv0.5.0 ssd 1541757555.675523043 (train.py:553) train_epoch: 26
Iteration:   1520, Loss function: 4.429, Average Loss: 4.277, avg. samples / sec: 24447.55
Iteration:   1520, Loss function: 4.143, Average Loss: 4.280, avg. samples / sec: 24498.39
Iteration:   1520, Loss function: 4.666, Average Loss: 4.272, avg. samples / sec: 24442.38
Iteration:   1520, Loss function: 4.647, Average Loss: 4.274, avg. samples / sec: 24432.68
Iteration:   1520, Loss function: 4.028, Average Loss: 4.280, avg. samples / sec: 24452.31
Iteration:   1520, Loss function: 4.540, Average Loss: 4.272, avg. samples / sec: 24391.04
Iteration:   1520, Loss function: 4.916, Average Loss: 4.263, avg. samples / sec: 24425.47
Iteration:   1520, Loss function: 5.050, Average Loss: 4.281, avg. samples / sec: 24419.58
Iteration:   1540, Loss function: 4.513, Average Loss: 4.266, avg. samples / sec: 24541.14
Iteration:   1540, Loss function: 4.495, Average Loss: 4.274, avg. samples / sec: 24493.67
Iteration:   1540, Loss function: 3.830, Average Loss: 4.281, avg. samples / sec: 24476.63
Iteration:   1540, Loss function: 4.094, Average Loss: 4.285, avg. samples / sec: 24511.77
Iteration:   1540, Loss function: 4.229, Average Loss: 4.280, avg. samples / sec: 24470.85
Iteration:   1540, Loss function: 4.901, Average Loss: 4.272, avg. samples / sec: 24445.43
Iteration:   1540, Loss function: 4.379, Average Loss: 4.280, avg. samples / sec: 24429.16
Iteration:   1540, Loss function: 4.252, Average Loss: 4.274, avg. samples / sec: 24461.22

:::MLPv0.5.0 ssd 1541757560.529358387 (train.py:553) train_epoch: 27
Iteration:   1560, Loss function: 4.344, Average Loss: 4.267, avg. samples / sec: 24469.69
Iteration:   1560, Loss function: 4.546, Average Loss: 4.276, avg. samples / sec: 24462.67
Iteration:   1560, Loss function: 4.761, Average Loss: 4.271, avg. samples / sec: 24497.46
Iteration:   1560, Loss function: 4.184, Average Loss: 4.280, avg. samples / sec: 24460.53
Iteration:   1560, Loss function: 4.473, Average Loss: 4.281, avg. samples / sec: 24496.48
Iteration:   1560, Loss function: 4.334, Average Loss: 4.282, avg. samples / sec: 24464.10
Iteration:   1560, Loss function: 4.217, Average Loss: 4.276, avg. samples / sec: 24486.05
Iteration:   1560, Loss function: 4.472, Average Loss: 4.287, avg. samples / sec: 24441.17
Iteration:   1580, Loss function: 3.941, Average Loss: 4.265, avg. samples / sec: 24508.56
Iteration:   1580, Loss function: 4.597, Average Loss: 4.284, avg. samples / sec: 24539.85
Iteration:   1580, Loss function: 3.656, Average Loss: 4.270, avg. samples / sec: 24512.49
Iteration:   1580, Loss function: 4.661, Average Loss: 4.275, avg. samples / sec: 24507.48
Iteration:   1580, Loss function: 4.687, Average Loss: 4.289, avg. samples / sec: 24537.62
Iteration:   1580, Loss function: 4.496, Average Loss: 4.283, avg. samples / sec: 24489.29
Iteration:   1580, Loss function: 4.259, Average Loss: 4.276, avg. samples / sec: 24500.81
Iteration:   1580, Loss function: 3.904, Average Loss: 4.281, avg. samples / sec: 24463.80
Iteration:   1600, Loss function: 4.248, Average Loss: 4.265, avg. samples / sec: 24504.10
Iteration:   1600, Loss function: 4.605, Average Loss: 4.281, avg. samples / sec: 24563.68
Iteration:   1600, Loss function: 4.080, Average Loss: 4.272, avg. samples / sec: 24502.51
Iteration:   1600, Loss function: 4.351, Average Loss: 4.275, avg. samples / sec: 24501.21
Iteration:   1600, Loss function: 4.246, Average Loss: 4.287, avg. samples / sec: 24498.11
Iteration:   1600, Loss function: 3.964, Average Loss: 4.284, avg. samples / sec: 24503.84
Iteration:   1600, Loss function: 4.541, Average Loss: 4.275, avg. samples / sec: 24509.18
Iteration:   1600, Loss function: 4.818, Average Loss: 4.283, avg. samples / sec: 24455.03

:::MLPv0.5.0 ssd 1541757565.377350807 (train.py:553) train_epoch: 28
Iteration:   1620, Loss function: 3.887, Average Loss: 4.281, avg. samples / sec: 24504.75
Iteration:   1620, Loss function: 4.860, Average Loss: 4.264, avg. samples / sec: 24491.87
Iteration:   1620, Loss function: 4.690, Average Loss: 4.279, avg. samples / sec: 24506.16
Iteration:   1620, Loss function: 4.496, Average Loss: 4.275, avg. samples / sec: 24481.86
Iteration:   1620, Loss function: 3.915, Average Loss: 4.286, avg. samples / sec: 24497.45
Iteration:   1620, Loss function: 3.812, Average Loss: 4.282, avg. samples / sec: 24503.80
Iteration:   1620, Loss function: 3.999, Average Loss: 4.274, avg. samples / sec: 24491.43
Iteration:   1620, Loss function: 4.085, Average Loss: 4.284, avg. samples / sec: 24466.33
Iteration:   1640, Loss function: 4.106, Average Loss: 4.262, avg. samples / sec: 24545.50
Iteration:   1640, Loss function: 4.165, Average Loss: 4.283, avg. samples / sec: 24538.70
Iteration:   1640, Loss function: 4.364, Average Loss: 4.284, avg. samples / sec: 24608.49
Iteration:   1640, Loss function: 4.110, Average Loss: 4.282, avg. samples / sec: 24583.08
Iteration:   1640, Loss function: 4.028, Average Loss: 4.271, avg. samples / sec: 24579.34
Iteration:   1640, Loss function: 4.165, Average Loss: 4.278, avg. samples / sec: 24518.65
Iteration:   1640, Loss function: 4.493, Average Loss: 4.277, avg. samples / sec: 24531.78
Iteration:   1640, Loss function: 4.663, Average Loss: 4.286, avg. samples / sec: 24518.17
Iteration:   1660, Loss function: 4.828, Average Loss: 4.282, avg. samples / sec: 24500.92
Iteration:   1660, Loss function: 4.529, Average Loss: 4.264, avg. samples / sec: 24487.82
Iteration:   1660, Loss function: 4.069, Average Loss: 4.283, avg. samples / sec: 24482.89
Iteration:   1660, Loss function: 4.220, Average Loss: 4.279, avg. samples / sec: 24482.41
Iteration:   1660, Loss function: 3.679, Average Loss: 4.272, avg. samples / sec: 24464.16
Iteration:   1660, Loss function: 4.694, Average Loss: 4.276, avg. samples / sec: 24471.68
Iteration:   1660, Loss function: 4.348, Average Loss: 4.284, avg. samples / sec: 24435.28
Iteration:   1660, Loss function: 4.340, Average Loss: 4.285, avg. samples / sec: 24490.87

:::MLPv0.5.0 ssd 1541757570.139524937 (train.py:553) train_epoch: 29
Iteration:   1680, Loss function: 4.323, Average Loss: 4.282, avg. samples / sec: 24512.65
Iteration:   1680, Loss function: 4.250, Average Loss: 4.281, avg. samples / sec: 24522.65
Iteration:   1680, Loss function: 4.328, Average Loss: 4.263, avg. samples / sec: 24512.78
Iteration:   1680, Loss function: 4.828, Average Loss: 4.286, avg. samples / sec: 24558.59
Iteration:   1680, Loss function: 3.988, Average Loss: 4.273, avg. samples / sec: 24552.89
Iteration:   1680, Loss function: 4.742, Average Loss: 4.271, avg. samples / sec: 24519.51
Iteration:   1680, Loss function: 3.838, Average Loss: 4.283, avg. samples / sec: 24526.68
Iteration:   1680, Loss function: 4.209, Average Loss: 4.278, avg. samples / sec: 24487.60
Iteration:   1700, Loss function: 3.860, Average Loss: 4.262, avg. samples / sec: 24505.10
Iteration:   1700, Loss function: 4.849, Average Loss: 4.281, avg. samples / sec: 24499.62
Iteration:   1700, Loss function: 4.100, Average Loss: 4.279, avg. samples / sec: 24552.81
Iteration:   1700, Loss function: 4.382, Average Loss: 4.280, avg. samples / sec: 24462.05
Iteration:   1700, Loss function: 3.907, Average Loss: 4.282, avg. samples / sec: 24494.97
Iteration:   1700, Loss function: 4.823, Average Loss: 4.285, avg. samples / sec: 24455.20
Iteration:   1700, Loss function: 4.403, Average Loss: 4.270, avg. samples / sec: 24486.84
Iteration:   1700, Loss function: 4.218, Average Loss: 4.273, avg. samples / sec: 24460.41
Iteration:   1720, Loss function: 4.612, Average Loss: 4.265, avg. samples / sec: 24500.32
Iteration:   1720, Loss function: 4.659, Average Loss: 4.281, avg. samples / sec: 24502.43
Iteration:   1720, Loss function: 3.957, Average Loss: 4.273, avg. samples / sec: 24554.66
Iteration:   1720, Loss function: 4.293, Average Loss: 4.285, avg. samples / sec: 24540.64
Iteration:   1720, Loss function: 4.437, Average Loss: 4.282, avg. samples / sec: 24529.97
Iteration:   1720, Loss function: 4.193, Average Loss: 4.281, avg. samples / sec: 24478.16
Iteration:   1720, Loss function: 3.623, Average Loss: 4.270, avg. samples / sec: 24515.33
Iteration:   1720, Loss function: 4.185, Average Loss: 4.283, avg. samples / sec: 24481.10

:::MLPv0.5.0 ssd 1541757574.990044832 (train.py:553) train_epoch: 30
Iteration:   1740, Loss function: 4.266, Average Loss: 4.277, avg. samples / sec: 24467.44
Iteration:   1740, Loss function: 4.362, Average Loss: 4.281, avg. samples / sec: 24487.48
Iteration:   1740, Loss function: 4.163, Average Loss: 4.273, avg. samples / sec: 24456.06
Iteration:   1740, Loss function: 4.345, Average Loss: 4.281, avg. samples / sec: 24466.69
Iteration:   1740, Loss function: 4.538, Average Loss: 4.283, avg. samples / sec: 24485.66
Iteration:   1740, Loss function: 4.062, Average Loss: 4.284, avg. samples / sec: 24437.48
Iteration:   1740, Loss function: 4.542, Average Loss: 4.263, avg. samples / sec: 24417.78
Iteration:   1740, Loss function: 4.459, Average Loss: 4.267, avg. samples / sec: 24454.21
Iteration:   1760, Loss function: 4.253, Average Loss: 4.282, avg. samples / sec: 24570.22
Iteration:   1760, Loss function: 4.121, Average Loss: 4.277, avg. samples / sec: 24534.20
Iteration:   1760, Loss function: 3.954, Average Loss: 4.279, avg. samples / sec: 24531.01
Iteration:   1760, Loss function: 4.591, Average Loss: 4.273, avg. samples / sec: 24542.14
Iteration:   1760, Loss function: 3.590, Average Loss: 4.282, avg. samples / sec: 24537.66
Iteration:   1760, Loss function: 4.453, Average Loss: 4.266, avg. samples / sec: 24543.11
Iteration:   1760, Loss function: 3.723, Average Loss: 4.283, avg. samples / sec: 24528.72
Iteration:   1760, Loss function: 4.778, Average Loss: 4.265, avg. samples / sec: 24521.79
Iteration:   1780, Loss function: 4.572, Average Loss: 4.277, avg. samples / sec: 24503.48
Iteration:   1780, Loss function: 4.110, Average Loss: 4.281, avg. samples / sec: 24493.99
Iteration:   1780, Loss function: 4.069, Average Loss: 4.273, avg. samples / sec: 24502.11
Iteration:   1780, Loss function: 4.055, Average Loss: 4.279, avg. samples / sec: 24500.34
Iteration:   1780, Loss function: 3.670, Average Loss: 4.280, avg. samples / sec: 24522.50
Iteration:   1780, Loss function: 4.443, Average Loss: 4.266, avg. samples / sec: 24506.17
Iteration:   1780, Loss function: 3.345, Average Loss: 4.283, avg. samples / sec: 24493.05
Iteration:   1780, Loss function: 4.289, Average Loss: 4.263, avg. samples / sec: 24511.21

:::MLPv0.5.0 ssd 1541757579.837930441 (train.py:553) train_epoch: 31
Iteration:   1800, Loss function: 3.910, Average Loss: 4.274, avg. samples / sec: 24469.79
Iteration:   1800, Loss function: 4.684, Average Loss: 4.263, avg. samples / sec: 24516.99
Iteration:   1800, Loss function: 3.990, Average Loss: 4.279, avg. samples / sec: 24473.02
Iteration:   1800, Loss function: 4.181, Average Loss: 4.272, avg. samples / sec: 24466.47
Iteration:   1800, Loss function: 4.752, Average Loss: 4.280, avg. samples / sec: 24498.85
Iteration:   1800, Loss function: 4.786, Average Loss: 4.279, avg. samples / sec: 24459.97
Iteration:   1800, Loss function: 4.390, Average Loss: 4.265, avg. samples / sec: 24467.14
Iteration:   1800, Loss function: 4.279, Average Loss: 4.279, avg. samples / sec: 24422.89
Iteration:   1820, Loss function: 3.936, Average Loss: 4.275, avg. samples / sec: 24441.51
Iteration:   1820, Loss function: 4.222, Average Loss: 4.271, avg. samples / sec: 24419.63
Iteration:   1820, Loss function: 4.018, Average Loss: 4.277, avg. samples / sec: 24422.56
Iteration:   1820, Loss function: 3.960, Average Loss: 4.275, avg. samples / sec: 24399.35
Iteration:   1820, Loss function: 4.512, Average Loss: 4.277, avg. samples / sec: 24448.70
Iteration:   1820, Loss function: 4.372, Average Loss: 4.277, avg. samples / sec: 24423.35
Iteration:   1820, Loss function: 4.174, Average Loss: 4.264, avg. samples / sec: 24374.06
Iteration:   1820, Loss function: 4.274, Average Loss: 4.264, avg. samples / sec: 24413.10
Iteration:   1840, Loss function: 4.011, Average Loss: 4.262, avg. samples / sec: 24541.87
Iteration:   1840, Loss function: 4.690, Average Loss: 4.274, avg. samples / sec: 24469.83
Iteration:   1840, Loss function: 3.882, Average Loss: 4.275, avg. samples / sec: 24502.08
Iteration:   1840, Loss function: 4.262, Average Loss: 4.274, avg. samples / sec: 24523.56
Iteration:   1840, Loss function: 4.550, Average Loss: 4.273, avg. samples / sec: 24501.30
Iteration:   1840, Loss function: 4.088, Average Loss: 4.277, avg. samples / sec: 24499.14
Iteration:   1840, Loss function: 4.638, Average Loss: 4.264, avg. samples / sec: 24493.48
Iteration:   1840, Loss function: 4.489, Average Loss: 4.272, avg. samples / sec: 24451.27

:::MLPv0.5.0 ssd 1541757584.695349693 (train.py:553) train_epoch: 32
Iteration:   1860, Loss function: 4.469, Average Loss: 4.272, avg. samples / sec: 24499.37
Iteration:   1860, Loss function: 4.040, Average Loss: 4.272, avg. samples / sec: 24443.33
Iteration:   1860, Loss function: 4.414, Average Loss: 4.265, avg. samples / sec: 24443.46
Iteration:   1860, Loss function: 4.321, Average Loss: 4.274, avg. samples / sec: 24447.99
Iteration:   1860, Loss function: 4.017, Average Loss: 4.278, avg. samples / sec: 24457.82
Iteration:   1860, Loss function: 4.204, Average Loss: 4.273, avg. samples / sec: 24434.31
Iteration:   1860, Loss function: 4.345, Average Loss: 4.274, avg. samples / sec: 24428.51
Iteration:   1860, Loss function: 4.088, Average Loss: 4.264, avg. samples / sec: 24460.27

































































:::MLPv0.5.0 ssd 1541757586.956685543 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757586.957229853 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757586.957690001 (train.py:220) eval_start: 32
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 46
Predicting Ended, total time: 6.20 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.20 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.20 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.20 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.20 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.20 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.20 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.20 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(316925, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/316925
Converting ndarray to lists...
(316925, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(316925, 7)
(316925, 7)
Loading and preparing results...
(316925, 7)
Loading and preparing results...
(316925, 7)
Loading and preparing results...
(316925, 7)
(316925, 7)
0/316925
Loading and preparing results...
0/316925
0/316925
Loading and preparing results...
0/316925
Converting ndarray to lists...
0/316925
0/316925
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/316925
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(316925, 7)
Loading and preparing results...
(316925, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(316925, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(316925, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/316925
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(316925, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(316925, 7)
0/316925
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(316925, 7)
Converting ndarray to lists...
(316925, 7)
Loading and preparing results...
(316925, 7)
Loading and preparing results...
(316925, 7)
(316925, 7)
0/316925
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(316925, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/316925
Converting ndarray to lists...
(316925, 7)
0/316925
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(316925, 7)
(316925, 7)
(316925, 7)
(316925, 7)
(316925, 7)
(316925, 7)
0/316925
Converting ndarray to lists...
(316925, 7)
Loading and preparing results...
(316925, 7)
(316925, 7)
0/316925
Loading and preparing results...
(316925, 7)
0/316925
(316925, 7)
Converting ndarray to lists...
(316925, 7)
0/316925
Converting ndarray to lists...
0/316925
Converting ndarray to lists...
0/316925
0/316925
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(316925, 7)
(316925, 7)
0/316925
Converting ndarray to lists...
(316925, 7)
Loading and preparing results...
(316925, 7)
(316925, 7)
(316925, 7)
0/316925
Converting ndarray to lists...
Converting ndarray to lists...
(316925, 7)
Loading and preparing results...
0/316925
Converting ndarray to lists...
0/316925
Converting ndarray to lists...
Loading and preparing results...
0/316925
0/316925
(316925, 7)
0/316925
0/316925
(316925, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/316925
0/316925
0/316925
0/316925
Converting ndarray to lists...
(316925, 7)
Converting ndarray to lists...
Loading and preparing results...
0/316925
0/316925
Loading and preparing results...
Loading and preparing results...
0/316925
(316925, 7)
(316925, 7)
0/316925
0/316925
(316925, 7)
0/316925
0/316925
Converting ndarray to lists...
Converting ndarray to lists...
(316925, 7)
0/316925
(316925, 7)
Converting ndarray to lists...
(316925, 7)
(316925, 7)
0/316925
0/316925
(316925, 7)
Converting ndarray to lists...
(316925, 7)
(316925, 7)
(316925, 7)
Loading and preparing results...
0/316925
(316925, 7)
0/316925
0/316925
(316925, 7)
0/316925
0/316925
0/316925
0/316925
0/316925
(316925, 7)
(316925, 7)
(316925, 7)
Converting ndarray to lists...
0/316925
0/316925
(316925, 7)
Loading and preparing results...
0/316925
0/316925
Converting ndarray to lists...
0/316925
Converting ndarray to lists...
0/316925
(316925, 7)
0/316925
0/316925
(316925, 7)
(316925, 7)
0/316925
0/316925
0/316925
0/316925
Converting ndarray to lists...
0/316925
(316925, 7)
0/316925
DONE (t=2.13s)
creating index...
DONE (t=2.14s)
DONE (t=2.14s)
creating index...
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
DONE (t=2.15s)
creating index...
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
DONE (t=2.17s)
creating index...
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.24s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=3.21s).
Accumulating evaluation results...
DONE (t=3.25s).
Accumulating evaluation results...
DONE (t=3.23s).
Accumulating evaluation results...
DONE (t=3.22s).
Accumulating evaluation results...
DONE (t=3.24s).
Accumulating evaluation results...
DONE (t=3.24s).
Accumulating evaluation results...
DONE (t=3.25s).
Accumulating evaluation results...
DONE (t=3.25s).
Accumulating evaluation results...
DONE (t=1.12s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.254
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.121
DONE (t=1.13s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.150
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.218
DONE (t=1.12s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.254
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.229
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.239
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.342
Current AP: 0.12934 AP goal: 0.21200
DONE (t=1.14s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.121
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.129
DONE (t=1.14s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.254
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.254
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.121
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.129
DONE (t=1.15s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.150
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.121
DONE (t=1.15s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.229
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.239
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.342
Current AP: 0.12934 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.254
DONE (t=1.14s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.121
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.129
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.129
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.254
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.254
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.229
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.239
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.342
Current AP: 0.12934 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.121
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.254
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.229
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.239
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.342
Current AP: 0.12934 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.121
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.121
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.150
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.229
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.239
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.342
Current AP: 0.12934 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.150
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.150
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.229
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.239
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.342
Current AP: 0.12934 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.197
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.229
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.239
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.342
Current AP: 0.12934 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.150
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.218
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.229
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.239
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.342
Current AP: 0.12934 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757600.018326759 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757600.018948078 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.1293404925916015}

:::MLPv0.5.0 ssd 1541757600.019403696 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.1293404925916015}

:::MLPv0.5.0 ssd 1541757600.019843578 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757600.020300150 (train.py:338) eval_stop: 32
Iteration:   1880, Loss function: 4.413, Average Loss: 4.273, avg. samples / sec: 2661.16
Iteration:   1880, Loss function: 4.471, Average Loss: 4.271, avg. samples / sec: 2661.17
Iteration:   1880, Loss function: 4.007, Average Loss: 4.264, avg. samples / sec: 2661.50
Iteration:   1880, Loss function: 4.274, Average Loss: 4.273, avg. samples / sec: 2661.03
Iteration:   1880, Loss function: 4.203, Average Loss: 4.273, avg. samples / sec: 2661.29
Iteration:   1880, Loss function: 4.253, Average Loss: 4.264, avg. samples / sec: 2660.84
Iteration:   1880, Loss function: 4.053, Average Loss: 4.276, avg. samples / sec: 2660.95
Iteration:   1880, Loss function: 5.048, Average Loss: 4.273, avg. samples / sec: 2660.94
Iteration:   1900, Loss function: 4.314, Average Loss: 4.261, avg. samples / sec: 24528.65
Iteration:   1900, Loss function: 4.301, Average Loss: 4.270, avg. samples / sec: 24511.25
Iteration:   1900, Loss function: 3.504, Average Loss: 4.269, avg. samples / sec: 24494.95
Iteration:   1900, Loss function: 3.958, Average Loss: 4.274, avg. samples / sec: 24519.85
Iteration:   1900, Loss function: 3.707, Average Loss: 4.270, avg. samples / sec: 24484.53
Iteration:   1900, Loss function: 4.269, Average Loss: 4.270, avg. samples / sec: 24496.97
Iteration:   1900, Loss function: 4.294, Average Loss: 4.270, avg. samples / sec: 24441.08
Iteration:   1900, Loss function: 4.022, Average Loss: 4.263, avg. samples / sec: 24447.75

:::MLPv0.5.0 ssd 1541757603.182087183 (train.py:553) train_epoch: 33
Iteration:   1920, Loss function: 4.351, Average Loss: 4.269, avg. samples / sec: 24498.02
Iteration:   1920, Loss function: 3.940, Average Loss: 4.272, avg. samples / sec: 24497.50
Iteration:   1920, Loss function: 3.858, Average Loss: 4.261, avg. samples / sec: 24547.51
Iteration:   1920, Loss function: 4.573, Average Loss: 4.269, avg. samples / sec: 24489.87
Iteration:   1920, Loss function: 4.583, Average Loss: 4.270, avg. samples / sec: 24519.41
Iteration:   1920, Loss function: 4.773, Average Loss: 4.269, avg. samples / sec: 24522.57
Iteration:   1920, Loss function: 4.422, Average Loss: 4.271, avg. samples / sec: 24492.81
Iteration:   1920, Loss function: 4.691, Average Loss: 4.261, avg. samples / sec: 24434.14
Iteration:   1940, Loss function: 4.224, Average Loss: 4.267, avg. samples / sec: 21109.84
Iteration:   1940, Loss function: 3.820, Average Loss: 4.267, avg. samples / sec: 21123.47
Iteration:   1940, Loss function: 4.458, Average Loss: 4.268, avg. samples / sec: 21102.22
Iteration:   1940, Loss function: 3.507, Average Loss: 4.266, avg. samples / sec: 21107.78
Iteration:   1940, Loss function: 4.386, Average Loss: 4.268, avg. samples / sec: 21103.41
Iteration:   1940, Loss function: 4.473, Average Loss: 4.259, avg. samples / sec: 21065.04
Iteration:   1940, Loss function: 4.041, Average Loss: 4.267, avg. samples / sec: 21060.36
Iteration:   1940, Loss function: 4.338, Average Loss: 4.260, avg. samples / sec: 21101.39
Iteration:   1960, Loss function: 4.479, Average Loss: 4.266, avg. samples / sec: 24554.41
Iteration:   1960, Loss function: 4.097, Average Loss: 4.264, avg. samples / sec: 24555.86
Iteration:   1960, Loss function: 4.786, Average Loss: 4.266, avg. samples / sec: 24546.43
Iteration:   1960, Loss function: 3.601, Average Loss: 4.253, avg. samples / sec: 24602.81
Iteration:   1960, Loss function: 4.130, Average Loss: 4.264, avg. samples / sec: 24598.26
Iteration:   1960, Loss function: 4.207, Average Loss: 4.262, avg. samples / sec: 24546.47
Iteration:   1960, Loss function: 4.167, Average Loss: 4.265, avg. samples / sec: 24558.00
Iteration:   1960, Loss function: 3.878, Average Loss: 4.254, avg. samples / sec: 24568.83

:::MLPv0.5.0 ssd 1541757608.296120882 (train.py:553) train_epoch: 34
Iteration:   1980, Loss function: 3.937, Average Loss: 4.261, avg. samples / sec: 24551.57
Iteration:   1980, Loss function: 4.518, Average Loss: 4.249, avg. samples / sec: 24585.20
Iteration:   1980, Loss function: 3.780, Average Loss: 4.258, avg. samples / sec: 24548.06
Iteration:   1980, Loss function: 4.166, Average Loss: 4.251, avg. samples / sec: 24526.82
Iteration:   1980, Loss function: 4.019, Average Loss: 4.263, avg. samples / sec: 24547.06
Iteration:   1980, Loss function: 4.342, Average Loss: 4.265, avg. samples / sec: 24513.23
Iteration:   1980, Loss function: 4.040, Average Loss: 4.264, avg. samples / sec: 24507.74
Iteration:   1980, Loss function: 3.910, Average Loss: 4.261, avg. samples / sec: 24485.63
Iteration:   2000, Loss function: 4.611, Average Loss: 4.264, avg. samples / sec: 24530.92
Iteration:   2000, Loss function: 4.193, Average Loss: 4.265, avg. samples / sec: 24550.99
Iteration:   2000, Loss function: 4.409, Average Loss: 4.262, avg. samples / sec: 24585.11
Iteration:   2000, Loss function: 3.978, Average Loss: 4.253, avg. samples / sec: 24546.24
Iteration:   2000, Loss function: 4.094, Average Loss: 4.258, avg. samples / sec: 24524.89
Iteration:   2000, Loss function: 3.569, Average Loss: 4.267, avg. samples / sec: 24534.27
Iteration:   2000, Loss function: 4.292, Average Loss: 4.265, avg. samples / sec: 24512.12
Iteration:   2000, Loss function: 4.168, Average Loss: 4.249, avg. samples / sec: 24470.28
Iteration:   2020, Loss function: 4.217, Average Loss: 4.264, avg. samples / sec: 24528.72
Iteration:   2020, Loss function: 4.114, Average Loss: 4.260, avg. samples / sec: 24533.68
Iteration:   2020, Loss function: 3.753, Average Loss: 4.257, avg. samples / sec: 24534.24
Iteration:   2020, Loss function: 3.387, Average Loss: 4.263, avg. samples / sec: 24505.34
Iteration:   2020, Loss function: 4.223, Average Loss: 4.264, avg. samples / sec: 24539.75
Iteration:   2020, Loss function: 3.763, Average Loss: 4.247, avg. samples / sec: 24542.80
Iteration:   2020, Loss function: 3.916, Average Loss: 4.251, avg. samples / sec: 24484.77
Iteration:   2020, Loss function: 3.668, Average Loss: 4.263, avg. samples / sec: 24490.77

:::MLPv0.5.0 ssd 1541757613.138071060 (train.py:553) train_epoch: 35
Iteration:   2040, Loss function: 4.240, Average Loss: 4.263, avg. samples / sec: 24491.17
Iteration:   2040, Loss function: 3.699, Average Loss: 4.258, avg. samples / sec: 24554.34
Iteration:   2040, Loss function: 4.107, Average Loss: 4.258, avg. samples / sec: 24488.91
Iteration:   2040, Loss function: 3.642, Average Loss: 4.246, avg. samples / sec: 24538.57
Iteration:   2040, Loss function: 4.261, Average Loss: 4.253, avg. samples / sec: 24469.03
Iteration:   2040, Loss function: 3.963, Average Loss: 4.261, avg. samples / sec: 24499.81
Iteration:   2040, Loss function: 4.139, Average Loss: 4.242, avg. samples / sec: 24479.68
Iteration:   2040, Loss function: 3.901, Average Loss: 4.261, avg. samples / sec: 24458.00
Iteration:   2060, Loss function: 3.970, Average Loss: 4.261, avg. samples / sec: 24513.98
Iteration:   2060, Loss function: 4.681, Average Loss: 4.258, avg. samples / sec: 24572.55
Iteration:   2060, Loss function: 4.209, Average Loss: 4.258, avg. samples / sec: 24509.46
Iteration:   2060, Loss function: 4.395, Average Loss: 4.244, avg. samples / sec: 24498.44
Iteration:   2060, Loss function: 4.161, Average Loss: 4.260, avg. samples / sec: 24526.69
Iteration:   2060, Loss function: 3.858, Average Loss: 4.256, avg. samples / sec: 24481.82
Iteration:   2060, Loss function: 3.911, Average Loss: 4.237, avg. samples / sec: 24521.41
Iteration:   2060, Loss function: 3.571, Average Loss: 4.251, avg. samples / sec: 24479.20

:::MLPv0.5.0 ssd 1541757617.987295628 (train.py:553) train_epoch: 36
Iteration:   2080, Loss function: 3.372, Average Loss: 4.259, avg. samples / sec: 24474.85
Iteration:   2080, Loss function: 3.929, Average Loss: 4.248, avg. samples / sec: 24532.31
Iteration:   2080, Loss function: 4.158, Average Loss: 4.253, avg. samples / sec: 24507.62
Iteration:   2080, Loss function: 3.782, Average Loss: 4.240, avg. samples / sec: 24481.24
Iteration:   2080, Loss function: 3.960, Average Loss: 4.255, avg. samples / sec: 24452.71
Iteration:   2080, Loss function: 4.248, Average Loss: 4.256, avg. samples / sec: 24434.54
Iteration:   2080, Loss function: 4.000, Average Loss: 4.256, avg. samples / sec: 24454.34
Iteration:   2080, Loss function: 4.071, Average Loss: 4.234, avg. samples / sec: 24475.25
Iteration:   2100, Loss function: 4.179, Average Loss: 4.256, avg. samples / sec: 24559.19
Iteration:   2100, Loss function: 3.641, Average Loss: 4.250, avg. samples / sec: 24556.66
Iteration:   2100, Loss function: 4.587, Average Loss: 4.238, avg. samples / sec: 24559.15
Iteration:   2100, Loss function: 4.238, Average Loss: 4.254, avg. samples / sec: 24561.24
Iteration:   2100, Loss function: 4.316, Average Loss: 4.244, avg. samples / sec: 24505.46
Iteration:   2100, Loss function: 4.102, Average Loss: 4.232, avg. samples / sec: 24549.68
Iteration:   2100, Loss function: 4.097, Average Loss: 4.256, avg. samples / sec: 24537.77
Iteration:   2100, Loss function: 4.440, Average Loss: 4.251, avg. samples / sec: 24520.83
Iteration:   2120, Loss function: 3.829, Average Loss: 4.252, avg. samples / sec: 24497.64
Iteration:   2120, Loss function: 4.061, Average Loss: 4.234, avg. samples / sec: 24502.92
Iteration:   2120, Loss function: 4.095, Average Loss: 4.253, avg. samples / sec: 24524.82
Iteration:   2120, Loss function: 3.498, Average Loss: 4.245, avg. samples / sec: 24471.94
Iteration:   2120, Loss function: 3.600, Average Loss: 4.250, avg. samples / sec: 24518.30
Iteration:   2120, Loss function: 4.142, Average Loss: 4.244, avg. samples / sec: 24503.21
Iteration:   2120, Loss function: 3.977, Average Loss: 4.231, avg. samples / sec: 24494.49
Iteration:   2120, Loss function: 3.965, Average Loss: 4.254, avg. samples / sec: 24490.78

:::MLPv0.5.0 ssd 1541757623.327039003 (train.py:553) train_epoch: 37
Iteration:   2140, Loss function: 3.977, Average Loss: 4.248, avg. samples / sec: 18902.18
Iteration:   2140, Loss function: 3.923, Average Loss: 4.247, avg. samples / sec: 18944.38
Iteration:   2140, Loss function: 4.634, Average Loss: 4.245, avg. samples / sec: 18926.86
Iteration:   2140, Loss function: 4.218, Average Loss: 4.242, avg. samples / sec: 18913.95
Iteration:   2140, Loss function: 4.743, Average Loss: 4.247, avg. samples / sec: 18885.02
Iteration:   2140, Loss function: 4.041, Average Loss: 4.225, avg. samples / sec: 18909.25
Iteration:   2140, Loss function: 3.556, Average Loss: 4.230, avg. samples / sec: 18874.99
Iteration:   2140, Loss function: 4.338, Average Loss: 4.242, avg. samples / sec: 18892.18
Iteration:   2160, Loss function: 3.880, Average Loss: 4.244, avg. samples / sec: 24530.53
Iteration:   2160, Loss function: 3.800, Average Loss: 4.241, avg. samples / sec: 24535.37
Iteration:   2160, Loss function: 4.034, Average Loss: 4.239, avg. samples / sec: 24590.49
Iteration:   2160, Loss function: 4.332, Average Loss: 4.235, avg. samples / sec: 24520.63
Iteration:   2160, Loss function: 3.909, Average Loss: 4.243, avg. samples / sec: 24545.92
Iteration:   2160, Loss function: 4.132, Average Loss: 4.226, avg. samples / sec: 24543.23
Iteration:   2160, Loss function: 4.702, Average Loss: 4.223, avg. samples / sec: 24528.32
Iteration:   2160, Loss function: 4.027, Average Loss: 4.241, avg. samples / sec: 24478.33
Iteration:   2180, Loss function: 4.141, Average Loss: 4.241, avg. samples / sec: 24531.03
Iteration:   2180, Loss function: 4.732, Average Loss: 4.238, avg. samples / sec: 24507.00
Iteration:   2180, Loss function: 3.994, Average Loss: 4.241, avg. samples / sec: 24485.18
Iteration:   2180, Loss function: 3.332, Average Loss: 4.232, avg. samples / sec: 24504.03
Iteration:   2180, Loss function: 3.693, Average Loss: 4.221, avg. samples / sec: 24520.98
Iteration:   2180, Loss function: 4.095, Average Loss: 4.225, avg. samples / sec: 24509.27
Iteration:   2180, Loss function: 4.192, Average Loss: 4.239, avg. samples / sec: 24522.01
Iteration:   2180, Loss function: 4.059, Average Loss: 4.239, avg. samples / sec: 24467.68

:::MLPv0.5.0 ssd 1541757628.088692188 (train.py:553) train_epoch: 38
Iteration:   2200, Loss function: 4.318, Average Loss: 4.239, avg. samples / sec: 24515.88
Iteration:   2200, Loss function: 4.307, Average Loss: 4.239, avg. samples / sec: 24550.31
Iteration:   2200, Loss function: 4.371, Average Loss: 4.225, avg. samples / sec: 24542.15
Iteration:   2200, Loss function: 3.937, Average Loss: 4.238, avg. samples / sec: 24499.69
Iteration:   2200, Loss function: 4.099, Average Loss: 4.232, avg. samples / sec: 24502.69
Iteration:   2200, Loss function: 4.339, Average Loss: 4.236, avg. samples / sec: 24498.38
Iteration:   2200, Loss function: 3.257, Average Loss: 4.240, avg. samples / sec: 24476.19
Iteration:   2200, Loss function: 4.421, Average Loss: 4.222, avg. samples / sec: 24488.41
Iteration:   2220, Loss function: 4.435, Average Loss: 4.236, avg. samples / sec: 24390.65
Iteration:   2220, Loss function: 3.558, Average Loss: 4.237, avg. samples / sec: 24387.13
Iteration:   2220, Loss function: 3.593, Average Loss: 4.233, avg. samples / sec: 24432.97
Iteration:   2220, Loss function: 3.891, Average Loss: 4.237, avg. samples / sec: 24426.19
Iteration:   2220, Loss function: 4.165, Average Loss: 4.235, avg. samples / sec: 24377.47
Iteration:   2220, Loss function: 4.502, Average Loss: 4.228, avg. samples / sec: 24389.99
Iteration:   2220, Loss function: 3.989, Average Loss: 4.219, avg. samples / sec: 24411.25
Iteration:   2220, Loss function: 4.108, Average Loss: 4.221, avg. samples / sec: 24339.16
Iteration:   2240, Loss function: 3.811, Average Loss: 4.235, avg. samples / sec: 24553.47
Iteration:   2240, Loss function: 4.115, Average Loss: 4.230, avg. samples / sec: 24548.12
Iteration:   2240, Loss function: 4.416, Average Loss: 4.233, avg. samples / sec: 24524.22
Iteration:   2240, Loss function: 4.301, Average Loss: 4.226, avg. samples / sec: 24536.11
Iteration:   2240, Loss function: 3.370, Average Loss: 4.231, avg. samples / sec: 24502.95
Iteration:   2240, Loss function: 4.058, Average Loss: 4.216, avg. samples / sec: 24556.39
Iteration:   2240, Loss function: 4.204, Average Loss: 4.218, avg. samples / sec: 24532.33
Iteration:   2240, Loss function: 3.820, Average Loss: 4.228, avg. samples / sec: 24502.31

:::MLPv0.5.0 ssd 1541757632.943763733 (train.py:553) train_epoch: 39
Iteration:   2260, Loss function: 4.454, Average Loss: 4.226, avg. samples / sec: 24507.76
Iteration:   2260, Loss function: 4.330, Average Loss: 4.213, avg. samples / sec: 24510.59
Iteration:   2260, Loss function: 3.826, Average Loss: 4.226, avg. samples / sec: 24504.01
Iteration:   2260, Loss function: 3.713, Average Loss: 4.232, avg. samples / sec: 24445.42
Iteration:   2260, Loss function: 4.125, Average Loss: 4.225, avg. samples / sec: 24456.22
Iteration:   2260, Loss function: 4.185, Average Loss: 4.215, avg. samples / sec: 24465.17
Iteration:   2260, Loss function: 4.473, Average Loss: 4.223, avg. samples / sec: 24457.19
Iteration:   2260, Loss function: 4.090, Average Loss: 4.229, avg. samples / sec: 24441.96
Iteration:   2280, Loss function: 4.212, Average Loss: 4.213, avg. samples / sec: 24520.33
Iteration:   2280, Loss function: 4.653, Average Loss: 4.222, avg. samples / sec: 24511.07
Iteration:   2280, Loss function: 3.800, Average Loss: 4.221, avg. samples / sec: 24510.84
Iteration:   2280, Loss function: 3.796, Average Loss: 4.225, avg. samples / sec: 24542.08
Iteration:   2280, Loss function: 3.693, Average Loss: 4.228, avg. samples / sec: 24517.78
Iteration:   2280, Loss function: 3.949, Average Loss: 4.211, avg. samples / sec: 24507.31
Iteration:   2280, Loss function: 3.955, Average Loss: 4.222, avg. samples / sec: 24487.21
Iteration:   2280, Loss function: 4.339, Average Loss: 4.220, avg. samples / sec: 24505.23
Iteration:   2300, Loss function: 4.168, Average Loss: 4.208, avg. samples / sec: 24568.28
Iteration:   2300, Loss function: 3.758, Average Loss: 4.218, avg. samples / sec: 24521.82
Iteration:   2300, Loss function: 4.179, Average Loss: 4.210, avg. samples / sec: 24487.91
Iteration:   2300, Loss function: 4.225, Average Loss: 4.223, avg. samples / sec: 24507.11
Iteration:   2300, Loss function: 3.748, Average Loss: 4.218, avg. samples / sec: 24494.11
Iteration:   2300, Loss function: 3.504, Average Loss: 4.216, avg. samples / sec: 24530.84
Iteration:   2300, Loss function: 4.457, Average Loss: 4.216, avg. samples / sec: 24524.97
Iteration:   2300, Loss function: 4.437, Average Loss: 4.221, avg. samples / sec: 24474.33

:::MLPv0.5.0 ssd 1541757637.789567709 (train.py:553) train_epoch: 40
Iteration:   2320, Loss function: 3.682, Average Loss: 4.214, avg. samples / sec: 24580.98
Iteration:   2320, Loss function: 3.723, Average Loss: 4.205, avg. samples / sec: 24528.20
Iteration:   2320, Loss function: 4.151, Average Loss: 4.215, avg. samples / sec: 24557.59
Iteration:   2320, Loss function: 4.078, Average Loss: 4.216, avg. samples / sec: 24521.90
Iteration:   2320, Loss function: 4.161, Average Loss: 4.205, avg. samples / sec: 24531.88
Iteration:   2320, Loss function: 4.061, Average Loss: 4.217, avg. samples / sec: 24562.04
Iteration:   2320, Loss function: 3.938, Average Loss: 4.216, avg. samples / sec: 24532.24
Iteration:   2320, Loss function: 3.959, Average Loss: 4.221, avg. samples / sec: 24516.33
Iteration:   2340, Loss function: 4.106, Average Loss: 4.219, avg. samples / sec: 24603.71
Iteration:   2340, Loss function: 4.251, Average Loss: 4.217, avg. samples / sec: 24586.11
Iteration:   2340, Loss function: 4.097, Average Loss: 4.211, avg. samples / sec: 24555.60
Iteration:   2340, Loss function: 4.397, Average Loss: 4.213, avg. samples / sec: 24532.67
Iteration:   2340, Loss function: 4.804, Average Loss: 4.205, avg. samples / sec: 24553.69
Iteration:   2340, Loss function: 3.850, Average Loss: 4.213, avg. samples / sec: 24530.01
Iteration:   2340, Loss function: 4.420, Average Loss: 4.212, avg. samples / sec: 24552.92
Iteration:   2340, Loss function: 4.754, Average Loss: 4.205, avg. samples / sec: 24504.76
Iteration:   2360, Loss function: 3.725, Average Loss: 4.206, avg. samples / sec: 24568.03
Iteration:   2360, Loss function: 3.874, Average Loss: 4.216, avg. samples / sec: 24515.75
Iteration:   2360, Loss function: 4.020, Average Loss: 4.207, avg. samples / sec: 24546.90
Iteration:   2360, Loss function: 4.045, Average Loss: 4.201, avg. samples / sec: 24519.65
Iteration:   2360, Loss function: 3.923, Average Loss: 4.208, avg. samples / sec: 24499.73
Iteration:   2360, Loss function: 5.026, Average Loss: 4.202, avg. samples / sec: 24526.83
Iteration:   2360, Loss function: 3.878, Average Loss: 4.211, avg. samples / sec: 24493.09
Iteration:   2360, Loss function: 3.999, Average Loss: 4.212, avg. samples / sec: 24475.43

:::MLPv0.5.0 ssd 1541757642.632128239 (train.py:553) train_epoch: 41
Iteration:   2380, Loss function: 3.927, Average Loss: 4.202, avg. samples / sec: 24558.76
Iteration:   2380, Loss function: 4.710, Average Loss: 4.206, avg. samples / sec: 24541.95
Iteration:   2380, Loss function: 3.965, Average Loss: 4.212, avg. samples / sec: 24567.07
Iteration:   2380, Loss function: 4.474, Average Loss: 4.215, avg. samples / sec: 24524.46
Iteration:   2380, Loss function: 4.237, Average Loss: 4.211, avg. samples / sec: 24558.66
Iteration:   2380, Loss function: 4.855, Average Loss: 4.200, avg. samples / sec: 24553.01
Iteration:   2380, Loss function: 4.824, Average Loss: 4.208, avg. samples / sec: 24495.22
Iteration:   2380, Loss function: 3.966, Average Loss: 4.206, avg. samples / sec: 24522.34
Iteration:   2400, Loss function: 4.272, Average Loss: 4.205, avg. samples / sec: 24497.41
Iteration:   2400, Loss function: 4.791, Average Loss: 4.210, avg. samples / sec: 24512.84
Iteration:   2400, Loss function: 3.865, Average Loss: 4.212, avg. samples / sec: 24505.05
Iteration:   2400, Loss function: 4.417, Average Loss: 4.206, avg. samples / sec: 24519.13
Iteration:   2400, Loss function: 4.135, Average Loss: 4.198, avg. samples / sec: 24477.77
Iteration:   2400, Loss function: 4.356, Average Loss: 4.208, avg. samples / sec: 24468.13
Iteration:   2400, Loss function: 4.924, Average Loss: 4.208, avg. samples / sec: 24481.10
Iteration:   2400, Loss function: 4.193, Average Loss: 4.199, avg. samples / sec: 24437.69
Iteration:   2420, Loss function: 4.975, Average Loss: 4.211, avg. samples / sec: 24559.85
Iteration:   2420, Loss function: 4.273, Average Loss: 4.199, avg. samples / sec: 24606.01
Iteration:   2420, Loss function: 4.432, Average Loss: 4.211, avg. samples / sec: 24553.43
Iteration:   2420, Loss function: 3.811, Average Loss: 4.203, avg. samples / sec: 24554.27
Iteration:   2420, Loss function: 3.961, Average Loss: 4.206, avg. samples / sec: 24594.06
Iteration:   2420, Loss function: 3.797, Average Loss: 4.202, avg. samples / sec: 24534.98
Iteration:   2420, Loss function: 4.040, Average Loss: 4.197, avg. samples / sec: 24542.38
Iteration:   2420, Loss function: 4.137, Average Loss: 4.206, avg. samples / sec: 24537.69

:::MLPv0.5.0 ssd 1541757647.391945362 (train.py:553) train_epoch: 42
Iteration:   2440, Loss function: 3.412, Average Loss: 4.204, avg. samples / sec: 24558.06
Iteration:   2440, Loss function: 4.105, Average Loss: 4.201, avg. samples / sec: 24508.81
Iteration:   2440, Loss function: 3.868, Average Loss: 4.209, avg. samples / sec: 24496.34
Iteration:   2440, Loss function: 4.382, Average Loss: 4.199, avg. samples / sec: 24502.98
Iteration:   2440, Loss function: 4.397, Average Loss: 4.204, avg. samples / sec: 24494.61
Iteration:   2440, Loss function: 3.752, Average Loss: 4.208, avg. samples / sec: 24467.25
Iteration:   2440, Loss function: 4.312, Average Loss: 4.196, avg. samples / sec: 24447.95
Iteration:   2440, Loss function: 3.969, Average Loss: 4.193, avg. samples / sec: 24488.97
Iteration:   2460, Loss function: 3.885, Average Loss: 4.191, avg. samples / sec: 24535.73
Iteration:   2460, Loss function: 4.445, Average Loss: 4.201, avg. samples / sec: 24468.62
Iteration:   2460, Loss function: 3.859, Average Loss: 4.196, avg. samples / sec: 24464.20
Iteration:   2460, Loss function: 3.674, Average Loss: 4.185, avg. samples / sec: 24524.88
Iteration:   2460, Loss function: 4.037, Average Loss: 4.201, avg. samples / sec: 24488.56
Iteration:   2460, Loss function: 3.587, Average Loss: 4.205, avg. samples / sec: 24463.31
Iteration:   2460, Loss function: 3.807, Average Loss: 4.196, avg. samples / sec: 24455.71
Iteration:   2460, Loss function: 3.596, Average Loss: 4.199, avg. samples / sec: 24449.78
Iteration:   2480, Loss function: 3.852, Average Loss: 4.201, avg. samples / sec: 24533.30
Iteration:   2480, Loss function: 4.014, Average Loss: 4.193, avg. samples / sec: 24526.64
Iteration:   2480, Loss function: 3.940, Average Loss: 4.192, avg. samples / sec: 24499.88
Iteration:   2480, Loss function: 4.276, Average Loss: 4.196, avg. samples / sec: 24494.77
Iteration:   2480, Loss function: 4.211, Average Loss: 4.196, avg. samples / sec: 24521.43
Iteration:   2480, Loss function: 3.613, Average Loss: 4.185, avg. samples / sec: 24474.12
Iteration:   2480, Loss function: 3.697, Average Loss: 4.181, avg. samples / sec: 24481.50
Iteration:   2480, Loss function: 3.818, Average Loss: 4.196, avg. samples / sec: 24472.54

:::MLPv0.5.0 ssd 1541757652.241811752 (train.py:553) train_epoch: 43
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1

:::MLPv0.5.0 ssd 1541757653.584006071 (train.py:578) opt_learning_rate: 0.016
Iteration:   2500, Loss function: 4.305, Average Loss: 4.197, avg. samples / sec: 24459.48
Iteration:   2500, Loss function: 4.066, Average Loss: 4.191, avg. samples / sec: 24501.08
Iteration:   2500, Loss function: 3.937, Average Loss: 4.194, avg. samples / sec: 24490.01
Iteration:   2500, Loss function: 3.916, Average Loss: 4.190, avg. samples / sec: 24472.32
Iteration:   2500, Loss function: 3.941, Average Loss: 4.179, avg. samples / sec: 24475.09
Iteration:   2500, Loss function: 4.118, Average Loss: 4.176, avg. samples / sec: 24463.22
Iteration:   2500, Loss function: 4.039, Average Loss: 4.189, avg. samples / sec: 24430.63
Iteration:   2500, Loss function: 4.382, Average Loss: 4.193, avg. samples / sec: 24426.12

































































:::MLPv0.5.0 ssd 1541757653.667719126 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757653.668266296 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757653.668752432 (train.py:220) eval_start: 43
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.38 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.38 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.38 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.38 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.38 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.38 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.38 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.38 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(357918, 7)
Loading and preparing results...
(357918, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(357918, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(357918, 7)
Converting ndarray to lists...
(357918, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/357918
Converting ndarray to lists...
Converting ndarray to lists...
(357918, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/357918
(357918, 7)
Loading and preparing results...
0/357918
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(357918, 7)
Loading and preparing results...
(357918, 7)
Loading and preparing results...
Loading and preparing results...
(357918, 7)
Loading and preparing results...
Loading and preparing results...
(357918, 7)
0/357918
Loading and preparing results...
Converting ndarray to lists...
(357918, 7)
Loading and preparing results...
(357918, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/357918
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/357918
0/357918
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(357918, 7)
0/357918
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/357918
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(357918, 7)
Loading and preparing results...
0/357918
(357918, 7)
0/357918
Converting ndarray to lists...
0/357918
Converting ndarray to lists...
Loading and preparing results...
0/357918
Converting ndarray to lists...
Loading and preparing results...
(357918, 7)
Loading and preparing results...
(357918, 7)
Converting ndarray to lists...
(357918, 7)
Converting ndarray to lists...
Loading and preparing results...
(357918, 7)
(357918, 7)
(357918, 7)
(357918, 7)
Loading and preparing results...
(357918, 7)
(357918, 7)
Loading and preparing results...
0/357918
(357918, 7)
(357918, 7)
Converting ndarray to lists...
(357918, 7)
(357918, 7)
Loading and preparing results...
0/357918
(357918, 7)
0/357918
0/357918
0/357918
(357918, 7)
0/357918
(357918, 7)
(357918, 7)
Loading and preparing results...
Converting ndarray to lists...
(357918, 7)
0/357918
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(357918, 7)
0/357918
Converting ndarray to lists...
(357918, 7)
Loading and preparing results...
0/357918
0/357918
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/357918
(357918, 7)
(357918, 7)
Converting ndarray to lists...
(357918, 7)
0/357918
0/357918
0/357918
(357918, 7)
0/357918
Converting ndarray to lists...
(357918, 7)
Converting ndarray to lists...
0/357918
0/357918
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(357918, 7)
0/357918
Converting ndarray to lists...
0/357918
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/357918
0/357918
(357918, 7)
0/357918
Loading and preparing results...
(357918, 7)
(357918, 7)
(357918, 7)
Loading and preparing results...
0/357918
(357918, 7)
Loading and preparing results...
0/357918
(357918, 7)
0/357918
Converting ndarray to lists...
0/357918
0/357918
Converting ndarray to lists...
Converting ndarray to lists...
(357918, 7)
(357918, 7)
(357918, 7)
0/357918
0/357918
0/357918
0/357918
0/357918
(357918, 7)
(357918, 7)
0/357918
0/357918
Converting ndarray to lists...
Converting ndarray to lists...
(357918, 7)
(357918, 7)
(357918, 7)
0/357918
0/357918
(357918, 7)
Loading and preparing results...
Converting ndarray to lists...
0/357918
Loading and preparing results...
0/357918
0/357918
0/357918
Converting ndarray to lists...
(357918, 7)
0/357918
0/357918
0/357918
(357918, 7)
(357918, 7)
0/357918
(357918, 7)
Converting ndarray to lists...
0/357918
(357918, 7)
0/357918
0/357918
Loading and preparing results...
0/357918
0/357918
Converting ndarray to lists...
(357918, 7)
0/357918
Converting ndarray to lists...
(357918, 7)
0/357918
DONE (t=1.85s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
DONE (t=1.90s)
creating index...
creating index...
DONE (t=1.91s)
creating index...
DONE (t=1.91s)
creating index...
DONE (t=1.92s)
creating index...
DONE (t=1.92s)
creating index...
DONE (t=1.92s)
creating index...
DONE (t=1.92s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.95s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.23s)
creating index...
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=3.64s).
Accumulating evaluation results...
DONE (t=3.67s).
Accumulating evaluation results...
DONE (t=3.67s).
Accumulating evaluation results...
DONE (t=3.66s).
Accumulating evaluation results...
DONE (t=3.66s).
Accumulating evaluation results...
DONE (t=3.66s).
Accumulating evaluation results...
DONE (t=3.70s).
Accumulating evaluation results...
DONE (t=3.73s).
Accumulating evaluation results...
DONE (t=1.22s).
DONE (t=1.22s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
DONE (t=1.22s).
DONE (t=1.22s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
DONE (t=1.23s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
DONE (t=1.26s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.236
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14473 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.236
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14473 AP goal: 0.21200
DONE (t=1.22s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.236
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.236
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.236
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14473 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14473 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14473 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.236
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14473 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.236
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14473 AP goal: 0.21200
DONE (t=1.17s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.227
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.236
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.373
Current AP: 0.14473 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757665.483690977 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757665.484318495 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.14473382037486368}

:::MLPv0.5.0 ssd 1541757665.484800339 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.14473382037486368}

:::MLPv0.5.0 ssd 1541757665.485258341 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757665.485714197 (train.py:338) eval_stop: 43
Iteration:   2520, Loss function: 3.326, Average Loss: 4.182, avg. samples / sec: 2942.57
Iteration:   2520, Loss function: 3.962, Average Loss: 4.187, avg. samples / sec: 2942.11
Iteration:   2520, Loss function: 3.386, Average Loss: 4.171, avg. samples / sec: 2942.31
Iteration:   2520, Loss function: 3.767, Average Loss: 4.182, avg. samples / sec: 2942.17
Iteration:   2520, Loss function: 3.467, Average Loss: 4.167, avg. samples / sec: 2942.43
Iteration:   2520, Loss function: 3.719, Average Loss: 4.180, avg. samples / sec: 2942.47
Iteration:   2520, Loss function: 3.506, Average Loss: 4.184, avg. samples / sec: 2941.77
Iteration:   2520, Loss function: 3.686, Average Loss: 4.184, avg. samples / sec: 2942.54
Iteration:   2540, Loss function: 3.239, Average Loss: 4.172, avg. samples / sec: 24577.22
Iteration:   2540, Loss function: 3.417, Average Loss: 4.157, avg. samples / sec: 24553.59
Iteration:   2540, Loss function: 3.255, Average Loss: 4.174, avg. samples / sec: 24543.88
Iteration:   2540, Loss function: 3.593, Average Loss: 4.171, avg. samples / sec: 24520.18
Iteration:   2540, Loss function: 2.989, Average Loss: 4.168, avg. samples / sec: 24463.57
Iteration:   2540, Loss function: 3.190, Average Loss: 4.165, avg. samples / sec: 24514.68
Iteration:   2540, Loss function: 3.766, Average Loss: 4.155, avg. samples / sec: 24500.38
Iteration:   2540, Loss function: 3.408, Average Loss: 4.171, avg. samples / sec: 24515.49

:::MLPv0.5.0 ssd 1541757669.340318918 (train.py:553) train_epoch: 44
Iteration:   2560, Loss function: 3.332, Average Loss: 4.159, avg. samples / sec: 24538.41
Iteration:   2560, Loss function: 3.329, Average Loss: 4.158, avg. samples / sec: 24533.41
Iteration:   2560, Loss function: 3.392, Average Loss: 4.158, avg. samples / sec: 24557.12
Iteration:   2560, Loss function: 3.730, Average Loss: 4.143, avg. samples / sec: 24502.23
Iteration:   2560, Loss function: 3.459, Average Loss: 4.152, avg. samples / sec: 24557.29
Iteration:   2560, Loss function: 3.319, Average Loss: 4.149, avg. samples / sec: 24553.60
Iteration:   2560, Loss function: 3.808, Average Loss: 4.140, avg. samples / sec: 24554.45
Iteration:   2560, Loss function: 3.572, Average Loss: 4.159, avg. samples / sec: 24548.31
Iteration:   2580, Loss function: 2.987, Average Loss: 4.143, avg. samples / sec: 24499.91
Iteration:   2580, Loss function: 3.439, Average Loss: 4.144, avg. samples / sec: 24501.64
Iteration:   2580, Loss function: 3.169, Average Loss: 4.133, avg. samples / sec: 24517.57
Iteration:   2580, Loss function: 2.884, Average Loss: 4.127, avg. samples / sec: 24500.47
Iteration:   2580, Loss function: 3.004, Average Loss: 4.127, avg. samples / sec: 24504.29
Iteration:   2580, Loss function: 3.911, Average Loss: 4.145, avg. samples / sec: 24452.67
Iteration:   2580, Loss function: 3.675, Average Loss: 4.138, avg. samples / sec: 24480.88
Iteration:   2580, Loss function: 3.254, Average Loss: 4.143, avg. samples / sec: 24492.47

:::MLPv0.5.0 ssd 1541757674.183767319 (train.py:553) train_epoch: 45
Iteration:   2600, Loss function: 3.263, Average Loss: 4.128, avg. samples / sec: 24521.82
Iteration:   2600, Loss function: 3.937, Average Loss: 4.126, avg. samples / sec: 24508.40
Iteration:   2600, Loss function: 3.587, Average Loss: 4.123, avg. samples / sec: 24564.40
Iteration:   2600, Loss function: 3.179, Average Loss: 4.114, avg. samples / sec: 24523.98
Iteration:   2600, Loss function: 3.176, Average Loss: 4.118, avg. samples / sec: 24514.38
Iteration:   2600, Loss function: 3.498, Average Loss: 4.113, avg. samples / sec: 24503.43
Iteration:   2600, Loss function: 3.146, Average Loss: 4.127, avg. samples / sec: 24505.39
Iteration:   2600, Loss function: 3.519, Average Loss: 4.127, avg. samples / sec: 24520.51
Iteration:   2620, Loss function: 3.979, Average Loss: 4.112, avg. samples / sec: 24572.24
Iteration:   2620, Loss function: 2.800, Average Loss: 4.105, avg. samples / sec: 24576.67
Iteration:   2620, Loss function: 3.563, Average Loss: 4.098, avg. samples / sec: 24606.74
Iteration:   2620, Loss function: 3.636, Average Loss: 4.101, avg. samples / sec: 24565.44
Iteration:   2620, Loss function: 3.187, Average Loss: 4.102, avg. samples / sec: 24568.15
Iteration:   2620, Loss function: 3.659, Average Loss: 4.112, avg. samples / sec: 24600.04
Iteration:   2620, Loss function: 3.220, Average Loss: 4.112, avg. samples / sec: 24527.33
Iteration:   2620, Loss function: 3.150, Average Loss: 4.112, avg. samples / sec: 24569.93
Iteration:   2640, Loss function: 3.147, Average Loss: 4.086, avg. samples / sec: 24504.49
Iteration:   2640, Loss function: 3.170, Average Loss: 4.083, avg. samples / sec: 24493.65
Iteration:   2640, Loss function: 3.124, Average Loss: 4.099, avg. samples / sec: 24521.88
Iteration:   2640, Loss function: 3.311, Average Loss: 4.094, avg. samples / sec: 24468.95
Iteration:   2640, Loss function: 2.951, Average Loss: 4.086, avg. samples / sec: 24478.46
Iteration:   2640, Loss function: 3.089, Average Loss: 4.086, avg. samples / sec: 24436.83
Iteration:   2640, Loss function: 3.001, Average Loss: 4.096, avg. samples / sec: 24487.15
Iteration:   2640, Loss function: 3.138, Average Loss: 4.096, avg. samples / sec: 24450.48

:::MLPv0.5.0 ssd 1541757678.943210840 (train.py:553) train_epoch: 46
Iteration:   2660, Loss function: 3.245, Average Loss: 4.069, avg. samples / sec: 24552.64
Iteration:   2660, Loss function: 3.418, Average Loss: 4.083, avg. samples / sec: 24551.16
Iteration:   2660, Loss function: 2.844, Average Loss: 4.071, avg. samples / sec: 24541.08
Iteration:   2660, Loss function: 3.217, Average Loss: 4.081, avg. samples / sec: 24550.40
Iteration:   2660, Loss function: 3.722, Average Loss: 4.070, avg. samples / sec: 24545.48
Iteration:   2660, Loss function: 2.957, Average Loss: 4.070, avg. samples / sec: 24555.65
Iteration:   2660, Loss function: 3.452, Average Loss: 4.080, avg. samples / sec: 24567.23
Iteration:   2660, Loss function: 3.151, Average Loss: 4.080, avg. samples / sec: 24541.76
Iteration:   2680, Loss function: 3.526, Average Loss: 4.054, avg. samples / sec: 24498.38
Iteration:   2680, Loss function: 3.156, Average Loss: 4.056, avg. samples / sec: 24527.59
Iteration:   2680, Loss function: 3.814, Average Loss: 4.057, avg. samples / sec: 24503.90
Iteration:   2680, Loss function: 2.978, Average Loss: 4.053, avg. samples / sec: 24528.99
Iteration:   2680, Loss function: 3.345, Average Loss: 4.065, avg. samples / sec: 24523.88
Iteration:   2680, Loss function: 2.983, Average Loss: 4.064, avg. samples / sec: 24472.18
Iteration:   2680, Loss function: 2.967, Average Loss: 4.068, avg. samples / sec: 24443.94
Iteration:   2680, Loss function: 2.950, Average Loss: 4.065, avg. samples / sec: 24469.59
Iteration:   2700, Loss function: 3.472, Average Loss: 4.049, avg. samples / sec: 24578.52
Iteration:   2700, Loss function: 3.458, Average Loss: 4.042, avg. samples / sec: 24544.26
Iteration:   2700, Loss function: 3.177, Average Loss: 4.041, avg. samples / sec: 24539.08
Iteration:   2700, Loss function: 3.416, Average Loss: 4.040, avg. samples / sec: 24539.13
Iteration:   2700, Loss function: 3.039, Average Loss: 4.051, avg. samples / sec: 24589.05
Iteration:   2700, Loss function: 3.163, Average Loss: 4.052, avg. samples / sec: 24554.29
Iteration:   2700, Loss function: 3.204, Average Loss: 4.041, avg. samples / sec: 24519.54
Iteration:   2700, Loss function: 3.665, Average Loss: 4.051, avg. samples / sec: 24564.36

:::MLPv0.5.0 ssd 1541757683.785570383 (train.py:553) train_epoch: 47
Iteration:   2720, Loss function: 3.083, Average Loss: 4.026, avg. samples / sec: 24545.13
Iteration:   2720, Loss function: 3.930, Average Loss: 4.027, avg. samples / sec: 24565.13
Iteration:   2720, Loss function: 2.779, Average Loss: 4.026, avg. samples / sec: 24534.24
Iteration:   2720, Loss function: 3.192, Average Loss: 4.036, avg. samples / sec: 24568.44
Iteration:   2720, Loss function: 3.308, Average Loss: 4.027, avg. samples / sec: 24520.79
Iteration:   2720, Loss function: 3.043, Average Loss: 4.037, avg. samples / sec: 24517.87
Iteration:   2720, Loss function: 3.140, Average Loss: 4.035, avg. samples / sec: 24477.80
Iteration:   2720, Loss function: 3.365, Average Loss: 4.036, avg. samples / sec: 24492.02
Iteration:   2740, Loss function: 3.208, Average Loss: 4.010, avg. samples / sec: 24579.99
Iteration:   2740, Loss function: 3.351, Average Loss: 4.012, avg. samples / sec: 24570.45
Iteration:   2740, Loss function: 3.442, Average Loss: 4.019, avg. samples / sec: 24624.66
Iteration:   2740, Loss function: 3.476, Average Loss: 4.014, avg. samples / sec: 24570.60
Iteration:   2740, Loss function: 3.615, Average Loss: 4.022, avg. samples / sec: 24589.31
Iteration:   2740, Loss function: 2.884, Average Loss: 4.013, avg. samples / sec: 24529.47
Iteration:   2740, Loss function: 3.224, Average Loss: 4.020, avg. samples / sec: 24579.11
Iteration:   2740, Loss function: 3.172, Average Loss: 4.023, avg. samples / sec: 24526.65
Iteration:   2760, Loss function: 3.820, Average Loss: 4.000, avg. samples / sec: 24494.60
Iteration:   2760, Loss function: 3.457, Average Loss: 3.996, avg. samples / sec: 24492.90
Iteration:   2760, Loss function: 3.582, Average Loss: 4.005, avg. samples / sec: 24497.30
Iteration:   2760, Loss function: 3.101, Average Loss: 4.006, avg. samples / sec: 24488.54
Iteration:   2760, Loss function: 3.821, Average Loss: 4.002, avg. samples / sec: 24487.50
Iteration:   2760, Loss function: 3.048, Average Loss: 4.006, avg. samples / sec: 24507.53
Iteration:   2760, Loss function: 2.892, Average Loss: 4.008, avg. samples / sec: 24503.71
Iteration:   2760, Loss function: 3.462, Average Loss: 3.999, avg. samples / sec: 24482.44

:::MLPv0.5.0 ssd 1541757688.630357981 (train.py:553) train_epoch: 48
Iteration:   2780, Loss function: 3.102, Average Loss: 3.980, avg. samples / sec: 24513.79
Iteration:   2780, Loss function: 2.806, Average Loss: 3.990, avg. samples / sec: 24547.12
Iteration:   2780, Loss function: 3.480, Average Loss: 3.986, avg. samples / sec: 24508.98
Iteration:   2780, Loss function: 3.160, Average Loss: 3.993, avg. samples / sec: 24561.81
Iteration:   2780, Loss function: 3.470, Average Loss: 3.991, avg. samples / sec: 24509.17
Iteration:   2780, Loss function: 3.306, Average Loss: 3.990, avg. samples / sec: 24532.55
Iteration:   2780, Loss function: 3.444, Average Loss: 3.991, avg. samples / sec: 24513.31
Iteration:   2780, Loss function: 3.626, Average Loss: 3.985, avg. samples / sec: 24535.27
Iteration:   2800, Loss function: 3.412, Average Loss: 3.965, avg. samples / sec: 24505.14
Iteration:   2800, Loss function: 3.536, Average Loss: 3.972, avg. samples / sec: 24506.42
Iteration:   2800, Loss function: 3.286, Average Loss: 3.975, avg. samples / sec: 24500.26
Iteration:   2800, Loss function: 3.284, Average Loss: 3.976, avg. samples / sec: 24493.30
Iteration:   2800, Loss function: 3.549, Average Loss: 3.977, avg. samples / sec: 24507.38
Iteration:   2800, Loss function: 3.389, Average Loss: 3.973, avg. samples / sec: 24488.21
Iteration:   2800, Loss function: 3.658, Average Loss: 3.970, avg. samples / sec: 24487.93
Iteration:   2800, Loss function: 3.954, Average Loss: 3.978, avg. samples / sec: 24444.58

































































:::MLPv0.5.0 ssd 1541757691.976219177 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757691.976779461 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757691.977250338 (train.py:220) eval_start: 48
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.31 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.31 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.31 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.31 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.31 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.31 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.31 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.31 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(318810, 7)
Converting ndarray to lists...
Loading and preparing results...
(318810, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/318810
0/318810
Converting ndarray to lists...
(318810, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(318810, 7)
Converting ndarray to lists...
(318810, 7)
(318810, 7)
Loading and preparing results...
(318810, 7)
0/318810
(318810, 7)
(318810, 7)
(318810, 7)
Loading and preparing results...
(318810, 7)
Loading and preparing results...
(318810, 7)
0/318810
Converting ndarray to lists...
(318810, 7)
Converting ndarray to lists...
(318810, 7)
Converting ndarray to lists...
Loading and preparing results...
0/318810
0/318810
(318810, 7)
0/318810
Loading and preparing results...
0/318810
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/318810
0/318810
Loading and preparing results...
Loading and preparing results...
0/318810
Converting ndarray to lists...
0/318810
(318810, 7)
Loading and preparing results...
Converting ndarray to lists...
(318810, 7)
Loading and preparing results...
Converting ndarray to lists...
(318810, 7)
Converting ndarray to lists...
0/318810
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/318810
Loading and preparing results...
Loading and preparing results...
0/318810
0/318810
Loading and preparing results...
(318810, 7)
0/318810
(318810, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(318810, 7)
Converting ndarray to lists...
Loading and preparing results...
(318810, 7)
Loading and preparing results...
Loading and preparing results...
(318810, 7)
(318810, 7)
(318810, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/318810
0/318810
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/318810
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(318810, 7)
Loading and preparing results...
0/318810
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(318810, 7)
Loading and preparing results...
0/318810
0/318810
Loading and preparing results...
(318810, 7)
0/318810
0/318810
0/318810
(318810, 7)
(318810, 7)
(318810, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(318810, 7)
0/318810
Converting ndarray to lists...
(318810, 7)
(318810, 7)
(318810, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(318810, 7)
(318810, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/318810
Loading and preparing results...
(318810, 7)
(318810, 7)
0/318810
(318810, 7)
(318810, 7)
(318810, 7)
0/318810
(318810, 7)
Converting ndarray to lists...
0/318810
(318810, 7)
(318810, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/318810
0/318810
Loading and preparing results...
Loading and preparing results...
0/318810
0/318810
(318810, 7)
0/318810
0/318810
Converting ndarray to lists...
0/318810
(318810, 7)
Converting ndarray to lists...
(318810, 7)
(318810, 7)
Converting ndarray to lists...
0/318810
0/318810
Converting ndarray to lists...
0/318810
Loading and preparing results...
(318810, 7)
0/318810
Converting ndarray to lists...
(318810, 7)
Converting ndarray to lists...
0/318810
0/318810
0/318810
Loading and preparing results...
0/318810
(318810, 7)
(318810, 7)
0/318810
Converting ndarray to lists...
0/318810
0/318810
0/318810
Loading and preparing results...
(318810, 7)
Converting ndarray to lists...
(318810, 7)
0/318810
Converting ndarray to lists...
(318810, 7)
0/318810
(318810, 7)
Converting ndarray to lists...
0/318810
Converting ndarray to lists...
0/318810
(318810, 7)
0/318810
0/318810
(318810, 7)
0/318810
(318810, 7)
Loading and preparing results...
(318810, 7)
Converting ndarray to lists...
0/318810
Loading and preparing results...
0/318810
Converting ndarray to lists...
Converting ndarray to lists...
(318810, 7)
0/318810
0/318810
(318810, 7)
0/318810
(318810, 7)
0/318810
0/318810
DONE (t=1.54s)
creating index...
DONE (t=1.54s)
creating index...
DONE (t=1.56s)
creating index...
DONE (t=1.56s)
creating index...
index created!
index created!
index created!
index created!
DONE (t=1.83s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.91s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.02s)
creating index...
index created!
index created!
DONE (t=2.02s)
creating index...
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.03s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.09s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.12s)
creating index...
DONE (t=2.12s)
creating index...
DONE (t=2.13s)
creating index...
index created!
index created!
index created!
index created!
index created!
DONE (t=2.15s)
creating index...
index created!
index created!
index created!
DONE (t=2.17s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
DONE (t=2.19s)
creating index...
index created!
DONE (t=2.19s)
creating index...
index created!
index created!
index created!
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
index created!
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
index created!
index created!
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
index created!
DONE (t=2.25s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.34s).
Accumulating evaluation results...
DONE (t=3.35s).
Accumulating evaluation results...
DONE (t=3.39s).
Accumulating evaluation results...
DONE (t=3.35s).
Accumulating evaluation results...
DONE (t=3.39s).
Accumulating evaluation results...
DONE (t=3.38s).
Accumulating evaluation results...
DONE (t=3.41s).
Accumulating evaluation results...
DONE (t=3.38s).
Accumulating evaluation results...
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21049 AP goal: 0.21200
DONE (t=1.09s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21049 AP goal: 0.21200
DONE (t=1.09s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
DONE (t=1.11s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21049 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
DONE (t=1.09s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21049 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
DONE (t=1.07s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
DONE (t=1.09s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21049 AP goal: 0.21200
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21049 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21049 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.302
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.087
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21049 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757703.237800121 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757703.238424778 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.2104867049894556}

:::MLPv0.5.0 ssd 1541757703.238899946 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.2104867049894556}

:::MLPv0.5.0 ssd 1541757703.239355564 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757703.239812136 (train.py:338) eval_stop: 48
Iteration:   2820, Loss function: 3.520, Average Loss: 3.955, avg. samples / sec: 3064.56
Iteration:   2820, Loss function: 2.954, Average Loss: 3.959, avg. samples / sec: 3065.01
Iteration:   2820, Loss function: 2.981, Average Loss: 3.951, avg. samples / sec: 3064.47
Iteration:   2820, Loss function: 3.243, Average Loss: 3.959, avg. samples / sec: 3064.62
Iteration:   2820, Loss function: 2.912, Average Loss: 3.964, avg. samples / sec: 3064.85
Iteration:   2820, Loss function: 3.383, Average Loss: 3.961, avg. samples / sec: 3064.49
Iteration:   2820, Loss function: 3.171, Average Loss: 3.964, avg. samples / sec: 3065.24
Iteration:   2820, Loss function: 3.349, Average Loss: 3.956, avg. samples / sec: 3064.55

:::MLPv0.5.0 ssd 1541757705.170395136 (train.py:553) train_epoch: 49
Iteration:   2840, Loss function: 3.861, Average Loss: 3.937, avg. samples / sec: 24506.37
Iteration:   2840, Loss function: 3.126, Average Loss: 3.941, avg. samples / sec: 24501.20
Iteration:   2840, Loss function: 3.948, Average Loss: 3.946, avg. samples / sec: 24503.97
Iteration:   2840, Loss function: 3.085, Average Loss: 3.944, avg. samples / sec: 24503.36
Iteration:   2840, Loss function: 3.112, Average Loss: 3.948, avg. samples / sec: 24510.63
Iteration:   2840, Loss function: 3.295, Average Loss: 3.950, avg. samples / sec: 24465.76
Iteration:   2840, Loss function: 3.044, Average Loss: 3.942, avg. samples / sec: 24500.33
Iteration:   2840, Loss function: 3.573, Average Loss: 3.945, avg. samples / sec: 24462.17
Iteration:   2860, Loss function: 3.243, Average Loss: 3.933, avg. samples / sec: 24463.87
Iteration:   2860, Loss function: 3.372, Average Loss: 3.927, avg. samples / sec: 24460.07
Iteration:   2860, Loss function: 3.037, Average Loss: 3.931, avg. samples / sec: 24461.99
Iteration:   2860, Loss function: 3.142, Average Loss: 3.934, avg. samples / sec: 24495.01
Iteration:   2860, Loss function: 3.324, Average Loss: 3.931, avg. samples / sec: 24501.78
Iteration:   2860, Loss function: 3.166, Average Loss: 3.922, avg. samples / sec: 24432.08
Iteration:   2860, Loss function: 2.969, Average Loss: 3.935, avg. samples / sec: 24425.98
Iteration:   2860, Loss function: 3.015, Average Loss: 3.926, avg. samples / sec: 24468.36
Iteration:   2880, Loss function: 3.088, Average Loss: 3.919, avg. samples / sec: 24506.45
Iteration:   2880, Loss function: 2.916, Average Loss: 3.911, avg. samples / sec: 24494.63
Iteration:   2880, Loss function: 2.965, Average Loss: 3.921, avg. samples / sec: 24541.40
Iteration:   2880, Loss function: 3.910, Average Loss: 3.922, avg. samples / sec: 24489.70
Iteration:   2880, Loss function: 3.106, Average Loss: 3.912, avg. samples / sec: 24535.77
Iteration:   2880, Loss function: 3.157, Average Loss: 3.917, avg. samples / sec: 24488.57
Iteration:   2880, Loss function: 2.924, Average Loss: 3.917, avg. samples / sec: 24497.39
Iteration:   2880, Loss function: 2.791, Average Loss: 3.907, avg. samples / sec: 24495.72

:::MLPv0.5.0 ssd 1541757709.939945221 (train.py:553) train_epoch: 50
Iteration:   2900, Loss function: 3.271, Average Loss: 3.907, avg. samples / sec: 24527.52
Iteration:   2900, Loss function: 3.530, Average Loss: 3.910, avg. samples / sec: 24528.41
Iteration:   2900, Loss function: 3.147, Average Loss: 3.903, avg. samples / sec: 24528.80
Iteration:   2900, Loss function: 3.075, Average Loss: 3.898, avg. samples / sec: 24526.61
Iteration:   2900, Loss function: 3.628, Average Loss: 3.896, avg. samples / sec: 24547.11
Iteration:   2900, Loss function: 3.170, Average Loss: 3.902, avg. samples / sec: 24531.44
Iteration:   2900, Loss function: 2.997, Average Loss: 3.904, avg. samples / sec: 24534.26
Iteration:   2900, Loss function: 3.318, Average Loss: 3.899, avg. samples / sec: 24494.23
Iteration:   2920, Loss function: 3.302, Average Loss: 3.884, avg. samples / sec: 24552.57
Iteration:   2920, Loss function: 3.403, Average Loss: 3.887, avg. samples / sec: 24548.74
Iteration:   2920, Loss function: 2.611, Average Loss: 3.891, avg. samples / sec: 24548.37
Iteration:   2920, Loss function: 3.541, Average Loss: 3.892, avg. samples / sec: 24535.95
Iteration:   2920, Loss function: 3.067, Average Loss: 3.896, avg. samples / sec: 24533.28
Iteration:   2920, Loss function: 3.061, Average Loss: 3.885, avg. samples / sec: 24527.12
Iteration:   2920, Loss function: 3.158, Average Loss: 3.886, avg. samples / sec: 24536.33
Iteration:   2920, Loss function: 2.710, Average Loss: 3.888, avg. samples / sec: 24486.71
Iteration:   2940, Loss function: 3.852, Average Loss: 3.880, avg. samples / sec: 24525.17
Iteration:   2940, Loss function: 3.102, Average Loss: 3.872, avg. samples / sec: 24562.76
Iteration:   2940, Loss function: 2.815, Average Loss: 3.873, avg. samples / sec: 24514.17
Iteration:   2940, Loss function: 3.438, Average Loss: 3.870, avg. samples / sec: 24504.14
Iteration:   2940, Loss function: 3.671, Average Loss: 3.878, avg. samples / sec: 24506.22
Iteration:   2940, Loss function: 3.220, Average Loss: 3.873, avg. samples / sec: 24515.02
Iteration:   2940, Loss function: 2.697, Average Loss: 3.882, avg. samples / sec: 24496.81
Iteration:   2940, Loss function: 3.779, Average Loss: 3.874, avg. samples / sec: 24519.18

:::MLPv0.5.0 ssd 1541757714.783671141 (train.py:553) train_epoch: 51
Iteration:   2960, Loss function: 3.437, Average Loss: 3.860, avg. samples / sec: 24501.97
Iteration:   2960, Loss function: 2.871, Average Loss: 3.867, avg. samples / sec: 24495.08
Iteration:   2960, Loss function: 2.919, Average Loss: 3.869, avg. samples / sec: 24527.45
Iteration:   2960, Loss function: 2.958, Average Loss: 3.859, avg. samples / sec: 24497.57
Iteration:   2960, Loss function: 3.642, Average Loss: 3.861, avg. samples / sec: 24487.78
Iteration:   2960, Loss function: 2.658, Average Loss: 3.859, avg. samples / sec: 24509.79
Iteration:   2960, Loss function: 3.696, Average Loss: 3.866, avg. samples / sec: 24456.77
Iteration:   2960, Loss function: 3.406, Average Loss: 3.856, avg. samples / sec: 24443.02
Iteration:   2980, Loss function: 2.900, Average Loss: 3.852, avg. samples / sec: 24501.96
Iteration:   2980, Loss function: 3.086, Average Loss: 3.855, avg. samples / sec: 24500.30
Iteration:   2980, Loss function: 2.799, Average Loss: 3.845, avg. samples / sec: 24528.82
Iteration:   2980, Loss function: 3.298, Average Loss: 3.844, avg. samples / sec: 24533.89
Iteration:   2980, Loss function: 3.148, Average Loss: 3.848, avg. samples / sec: 24468.40
Iteration:   2980, Loss function: 3.087, Average Loss: 3.852, avg. samples / sec: 24515.06
Iteration:   2980, Loss function: 3.265, Average Loss: 3.840, avg. samples / sec: 24499.49
Iteration:   2980, Loss function: 2.577, Average Loss: 3.843, avg. samples / sec: 24436.26
Iteration:   3000, Loss function: 3.238, Average Loss: 3.842, avg. samples / sec: 24531.68
Iteration:   3000, Loss function: 3.187, Average Loss: 3.838, avg. samples / sec: 24528.06
Iteration:   3000, Loss function: 2.945, Average Loss: 3.833, avg. samples / sec: 24529.81
Iteration:   3000, Loss function: 3.205, Average Loss: 3.838, avg. samples / sec: 24521.59
Iteration:   3000, Loss function: 3.191, Average Loss: 3.831, avg. samples / sec: 24544.24
Iteration:   3000, Loss function: 3.336, Average Loss: 3.829, avg. samples / sec: 24531.74
Iteration:   3000, Loss function: 3.196, Average Loss: 3.831, avg. samples / sec: 24475.91
Iteration:   3000, Loss function: 3.253, Average Loss: 3.834, avg. samples / sec: 24470.88

:::MLPv0.5.0 ssd 1541757719.628237486 (train.py:553) train_epoch: 52
Iteration:   3020, Loss function: 3.245, Average Loss: 3.831, avg. samples / sec: 24476.47
Iteration:   3020, Loss function: 2.898, Average Loss: 3.816, avg. samples / sec: 24529.27
Iteration:   3020, Loss function: 3.405, Average Loss: 3.816, avg. samples / sec: 24511.13
Iteration:   3020, Loss function: 3.076, Average Loss: 3.823, avg. samples / sec: 24457.14
Iteration:   3020, Loss function: 3.623, Average Loss: 3.820, avg. samples / sec: 24510.79
Iteration:   3020, Loss function: 2.933, Average Loss: 3.821, avg. samples / sec: 24503.35
Iteration:   3020, Loss function: 3.614, Average Loss: 3.822, avg. samples / sec: 24482.72
Iteration:   3020, Loss function: 3.049, Average Loss: 3.818, avg. samples / sec: 24446.34
Iteration:   3040, Loss function: 2.598, Average Loss: 3.806, avg. samples / sec: 24523.03
Iteration:   3040, Loss function: 3.084, Average Loss: 3.809, avg. samples / sec: 24503.56
Iteration:   3040, Loss function: 2.993, Average Loss: 3.815, avg. samples / sec: 24481.71
Iteration:   3040, Loss function: 3.160, Average Loss: 3.801, avg. samples / sec: 24486.71
Iteration:   3040, Loss function: 3.202, Average Loss: 3.805, avg. samples / sec: 24511.44
Iteration:   3040, Loss function: 2.782, Average Loss: 3.806, avg. samples / sec: 24478.21
Iteration:   3040, Loss function: 2.841, Average Loss: 3.803, avg. samples / sec: 24438.00
Iteration:   3040, Loss function: 2.960, Average Loss: 3.810, avg. samples / sec: 24471.67
Iteration:   3060, Loss function: 2.996, Average Loss: 3.796, avg. samples / sec: 24530.24
Iteration:   3060, Loss function: 3.871, Average Loss: 3.803, avg. samples / sec: 24529.08
Iteration:   3060, Loss function: 3.002, Average Loss: 3.788, avg. samples / sec: 24538.25
Iteration:   3060, Loss function: 3.263, Average Loss: 3.796, avg. samples / sec: 24578.40
Iteration:   3060, Loss function: 2.646, Average Loss: 3.794, avg. samples / sec: 24498.60
Iteration:   3060, Loss function: 3.339, Average Loss: 3.792, avg. samples / sec: 24539.96
Iteration:   3060, Loss function: 3.043, Average Loss: 3.793, avg. samples / sec: 24511.23
Iteration:   3060, Loss function: 3.427, Average Loss: 3.789, avg. samples / sec: 24501.96

:::MLPv0.5.0 ssd 1541757724.477827072 (train.py:553) train_epoch: 53
Iteration:   3080, Loss function: 3.097, Average Loss: 3.789, avg. samples / sec: 24428.93
Iteration:   3080, Loss function: 2.987, Average Loss: 3.774, avg. samples / sec: 24481.71
Iteration:   3080, Loss function: 3.579, Average Loss: 3.783, avg. samples / sec: 24422.57
Iteration:   3080, Loss function: 3.270, Average Loss: 3.774, avg. samples / sec: 24429.71
Iteration:   3080, Loss function: 3.085, Average Loss: 3.784, avg. samples / sec: 24413.38
Iteration:   3080, Loss function: 2.597, Average Loss: 3.779, avg. samples / sec: 24446.60
Iteration:   3080, Loss function: 3.174, Average Loss: 3.781, avg. samples / sec: 24410.96
Iteration:   3080, Loss function: 2.726, Average Loss: 3.777, avg. samples / sec: 24424.35
Iteration:   3100, Loss function: 3.056, Average Loss: 3.775, avg. samples / sec: 24530.26
Iteration:   3100, Loss function: 2.882, Average Loss: 3.760, avg. samples / sec: 24533.23
Iteration:   3100, Loss function: 3.692, Average Loss: 3.771, avg. samples / sec: 24527.32
Iteration:   3100, Loss function: 3.165, Average Loss: 3.768, avg. samples / sec: 24563.46
Iteration:   3100, Loss function: 3.111, Average Loss: 3.761, avg. samples / sec: 24521.61
Iteration:   3100, Loss function: 2.979, Average Loss: 3.767, avg. samples / sec: 24522.57
Iteration:   3100, Loss function: 3.405, Average Loss: 3.766, avg. samples / sec: 24549.97
Iteration:   3100, Loss function: 3.075, Average Loss: 3.771, avg. samples / sec: 24481.78

:::MLPv0.5.0 ssd 1541757729.246144533 (train.py:553) train_epoch: 54
Iteration:   3120, Loss function: 3.098, Average Loss: 3.761, avg. samples / sec: 24485.22
Iteration:   3120, Loss function: 3.205, Average Loss: 3.755, avg. samples / sec: 24509.65
Iteration:   3120, Loss function: 2.994, Average Loss: 3.749, avg. samples / sec: 24491.73
Iteration:   3120, Loss function: 2.845, Average Loss: 3.759, avg. samples / sec: 24474.91
Iteration:   3120, Loss function: 3.118, Average Loss: 3.748, avg. samples / sec: 24449.36
Iteration:   3120, Loss function: 2.884, Average Loss: 3.758, avg. samples / sec: 24507.24
Iteration:   3120, Loss function: 3.013, Average Loss: 3.753, avg. samples / sec: 24471.44
Iteration:   3120, Loss function: 3.185, Average Loss: 3.755, avg. samples / sec: 24433.23
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2

:::MLPv0.5.0 ssd 1541757729.751735926 (train.py:586) opt_learning_rate: 0.0016

































































:::MLPv0.5.0 ssd 1541757729.835290670 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757729.835817099 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757729.836273909 (train.py:220) eval_start: 54
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 25
Predicting Ended, total time: 4.10 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.10 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.10 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.10 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.10 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.10 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.10 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(301075, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(301075, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(301075, 7)
Converting ndarray to lists...
Loading and preparing results...
(301075, 7)
(301075, 7)
Loading and preparing results...
Converting ndarray to lists...
(301075, 7)
0/301075
0/301075
(301075, 7)
(301075, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/301075
(301075, 7)
Converting ndarray to lists...
(301075, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.10 s
Converting ndarray to lists...
0/301075
(301075, 7)
0/301075
Loading and preparing results...
Converting ndarray to lists...
0/301075
Converting ndarray to lists...
Converting ndarray to lists...
(301075, 7)
Loading and preparing results...
(301075, 7)
(301075, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/301075
0/301075
(301075, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(301075, 7)
(301075, 7)
(301075, 7)
(301075, 7)
0/301075
Loading and preparing results...
Converting ndarray to lists...
0/301075
(301075, 7)
Converting ndarray to lists...
0/301075
Converting ndarray to lists...
Loading and preparing results...
0/301075
(301075, 7)
Converting ndarray to lists...
Loading and preparing results...
0/301075
Converting ndarray to lists...
0/301075
Converting ndarray to lists...
Loading and preparing results...
0/301075
Loading and preparing results...
Converting ndarray to lists...
(301075, 7)
0/301075
0/301075
Converting ndarray to lists...
Loading and preparing results...
(301075, 7)
Loading and preparing results...
(301075, 7)
Loading and preparing results...
(301075, 7)
0/301075
Converting ndarray to lists...
0/301075
0/301075
Loading and preparing results...
(301075, 7)
(301075, 7)
Converting ndarray to lists...
Loading and preparing results...
(301075, 7)
Converting ndarray to lists...
0/301075
(301075, 7)
Converting ndarray to lists...
Loading and preparing results...
0/301075
Converting ndarray to lists...
Converting ndarray to lists...
(301075, 7)
Converting ndarray to lists...
Loading and preparing results...
0/301075
(301075, 7)
Loading and preparing results...
Loading and preparing results...
(301075, 7)
Loading and preparing results...
(301075, 7)
Loading and preparing results...
0/301075
Converting ndarray to lists...
0/301075
Loading and preparing results...
0/301075
(301075, 7)
0/301075
0/301075
0/301075
(301075, 7)
Converting ndarray to lists...
0/301075
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(301075, 7)
Loading and preparing results...
(301075, 7)
(301075, 7)
(301075, 7)
(301075, 7)
0/301075
Loading and preparing results...
0/301075
Converting ndarray to lists...
Loading and preparing results...
0/301075
Converting ndarray to lists...
0/301075
Loading and preparing results...
0/301075
0/301075
(301075, 7)
0/301075
Converting ndarray to lists...
Loading and preparing results...
(301075, 7)
Converting ndarray to lists...
0/301075
Converting ndarray to lists...
(301075, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/301075
0/301075
Converting ndarray to lists...
(301075, 7)
Converting ndarray to lists...
0/301075
Converting ndarray to lists...
0/301075
Converting ndarray to lists...
0/301075
Converting ndarray to lists...
(301075, 7)
Loading and preparing results...
Converting ndarray to lists...
(301075, 7)
(301075, 7)
0/301075
Loading and preparing results...
(301075, 7)
0/301075
Loading and preparing results...
(301075, 7)
(301075, 7)
(301075, 7)
(301075, 7)
Loading and preparing results...
0/301075
(301075, 7)
0/301075
(301075, 7)
Converting ndarray to lists...
0/301075
(301075, 7)
0/301075
0/301075
0/301075
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(301075, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(301075, 7)
0/301075
0/301075
Loading and preparing results...
0/301075
0/301075
0/301075
0/301075
Converting ndarray to lists...
(301075, 7)
(301075, 7)
(301075, 7)
(301075, 7)
Loading and preparing results...
0/301075
0/301075
Converting ndarray to lists...
Converting ndarray to lists...
(301075, 7)
(301075, 7)
0/301075
0/301075
0/301075
0/301075
(301075, 7)
0/301075
DONE (t=1.44s)
creating index...
DONE (t=1.46s)
creating index...
DONE (t=1.47s)
creating index...
DONE (t=1.49s)
creating index...
DONE (t=1.49s)
creating index...
DONE (t=1.50s)
creating index...
index created!
index created!
index created!
index created!
index created!
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.86s)
creating index...
index created!
index created!
index created!
DONE (t=1.88s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=1.91s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=2.01s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.03s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.09s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.11s)
creating index...
DONE (t=2.12s)
creating index...
DONE (t=2.13s)
creating index...
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=3.27s).
Accumulating evaluation results...
DONE (t=3.28s).
Accumulating evaluation results...
DONE (t=3.30s).
Accumulating evaluation results...
DONE (t=3.26s).
Accumulating evaluation results...
DONE (t=3.33s).
Accumulating evaluation results...
DONE (t=3.31s).
Accumulating evaluation results...
DONE (t=3.27s).
Accumulating evaluation results...
DONE (t=3.31s).
Accumulating evaluation results...
DONE (t=1.06s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
DONE (t=1.06s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
DONE (t=1.06s).
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.340
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21233 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.340
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21233 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.340
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21233 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21233 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21233 AP goal: 0.21200
DONE (t=1.06s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
DONE (t=1.03s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21233 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21233 AP goal: 0.21200
DONE (t=1.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.052
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.210
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.086
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21233 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757740.617220640 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757740.617831707 (train.py:333) eval_accuracy: {"epoch": 54, "value": 0.21233159417039174}

:::MLPv0.5.0 ssd 1541757740.618313551 (train.py:336) eval_iteration_accuracy: {"epoch": 54, "value": 0.21233159417039174}

:::MLPv0.5.0 ssd 1541757740.618768215 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757740.619227409 (train.py:338) eval_stop: 54

:::MLPv0.5.0 ssd 1541757741.703804016 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1541757741.704325914 (train.py:707) run_final
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2018-11-09 10:02:37 AM
RESULT,OBJECT_DETECTION,,368,nvidia,2018-11-09 09:56:29 AM
ENDING TIMING RUN AT 2018-11-09 10:02:29 AM
RESULT,OBJECT_DETECTION,,368,nvidia,2018-11-09 09:56:21 AM
ENDING TIMING RUN AT 2018-11-09 10:02:34 AM
RESULT,OBJECT_DETECTION,,368,nvidia,2018-11-09 09:56:26 AM
ENDING TIMING RUN AT 2018-11-09 10:02:44 AM
RESULT,OBJECT_DETECTION,,368,nvidia,2018-11-09 09:56:36 AM
ENDING TIMING RUN AT 2018-11-09 10:02:34 AM
RESULT,OBJECT_DETECTION,,368,nvidia,2018-11-09 09:56:26 AM
ENDING TIMING RUN AT 2018-11-09 10:02:28 AM
RESULT,OBJECT_DETECTION,,369,nvidia,2018-11-09 09:56:19 AM
ENDING TIMING RUN AT 2018-11-09 10:02:37 AM
RESULT,OBJECT_DETECTION,,368,nvidia,2018-11-09 09:56:29 AM
ENDING TIMING RUN AT 2018-11-09 10:02:39 AM
RESULT,OBJECT_DETECTION,,368,nvidia,2018-11-09 09:56:31 AM
