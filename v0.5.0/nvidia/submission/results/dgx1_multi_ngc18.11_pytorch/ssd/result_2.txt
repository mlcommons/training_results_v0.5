Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3

:::MLPv0.5.0 ssd 1541710824.732809544 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.785216808 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.881508589 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.881376028 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.888578653 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.912857533 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.919791937 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710824.943967342 (<string>:1) run_clear_caches
Launching on node sc-sdgx-784
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-794
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-801
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-784
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-802
+ srun -N 1 -n 1 -w sc-sdgx-784 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=0 --master_addr=172.22.2.73 --master_port=4242' -e SLURM_JOB_ID=155376 -e SLURM_NTASKS_PER_NODE=8 cont_155376 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-794
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-807
+ srun -N 1 -n 1 -w sc-sdgx-794 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=1 --master_addr=172.22.2.73 --master_port=4242' -e SLURM_JOB_ID=155376 -e SLURM_NTASKS_PER_NODE=8 cont_155376 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-801
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-828
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-802
+ srun -N 1 -n 1 -w sc-sdgx-801 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=2 --master_addr=172.22.2.73 --master_port=4242' -e SLURM_JOB_ID=155376 -e SLURM_NTASKS_PER_NODE=8 cont_155376 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-836
+ srun -N 1 -n 1 -w sc-sdgx-802 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=3 --master_addr=172.22.2.73 --master_port=4242' -e SLURM_JOB_ID=155376 -e SLURM_NTASKS_PER_NODE=8 cont_155376 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-807
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-837
+ srun -N 1 -n 1 -w sc-sdgx-807 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=4 --master_addr=172.22.2.73 --master_port=4242' -e SLURM_JOB_ID=155376 -e SLURM_NTASKS_PER_NODE=8 cont_155376 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-828
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-836
+ srun -N 1 -n 1 -w sc-sdgx-828 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=5 --master_addr=172.22.2.73 --master_port=4242' -e SLURM_JOB_ID=155376 -e SLURM_NTASKS_PER_NODE=8 cont_155376 ./run_and_time.sh
+ srun -N 1 -n 1 -w sc-sdgx-836 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=6 --master_addr=172.22.2.73 --master_port=4242' -e SLURM_JOB_ID=155376 -e SLURM_NTASKS_PER_NODE=8 cont_155376 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-837
+ srun -N 1 -n 1 -w sc-sdgx-837 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=7 --master_addr=172.22.2.73 --master_port=4242' -e SLURM_JOB_ID=155376 -e SLURM_NTASKS_PER_NODE=8 cont_155376 ./run_and_time.sh
Run vars: id 155376 gpus 8 mparams  --nnodes=8 --node_rank=0 --master_addr=172.22.2.73 --master_port=4242
Run vars: id 155376 gpus 8 mparams  --nnodes=8 --node_rank=5 --master_addr=172.22.2.73 --master_port=4242
Run vars: id 155376 gpus 8 mparams  --nnodes=8 --node_rank=2 --master_addr=172.22.2.73 --master_port=4242
Run vars: id 155376 gpus 8 mparams  --nnodes=8 --node_rank=4 --master_addr=172.22.2.73 --master_port=4242
Run vars: id 155376 gpus 8 mparams  --nnodes=8 --node_rank=6 --master_addr=172.22.2.73 --master_port=4242
Run vars: id 155376 gpus 8 mparams  --nnodes=8 --node_rank=1 --master_addr=172.22.2.73 --master_port=4242
Run vars: id 155376 gpus 8 mparams  --nnodes=8 --node_rank=7 --master_addr=172.22.2.73 --master_port=4242
Run vars: id 155376 gpus 8 mparams  --nnodes=8 --node_rank=3 --master_addr=172.22.2.73 --master_port=4242
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=0 --master_addr=172.22.2.73 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=5 --master_addr=172.22.2.73 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=2 --master_addr=172.22.2.73 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=4 --master_addr=172.22.2.73 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=6 --master_addr=172.22.2.73 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=1 --master_addr=172.22.2.73 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=7 --master_addr=172.22.2.73 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:25 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=3 --master_addr=172.22.2.73 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
0 Using seed = 1453160937
1 Using seed = 1453160938
3 Using seed = 1453160940
5 Using seed = 1453160942
7 Using seed = 1453160944
4 Using seed = 1453160941
12 Using seed = 1453160949
14 Using seed = 1453160951
13 Using seed = 1453160950
15 Using seed = 1453160952
8 Using seed = 1453160945
9 Using seed = 1453160946
10 Using seed = 1453160947
11 Using seed = 1453160948
19 Using seed = 1453160956
18 Using seed = 1453160955
16 Using seed = 1453160953
17 Using seed = 1453160954
22 Using seed = 1453160959
23 Using seed = 1453160960
20 Using seed = 1453160957
21 Using seed = 1453160958
25 Using seed = 1453160962
26 Using seed = 1453160963
24 Using seed = 1453160961
27 Using seed = 1453160964
29 Using seed = 1453160966
30 Using seed = 1453160967
28 Using seed = 1453160965
31 Using seed = 1453160968
33 Using seed = 1453160970
34 Using seed = 1453160971
32 Using seed = 1453160969
35 Using seed = 1453160972
38 Using seed = 1453160975
37 Using seed = 1453160974
39 Using seed = 1453160976
36 Using seed = 1453160973
47 Using seed = 1453160984
45 Using seed = 1453160982
44 Using seed = 1453160981
46 Using seed = 1453160983
43 Using seed = 1453160980
41 Using seed = 1453160978
42 Using seed = 1453160979
40 Using seed = 1453160977
49 Using seed = 1453160986
50 Using seed = 1453160987
51 Using seed = 1453160988
48 Using seed = 1453160985
54 Using seed = 1453160991
55 Using seed = 1453160992
53 Using seed = 1453160990
52 Using seed = 1453160989
62 Using seed = 1453160999
61 Using seed = 1453160998
63 Using seed = 1453161000
60 Using seed = 1453160997
56 Using seed = 1453160993
58 Using seed = 1453160995
57 Using seed = 1453160994
59 Using seed = 1453160996
2 Using seed = 1453160939
6 Using seed = 1453160943

:::MLPv0.5.0 ssd 1541710836.208863258 (train.py:371) run_start

:::MLPv0.5.0 ssd 1541710836.211500406 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1541710836.224448442 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1541710836.237371445 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1541710836.238197803 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1541710836.286540747 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1541710836.306979656 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1541710836.326973915 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1541710836.340323687 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1541710836.362722397 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1541710836.380258083 (train.py:382) input_size: 300
loading annotations into memory...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
index created!
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
index created!
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
Done (t=0.49s)
creating index...
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
index created!
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
time_check a: 1541710837.336171865
time_check a: 1541710837.346779585
time_check a: 1541710837.347385883
time_check a: 1541710837.348732233
time_check a: 1541710837.349868298
time_check a: 1541710837.351748943
time_check a: 1541710837.352718115
time_check a: 1541710837.362654686
time_check b: 1541710860.956906557
time_check b: 1541710860.968643665
time_check b: 1541710860.973338366
time_check b: 1541710861.056093454
time_check b: 1541710861.055995464
time_check b: 1541710861.085500002
time_check b: 1541710861.110530615
time_check b: 1541710862.099496841

:::MLPv0.5.0 ssd 1541710862.880234718 (train.py:413) input_order

:::MLPv0.5.0 ssd 1541710862.886832952 (train.py:414) input_batch_size: 32

:::MLPv0.5.0 ssd 1541710867.113412619 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710867.114299536 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710867.143710375 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1541710868.157032490 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1541710868.157888651 (train.py:477) opt_learning_rate: 0.16

:::MLPv0.5.0 ssd 1541710868.158619881 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1541710868.159336090 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1541710868.160068274 (train.py:483) opt_learning_rate_warmup_steps: 900

:::MLPv0.5.0 ssd 1541710872.334412336 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710872.335326195 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710872.364557028 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss

:::MLPv0.5.0 ssd 1541710875.240859985 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1541710875.241725683 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1541710875.245365143 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 21.873, Average Loss: 0.022, avg. samples / sec: 39618.36
Iteration:      0, Loss function: 22.414, Average Loss: 0.022, avg. samples / sec: 13083.34
Iteration:      0, Loss function: 23.091, Average Loss: 0.023, avg. samples / sec: 7832.36
Iteration:      0, Loss function: 22.618, Average Loss: 0.023, avg. samples / sec: 23873.73
Iteration:      0, Loss function: 22.815, Average Loss: 0.023, avg. samples / sec: 12975.24
Iteration:      0, Loss function: 22.618, Average Loss: 0.023, avg. samples / sec: 16662.18
Iteration:      0, Loss function: 22.489, Average Loss: 0.022, avg. samples / sec: 18357.54
Iteration:      0, Loss function: 22.654, Average Loss: 0.023, avg. samples / sec: 19803.15

:::MLPv0.5.0 ssd 1541710877.172374249 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.0001777777777777767}

:::MLPv0.5.0 ssd 1541710877.405073643 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0003555555555555534}

:::MLPv0.5.0 ssd 1541710877.518863201 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0005333333333333301}

:::MLPv0.5.0 ssd 1541710877.632333279 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0007111111111111068}

:::MLPv0.5.0 ssd 1541710877.756395817 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0008888888888888835}

:::MLPv0.5.0 ssd 1541710877.864995003 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0010666666666666602}

:::MLPv0.5.0 ssd 1541710877.972057581 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.001244444444444437}

:::MLPv0.5.0 ssd 1541710878.080356359 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0014222222222222136}

:::MLPv0.5.0 ssd 1541710878.187871695 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0015999999999999903}

:::MLPv0.5.0 ssd 1541710878.289724350 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.001777777777777767}

:::MLPv0.5.0 ssd 1541710878.394050360 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0019555555555555437}

:::MLPv0.5.0 ssd 1541710878.498651743 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.0021333333333333204}

:::MLPv0.5.0 ssd 1541710878.602276564 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.002311111111111097}

:::MLPv0.5.0 ssd 1541710878.715766430 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.002488888888888874}

:::MLPv0.5.0 ssd 1541710878.819252729 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.0026666666666666505}

:::MLPv0.5.0 ssd 1541710878.924944401 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.0028444444444444272}

:::MLPv0.5.0 ssd 1541710879.031004906 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.0030222222222222317}

:::MLPv0.5.0 ssd 1541710879.135667086 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.0032000000000000084}

:::MLPv0.5.0 ssd 1541710879.236204386 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.003377777777777785}

:::MLPv0.5.0 ssd 1541710879.340886831 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.003555555555555562}
Iteration:     20, Loss function: 20.054, Average Loss: 0.440, avg. samples / sec: 10037.68
Iteration:     20, Loss function: 19.646, Average Loss: 0.436, avg. samples / sec: 10038.33
Iteration:     20, Loss function: 21.068, Average Loss: 0.440, avg. samples / sec: 10033.54
Iteration:     20, Loss function: 20.674, Average Loss: 0.443, avg. samples / sec: 10030.41
Iteration:     20, Loss function: 20.964, Average Loss: 0.441, avg. samples / sec: 10031.62
Iteration:     20, Loss function: 20.662, Average Loss: 0.437, avg. samples / sec: 10032.31
Iteration:     20, Loss function: 20.868, Average Loss: 0.443, avg. samples / sec: 10027.37
Iteration:     20, Loss function: 20.563, Average Loss: 0.440, avg. samples / sec: 10044.06

:::MLPv0.5.0 ssd 1541710879.443701982 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.0037333333333333385}

:::MLPv0.5.0 ssd 1541710879.548901320 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.003911111111111115}

:::MLPv0.5.0 ssd 1541710879.654267073 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.004088888888888892}

:::MLPv0.5.0 ssd 1541710879.759694099 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.004266666666666669}

:::MLPv0.5.0 ssd 1541710879.860710859 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.004444444444444445}

:::MLPv0.5.0 ssd 1541710879.967088699 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.004622222222222222}

:::MLPv0.5.0 ssd 1541710880.074193001 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.004799999999999999}

:::MLPv0.5.0 ssd 1541710880.176428556 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.004977777777777775}

:::MLPv0.5.0 ssd 1541710880.274727345 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.005155555555555552}

:::MLPv0.5.0 ssd 1541710880.375226974 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.005333333333333329}

:::MLPv0.5.0 ssd 1541710880.474595308 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.0055111111111111055}

:::MLPv0.5.0 ssd 1541710880.574046373 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.005688888888888882}

:::MLPv0.5.0 ssd 1541710880.671861172 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.005866666666666659}

:::MLPv0.5.0 ssd 1541710880.776548624 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.006044444444444436}

:::MLPv0.5.0 ssd 1541710880.876424789 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.006222222222222212}

:::MLPv0.5.0 ssd 1541710880.974937677 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.006399999999999989}

:::MLPv0.5.0 ssd 1541710881.078192234 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.006577777777777766}

:::MLPv0.5.0 ssd 1541710881.176609993 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.0067555555555555424}

:::MLPv0.5.0 ssd 1541710881.278747320 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.006933333333333319}

:::MLPv0.5.0 ssd 1541710881.388607264 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.007111111111111096}
Iteration:     40, Loss function: 15.941, Average Loss: 0.812, avg. samples / sec: 20000.45
Iteration:     40, Loss function: 16.449, Average Loss: 0.817, avg. samples / sec: 19994.38
Iteration:     40, Loss function: 15.838, Average Loss: 0.814, avg. samples / sec: 20024.67
Iteration:     40, Loss function: 15.490, Average Loss: 0.816, avg. samples / sec: 20037.64
Iteration:     40, Loss function: 16.250, Average Loss: 0.823, avg. samples / sec: 20010.34
Iteration:     40, Loss function: 15.924, Average Loss: 0.819, avg. samples / sec: 19995.86
Iteration:     40, Loss function: 16.057, Average Loss: 0.820, avg. samples / sec: 20002.62
Iteration:     40, Loss function: 15.410, Average Loss: 0.821, avg. samples / sec: 19971.15

:::MLPv0.5.0 ssd 1541710881.489349365 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.0072888888888888725}

:::MLPv0.5.0 ssd 1541710881.592482328 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.007466666666666649}

:::MLPv0.5.0 ssd 1541710881.691988230 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.007644444444444454}

:::MLPv0.5.0 ssd 1541710881.797847748 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.00782222222222223}

:::MLPv0.5.0 ssd 1541710881.899516344 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.008000000000000007}

:::MLPv0.5.0 ssd 1541710882.005786419 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.008177777777777784}

:::MLPv0.5.0 ssd 1541710882.110618353 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.00835555555555556}

:::MLPv0.5.0 ssd 1541710882.209187746 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.008533333333333337}

:::MLPv0.5.0 ssd 1541710882.314617634 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.008711111111111114}

:::MLPv0.5.0 ssd 1541710882.417194605 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.00888888888888889}

:::MLPv0.5.0 ssd 1541710882.517366171 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.009066666666666667}

:::MLPv0.5.0 ssd 1541710882.619792938 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.009244444444444444}

:::MLPv0.5.0 ssd 1541710882.725880623 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.00942222222222222}

:::MLPv0.5.0 ssd 1541710882.830340862 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.009599999999999997}

:::MLPv0.5.0 ssd 1541710882.933535337 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.009777777777777774}

:::MLPv0.5.0 ssd 1541710883.035372257 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.00995555555555555}

:::MLPv0.5.0 ssd 1541710883.133367777 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.010133333333333328}

:::MLPv0.5.0 ssd 1541710883.226930618 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1541710883.232568502 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 58, "value": 0.010311111111111104}

:::MLPv0.5.0 ssd 1541710883.331646919 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 59, "value": 0.010488888888888881}

:::MLPv0.5.0 ssd 1541710883.430711031 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 60, "value": 0.010666666666666658}
Iteration:     60, Loss function: 11.429, Average Loss: 1.060, avg. samples / sec: 20066.06
Iteration:     60, Loss function: 10.377, Average Loss: 1.063, avg. samples / sec: 20067.77
Iteration:     60, Loss function: 10.456, Average Loss: 1.061, avg. samples / sec: 20066.04
Iteration:     60, Loss function: 11.358, Average Loss: 1.067, avg. samples / sec: 20068.54
Iteration:     60, Loss function: 11.211, Average Loss: 1.065, avg. samples / sec: 20076.18
Iteration:     60, Loss function: 10.715, Average Loss: 1.066, avg. samples / sec: 20105.51
Iteration:     60, Loss function: 10.854, Average Loss: 1.066, avg. samples / sec: 20028.69
Iteration:     60, Loss function: 11.695, Average Loss: 1.059, avg. samples / sec: 20008.08

:::MLPv0.5.0 ssd 1541710883.532746792 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 61, "value": 0.010844444444444434}

:::MLPv0.5.0 ssd 1541710883.638069391 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 62, "value": 0.011022222222222211}

:::MLPv0.5.0 ssd 1541710883.738738060 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 63, "value": 0.011199999999999988}

:::MLPv0.5.0 ssd 1541710883.835340977 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 64, "value": 0.011377777777777764}

:::MLPv0.5.0 ssd 1541710883.935074329 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 65, "value": 0.011555555555555541}

:::MLPv0.5.0 ssd 1541710884.032331705 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 66, "value": 0.011733333333333318}

:::MLPv0.5.0 ssd 1541710884.139140606 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 67, "value": 0.011911111111111095}

:::MLPv0.5.0 ssd 1541710884.238183737 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 68, "value": 0.012088888888888899}

:::MLPv0.5.0 ssd 1541710884.336189747 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 69, "value": 0.012266666666666676}

:::MLPv0.5.0 ssd 1541710884.433588505 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 70, "value": 0.012444444444444452}

:::MLPv0.5.0 ssd 1541710884.530851841 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 71, "value": 0.012622222222222229}

:::MLPv0.5.0 ssd 1541710884.629383087 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 72, "value": 0.012800000000000006}

:::MLPv0.5.0 ssd 1541710884.728030205 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 73, "value": 0.012977777777777783}

:::MLPv0.5.0 ssd 1541710884.826861620 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 74, "value": 0.01315555555555556}

:::MLPv0.5.0 ssd 1541710884.925920486 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 75, "value": 0.013333333333333336}

:::MLPv0.5.0 ssd 1541710885.021873474 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 76, "value": 0.013511111111111113}

:::MLPv0.5.0 ssd 1541710885.116854191 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 77, "value": 0.01368888888888889}

:::MLPv0.5.0 ssd 1541710885.217284203 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 78, "value": 0.013866666666666666}

:::MLPv0.5.0 ssd 1541710885.323637009 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 79, "value": 0.014044444444444443}

:::MLPv0.5.0 ssd 1541710885.420950890 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 80, "value": 0.01422222222222222}
Iteration:     80, Loss function: 9.914, Average Loss: 1.258, avg. samples / sec: 20588.93
Iteration:     80, Loss function: 9.521, Average Loss: 1.249, avg. samples / sec: 20579.60
Iteration:     80, Loss function: 9.839, Average Loss: 1.247, avg. samples / sec: 20581.86
Iteration:     80, Loss function: 9.942, Average Loss: 1.248, avg. samples / sec: 20633.90
Iteration:     80, Loss function: 10.076, Average Loss: 1.258, avg. samples / sec: 20599.96
Iteration:     80, Loss function: 9.319, Average Loss: 1.251, avg. samples / sec: 20579.63
Iteration:     80, Loss function: 9.977, Average Loss: 1.251, avg. samples / sec: 20576.59
Iteration:     80, Loss function: 9.773, Average Loss: 1.256, avg. samples / sec: 20615.78

:::MLPv0.5.0 ssd 1541710885.516537428 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 81, "value": 0.014399999999999996}

:::MLPv0.5.0 ssd 1541710885.615102768 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 82, "value": 0.014577777777777773}

:::MLPv0.5.0 ssd 1541710885.709644318 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 83, "value": 0.01475555555555555}

:::MLPv0.5.0 ssd 1541710885.807363510 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 84, "value": 0.014933333333333326}

:::MLPv0.5.0 ssd 1541710885.902506590 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 85, "value": 0.015111111111111103}

:::MLPv0.5.0 ssd 1541710886.016604662 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 86, "value": 0.01528888888888888}

:::MLPv0.5.0 ssd 1541710886.114514112 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 87, "value": 0.015466666666666656}

:::MLPv0.5.0 ssd 1541710886.213821173 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 88, "value": 0.015644444444444433}

:::MLPv0.5.0 ssd 1541710886.310659170 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 89, "value": 0.01582222222222221}

:::MLPv0.5.0 ssd 1541710886.408153534 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 90, "value": 0.015999999999999986}

:::MLPv0.5.0 ssd 1541710886.507619858 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 91, "value": 0.016177777777777763}

:::MLPv0.5.0 ssd 1541710886.604987383 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 92, "value": 0.01635555555555554}

:::MLPv0.5.0 ssd 1541710886.704111338 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 93, "value": 0.016533333333333317}

:::MLPv0.5.0 ssd 1541710886.801498175 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 94, "value": 0.01671111111111112}

:::MLPv0.5.0 ssd 1541710886.906107426 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 95, "value": 0.016888888888888898}

:::MLPv0.5.0 ssd 1541710887.003528357 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 96, "value": 0.017066666666666674}

:::MLPv0.5.0 ssd 1541710887.104461908 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 97, "value": 0.01724444444444445}

:::MLPv0.5.0 ssd 1541710887.204093695 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 98, "value": 0.017422222222222228}

:::MLPv0.5.0 ssd 1541710887.303402662 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 99, "value": 0.017600000000000005}

:::MLPv0.5.0 ssd 1541710887.404546976 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 100, "value": 0.01777777777777778}
Iteration:    100, Loss function: 9.053, Average Loss: 1.418, avg. samples / sec: 20647.63
Iteration:    100, Loss function: 9.653, Average Loss: 1.412, avg. samples / sec: 20646.32
Iteration:    100, Loss function: 9.112, Average Loss: 1.414, avg. samples / sec: 20655.24
Iteration:    100, Loss function: 9.104, Average Loss: 1.414, avg. samples / sec: 20645.15
Iteration:    100, Loss function: 9.249, Average Loss: 1.410, avg. samples / sec: 20643.48
Iteration:    100, Loss function: 9.305, Average Loss: 1.421, avg. samples / sec: 20641.93
Iteration:    100, Loss function: 9.251, Average Loss: 1.412, avg. samples / sec: 20634.70
Iteration:    100, Loss function: 9.279, Average Loss: 1.418, avg. samples / sec: 20638.99

:::MLPv0.5.0 ssd 1541710887.503904581 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 101, "value": 0.017955555555555558}

:::MLPv0.5.0 ssd 1541710887.598007441 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 102, "value": 0.018133333333333335}

:::MLPv0.5.0 ssd 1541710887.694247961 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 103, "value": 0.01831111111111111}

:::MLPv0.5.0 ssd 1541710887.788241386 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 104, "value": 0.018488888888888888}

:::MLPv0.5.0 ssd 1541710887.886642218 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 105, "value": 0.018666666666666665}

:::MLPv0.5.0 ssd 1541710887.984859228 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 106, "value": 0.01884444444444444}

:::MLPv0.5.0 ssd 1541710888.083275080 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 107, "value": 0.019022222222222218}

:::MLPv0.5.0 ssd 1541710888.180223942 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 108, "value": 0.019199999999999995}

:::MLPv0.5.0 ssd 1541710888.276486158 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 109, "value": 0.01937777777777777}

:::MLPv0.5.0 ssd 1541710888.374371290 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 110, "value": 0.019555555555555548}

:::MLPv0.5.0 ssd 1541710888.469748974 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 111, "value": 0.019733333333333325}

:::MLPv0.5.0 ssd 1541710888.566727638 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 112, "value": 0.0199111111111111}

:::MLPv0.5.0 ssd 1541710888.675229549 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 113, "value": 0.02008888888888888}

:::MLPv0.5.0 ssd 1541710888.772131443 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 114, "value": 0.020266666666666655}

:::MLPv0.5.0 ssd 1541710888.866852760 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 115, "value": 0.020444444444444432}

:::MLPv0.5.0 ssd 1541710888.959636450 (train.py:553) train_epoch: 2

:::MLPv0.5.0 ssd 1541710888.964948893 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 116, "value": 0.02062222222222221}

:::MLPv0.5.0 ssd 1541710889.062298775 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 117, "value": 0.020799999999999985}

:::MLPv0.5.0 ssd 1541710889.157596588 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 118, "value": 0.020977777777777762}

:::MLPv0.5.0 ssd 1541710889.255426645 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 119, "value": 0.02115555555555554}

:::MLPv0.5.0 ssd 1541710889.354939222 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 120, "value": 0.021333333333333343}
Iteration:    120, Loss function: 8.993, Average Loss: 1.567, avg. samples / sec: 21029.13
Iteration:    120, Loss function: 8.684, Average Loss: 1.567, avg. samples / sec: 21001.58
Iteration:    120, Loss function: 8.944, Average Loss: 1.563, avg. samples / sec: 21005.53
Iteration:    120, Loss function: 8.913, Average Loss: 1.561, avg. samples / sec: 21006.95
Iteration:    120, Loss function: 8.512, Average Loss: 1.565, avg. samples / sec: 21007.57
Iteration:    120, Loss function: 8.839, Average Loss: 1.563, avg. samples / sec: 21012.26
Iteration:    120, Loss function: 7.976, Average Loss: 1.564, avg. samples / sec: 21001.24
Iteration:    120, Loss function: 9.070, Average Loss: 1.569, avg. samples / sec: 21000.79

:::MLPv0.5.0 ssd 1541710889.455918074 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 121, "value": 0.02151111111111112}

:::MLPv0.5.0 ssd 1541710889.554055452 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 122, "value": 0.021688888888888896}

:::MLPv0.5.0 ssd 1541710889.650530338 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 123, "value": 0.021866666666666673}

:::MLPv0.5.0 ssd 1541710889.747709990 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 124, "value": 0.02204444444444445}

:::MLPv0.5.0 ssd 1541710889.852650166 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 125, "value": 0.022222222222222227}

:::MLPv0.5.0 ssd 1541710889.962084055 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 126, "value": 0.022400000000000003}

:::MLPv0.5.0 ssd 1541710890.056693077 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 127, "value": 0.02257777777777778}

:::MLPv0.5.0 ssd 1541710890.156857491 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 128, "value": 0.022755555555555557}

:::MLPv0.5.0 ssd 1541710890.253793001 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 129, "value": 0.022933333333333333}

:::MLPv0.5.0 ssd 1541710890.349674940 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 130, "value": 0.02311111111111111}

:::MLPv0.5.0 ssd 1541710890.448781967 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 131, "value": 0.023288888888888887}

:::MLPv0.5.0 ssd 1541710890.545740604 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 132, "value": 0.023466666666666663}

:::MLPv0.5.0 ssd 1541710890.640651464 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 133, "value": 0.02364444444444444}

:::MLPv0.5.0 ssd 1541710890.736563921 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 134, "value": 0.023822222222222217}

:::MLPv0.5.0 ssd 1541710890.833635807 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 135, "value": 0.023999999999999994}

:::MLPv0.5.0 ssd 1541710890.937144518 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 136, "value": 0.02417777777777777}

:::MLPv0.5.0 ssd 1541710891.034316063 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 137, "value": 0.024355555555555547}

:::MLPv0.5.0 ssd 1541710891.130083084 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 138, "value": 0.024533333333333324}

:::MLPv0.5.0 ssd 1541710891.226252794 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 139, "value": 0.0247111111111111}

:::MLPv0.5.0 ssd 1541710891.323683023 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 140, "value": 0.024888888888888877}
Iteration:    140, Loss function: 8.516, Average Loss: 1.703, avg. samples / sec: 20813.97
Iteration:    140, Loss function: 8.768, Average Loss: 1.708, avg. samples / sec: 20815.43
Iteration:    140, Loss function: 8.573, Average Loss: 1.707, avg. samples / sec: 20808.31
Iteration:    140, Loss function: 9.226, Average Loss: 1.709, avg. samples / sec: 20803.06
Iteration:    140, Loss function: 8.920, Average Loss: 1.705, avg. samples / sec: 20804.55
Iteration:    140, Loss function: 8.618, Average Loss: 1.703, avg. samples / sec: 20803.98
Iteration:    140, Loss function: 8.610, Average Loss: 1.712, avg. samples / sec: 20806.88
Iteration:    140, Loss function: 8.574, Average Loss: 1.704, avg. samples / sec: 20786.35

:::MLPv0.5.0 ssd 1541710891.419071674 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 141, "value": 0.025066666666666654}

:::MLPv0.5.0 ssd 1541710891.518739462 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 142, "value": 0.02524444444444443}

:::MLPv0.5.0 ssd 1541710891.622120142 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 143, "value": 0.025422222222222207}

:::MLPv0.5.0 ssd 1541710891.721450329 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 144, "value": 0.025599999999999984}

:::MLPv0.5.0 ssd 1541710891.819676876 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 145, "value": 0.02577777777777779}

:::MLPv0.5.0 ssd 1541710891.914827108 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 146, "value": 0.025955555555555565}

:::MLPv0.5.0 ssd 1541710892.011578798 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 147, "value": 0.026133333333333342}

:::MLPv0.5.0 ssd 1541710892.107454538 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 148, "value": 0.02631111111111112}

:::MLPv0.5.0 ssd 1541710892.202267885 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 149, "value": 0.026488888888888895}

:::MLPv0.5.0 ssd 1541710892.297687292 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 150, "value": 0.026666666666666672}

:::MLPv0.5.0 ssd 1541710892.396019936 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 151, "value": 0.02684444444444445}

:::MLPv0.5.0 ssd 1541710892.495300531 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 152, "value": 0.027022222222222225}

:::MLPv0.5.0 ssd 1541710892.590557814 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 153, "value": 0.027200000000000002}

:::MLPv0.5.0 ssd 1541710892.691408396 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 154, "value": 0.02737777777777778}

:::MLPv0.5.0 ssd 1541710892.787167549 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 155, "value": 0.027555555555555555}

:::MLPv0.5.0 ssd 1541710892.882796764 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 156, "value": 0.027733333333333332}

:::MLPv0.5.0 ssd 1541710892.979339838 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 157, "value": 0.02791111111111111}

:::MLPv0.5.0 ssd 1541710893.077308178 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 158, "value": 0.028088888888888885}

:::MLPv0.5.0 ssd 1541710893.175257206 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 159, "value": 0.028266666666666662}

:::MLPv0.5.0 ssd 1541710893.273197651 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 160, "value": 0.02844444444444444}
Iteration:    160, Loss function: 8.200, Average Loss: 1.837, avg. samples / sec: 21009.91
Iteration:    160, Loss function: 7.894, Average Loss: 1.843, avg. samples / sec: 21010.94
Iteration:    160, Loss function: 8.807, Average Loss: 1.840, avg. samples / sec: 21018.39
Iteration:    160, Loss function: 8.361, Average Loss: 1.846, avg. samples / sec: 21011.06
Iteration:    160, Loss function: 8.137, Average Loss: 1.841, avg. samples / sec: 21034.80
Iteration:    160, Loss function: 8.785, Average Loss: 1.843, avg. samples / sec: 21007.71
Iteration:    160, Loss function: 8.583, Average Loss: 1.847, avg. samples / sec: 21018.75
Iteration:    160, Loss function: 7.883, Average Loss: 1.843, avg. samples / sec: 21008.29

:::MLPv0.5.0 ssd 1541710893.370354652 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 161, "value": 0.028622222222222216}

:::MLPv0.5.0 ssd 1541710893.467111111 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 162, "value": 0.028799999999999992}

:::MLPv0.5.0 ssd 1541710893.566114664 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 163, "value": 0.02897777777777777}

:::MLPv0.5.0 ssd 1541710893.661400080 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 164, "value": 0.029155555555555546}

:::MLPv0.5.0 ssd 1541710893.757077932 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 165, "value": 0.029333333333333322}

:::MLPv0.5.0 ssd 1541710893.853227854 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 166, "value": 0.0295111111111111}

:::MLPv0.5.0 ssd 1541710893.948895931 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 167, "value": 0.029688888888888876}

:::MLPv0.5.0 ssd 1541710894.043828249 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 168, "value": 0.029866666666666652}

:::MLPv0.5.0 ssd 1541710894.138026714 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 169, "value": 0.03004444444444443}

:::MLPv0.5.0 ssd 1541710894.234224558 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 170, "value": 0.030222222222222206}

:::MLPv0.5.0 ssd 1541710894.329711199 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 171, "value": 0.03040000000000001}

:::MLPv0.5.0 ssd 1541710894.425981522 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 172, "value": 0.030577777777777787}

:::MLPv0.5.0 ssd 1541710894.523180246 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 173, "value": 0.030755555555555564}

:::MLPv0.5.0 ssd 1541710894.617511272 (train.py:553) train_epoch: 3

:::MLPv0.5.0 ssd 1541710894.622771502 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 174, "value": 0.03093333333333334}

:::MLPv0.5.0 ssd 1541710894.720479250 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 175, "value": 0.031111111111111117}

:::MLPv0.5.0 ssd 1541710894.819171190 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 176, "value": 0.031288888888888894}

:::MLPv0.5.0 ssd 1541710894.913410902 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 177, "value": 0.03146666666666667}

:::MLPv0.5.0 ssd 1541710895.009992361 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 178, "value": 0.03164444444444445}

:::MLPv0.5.0 ssd 1541710895.105843544 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 179, "value": 0.031822222222222224}

:::MLPv0.5.0 ssd 1541710895.202407122 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 180, "value": 0.032}
Iteration:    180, Loss function: 8.202, Average Loss: 1.968, avg. samples / sec: 21240.53
Iteration:    180, Loss function: 7.787, Average Loss: 1.972, avg. samples / sec: 21228.90
Iteration:    180, Loss function: 8.389, Average Loss: 1.964, avg. samples / sec: 21227.82
Iteration:    180, Loss function: 8.456, Average Loss: 1.974, avg. samples / sec: 21231.12
Iteration:    180, Loss function: 7.864, Average Loss: 1.966, avg. samples / sec: 21230.91
Iteration:    180, Loss function: 7.961, Average Loss: 1.974, avg. samples / sec: 21231.10
Iteration:    180, Loss function: 8.416, Average Loss: 1.965, avg. samples / sec: 21221.85
Iteration:    180, Loss function: 7.618, Average Loss: 1.966, avg. samples / sec: 21230.34

:::MLPv0.5.0 ssd 1541710895.298399925 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 181, "value": 0.03217777777777778}

:::MLPv0.5.0 ssd 1541710895.393211603 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 182, "value": 0.032355555555555554}

:::MLPv0.5.0 ssd 1541710895.488506317 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 183, "value": 0.03253333333333333}

:::MLPv0.5.0 ssd 1541710895.586228371 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 184, "value": 0.03271111111111111}

:::MLPv0.5.0 ssd 1541710895.683440447 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 185, "value": 0.032888888888888884}

:::MLPv0.5.0 ssd 1541710895.779440880 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 186, "value": 0.03306666666666666}

:::MLPv0.5.0 ssd 1541710895.874334097 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 187, "value": 0.03324444444444444}

:::MLPv0.5.0 ssd 1541710895.968549013 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 188, "value": 0.033422222222222214}

:::MLPv0.5.0 ssd 1541710896.069353580 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 189, "value": 0.03359999999999999}

:::MLPv0.5.0 ssd 1541710896.167482853 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 190, "value": 0.03377777777777777}

:::MLPv0.5.0 ssd 1541710896.266978979 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 191, "value": 0.033955555555555544}

:::MLPv0.5.0 ssd 1541710896.363371849 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 192, "value": 0.03413333333333332}

:::MLPv0.5.0 ssd 1541710896.460624218 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 193, "value": 0.0343111111111111}

:::MLPv0.5.0 ssd 1541710896.555797577 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 194, "value": 0.034488888888888874}

:::MLPv0.5.0 ssd 1541710896.658670425 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 195, "value": 0.03466666666666665}

:::MLPv0.5.0 ssd 1541710896.759209871 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 196, "value": 0.03484444444444443}

:::MLPv0.5.0 ssd 1541710896.855309725 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 197, "value": 0.03502222222222222}

:::MLPv0.5.0 ssd 1541710896.949539661 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 198, "value": 0.035199999999999995}

:::MLPv0.5.0 ssd 1541710897.044477463 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 199, "value": 0.03537777777777777}

:::MLPv0.5.0 ssd 1541710897.139986753 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 200, "value": 0.03555555555555555}
Iteration:    200, Loss function: 8.277, Average Loss: 2.092, avg. samples / sec: 21140.18
Iteration:    200, Loss function: 8.869, Average Loss: 2.088, avg. samples / sec: 21145.76
Iteration:    200, Loss function: 7.910, Average Loss: 2.088, avg. samples / sec: 21143.56
Iteration:    200, Loss function: 8.204, Average Loss: 2.094, avg. samples / sec: 21140.49
Iteration:    200, Loss function: 8.258, Average Loss: 2.088, avg. samples / sec: 21146.46
Iteration:    200, Loss function: 8.643, Average Loss: 2.089, avg. samples / sec: 21144.26
Iteration:    200, Loss function: 7.997, Average Loss: 2.097, avg. samples / sec: 21134.10
Iteration:    200, Loss function: 8.021, Average Loss: 2.096, avg. samples / sec: 21134.33

:::MLPv0.5.0 ssd 1541710897.235170126 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 201, "value": 0.035733333333333325}

:::MLPv0.5.0 ssd 1541710897.331794262 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 202, "value": 0.0359111111111111}

:::MLPv0.5.0 ssd 1541710897.432866096 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 203, "value": 0.03608888888888889}

:::MLPv0.5.0 ssd 1541710897.528980017 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 204, "value": 0.03626666666666667}

:::MLPv0.5.0 ssd 1541710897.622475863 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 205, "value": 0.036444444444444446}

:::MLPv0.5.0 ssd 1541710897.719218016 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 206, "value": 0.03662222222222222}

:::MLPv0.5.0 ssd 1541710897.816054344 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 207, "value": 0.0368}

:::MLPv0.5.0 ssd 1541710897.915254593 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 208, "value": 0.036977777777777776}

:::MLPv0.5.0 ssd 1541710898.012357235 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 209, "value": 0.03715555555555555}

:::MLPv0.5.0 ssd 1541710898.107764721 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 210, "value": 0.03733333333333333}

:::MLPv0.5.0 ssd 1541710898.206243753 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 211, "value": 0.037511111111111106}

:::MLPv0.5.0 ssd 1541710898.308723927 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 212, "value": 0.03768888888888888}

:::MLPv0.5.0 ssd 1541710898.404026508 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 213, "value": 0.03786666666666666}

:::MLPv0.5.0 ssd 1541710898.501790047 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 214, "value": 0.038044444444444436}

:::MLPv0.5.0 ssd 1541710898.598208189 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 215, "value": 0.03822222222222221}

:::MLPv0.5.0 ssd 1541710898.694081068 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 216, "value": 0.038400000000000004}

:::MLPv0.5.0 ssd 1541710898.788737059 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 217, "value": 0.03857777777777778}

:::MLPv0.5.0 ssd 1541710898.891764879 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 218, "value": 0.03875555555555556}

:::MLPv0.5.0 ssd 1541710898.992781639 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 219, "value": 0.038933333333333334}

:::MLPv0.5.0 ssd 1541710899.088243246 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 220, "value": 0.03911111111111111}
Iteration:    220, Loss function: 7.553, Average Loss: 2.206, avg. samples / sec: 21022.83
Iteration:    220, Loss function: 7.701, Average Loss: 2.210, avg. samples / sec: 21025.94
Iteration:    220, Loss function: 7.462, Average Loss: 2.205, avg. samples / sec: 21023.54
Iteration:    220, Loss function: 7.536, Average Loss: 2.207, avg. samples / sec: 21017.50
Iteration:    220, Loss function: 7.808, Average Loss: 2.211, avg. samples / sec: 21034.28
Iteration:    220, Loss function: 7.862, Average Loss: 2.205, avg. samples / sec: 21025.92
Iteration:    220, Loss function: 7.774, Average Loss: 2.211, avg. samples / sec: 21026.14
Iteration:    220, Loss function: 7.673, Average Loss: 2.202, avg. samples / sec: 21017.51

:::MLPv0.5.0 ssd 1541710899.182460308 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 221, "value": 0.03928888888888889}

:::MLPv0.5.0 ssd 1541710899.277294397 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 222, "value": 0.039466666666666664}

:::MLPv0.5.0 ssd 1541710899.374735594 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 223, "value": 0.03964444444444444}

:::MLPv0.5.0 ssd 1541710899.471323967 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 224, "value": 0.03982222222222222}

:::MLPv0.5.0 ssd 1541710899.566821098 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 225, "value": 0.039999999999999994}

:::MLPv0.5.0 ssd 1541710899.662893534 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 226, "value": 0.04017777777777777}

:::MLPv0.5.0 ssd 1541710899.757525444 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 227, "value": 0.04035555555555555}

:::MLPv0.5.0 ssd 1541710899.852409601 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 228, "value": 0.04053333333333334}

:::MLPv0.5.0 ssd 1541710899.949689388 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 229, "value": 0.040711111111111115}

:::MLPv0.5.0 ssd 1541710900.046004057 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 230, "value": 0.04088888888888889}

:::MLPv0.5.0 ssd 1541710900.145615578 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 231, "value": 0.04106666666666667}

:::MLPv0.5.0 ssd 1541710900.238996506 (train.py:553) train_epoch: 4

:::MLPv0.5.0 ssd 1541710900.244413137 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 232, "value": 0.041244444444444445}

:::MLPv0.5.0 ssd 1541710900.338856936 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 233, "value": 0.04142222222222222}

:::MLPv0.5.0 ssd 1541710900.433509350 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 234, "value": 0.0416}

:::MLPv0.5.0 ssd 1541710900.531265974 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 235, "value": 0.041777777777777775}

:::MLPv0.5.0 ssd 1541710900.631549358 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 236, "value": 0.04195555555555555}

:::MLPv0.5.0 ssd 1541710900.726286888 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 237, "value": 0.04213333333333333}

:::MLPv0.5.0 ssd 1541710900.822762251 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 238, "value": 0.042311111111111105}

:::MLPv0.5.0 ssd 1541710900.917324543 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 239, "value": 0.04248888888888888}

:::MLPv0.5.0 ssd 1541710901.015825033 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 240, "value": 0.04266666666666666}
Iteration:    240, Loss function: 7.204, Average Loss: 2.314, avg. samples / sec: 21257.64
Iteration:    240, Loss function: 7.579, Average Loss: 2.311, avg. samples / sec: 21246.83
Iteration:    240, Loss function: 7.428, Average Loss: 2.311, avg. samples / sec: 21255.34
Iteration:    240, Loss function: 7.844, Average Loss: 2.312, avg. samples / sec: 21256.62
Iteration:    240, Loss function: 7.612, Average Loss: 2.317, avg. samples / sec: 21253.85
Iteration:    240, Loss function: 7.954, Average Loss: 2.314, avg. samples / sec: 21246.31
Iteration:    240, Loss function: 8.000, Average Loss: 2.316, avg. samples / sec: 21248.76
Iteration:    240, Loss function: 8.009, Average Loss: 2.311, avg. samples / sec: 21248.10

:::MLPv0.5.0 ssd 1541710901.115405560 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 241, "value": 0.04284444444444445}

:::MLPv0.5.0 ssd 1541710901.210017204 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 242, "value": 0.043022222222222226}

:::MLPv0.5.0 ssd 1541710901.305290222 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 243, "value": 0.0432}

:::MLPv0.5.0 ssd 1541710901.400324583 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 244, "value": 0.04337777777777778}

:::MLPv0.5.0 ssd 1541710901.495771170 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 245, "value": 0.043555555555555556}

:::MLPv0.5.0 ssd 1541710901.589629889 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 246, "value": 0.04373333333333333}

:::MLPv0.5.0 ssd 1541710901.684286356 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 247, "value": 0.04391111111111111}

:::MLPv0.5.0 ssd 1541710901.780257463 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 248, "value": 0.044088888888888886}

:::MLPv0.5.0 ssd 1541710901.877378702 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 249, "value": 0.04426666666666666}

:::MLPv0.5.0 ssd 1541710901.973265886 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 250, "value": 0.04444444444444444}

:::MLPv0.5.0 ssd 1541710902.067945957 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 251, "value": 0.044622222222222216}

:::MLPv0.5.0 ssd 1541710902.164710760 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 252, "value": 0.04479999999999999}

:::MLPv0.5.0 ssd 1541710902.259386301 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 253, "value": 0.04497777777777777}

:::MLPv0.5.0 ssd 1541710902.355964899 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 254, "value": 0.04515555555555556}

:::MLPv0.5.0 ssd 1541710902.454231977 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 255, "value": 0.04533333333333334}

:::MLPv0.5.0 ssd 1541710902.552056551 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 256, "value": 0.04551111111111111}

:::MLPv0.5.0 ssd 1541710902.646777630 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 257, "value": 0.04568888888888889}

:::MLPv0.5.0 ssd 1541710902.741445303 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 258, "value": 0.04586666666666667}

:::MLPv0.5.0 ssd 1541710902.836756945 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 259, "value": 0.04604444444444444}

:::MLPv0.5.0 ssd 1541710902.932119846 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 260, "value": 0.04622222222222222}
Iteration:    260, Loss function: 7.660, Average Loss: 2.416, avg. samples / sec: 21384.98
Iteration:    260, Loss function: 7.126, Average Loss: 2.416, avg. samples / sec: 21372.66
Iteration:    260, Loss function: 7.283, Average Loss: 2.419, avg. samples / sec: 21381.12
Iteration:    260, Loss function: 7.199, Average Loss: 2.412, avg. samples / sec: 21378.95
Iteration:    260, Loss function: 7.083, Average Loss: 2.415, avg. samples / sec: 21375.88
Iteration:    260, Loss function: 7.143, Average Loss: 2.416, avg. samples / sec: 21372.00
Iteration:    260, Loss function: 7.426, Average Loss: 2.418, avg. samples / sec: 21372.47
Iteration:    260, Loss function: 7.310, Average Loss: 2.415, avg. samples / sec: 21366.46

:::MLPv0.5.0 ssd 1541710903.029428005 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 261, "value": 0.0464}

:::MLPv0.5.0 ssd 1541710903.124703169 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 262, "value": 0.046577777777777774}

:::MLPv0.5.0 ssd 1541710903.222360373 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 263, "value": 0.04675555555555555}

:::MLPv0.5.0 ssd 1541710903.319324493 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 264, "value": 0.04693333333333333}

:::MLPv0.5.0 ssd 1541710903.415690660 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 265, "value": 0.047111111111111104}

:::MLPv0.5.0 ssd 1541710903.512580395 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 266, "value": 0.04728888888888888}

:::MLPv0.5.0 ssd 1541710903.606869936 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 267, "value": 0.04746666666666667}

:::MLPv0.5.0 ssd 1541710903.703386784 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 268, "value": 0.04764444444444445}

:::MLPv0.5.0 ssd 1541710903.805615664 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 269, "value": 0.047822222222222224}

:::MLPv0.5.0 ssd 1541710903.900250196 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 270, "value": 0.048}

:::MLPv0.5.0 ssd 1541710903.995742798 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 271, "value": 0.04817777777777778}

:::MLPv0.5.0 ssd 1541710904.089518547 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 272, "value": 0.048355555555555554}

:::MLPv0.5.0 ssd 1541710904.183562517 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 273, "value": 0.04853333333333333}

:::MLPv0.5.0 ssd 1541710904.282220364 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 274, "value": 0.04871111111111111}

:::MLPv0.5.0 ssd 1541710904.376876116 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 275, "value": 0.048888888888888885}

:::MLPv0.5.0 ssd 1541710904.469890118 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 276, "value": 0.04906666666666666}

:::MLPv0.5.0 ssd 1541710904.563000917 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 277, "value": 0.04924444444444444}

:::MLPv0.5.0 ssd 1541710904.658096313 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 278, "value": 0.049422222222222215}

:::MLPv0.5.0 ssd 1541710904.755166054 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 279, "value": 0.04959999999999999}

:::MLPv0.5.0 ssd 1541710904.853466511 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 280, "value": 0.04977777777777778}
Iteration:    280, Loss function: 7.192, Average Loss: 2.512, avg. samples / sec: 21324.94
Iteration:    280, Loss function: 6.927, Average Loss: 2.510, avg. samples / sec: 21327.35
Iteration:    280, Loss function: 7.052, Average Loss: 2.513, avg. samples / sec: 21318.17
Iteration:    280, Loss function: 6.624, Average Loss: 2.511, avg. samples / sec: 21330.37
Iteration:    280, Loss function: 7.638, Average Loss: 2.518, avg. samples / sec: 21316.07
Iteration:    280, Loss function: 6.898, Average Loss: 2.509, avg. samples / sec: 21317.78
Iteration:    280, Loss function: 7.252, Average Loss: 2.511, avg. samples / sec: 21310.47
Iteration:    280, Loss function: 7.229, Average Loss: 2.512, avg. samples / sec: 21299.43

:::MLPv0.5.0 ssd 1541710904.949229240 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 281, "value": 0.04995555555555556}

:::MLPv0.5.0 ssd 1541710905.045123339 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 282, "value": 0.050133333333333335}

:::MLPv0.5.0 ssd 1541710905.140434980 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 283, "value": 0.05031111111111111}

:::MLPv0.5.0 ssd 1541710905.247581720 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 284, "value": 0.05048888888888889}

:::MLPv0.5.0 ssd 1541710905.342902899 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 285, "value": 0.050666666666666665}

:::MLPv0.5.0 ssd 1541710905.438987255 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 286, "value": 0.05084444444444444}

:::MLPv0.5.0 ssd 1541710905.536745310 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 287, "value": 0.05102222222222222}

:::MLPv0.5.0 ssd 1541710905.634204388 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 288, "value": 0.051199999999999996}

:::MLPv0.5.0 ssd 1541710905.724544048 (train.py:553) train_epoch: 5

:::MLPv0.5.0 ssd 1541710905.729832172 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 289, "value": 0.05137777777777777}

:::MLPv0.5.0 ssd 1541710905.823833942 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 290, "value": 0.05155555555555555}

:::MLPv0.5.0 ssd 1541710905.919760704 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 291, "value": 0.051733333333333326}

:::MLPv0.5.0 ssd 1541710906.013708591 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 292, "value": 0.0519111111111111}

:::MLPv0.5.0 ssd 1541710906.107743263 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 293, "value": 0.05208888888888889}

:::MLPv0.5.0 ssd 1541710906.201747179 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 294, "value": 0.05226666666666667}

:::MLPv0.5.0 ssd 1541710906.295973063 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 295, "value": 0.052444444444444446}

:::MLPv0.5.0 ssd 1541710906.394272804 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 296, "value": 0.05262222222222222}

:::MLPv0.5.0 ssd 1541710906.489817381 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 297, "value": 0.0528}

:::MLPv0.5.0 ssd 1541710906.584210396 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 298, "value": 0.052977777777777776}

:::MLPv0.5.0 ssd 1541710906.680299759 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 299, "value": 0.05315555555555555}

:::MLPv0.5.0 ssd 1541710906.777614355 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 300, "value": 0.05333333333333333}
Iteration:    300, Loss function: 7.296, Average Loss: 2.605, avg. samples / sec: 21290.75
Iteration:    300, Loss function: 6.949, Average Loss: 2.609, avg. samples / sec: 21284.82
Iteration:    300, Loss function: 7.251, Average Loss: 2.607, avg. samples / sec: 21312.85
Iteration:    300, Loss function: 7.100, Average Loss: 2.608, avg. samples / sec: 21283.26
Iteration:    300, Loss function: 7.255, Average Loss: 2.608, avg. samples / sec: 21286.24
Iteration:    300, Loss function: 7.317, Average Loss: 2.606, avg. samples / sec: 21288.08
Iteration:    300, Loss function: 7.274, Average Loss: 2.607, avg. samples / sec: 21280.12
Iteration:    300, Loss function: 7.147, Average Loss: 2.603, avg. samples / sec: 21286.96

:::MLPv0.5.0 ssd 1541710906.873223066 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 301, "value": 0.053511111111111107}

:::MLPv0.5.0 ssd 1541710906.967351198 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 302, "value": 0.05368888888888888}

:::MLPv0.5.0 ssd 1541710907.066437960 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 303, "value": 0.05386666666666666}

:::MLPv0.5.0 ssd 1541710907.161810398 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 304, "value": 0.05404444444444444}

:::MLPv0.5.0 ssd 1541710907.256410122 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 305, "value": 0.05422222222222223}

:::MLPv0.5.0 ssd 1541710907.369773388 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 306, "value": 0.054400000000000004}

:::MLPv0.5.0 ssd 1541710907.467845440 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 307, "value": 0.05457777777777778}

:::MLPv0.5.0 ssd 1541710907.562348366 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 308, "value": 0.05475555555555556}

:::MLPv0.5.0 ssd 1541710907.657625675 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 309, "value": 0.054933333333333334}

:::MLPv0.5.0 ssd 1541710907.754180193 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 310, "value": 0.05511111111111111}

:::MLPv0.5.0 ssd 1541710907.849116564 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 311, "value": 0.05528888888888889}

:::MLPv0.5.0 ssd 1541710907.947271824 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 312, "value": 0.055466666666666664}

:::MLPv0.5.0 ssd 1541710908.043612242 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 313, "value": 0.05564444444444444}

:::MLPv0.5.0 ssd 1541710908.137824059 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 314, "value": 0.05582222222222222}

:::MLPv0.5.0 ssd 1541710908.234071732 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 315, "value": 0.055999999999999994}

:::MLPv0.5.0 ssd 1541710908.330250740 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 316, "value": 0.05617777777777777}

:::MLPv0.5.0 ssd 1541710908.425856590 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 317, "value": 0.05635555555555555}

:::MLPv0.5.0 ssd 1541710908.522901773 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 318, "value": 0.05653333333333334}

:::MLPv0.5.0 ssd 1541710908.618637562 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 319, "value": 0.056711111111111115}

:::MLPv0.5.0 ssd 1541710908.713314533 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 320, "value": 0.05688888888888889}
Iteration:    320, Loss function: 6.992, Average Loss: 2.694, avg. samples / sec: 21163.92
Iteration:    320, Loss function: 6.522, Average Loss: 2.699, avg. samples / sec: 21161.12
Iteration:    320, Loss function: 7.305, Average Loss: 2.694, avg. samples / sec: 21151.87
Iteration:    320, Loss function: 7.335, Average Loss: 2.697, avg. samples / sec: 21162.81
Iteration:    320, Loss function: 6.948, Average Loss: 2.691, avg. samples / sec: 21162.08
Iteration:    320, Loss function: 6.928, Average Loss: 2.697, avg. samples / sec: 21160.84
Iteration:    320, Loss function: 7.416, Average Loss: 2.697, avg. samples / sec: 21158.37
Iteration:    320, Loss function: 6.504, Average Loss: 2.694, avg. samples / sec: 21155.04

:::MLPv0.5.0 ssd 1541710908.807326794 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 321, "value": 0.05706666666666667}

:::MLPv0.5.0 ssd 1541710908.905787945 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 322, "value": 0.057244444444444445}

:::MLPv0.5.0 ssd 1541710909.001164198 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 323, "value": 0.05742222222222222}

:::MLPv0.5.0 ssd 1541710909.095668554 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 324, "value": 0.0576}

:::MLPv0.5.0 ssd 1541710909.190949202 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 325, "value": 0.057777777777777775}

:::MLPv0.5.0 ssd 1541710909.284872770 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 326, "value": 0.05795555555555555}

:::MLPv0.5.0 ssd 1541710909.380496502 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 327, "value": 0.05813333333333333}

:::MLPv0.5.0 ssd 1541710909.477349043 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 328, "value": 0.058311111111111105}

:::MLPv0.5.0 ssd 1541710909.572448730 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 329, "value": 0.05848888888888888}

:::MLPv0.5.0 ssd 1541710909.666967869 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 330, "value": 0.05866666666666666}

:::MLPv0.5.0 ssd 1541710909.762722731 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 331, "value": 0.05884444444444445}

:::MLPv0.5.0 ssd 1541710909.863175154 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 332, "value": 0.059022222222222226}

:::MLPv0.5.0 ssd 1541710909.959490299 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 333, "value": 0.0592}

:::MLPv0.5.0 ssd 1541710910.054533720 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 334, "value": 0.05937777777777778}

:::MLPv0.5.0 ssd 1541710910.149320364 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 335, "value": 0.059555555555555556}

:::MLPv0.5.0 ssd 1541710910.245314360 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 336, "value": 0.05973333333333333}

:::MLPv0.5.0 ssd 1541710910.338884592 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 337, "value": 0.05991111111111111}

:::MLPv0.5.0 ssd 1541710910.434839487 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 338, "value": 0.060088888888888886}

:::MLPv0.5.0 ssd 1541710910.529240847 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 339, "value": 0.06026666666666666}

:::MLPv0.5.0 ssd 1541710910.623710632 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 340, "value": 0.06044444444444444}
Iteration:    340, Loss function: 7.295, Average Loss: 2.778, avg. samples / sec: 21447.60
Iteration:    340, Loss function: 7.082, Average Loss: 2.777, avg. samples / sec: 21443.72
Iteration:    340, Loss function: 6.921, Average Loss: 2.781, avg. samples / sec: 21449.61
Iteration:    340, Loss function: 7.301, Average Loss: 2.777, avg. samples / sec: 21443.78
Iteration:    340, Loss function: 6.643, Average Loss: 2.783, avg. samples / sec: 21440.73
Iteration:    340, Loss function: 6.974, Average Loss: 2.780, avg. samples / sec: 21441.44
Iteration:    340, Loss function: 7.185, Average Loss: 2.779, avg. samples / sec: 21438.98
Iteration:    340, Loss function: 6.859, Average Loss: 2.781, avg. samples / sec: 21427.84

:::MLPv0.5.0 ssd 1541710910.719183683 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 341, "value": 0.060622222222222216}

:::MLPv0.5.0 ssd 1541710910.814424753 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 342, "value": 0.06079999999999999}

:::MLPv0.5.0 ssd 1541710910.909559011 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 343, "value": 0.06097777777777777}

:::MLPv0.5.0 ssd 1541710911.003913879 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 344, "value": 0.06115555555555556}

:::MLPv0.5.0 ssd 1541710911.099243402 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 345, "value": 0.06133333333333334}

:::MLPv0.5.0 ssd 1541710911.195936203 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 346, "value": 0.061511111111111114}

:::MLPv0.5.0 ssd 1541710911.286682129 (train.py:553) train_epoch: 6

:::MLPv0.5.0 ssd 1541710911.292584419 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 347, "value": 0.06168888888888889}

:::MLPv0.5.0 ssd 1541710911.391567707 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 348, "value": 0.06186666666666667}

:::MLPv0.5.0 ssd 1541710911.486069918 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 349, "value": 0.062044444444444444}

:::MLPv0.5.0 ssd 1541710911.586723089 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 350, "value": 0.06222222222222222}

:::MLPv0.5.0 ssd 1541710911.681276083 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 351, "value": 0.0624}

:::MLPv0.5.0 ssd 1541710911.777579069 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 352, "value": 0.06257777777777777}

:::MLPv0.5.0 ssd 1541710911.873025417 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 353, "value": 0.06275555555555555}

:::MLPv0.5.0 ssd 1541710911.969432831 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 354, "value": 0.06293333333333333}

:::MLPv0.5.0 ssd 1541710912.067872524 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 355, "value": 0.0631111111111111}

:::MLPv0.5.0 ssd 1541710912.163323641 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 356, "value": 0.0632888888888889}

:::MLPv0.5.0 ssd 1541710912.258886576 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 357, "value": 0.06346666666666667}

:::MLPv0.5.0 ssd 1541710912.353242636 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 358, "value": 0.06364444444444445}

:::MLPv0.5.0 ssd 1541710912.446684599 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 359, "value": 0.06382222222222222}

:::MLPv0.5.0 ssd 1541710912.543581724 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 360, "value": 0.064}
Iteration:    360, Loss function: 6.812, Average Loss: 2.856, avg. samples / sec: 21338.01
Iteration:    360, Loss function: 6.666, Average Loss: 2.865, avg. samples / sec: 21338.04
Iteration:    360, Loss function: 6.459, Average Loss: 2.858, avg. samples / sec: 21340.26
Iteration:    360, Loss function: 6.621, Average Loss: 2.861, avg. samples / sec: 21335.08
Iteration:    360, Loss function: 7.251, Average Loss: 2.861, avg. samples / sec: 21351.93
Iteration:    360, Loss function: 6.491, Average Loss: 2.855, avg. samples / sec: 21334.69
Iteration:    360, Loss function: 6.239, Average Loss: 2.855, avg. samples / sec: 21327.75
Iteration:    360, Loss function: 6.136, Average Loss: 2.859, avg. samples / sec: 21330.65

:::MLPv0.5.0 ssd 1541710912.637840033 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 361, "value": 0.06417777777777778}

:::MLPv0.5.0 ssd 1541710912.732100964 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 362, "value": 0.06435555555555555}

:::MLPv0.5.0 ssd 1541710912.827395678 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 363, "value": 0.06453333333333333}

:::MLPv0.5.0 ssd 1541710912.923223495 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 364, "value": 0.06471111111111111}

:::MLPv0.5.0 ssd 1541710913.018180609 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 365, "value": 0.06488888888888888}

:::MLPv0.5.0 ssd 1541710913.112656832 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 366, "value": 0.06506666666666666}

:::MLPv0.5.0 ssd 1541710913.208727360 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 367, "value": 0.06524444444444444}

:::MLPv0.5.0 ssd 1541710913.303216934 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 368, "value": 0.06542222222222221}

:::MLPv0.5.0 ssd 1541710913.398218870 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 369, "value": 0.0656}

:::MLPv0.5.0 ssd 1541710913.492782354 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 370, "value": 0.06577777777777778}

:::MLPv0.5.0 ssd 1541710913.589034796 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 371, "value": 0.06595555555555556}

:::MLPv0.5.0 ssd 1541710913.684278488 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 372, "value": 0.06613333333333334}

:::MLPv0.5.0 ssd 1541710913.784488440 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 373, "value": 0.06631111111111111}

:::MLPv0.5.0 ssd 1541710913.878943205 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 374, "value": 0.06648888888888889}

:::MLPv0.5.0 ssd 1541710913.973221540 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 375, "value": 0.06666666666666667}

:::MLPv0.5.0 ssd 1541710914.067064762 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 376, "value": 0.06684444444444444}

:::MLPv0.5.0 ssd 1541710914.162891865 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 377, "value": 0.06702222222222222}

:::MLPv0.5.0 ssd 1541710914.257860422 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 378, "value": 0.0672}

:::MLPv0.5.0 ssd 1541710914.358049393 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 379, "value": 0.06737777777777777}

:::MLPv0.5.0 ssd 1541710914.453122616 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 380, "value": 0.06755555555555555}
Iteration:    380, Loss function: 6.888, Average Loss: 2.932, avg. samples / sec: 21450.84
Iteration:    380, Loss function: 6.769, Average Loss: 2.937, avg. samples / sec: 21455.20
Iteration:    380, Loss function: 6.064, Average Loss: 2.930, avg. samples / sec: 21459.54
Iteration:    380, Loss function: 6.831, Average Loss: 2.932, avg. samples / sec: 21449.52
Iteration:    380, Loss function: 6.623, Average Loss: 2.937, avg. samples / sec: 21443.15
Iteration:    380, Loss function: 6.993, Average Loss: 2.940, avg. samples / sec: 21441.55
Iteration:    380, Loss function: 6.512, Average Loss: 2.934, avg. samples / sec: 21440.35
Iteration:    380, Loss function: 6.825, Average Loss: 2.935, avg. samples / sec: 21457.05

:::MLPv0.5.0 ssd 1541710914.547260761 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 381, "value": 0.06773333333333333}

:::MLPv0.5.0 ssd 1541710914.640586853 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 382, "value": 0.06791111111111112}

:::MLPv0.5.0 ssd 1541710914.734117985 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 383, "value": 0.0680888888888889}

:::MLPv0.5.0 ssd 1541710914.828557491 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 384, "value": 0.06826666666666667}

:::MLPv0.5.0 ssd 1541710914.923632622 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 385, "value": 0.06844444444444445}

:::MLPv0.5.0 ssd 1541710915.019026518 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 386, "value": 0.06862222222222222}

:::MLPv0.5.0 ssd 1541710915.112853527 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 387, "value": 0.0688}

:::MLPv0.5.0 ssd 1541710915.207225800 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 388, "value": 0.06897777777777778}

:::MLPv0.5.0 ssd 1541710915.301817179 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 389, "value": 0.06915555555555555}

:::MLPv0.5.0 ssd 1541710915.395304918 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 390, "value": 0.06933333333333333}

:::MLPv0.5.0 ssd 1541710915.490399122 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 391, "value": 0.0695111111111111}

:::MLPv0.5.0 ssd 1541710915.585126877 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 392, "value": 0.06968888888888888}

:::MLPv0.5.0 ssd 1541710915.679894924 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 393, "value": 0.06986666666666666}

:::MLPv0.5.0 ssd 1541710915.777277231 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 394, "value": 0.07004444444444444}

:::MLPv0.5.0 ssd 1541710915.872254133 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 395, "value": 0.07022222222222223}

:::MLPv0.5.0 ssd 1541710915.966607332 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 396, "value": 0.0704}

:::MLPv0.5.0 ssd 1541710916.063151598 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 397, "value": 0.07057777777777778}

:::MLPv0.5.0 ssd 1541710916.158179522 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 398, "value": 0.07075555555555556}

:::MLPv0.5.0 ssd 1541710916.252129555 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 399, "value": 0.07093333333333333}

:::MLPv0.5.0 ssd 1541710916.345397472 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 400, "value": 0.07111111111111111}
Iteration:    400, Loss function: 6.557, Average Loss: 3.010, avg. samples / sec: 21648.57
Iteration:    400, Loss function: 6.373, Average Loss: 3.008, avg. samples / sec: 21656.38
Iteration:    400, Loss function: 6.874, Average Loss: 3.004, avg. samples / sec: 21648.17
Iteration:    400, Loss function: 6.387, Average Loss: 3.010, avg. samples / sec: 21654.99
Iteration:    400, Loss function: 6.705, Average Loss: 3.003, avg. samples / sec: 21636.49
Iteration:    400, Loss function: 6.627, Average Loss: 3.000, avg. samples / sec: 21643.24
Iteration:    400, Loss function: 6.418, Average Loss: 3.005, avg. samples / sec: 21650.27
Iteration:    400, Loss function: 6.629, Average Loss: 3.007, avg. samples / sec: 21647.11

:::MLPv0.5.0 ssd 1541710916.439924955 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 401, "value": 0.07128888888888889}

:::MLPv0.5.0 ssd 1541710916.537491560 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 402, "value": 0.07146666666666666}

:::MLPv0.5.0 ssd 1541710916.630855322 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 403, "value": 0.07164444444444444}

:::MLPv0.5.0 ssd 1541710916.725738525 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 404, "value": 0.07182222222222222}

:::MLPv0.5.0 ssd 1541710916.816262484 (train.py:553) train_epoch: 7

:::MLPv0.5.0 ssd 1541710916.822424412 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 405, "value": 0.072}

:::MLPv0.5.0 ssd 1541710916.917545795 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 406, "value": 0.07217777777777777}

:::MLPv0.5.0 ssd 1541710917.014759064 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 407, "value": 0.07235555555555555}

:::MLPv0.5.0 ssd 1541710917.109589100 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 408, "value": 0.07253333333333334}

:::MLPv0.5.0 ssd 1541710917.206693888 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 409, "value": 0.07271111111111112}

:::MLPv0.5.0 ssd 1541710917.307358980 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 410, "value": 0.07288888888888889}

:::MLPv0.5.0 ssd 1541710917.405028820 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 411, "value": 0.07306666666666667}

:::MLPv0.5.0 ssd 1541710917.501361847 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 412, "value": 0.07324444444444445}

:::MLPv0.5.0 ssd 1541710917.595704556 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 413, "value": 0.07342222222222222}

:::MLPv0.5.0 ssd 1541710917.698356152 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 414, "value": 0.0736}

:::MLPv0.5.0 ssd 1541710917.794785738 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 415, "value": 0.07377777777777778}

:::MLPv0.5.0 ssd 1541710917.889110088 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 416, "value": 0.07395555555555555}

:::MLPv0.5.0 ssd 1541710917.985048532 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 417, "value": 0.07413333333333333}

:::MLPv0.5.0 ssd 1541710918.079817772 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 418, "value": 0.0743111111111111}

:::MLPv0.5.0 ssd 1541710918.179712296 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 419, "value": 0.07448888888888888}

:::MLPv0.5.0 ssd 1541710918.274091244 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 420, "value": 0.07466666666666666}
Iteration:    420, Loss function: 6.204, Average Loss: 3.083, avg. samples / sec: 21244.14
Iteration:    420, Loss function: 6.324, Average Loss: 3.083, avg. samples / sec: 21236.02
Iteration:    420, Loss function: 6.076, Average Loss: 3.083, avg. samples / sec: 21242.55
Iteration:    420, Loss function: 6.003, Average Loss: 3.081, avg. samples / sec: 21248.56
Iteration:    420, Loss function: 6.546, Average Loss: 3.075, avg. samples / sec: 21240.67
Iteration:    420, Loss function: 6.396, Average Loss: 3.078, avg. samples / sec: 21240.50
Iteration:    420, Loss function: 6.785, Average Loss: 3.076, avg. samples / sec: 21236.53
Iteration:    420, Loss function: 6.645, Average Loss: 3.080, avg. samples / sec: 21240.44

:::MLPv0.5.0 ssd 1541710918.368467569 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 421, "value": 0.07484444444444445}

:::MLPv0.5.0 ssd 1541710918.462711334 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 422, "value": 0.07502222222222223}

:::MLPv0.5.0 ssd 1541710918.561383963 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 423, "value": 0.0752}

:::MLPv0.5.0 ssd 1541710918.657398939 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 424, "value": 0.07537777777777778}

:::MLPv0.5.0 ssd 1541710918.752972126 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 425, "value": 0.07555555555555556}

:::MLPv0.5.0 ssd 1541710918.851332664 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 426, "value": 0.07573333333333333}

:::MLPv0.5.0 ssd 1541710918.946953297 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 427, "value": 0.07591111111111111}

:::MLPv0.5.0 ssd 1541710919.041664839 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 428, "value": 0.07608888888888889}

:::MLPv0.5.0 ssd 1541710919.135224342 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 429, "value": 0.07626666666666666}

:::MLPv0.5.0 ssd 1541710919.230765343 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 430, "value": 0.07644444444444444}

:::MLPv0.5.0 ssd 1541710919.326145172 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 431, "value": 0.07662222222222222}

:::MLPv0.5.0 ssd 1541710919.420090675 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 432, "value": 0.0768}

:::MLPv0.5.0 ssd 1541710919.514272928 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 433, "value": 0.07697777777777778}

:::MLPv0.5.0 ssd 1541710919.608098030 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 434, "value": 0.07715555555555556}

:::MLPv0.5.0 ssd 1541710919.701710939 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 435, "value": 0.07733333333333334}

:::MLPv0.5.0 ssd 1541710919.797307491 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 436, "value": 0.07751111111111111}

:::MLPv0.5.0 ssd 1541710919.892171383 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 437, "value": 0.07768888888888889}

:::MLPv0.5.0 ssd 1541710919.988163233 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 438, "value": 0.07786666666666667}

:::MLPv0.5.0 ssd 1541710920.081743002 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 439, "value": 0.07804444444444444}

:::MLPv0.5.0 ssd 1541710920.174929857 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 440, "value": 0.07822222222222222}
Iteration:    440, Loss function: 5.676, Average Loss: 3.146, avg. samples / sec: 21547.13
Iteration:    440, Loss function: 5.762, Average Loss: 3.144, avg. samples / sec: 21542.64
Iteration:    440, Loss function: 5.721, Average Loss: 3.138, avg. samples / sec: 21551.50
Iteration:    440, Loss function: 6.212, Average Loss: 3.137, avg. samples / sec: 21546.02
Iteration:    440, Loss function: 6.266, Average Loss: 3.144, avg. samples / sec: 21548.01
Iteration:    440, Loss function: 6.272, Average Loss: 3.145, avg. samples / sec: 21542.66
Iteration:    440, Loss function: 6.438, Average Loss: 3.138, avg. samples / sec: 21544.90
Iteration:    440, Loss function: 6.050, Average Loss: 3.147, avg. samples / sec: 21523.88

:::MLPv0.5.0 ssd 1541710920.271077156 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 441, "value": 0.0784}

:::MLPv0.5.0 ssd 1541710920.369385004 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 442, "value": 0.07857777777777777}

:::MLPv0.5.0 ssd 1541710920.463124990 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 443, "value": 0.07875555555555555}

:::MLPv0.5.0 ssd 1541710920.560069323 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 444, "value": 0.07893333333333333}

:::MLPv0.5.0 ssd 1541710920.655917168 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 445, "value": 0.0791111111111111}

:::MLPv0.5.0 ssd 1541710920.751022577 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 446, "value": 0.0792888888888889}

:::MLPv0.5.0 ssd 1541710920.844936132 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 447, "value": 0.07946666666666667}

:::MLPv0.5.0 ssd 1541710920.939144373 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 448, "value": 0.07964444444444445}

:::MLPv0.5.0 ssd 1541710921.034063816 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 449, "value": 0.07982222222222222}

:::MLPv0.5.0 ssd 1541710921.128984213 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 450, "value": 0.08}

:::MLPv0.5.0 ssd 1541710921.223722935 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 451, "value": 0.08017777777777778}

:::MLPv0.5.0 ssd 1541710921.318130732 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 452, "value": 0.08035555555555556}

:::MLPv0.5.0 ssd 1541710921.415162325 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 453, "value": 0.08053333333333333}

:::MLPv0.5.0 ssd 1541710921.509638071 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 454, "value": 0.08071111111111111}

:::MLPv0.5.0 ssd 1541710921.603786230 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 455, "value": 0.08088888888888889}

:::MLPv0.5.0 ssd 1541710921.702732563 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 456, "value": 0.08106666666666666}

:::MLPv0.5.0 ssd 1541710921.798187017 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 457, "value": 0.08124444444444444}

:::MLPv0.5.0 ssd 1541710921.892644882 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 458, "value": 0.08142222222222222}

:::MLPv0.5.0 ssd 1541710921.989078522 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 459, "value": 0.0816}

:::MLPv0.5.0 ssd 1541710922.083372355 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 460, "value": 0.08177777777777778}
Iteration:    460, Loss function: 6.022, Average Loss: 3.205, avg. samples / sec: 21486.29
Iteration:    460, Loss function: 6.176, Average Loss: 3.204, avg. samples / sec: 21469.24
Iteration:    460, Loss function: 5.732, Average Loss: 3.197, avg. samples / sec: 21467.04
Iteration:    460, Loss function: 6.142, Average Loss: 3.204, avg. samples / sec: 21459.17
Iteration:    460, Loss function: 6.002, Average Loss: 3.195, avg. samples / sec: 21469.04
Iteration:    460, Loss function: 5.629, Average Loss: 3.204, avg. samples / sec: 21456.74
Iteration:    460, Loss function: 5.912, Average Loss: 3.204, avg. samples / sec: 21461.74
Iteration:    460, Loss function: 6.105, Average Loss: 3.198, avg. samples / sec: 21457.41

:::MLPv0.5.0 ssd 1541710922.181021929 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 461, "value": 0.08195555555555556}

:::MLPv0.5.0 ssd 1541710922.275242567 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 462, "value": 0.08213333333333334}

:::MLPv0.5.0 ssd 1541710922.365885735 (train.py:553) train_epoch: 8

:::MLPv0.5.0 ssd 1541710922.371203899 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 463, "value": 0.08231111111111111}

:::MLPv0.5.0 ssd 1541710922.481229305 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 464, "value": 0.08248888888888889}

:::MLPv0.5.0 ssd 1541710922.582158804 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 465, "value": 0.08266666666666667}

:::MLPv0.5.0 ssd 1541710922.676471233 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 466, "value": 0.08284444444444444}

:::MLPv0.5.0 ssd 1541710922.771167278 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 467, "value": 0.08302222222222222}

:::MLPv0.5.0 ssd 1541710922.864275455 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 468, "value": 0.0832}

:::MLPv0.5.0 ssd 1541710922.960739851 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 469, "value": 0.08337777777777777}

:::MLPv0.5.0 ssd 1541710923.058585644 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 470, "value": 0.08355555555555555}

:::MLPv0.5.0 ssd 1541710923.155121088 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 471, "value": 0.08373333333333333}

:::MLPv0.5.0 ssd 1541710923.254228830 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 472, "value": 0.08391111111111112}

:::MLPv0.5.0 ssd 1541710923.348273993 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 473, "value": 0.0840888888888889}

:::MLPv0.5.0 ssd 1541710923.443057060 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 474, "value": 0.08426666666666667}

:::MLPv0.5.0 ssd 1541710923.538448811 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 475, "value": 0.08444444444444445}

:::MLPv0.5.0 ssd 1541710923.632949829 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 476, "value": 0.08462222222222222}

:::MLPv0.5.0 ssd 1541710923.727444649 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 477, "value": 0.0848}

:::MLPv0.5.0 ssd 1541710923.823016644 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 478, "value": 0.08497777777777778}

:::MLPv0.5.0 ssd 1541710923.918356419 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 479, "value": 0.08515555555555555}

:::MLPv0.5.0 ssd 1541710924.012711287 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 480, "value": 0.08533333333333333}
Iteration:    480, Loss function: 5.856, Average Loss: 3.263, avg. samples / sec: 21239.93
Iteration:    480, Loss function: 6.210, Average Loss: 3.270, avg. samples / sec: 21233.88
Iteration:    480, Loss function: 6.202, Average Loss: 3.268, avg. samples / sec: 21232.48
Iteration:    480, Loss function: 6.370, Average Loss: 3.260, avg. samples / sec: 21235.74
Iteration:    480, Loss function: 6.582, Average Loss: 3.269, avg. samples / sec: 21234.48
Iteration:    480, Loss function: 5.739, Average Loss: 3.271, avg. samples / sec: 21236.69
Iteration:    480, Loss function: 6.380, Average Loss: 3.261, avg. samples / sec: 21230.74
Iteration:    480, Loss function: 6.446, Average Loss: 3.270, avg. samples / sec: 21226.98

:::MLPv0.5.0 ssd 1541710924.110857248 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 481, "value": 0.08551111111111111}

:::MLPv0.5.0 ssd 1541710924.207273006 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 482, "value": 0.08568888888888888}

:::MLPv0.5.0 ssd 1541710924.305577993 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 483, "value": 0.08586666666666666}

:::MLPv0.5.0 ssd 1541710924.400604963 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 484, "value": 0.08604444444444445}

:::MLPv0.5.0 ssd 1541710924.494918823 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 485, "value": 0.08622222222222223}

:::MLPv0.5.0 ssd 1541710924.589730263 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 486, "value": 0.0864}

:::MLPv0.5.0 ssd 1541710924.684603453 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 487, "value": 0.08657777777777778}

:::MLPv0.5.0 ssd 1541710924.779182434 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 488, "value": 0.08675555555555556}

:::MLPv0.5.0 ssd 1541710924.874502659 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 489, "value": 0.08693333333333333}

:::MLPv0.5.0 ssd 1541710924.968823910 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 490, "value": 0.08711111111111111}

:::MLPv0.5.0 ssd 1541710925.063644171 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 491, "value": 0.08728888888888889}

:::MLPv0.5.0 ssd 1541710925.157887936 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 492, "value": 0.08746666666666666}

:::MLPv0.5.0 ssd 1541710925.252834797 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 493, "value": 0.08764444444444444}

:::MLPv0.5.0 ssd 1541710925.347769260 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 494, "value": 0.08782222222222222}

:::MLPv0.5.0 ssd 1541710925.441615582 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 495, "value": 0.088}

:::MLPv0.5.0 ssd 1541710925.537963152 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 496, "value": 0.08817777777777777}

:::MLPv0.5.0 ssd 1541710925.632454634 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 497, "value": 0.08835555555555556}

:::MLPv0.5.0 ssd 1541710925.727595806 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 498, "value": 0.08853333333333334}

:::MLPv0.5.0 ssd 1541710925.823984146 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 499, "value": 0.08871111111111112}

:::MLPv0.5.0 ssd 1541710925.918780088 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 500, "value": 0.08888888888888889}
Iteration:    500, Loss function: 5.914, Average Loss: 3.323, avg. samples / sec: 21491.71
Iteration:    500, Loss function: 5.628, Average Loss: 3.316, avg. samples / sec: 21493.08
Iteration:    500, Loss function: 5.660, Average Loss: 3.325, avg. samples / sec: 21491.12
Iteration:    500, Loss function: 6.327, Average Loss: 3.323, avg. samples / sec: 21488.92
Iteration:    500, Loss function: 5.924, Average Loss: 3.321, avg. samples / sec: 21489.09
Iteration:    500, Loss function: 6.245, Average Loss: 3.315, avg. samples / sec: 21489.65
Iteration:    500, Loss function: 5.558, Average Loss: 3.317, avg. samples / sec: 21482.94
Iteration:    500, Loss function: 5.706, Average Loss: 3.324, avg. samples / sec: 21491.73

:::MLPv0.5.0 ssd 1541710926.012430191 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 501, "value": 0.08906666666666667}

:::MLPv0.5.0 ssd 1541710926.107640982 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 502, "value": 0.08924444444444445}

:::MLPv0.5.0 ssd 1541710926.205235243 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 503, "value": 0.08942222222222222}

:::MLPv0.5.0 ssd 1541710926.298958302 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 504, "value": 0.0896}

:::MLPv0.5.0 ssd 1541710926.393281937 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 505, "value": 0.08977777777777778}

:::MLPv0.5.0 ssd 1541710926.486526489 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 506, "value": 0.08995555555555555}

:::MLPv0.5.0 ssd 1541710926.580912113 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 507, "value": 0.09013333333333333}

:::MLPv0.5.0 ssd 1541710926.675827980 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 508, "value": 0.0903111111111111}

:::MLPv0.5.0 ssd 1541710926.770497084 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 509, "value": 0.09048888888888888}

:::MLPv0.5.0 ssd 1541710926.866223097 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 510, "value": 0.09066666666666667}

:::MLPv0.5.0 ssd 1541710926.960533142 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 511, "value": 0.09084444444444445}

:::MLPv0.5.0 ssd 1541710927.054130316 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 512, "value": 0.09102222222222223}

:::MLPv0.5.0 ssd 1541710927.151373386 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 513, "value": 0.0912}

:::MLPv0.5.0 ssd 1541710927.246166706 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 514, "value": 0.09137777777777778}

:::MLPv0.5.0 ssd 1541710927.342554331 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 515, "value": 0.09155555555555556}

:::MLPv0.5.0 ssd 1541710927.438000441 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 516, "value": 0.09173333333333333}

:::MLPv0.5.0 ssd 1541710927.533529520 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 517, "value": 0.09191111111111111}

:::MLPv0.5.0 ssd 1541710927.627741337 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 518, "value": 0.09208888888888889}

:::MLPv0.5.0 ssd 1541710927.722134590 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 519, "value": 0.09226666666666666}

:::MLPv0.5.0 ssd 1541710927.814900875 (train.py:553) train_epoch: 9

:::MLPv0.5.0 ssd 1541710927.820465565 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 520, "value": 0.09244444444444444}
Iteration:    520, Loss function: 6.028, Average Loss: 3.372, avg. samples / sec: 21546.01
Iteration:    520, Loss function: 5.661, Average Loss: 3.379, avg. samples / sec: 21541.63
Iteration:    520, Loss function: 5.991, Average Loss: 3.370, avg. samples / sec: 21543.16
Iteration:    520, Loss function: 6.349, Average Loss: 3.374, avg. samples / sec: 21534.81
Iteration:    520, Loss function: 6.469, Average Loss: 3.379, avg. samples / sec: 21536.46
Iteration:    520, Loss function: 6.481, Average Loss: 3.379, avg. samples / sec: 21528.61
Iteration:    520, Loss function: 6.008, Average Loss: 3.378, avg. samples / sec: 21530.73
Iteration:    520, Loss function: 6.146, Average Loss: 3.378, avg. samples / sec: 21539.44

:::MLPv0.5.0 ssd 1541710927.920279741 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 521, "value": 0.09262222222222222}

:::MLPv0.5.0 ssd 1541710928.015542269 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 522, "value": 0.0928}

:::MLPv0.5.0 ssd 1541710928.111048460 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 523, "value": 0.09297777777777778}

:::MLPv0.5.0 ssd 1541710928.204971552 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 524, "value": 0.09315555555555556}

:::MLPv0.5.0 ssd 1541710928.298855066 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 525, "value": 0.09333333333333334}

:::MLPv0.5.0 ssd 1541710928.393204451 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 526, "value": 0.09351111111111111}

:::MLPv0.5.0 ssd 1541710928.489264488 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 527, "value": 0.09368888888888889}

:::MLPv0.5.0 ssd 1541710928.583545685 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 528, "value": 0.09386666666666667}

:::MLPv0.5.0 ssd 1541710928.678113699 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 529, "value": 0.09404444444444444}

:::MLPv0.5.0 ssd 1541710928.773015738 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 530, "value": 0.09422222222222222}

:::MLPv0.5.0 ssd 1541710928.867664337 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 531, "value": 0.0944}

:::MLPv0.5.0 ssd 1541710928.961678028 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 532, "value": 0.09457777777777777}

:::MLPv0.5.0 ssd 1541710929.057278633 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 533, "value": 0.09475555555555555}

:::MLPv0.5.0 ssd 1541710929.152126312 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 534, "value": 0.09493333333333333}

:::MLPv0.5.0 ssd 1541710929.245848894 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 535, "value": 0.0951111111111111}

:::MLPv0.5.0 ssd 1541710929.340998888 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 536, "value": 0.0952888888888889}

:::MLPv0.5.0 ssd 1541710929.436369658 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 537, "value": 0.09546666666666667}

:::MLPv0.5.0 ssd 1541710929.530919313 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 538, "value": 0.09564444444444445}

:::MLPv0.5.0 ssd 1541710929.626405716 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 539, "value": 0.09582222222222223}

:::MLPv0.5.0 ssd 1541710929.722386837 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 540, "value": 0.096}
Iteration:    540, Loss function: 5.588, Average Loss: 3.419, avg. samples / sec: 21542.50
Iteration:    540, Loss function: 5.414, Average Loss: 3.423, avg. samples / sec: 21539.16
Iteration:    540, Loss function: 5.721, Average Loss: 3.422, avg. samples / sec: 21542.78
Iteration:    540, Loss function: 6.049, Average Loss: 3.427, avg. samples / sec: 21538.83
Iteration:    540, Loss function: 5.419, Average Loss: 3.427, avg. samples / sec: 21546.51
Iteration:    540, Loss function: 6.336, Average Loss: 3.427, avg. samples / sec: 21541.21
Iteration:    540, Loss function: 5.666, Average Loss: 3.426, avg. samples / sec: 21535.89
Iteration:    540, Loss function: 5.505, Average Loss: 3.427, avg. samples / sec: 21529.61

:::MLPv0.5.0 ssd 1541710929.818100691 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 541, "value": 0.09617777777777778}

:::MLPv0.5.0 ssd 1541710929.913575649 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 542, "value": 0.09635555555555556}

:::MLPv0.5.0 ssd 1541710930.008504629 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 543, "value": 0.09653333333333333}

:::MLPv0.5.0 ssd 1541710930.103912115 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 544, "value": 0.09671111111111111}

:::MLPv0.5.0 ssd 1541710930.198435545 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 545, "value": 0.09688888888888889}

:::MLPv0.5.0 ssd 1541710930.292008638 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 546, "value": 0.09706666666666666}

:::MLPv0.5.0 ssd 1541710930.387118578 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 547, "value": 0.09724444444444444}

:::MLPv0.5.0 ssd 1541710930.482288361 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 548, "value": 0.09742222222222222}

:::MLPv0.5.0 ssd 1541710930.577120304 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 549, "value": 0.09759999999999999}

:::MLPv0.5.0 ssd 1541710930.671820641 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 550, "value": 0.09777777777777777}

:::MLPv0.5.0 ssd 1541710930.766563416 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 551, "value": 0.09795555555555555}

:::MLPv0.5.0 ssd 1541710930.860822439 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 552, "value": 0.09813333333333334}

:::MLPv0.5.0 ssd 1541710930.955807447 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 553, "value": 0.09831111111111111}

:::MLPv0.5.0 ssd 1541710931.052035332 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 554, "value": 0.09848888888888889}

:::MLPv0.5.0 ssd 1541710931.146835566 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 555, "value": 0.09866666666666667}

:::MLPv0.5.0 ssd 1541710931.241228342 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 556, "value": 0.09884444444444444}

:::MLPv0.5.0 ssd 1541710931.335952282 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 557, "value": 0.09902222222222222}

:::MLPv0.5.0 ssd 1541710931.430337191 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 558, "value": 0.09920000000000001}

:::MLPv0.5.0 ssd 1541710931.527214766 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 559, "value": 0.09937777777777779}

:::MLPv0.5.0 ssd 1541710931.624312401 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 560, "value": 0.09955555555555556}
Iteration:    560, Loss function: 5.625, Average Loss: 3.466, avg. samples / sec: 21530.86
Iteration:    560, Loss function: 5.902, Average Loss: 3.474, avg. samples / sec: 21533.23
Iteration:    560, Loss function: 5.810, Average Loss: 3.467, avg. samples / sec: 21530.46
Iteration:    560, Loss function: 5.849, Average Loss: 3.470, avg. samples / sec: 21536.44
Iteration:    560, Loss function: 5.993, Average Loss: 3.472, avg. samples / sec: 21535.86
Iteration:    560, Loss function: 5.384, Average Loss: 3.472, avg. samples / sec: 21527.19
Iteration:    560, Loss function: 5.448, Average Loss: 3.471, avg. samples / sec: 21544.61
Iteration:    560, Loss function: 6.231, Average Loss: 3.466, avg. samples / sec: 21520.77

:::MLPv0.5.0 ssd 1541710931.718480825 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 561, "value": 0.09973333333333334}

:::MLPv0.5.0 ssd 1541710931.815315723 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 562, "value": 0.09991111111111112}

:::MLPv0.5.0 ssd 1541710931.910270214 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 563, "value": 0.1000888888888889}

:::MLPv0.5.0 ssd 1541710932.005984783 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 564, "value": 0.10026666666666667}

:::MLPv0.5.0 ssd 1541710932.100831032 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 565, "value": 0.10044444444444445}

:::MLPv0.5.0 ssd 1541710932.198368311 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 566, "value": 0.10062222222222222}

:::MLPv0.5.0 ssd 1541710932.294220209 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 567, "value": 0.1008}

:::MLPv0.5.0 ssd 1541710932.389293909 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 568, "value": 0.10097777777777778}

:::MLPv0.5.0 ssd 1541710932.483255148 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 569, "value": 0.10115555555555555}

:::MLPv0.5.0 ssd 1541710932.578073025 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 570, "value": 0.10133333333333333}

:::MLPv0.5.0 ssd 1541710932.672446489 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 571, "value": 0.10151111111111111}

:::MLPv0.5.0 ssd 1541710932.767576694 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 572, "value": 0.10168888888888888}

:::MLPv0.5.0 ssd 1541710932.861980915 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 573, "value": 0.10186666666666666}

:::MLPv0.5.0 ssd 1541710932.956921339 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 574, "value": 0.10204444444444444}

:::MLPv0.5.0 ssd 1541710933.051539898 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 575, "value": 0.10222222222222221}

:::MLPv0.5.0 ssd 1541710933.145940065 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 576, "value": 0.10239999999999999}

:::MLPv0.5.0 ssd 1541710933.241398811 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 577, "value": 0.10257777777777778}

:::MLPv0.5.0 ssd 1541710933.332486629 (train.py:553) train_epoch: 10

:::MLPv0.5.0 ssd 1541710933.338407278 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 578, "value": 0.10275555555555556}

:::MLPv0.5.0 ssd 1541710933.435317993 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 579, "value": 0.10293333333333334}

:::MLPv0.5.0 ssd 1541710933.530223131 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 580, "value": 0.10311111111111111}
Iteration:    580, Loss function: 6.118, Average Loss: 3.518, avg. samples / sec: 21496.83
Iteration:    580, Loss function: 5.287, Average Loss: 3.514, avg. samples / sec: 21494.62
Iteration:    580, Loss function: 5.485, Average Loss: 3.522, avg. samples / sec: 21492.35
Iteration:    580, Loss function: 6.278, Average Loss: 3.517, avg. samples / sec: 21498.53
Iteration:    580, Loss function: 5.837, Average Loss: 3.522, avg. samples / sec: 21495.14
Iteration:    580, Loss function: 6.304, Average Loss: 3.521, avg. samples / sec: 21493.73
Iteration:    580, Loss function: 5.346, Average Loss: 3.516, avg. samples / sec: 21491.83
Iteration:    580, Loss function: 5.481, Average Loss: 3.521, avg. samples / sec: 21493.40

:::MLPv0.5.0 ssd 1541710933.624208689 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 581, "value": 0.10328888888888889}

:::MLPv0.5.0 ssd 1541710933.719928503 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 582, "value": 0.10346666666666667}

:::MLPv0.5.0 ssd 1541710933.814709187 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 583, "value": 0.10364444444444444}

:::MLPv0.5.0 ssd 1541710933.907865763 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 584, "value": 0.10382222222222223}

:::MLPv0.5.0 ssd 1541710934.002379179 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 585, "value": 0.10400000000000001}

:::MLPv0.5.0 ssd 1541710934.096652031 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 586, "value": 0.10417777777777779}

:::MLPv0.5.0 ssd 1541710934.189630270 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 587, "value": 0.10435555555555556}

:::MLPv0.5.0 ssd 1541710934.284152746 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 588, "value": 0.10453333333333334}

:::MLPv0.5.0 ssd 1541710934.378373146 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 589, "value": 0.10471111111111112}

:::MLPv0.5.0 ssd 1541710934.475381136 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 590, "value": 0.10488888888888889}

:::MLPv0.5.0 ssd 1541710934.570385695 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 591, "value": 0.10506666666666667}

:::MLPv0.5.0 ssd 1541710934.665986776 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 592, "value": 0.10524444444444445}

:::MLPv0.5.0 ssd 1541710934.759779692 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 593, "value": 0.10542222222222222}

:::MLPv0.5.0 ssd 1541710934.853798628 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 594, "value": 0.1056}

:::MLPv0.5.0 ssd 1541710934.949800253 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 595, "value": 0.10577777777777778}

:::MLPv0.5.0 ssd 1541710935.044261694 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 596, "value": 0.10595555555555555}

:::MLPv0.5.0 ssd 1541710935.139199495 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 597, "value": 0.10613333333333333}

:::MLPv0.5.0 ssd 1541710935.234654665 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 598, "value": 0.1063111111111111}

:::MLPv0.5.0 ssd 1541710935.328793526 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 599, "value": 0.10648888888888888}

:::MLPv0.5.0 ssd 1541710935.423198462 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 600, "value": 0.10666666666666666}
Iteration:    600, Loss function: 5.137, Average Loss: 3.561, avg. samples / sec: 21639.75
Iteration:    600, Loss function: 5.437, Average Loss: 3.555, avg. samples / sec: 21640.67
Iteration:    600, Loss function: 5.735, Average Loss: 3.564, avg. samples / sec: 21643.30
Iteration:    600, Loss function: 5.402, Average Loss: 3.562, avg. samples / sec: 21639.11
Iteration:    600, Loss function: 5.632, Average Loss: 3.564, avg. samples / sec: 21641.93
Iteration:    600, Loss function: 5.576, Average Loss: 3.563, avg. samples / sec: 21641.05
Iteration:    600, Loss function: 5.548, Average Loss: 3.555, avg. samples / sec: 21637.63
Iteration:    600, Loss function: 5.110, Average Loss: 3.558, avg. samples / sec: 21634.24

:::MLPv0.5.0 ssd 1541710935.516379595 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 601, "value": 0.10684444444444444}

:::MLPv0.5.0 ssd 1541710935.609714985 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 602, "value": 0.10702222222222221}

:::MLPv0.5.0 ssd 1541710935.703405619 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 603, "value": 0.1072}

:::MLPv0.5.0 ssd 1541710935.797189713 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 604, "value": 0.10737777777777778}

:::MLPv0.5.0 ssd 1541710935.891781569 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 605, "value": 0.10755555555555556}

:::MLPv0.5.0 ssd 1541710935.988165855 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 606, "value": 0.10773333333333333}

:::MLPv0.5.0 ssd 1541710936.083000660 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 607, "value": 0.10791111111111111}

:::MLPv0.5.0 ssd 1541710936.179190636 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 608, "value": 0.10808888888888889}

:::MLPv0.5.0 ssd 1541710936.273563623 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 609, "value": 0.10826666666666668}

:::MLPv0.5.0 ssd 1541710936.367980719 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 610, "value": 0.10844444444444445}

:::MLPv0.5.0 ssd 1541710936.462100983 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 611, "value": 0.10862222222222223}

:::MLPv0.5.0 ssd 1541710936.557129622 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 612, "value": 0.10880000000000001}

:::MLPv0.5.0 ssd 1541710936.654286861 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 613, "value": 0.10897777777777778}

:::MLPv0.5.0 ssd 1541710936.748708725 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 614, "value": 0.10915555555555556}

:::MLPv0.5.0 ssd 1541710936.843770266 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 615, "value": 0.10933333333333334}

:::MLPv0.5.0 ssd 1541710936.938545704 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 616, "value": 0.10951111111111111}

:::MLPv0.5.0 ssd 1541710937.033404827 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 617, "value": 0.10968888888888889}

:::MLPv0.5.0 ssd 1541710937.129473448 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 618, "value": 0.10986666666666667}

:::MLPv0.5.0 ssd 1541710937.223126888 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 619, "value": 0.11004444444444444}

:::MLPv0.5.0 ssd 1541710937.318171024 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 620, "value": 0.11022222222222222}
Iteration:    620, Loss function: 5.964, Average Loss: 3.593, avg. samples / sec: 21614.48
Iteration:    620, Loss function: 5.670, Average Loss: 3.597, avg. samples / sec: 21621.44
Iteration:    620, Loss function: 5.358, Average Loss: 3.594, avg. samples / sec: 21619.36
Iteration:    620, Loss function: 5.796, Average Loss: 3.603, avg. samples / sec: 21615.22
Iteration:    620, Loss function: 5.862, Average Loss: 3.601, avg. samples / sec: 21605.75
Iteration:    620, Loss function: 5.604, Average Loss: 3.601, avg. samples / sec: 21610.20
Iteration:    620, Loss function: 5.771, Average Loss: 3.601, avg. samples / sec: 21607.44
Iteration:    620, Loss function: 5.689, Average Loss: 3.598, avg. samples / sec: 21609.25

:::MLPv0.5.0 ssd 1541710937.414603472 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 621, "value": 0.1104}

:::MLPv0.5.0 ssd 1541710937.509635925 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 622, "value": 0.11057777777777777}

:::MLPv0.5.0 ssd 1541710937.605361462 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 623, "value": 0.11075555555555555}

:::MLPv0.5.0 ssd 1541710937.699316978 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 624, "value": 0.11093333333333333}

:::MLPv0.5.0 ssd 1541710937.793765545 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 625, "value": 0.1111111111111111}

:::MLPv0.5.0 ssd 1541710937.888568640 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 626, "value": 0.11128888888888888}

:::MLPv0.5.0 ssd 1541710937.982428312 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 627, "value": 0.11146666666666666}

:::MLPv0.5.0 ssd 1541710938.076495409 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 628, "value": 0.11164444444444445}

:::MLPv0.5.0 ssd 1541710938.171063662 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 629, "value": 0.11182222222222223}

:::MLPv0.5.0 ssd 1541710938.264634609 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 630, "value": 0.112}

:::MLPv0.5.0 ssd 1541710938.359263182 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 631, "value": 0.11217777777777778}

:::MLPv0.5.0 ssd 1541710938.454013348 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 632, "value": 0.11235555555555556}

:::MLPv0.5.0 ssd 1541710938.549067736 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 633, "value": 0.11253333333333333}

:::MLPv0.5.0 ssd 1541710938.643715858 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 634, "value": 0.11271111111111111}

:::MLPv0.5.0 ssd 1541710938.738658667 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 635, "value": 0.1128888888888889}

:::MLPv0.5.0 ssd 1541710938.829550028 (train.py:553) train_epoch: 11

:::MLPv0.5.0 ssd 1541710938.836230755 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 636, "value": 0.11306666666666668}

:::MLPv0.5.0 ssd 1541710938.931201935 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 637, "value": 0.11324444444444445}

:::MLPv0.5.0 ssd 1541710939.025127888 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 638, "value": 0.11342222222222223}

:::MLPv0.5.0 ssd 1541710939.120359421 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 639, "value": 0.1136}

:::MLPv0.5.0 ssd 1541710939.214141846 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 640, "value": 0.11377777777777778}
Iteration:    640, Loss function: 5.788, Average Loss: 3.640, avg. samples / sec: 21613.15
Iteration:    640, Loss function: 5.206, Average Loss: 3.639, avg. samples / sec: 21615.51
Iteration:    640, Loss function: 6.003, Average Loss: 3.635, avg. samples / sec: 21603.27
Iteration:    640, Loss function: 6.011, Average Loss: 3.642, avg. samples / sec: 21609.24
Iteration:    640, Loss function: 5.668, Average Loss: 3.637, avg. samples / sec: 21604.79
Iteration:    640, Loss function: 5.696, Average Loss: 3.635, avg. samples / sec: 21603.44
Iteration:    640, Loss function: 5.894, Average Loss: 3.644, avg. samples / sec: 21604.18
Iteration:    640, Loss function: 5.748, Average Loss: 3.642, avg. samples / sec: 21601.43

:::MLPv0.5.0 ssd 1541710939.307654619 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 641, "value": 0.11395555555555556}

:::MLPv0.5.0 ssd 1541710939.407047987 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 642, "value": 0.11413333333333334}

:::MLPv0.5.0 ssd 1541710939.502126217 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 643, "value": 0.11431111111111111}

:::MLPv0.5.0 ssd 1541710939.596381664 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 644, "value": 0.11448888888888889}

:::MLPv0.5.0 ssd 1541710939.691560984 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 645, "value": 0.11466666666666667}

:::MLPv0.5.0 ssd 1541710939.786102772 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 646, "value": 0.11484444444444444}

:::MLPv0.5.0 ssd 1541710939.880173206 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 647, "value": 0.11502222222222222}

:::MLPv0.5.0 ssd 1541710939.974412203 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 648, "value": 0.1152}

:::MLPv0.5.0 ssd 1541710940.068494320 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 649, "value": 0.11537777777777777}

:::MLPv0.5.0 ssd 1541710940.164505959 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 650, "value": 0.11555555555555555}

:::MLPv0.5.0 ssd 1541710940.259153843 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 651, "value": 0.11573333333333333}

:::MLPv0.5.0 ssd 1541710940.353666544 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 652, "value": 0.1159111111111111}

:::MLPv0.5.0 ssd 1541710940.448444366 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 653, "value": 0.11608888888888888}

:::MLPv0.5.0 ssd 1541710940.543305635 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 654, "value": 0.11626666666666667}

:::MLPv0.5.0 ssd 1541710940.637383699 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 655, "value": 0.11644444444444445}

:::MLPv0.5.0 ssd 1541710940.732672691 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 656, "value": 0.11662222222222222}

:::MLPv0.5.0 ssd 1541710940.827955723 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 657, "value": 0.1168}

:::MLPv0.5.0 ssd 1541710940.922973394 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 658, "value": 0.11697777777777778}

:::MLPv0.5.0 ssd 1541710941.017608643 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 659, "value": 0.11715555555555555}

:::MLPv0.5.0 ssd 1541710941.113770962 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 660, "value": 0.11733333333333333}
Iteration:    660, Loss function: 4.671, Average Loss: 3.675, avg. samples / sec: 21565.30
Iteration:    660, Loss function: 5.142, Average Loss: 3.673, avg. samples / sec: 21563.71
Iteration:    660, Loss function: 4.895, Average Loss: 3.669, avg. samples / sec: 21562.24
Iteration:    660, Loss function: 4.880, Average Loss: 3.669, avg. samples / sec: 21563.75
Iteration:    660, Loss function: 5.394, Average Loss: 3.676, avg. samples / sec: 21554.75
Iteration:    660, Loss function: 5.734, Average Loss: 3.679, avg. samples / sec: 21563.37
Iteration:    660, Loss function: 5.589, Average Loss: 3.677, avg. samples / sec: 21558.05
Iteration:    660, Loss function: 5.094, Average Loss: 3.674, avg. samples / sec: 21548.02

:::MLPv0.5.0 ssd 1541710941.209396362 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 661, "value": 0.11751111111111112}

:::MLPv0.5.0 ssd 1541710941.303738117 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 662, "value": 0.1176888888888889}

:::MLPv0.5.0 ssd 1541710941.399823189 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 663, "value": 0.11786666666666668}

:::MLPv0.5.0 ssd 1541710941.494699955 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 664, "value": 0.11804444444444445}

:::MLPv0.5.0 ssd 1541710941.588376999 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 665, "value": 0.11822222222222223}

:::MLPv0.5.0 ssd 1541710941.684010267 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 666, "value": 0.1184}

:::MLPv0.5.0 ssd 1541710941.777612209 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 667, "value": 0.11857777777777778}

:::MLPv0.5.0 ssd 1541710941.871437311 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 668, "value": 0.11875555555555556}

:::MLPv0.5.0 ssd 1541710941.965947151 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 669, "value": 0.11893333333333334}

:::MLPv0.5.0 ssd 1541710942.059340715 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 670, "value": 0.11911111111111111}

:::MLPv0.5.0 ssd 1541710942.153151512 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 671, "value": 0.11928888888888889}

:::MLPv0.5.0 ssd 1541710942.247695923 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 672, "value": 0.11946666666666667}

:::MLPv0.5.0 ssd 1541710942.342565060 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 673, "value": 0.11964444444444444}

:::MLPv0.5.0 ssd 1541710942.436921120 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 674, "value": 0.11982222222222222}

:::MLPv0.5.0 ssd 1541710942.531900167 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 675, "value": 0.12}

:::MLPv0.5.0 ssd 1541710942.627811909 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 676, "value": 0.12017777777777777}

:::MLPv0.5.0 ssd 1541710942.724179506 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 677, "value": 0.12035555555555555}

:::MLPv0.5.0 ssd 1541710942.817997456 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 678, "value": 0.12053333333333333}

:::MLPv0.5.0 ssd 1541710942.912539959 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 679, "value": 0.1207111111111111}

:::MLPv0.5.0 ssd 1541710943.007169247 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 680, "value": 0.12088888888888889}
Iteration:    680, Loss function: 5.301, Average Loss: 3.703, avg. samples / sec: 21638.55
Iteration:    680, Loss function: 5.395, Average Loss: 3.709, avg. samples / sec: 21648.80
Iteration:    680, Loss function: 5.321, Average Loss: 3.711, avg. samples / sec: 21641.04
Iteration:    680, Loss function: 5.875, Average Loss: 3.702, avg. samples / sec: 21639.88
Iteration:    680, Loss function: 5.444, Average Loss: 3.708, avg. samples / sec: 21633.16
Iteration:    680, Loss function: 5.742, Average Loss: 3.705, avg. samples / sec: 21636.37
Iteration:    680, Loss function: 5.166, Average Loss: 3.709, avg. samples / sec: 21638.04
Iteration:    680, Loss function: 5.602, Average Loss: 3.709, avg. samples / sec: 21645.94

:::MLPv0.5.0 ssd 1541710943.101169586 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 681, "value": 0.12106666666666667}

:::MLPv0.5.0 ssd 1541710943.195524454 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 682, "value": 0.12124444444444445}

:::MLPv0.5.0 ssd 1541710943.289964914 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 683, "value": 0.12142222222222222}

:::MLPv0.5.0 ssd 1541710943.383917809 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 684, "value": 0.1216}

:::MLPv0.5.0 ssd 1541710943.477871656 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 685, "value": 0.12177777777777778}

:::MLPv0.5.0 ssd 1541710943.572203159 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 686, "value": 0.12195555555555557}

:::MLPv0.5.0 ssd 1541710943.667316914 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 687, "value": 0.12213333333333334}

:::MLPv0.5.0 ssd 1541710943.772450209 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 688, "value": 0.12231111111111112}

:::MLPv0.5.0 ssd 1541710943.868066311 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 689, "value": 0.1224888888888889}

:::MLPv0.5.0 ssd 1541710943.962243795 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 690, "value": 0.12266666666666667}

:::MLPv0.5.0 ssd 1541710944.056118965 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 691, "value": 0.12284444444444445}

:::MLPv0.5.0 ssd 1541710944.150911331 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 692, "value": 0.12302222222222223}

:::MLPv0.5.0 ssd 1541710944.246284962 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 693, "value": 0.1232}

:::MLPv0.5.0 ssd 1541710944.339446068 (train.py:553) train_epoch: 12

:::MLPv0.5.0 ssd 1541710944.344738245 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 694, "value": 0.12337777777777778}

:::MLPv0.5.0 ssd 1541710944.440081835 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 695, "value": 0.12355555555555556}

:::MLPv0.5.0 ssd 1541710944.535475731 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 696, "value": 0.12373333333333333}

:::MLPv0.5.0 ssd 1541710944.637369156 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 697, "value": 0.12391111111111111}

:::MLPv0.5.0 ssd 1541710944.733965397 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 698, "value": 0.12408888888888889}

:::MLPv0.5.0 ssd 1541710944.830738068 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 699, "value": 0.12426666666666666}

:::MLPv0.5.0 ssd 1541710944.925227404 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 700, "value": 0.12444444444444444}
Iteration:    700, Loss function: 4.657, Average Loss: 3.734, avg. samples / sec: 21358.00
Iteration:    700, Loss function: 5.409, Average Loss: 3.741, avg. samples / sec: 21356.92
Iteration:    700, Loss function: 5.410, Average Loss: 3.734, avg. samples / sec: 21354.93
Iteration:    700, Loss function: 5.457, Average Loss: 3.741, avg. samples / sec: 21352.23
Iteration:    700, Loss function: 5.101, Average Loss: 3.736, avg. samples / sec: 21350.81
Iteration:    700, Loss function: 5.504, Average Loss: 3.740, avg. samples / sec: 21348.16
Iteration:    700, Loss function: 4.965, Average Loss: 3.741, avg. samples / sec: 21352.16
Iteration:    700, Loss function: 5.212, Average Loss: 3.739, avg. samples / sec: 21349.79

:::MLPv0.5.0 ssd 1541710945.024263144 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 701, "value": 0.12462222222222222}

:::MLPv0.5.0 ssd 1541710945.120113611 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 702, "value": 0.1248}

:::MLPv0.5.0 ssd 1541710945.214215517 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 703, "value": 0.12497777777777777}

:::MLPv0.5.0 ssd 1541710945.307327271 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 704, "value": 0.12515555555555555}

:::MLPv0.5.0 ssd 1541710945.402193069 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 705, "value": 0.12533333333333335}

:::MLPv0.5.0 ssd 1541710945.498848200 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 706, "value": 0.12551111111111113}

:::MLPv0.5.0 ssd 1541710945.597531319 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 707, "value": 0.1256888888888889}

:::MLPv0.5.0 ssd 1541710945.692299604 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 708, "value": 0.12586666666666668}

:::MLPv0.5.0 ssd 1541710945.786020279 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 709, "value": 0.12604444444444446}

:::MLPv0.5.0 ssd 1541710945.880491734 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 710, "value": 0.12622222222222224}

:::MLPv0.5.0 ssd 1541710945.975660324 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 711, "value": 0.1264}

:::MLPv0.5.0 ssd 1541710946.074376345 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 712, "value": 0.1265777777777778}

:::MLPv0.5.0 ssd 1541710946.168787479 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 713, "value": 0.12675555555555557}

:::MLPv0.5.0 ssd 1541710946.265084982 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 714, "value": 0.12693333333333334}

:::MLPv0.5.0 ssd 1541710946.360197544 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 715, "value": 0.12711111111111112}

:::MLPv0.5.0 ssd 1541710946.454517603 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 716, "value": 0.1272888888888889}

:::MLPv0.5.0 ssd 1541710946.549373627 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 717, "value": 0.12746666666666667}

:::MLPv0.5.0 ssd 1541710946.644088268 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 718, "value": 0.12764444444444445}

:::MLPv0.5.0 ssd 1541710946.738894701 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 719, "value": 0.12782222222222223}

:::MLPv0.5.0 ssd 1541710946.832770109 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 720, "value": 0.128}
Iteration:    720, Loss function: 4.917, Average Loss: 3.765, avg. samples / sec: 21473.18
Iteration:    720, Loss function: 4.942, Average Loss: 3.770, avg. samples / sec: 21480.18
Iteration:    720, Loss function: 4.990, Average Loss: 3.769, avg. samples / sec: 21476.16
Iteration:    720, Loss function: 5.854, Average Loss: 3.771, avg. samples / sec: 21469.58
Iteration:    720, Loss function: 4.849, Average Loss: 3.766, avg. samples / sec: 21469.64
Iteration:    720, Loss function: 5.286, Average Loss: 3.769, avg. samples / sec: 21473.65
Iteration:    720, Loss function: 5.715, Average Loss: 3.771, avg. samples / sec: 21473.23
Iteration:    720, Loss function: 4.812, Average Loss: 3.772, avg. samples / sec: 21466.42

:::MLPv0.5.0 ssd 1541710946.927800417 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 721, "value": 0.12817777777777778}

:::MLPv0.5.0 ssd 1541710947.022048235 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 722, "value": 0.12835555555555556}

:::MLPv0.5.0 ssd 1541710947.116352320 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 723, "value": 0.12853333333333333}

:::MLPv0.5.0 ssd 1541710947.210263729 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 724, "value": 0.1287111111111111}

:::MLPv0.5.0 ssd 1541710947.304414511 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 725, "value": 0.1288888888888889}

:::MLPv0.5.0 ssd 1541710947.397672653 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 726, "value": 0.12906666666666666}

:::MLPv0.5.0 ssd 1541710947.492246389 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 727, "value": 0.12924444444444444}

:::MLPv0.5.0 ssd 1541710947.586662054 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 728, "value": 0.12942222222222222}

:::MLPv0.5.0 ssd 1541710947.682252884 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 729, "value": 0.1296}

:::MLPv0.5.0 ssd 1541710947.776549816 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 730, "value": 0.12977777777777777}

:::MLPv0.5.0 ssd 1541710947.870835304 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 731, "value": 0.12995555555555555}

:::MLPv0.5.0 ssd 1541710947.964397669 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 732, "value": 0.13013333333333332}

:::MLPv0.5.0 ssd 1541710948.058460236 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 733, "value": 0.1303111111111111}

:::MLPv0.5.0 ssd 1541710948.153080940 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 734, "value": 0.13048888888888888}

:::MLPv0.5.0 ssd 1541710948.247882128 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 735, "value": 0.13066666666666665}

:::MLPv0.5.0 ssd 1541710948.343279839 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 736, "value": 0.13084444444444446}

:::MLPv0.5.0 ssd 1541710948.437504292 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 737, "value": 0.13102222222222223}

:::MLPv0.5.0 ssd 1541710948.531698942 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 738, "value": 0.1312}

:::MLPv0.5.0 ssd 1541710948.627115965 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 739, "value": 0.1313777777777778}

:::MLPv0.5.0 ssd 1541710948.721262932 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 740, "value": 0.13155555555555556}
Iteration:    740, Loss function: 5.670, Average Loss: 3.794, avg. samples / sec: 21690.89
Iteration:    740, Loss function: 5.645, Average Loss: 3.803, avg. samples / sec: 21690.89
Iteration:    740, Loss function: 5.733, Average Loss: 3.804, avg. samples / sec: 21701.22
Iteration:    740, Loss function: 5.281, Average Loss: 3.797, avg. samples / sec: 21694.04
Iteration:    740, Loss function: 6.092, Average Loss: 3.800, avg. samples / sec: 21691.66
Iteration:    740, Loss function: 6.049, Average Loss: 3.802, avg. samples / sec: 21691.27
Iteration:    740, Loss function: 5.441, Average Loss: 3.803, avg. samples / sec: 21694.23
Iteration:    740, Loss function: 6.081, Average Loss: 3.803, avg. samples / sec: 21687.30

:::MLPv0.5.0 ssd 1541710948.816882849 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 741, "value": 0.13173333333333334}

:::MLPv0.5.0 ssd 1541710948.910950661 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 742, "value": 0.13191111111111112}

:::MLPv0.5.0 ssd 1541710949.005648375 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 743, "value": 0.1320888888888889}

:::MLPv0.5.0 ssd 1541710949.099668741 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 744, "value": 0.13226666666666667}

:::MLPv0.5.0 ssd 1541710949.193907499 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 745, "value": 0.13244444444444445}

:::MLPv0.5.0 ssd 1541710949.291732311 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 746, "value": 0.13262222222222222}

:::MLPv0.5.0 ssd 1541710949.385865450 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 747, "value": 0.1328}

:::MLPv0.5.0 ssd 1541710949.479947567 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 748, "value": 0.13297777777777778}

:::MLPv0.5.0 ssd 1541710949.573937654 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 749, "value": 0.13315555555555555}

:::MLPv0.5.0 ssd 1541710949.668402672 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 750, "value": 0.13333333333333333}

:::MLPv0.5.0 ssd 1541710949.758713245 (train.py:553) train_epoch: 13

:::MLPv0.5.0 ssd 1541710949.764016151 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 751, "value": 0.1335111111111111}

:::MLPv0.5.0 ssd 1541710949.858016014 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 752, "value": 0.13368888888888888}

:::MLPv0.5.0 ssd 1541710949.952261448 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 753, "value": 0.13386666666666666}

:::MLPv0.5.0 ssd 1541710950.045767307 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 754, "value": 0.13404444444444444}

:::MLPv0.5.0 ssd 1541710950.140147924 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 755, "value": 0.13422222222222221}

:::MLPv0.5.0 ssd 1541710950.235183477 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 756, "value": 0.1344}

:::MLPv0.5.0 ssd 1541710950.329235792 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 757, "value": 0.13457777777777777}

:::MLPv0.5.0 ssd 1541710950.422883987 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 758, "value": 0.13475555555555557}

:::MLPv0.5.0 ssd 1541710950.517170668 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 759, "value": 0.13493333333333335}

:::MLPv0.5.0 ssd 1541710950.614333868 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 760, "value": 0.13511111111111113}
Iteration:    760, Loss function: 6.308, Average Loss: 3.848, avg. samples / sec: 21637.77
Iteration:    760, Loss function: 5.812, Average Loss: 3.838, avg. samples / sec: 21634.11
Iteration:    760, Loss function: 6.313, Average Loss: 3.846, avg. samples / sec: 21638.46
Iteration:    760, Loss function: 6.160, Average Loss: 3.848, avg. samples / sec: 21634.74
Iteration:    760, Loss function: 7.019, Average Loss: 3.843, avg. samples / sec: 21635.43
Iteration:    760, Loss function: 6.039, Average Loss: 3.844, avg. samples / sec: 21635.15
Iteration:    760, Loss function: 5.690, Average Loss: 3.843, avg. samples / sec: 21637.17
Iteration:    760, Loss function: 6.021, Average Loss: 3.848, avg. samples / sec: 21633.36

:::MLPv0.5.0 ssd 1541710950.708938122 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 761, "value": 0.1352888888888889}

:::MLPv0.5.0 ssd 1541710950.802442074 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 762, "value": 0.13546666666666668}

:::MLPv0.5.0 ssd 1541710950.896691799 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 763, "value": 0.13564444444444446}

:::MLPv0.5.0 ssd 1541710950.990263224 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 764, "value": 0.13582222222222223}

:::MLPv0.5.0 ssd 1541710951.084019899 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 765, "value": 0.136}

:::MLPv0.5.0 ssd 1541710951.177716017 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 766, "value": 0.1361777777777778}

:::MLPv0.5.0 ssd 1541710951.271314383 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 767, "value": 0.13635555555555556}

:::MLPv0.5.0 ssd 1541710951.366488218 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 768, "value": 0.13653333333333334}

:::MLPv0.5.0 ssd 1541710951.460350752 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 769, "value": 0.13671111111111112}

:::MLPv0.5.0 ssd 1541710951.555272102 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 770, "value": 0.1368888888888889}

:::MLPv0.5.0 ssd 1541710951.649180412 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 771, "value": 0.13706666666666667}

:::MLPv0.5.0 ssd 1541710951.743325233 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 772, "value": 0.13724444444444445}

:::MLPv0.5.0 ssd 1541710951.837990761 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 773, "value": 0.13742222222222222}

:::MLPv0.5.0 ssd 1541710951.931912899 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 774, "value": 0.1376}

:::MLPv0.5.0 ssd 1541710952.027258873 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 775, "value": 0.13777777777777778}

:::MLPv0.5.0 ssd 1541710952.121923685 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 776, "value": 0.13795555555555555}

:::MLPv0.5.0 ssd 1541710952.217938662 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 777, "value": 0.13813333333333333}

:::MLPv0.5.0 ssd 1541710952.311609030 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 778, "value": 0.1383111111111111}

:::MLPv0.5.0 ssd 1541710952.405875683 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 779, "value": 0.13848888888888888}

:::MLPv0.5.0 ssd 1541710952.501874685 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 780, "value": 0.13866666666666666}
Iteration:    780, Loss function: 5.125, Average Loss: 3.882, avg. samples / sec: 21708.56
Iteration:    780, Loss function: 5.406, Average Loss: 3.877, avg. samples / sec: 21706.46
Iteration:    780, Loss function: 5.517, Average Loss: 3.882, avg. samples / sec: 21700.37
Iteration:    780, Loss function: 5.558, Average Loss: 3.873, avg. samples / sec: 21700.45
Iteration:    780, Loss function: 5.602, Average Loss: 3.882, avg. samples / sec: 21701.02
Iteration:    780, Loss function: 4.952, Average Loss: 3.876, avg. samples / sec: 21702.84
Iteration:    780, Loss function: 5.769, Average Loss: 3.878, avg. samples / sec: 21705.26
Iteration:    780, Loss function: 5.467, Average Loss: 3.882, avg. samples / sec: 21704.18

:::MLPv0.5.0 ssd 1541710952.597071171 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 781, "value": 0.13884444444444444}

:::MLPv0.5.0 ssd 1541710952.691883802 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 782, "value": 0.1390222222222222}

:::MLPv0.5.0 ssd 1541710952.785968542 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 783, "value": 0.1392}

:::MLPv0.5.0 ssd 1541710952.880248308 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 784, "value": 0.13937777777777777}

:::MLPv0.5.0 ssd 1541710952.974295616 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 785, "value": 0.13955555555555554}

:::MLPv0.5.0 ssd 1541710953.075289249 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 786, "value": 0.13973333333333332}

:::MLPv0.5.0 ssd 1541710953.169566393 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 787, "value": 0.13991111111111112}

:::MLPv0.5.0 ssd 1541710953.264222860 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 788, "value": 0.1400888888888889}

:::MLPv0.5.0 ssd 1541710953.360111713 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 789, "value": 0.14026666666666668}

:::MLPv0.5.0 ssd 1541710953.453330517 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 790, "value": 0.14044444444444446}

:::MLPv0.5.0 ssd 1541710953.547355175 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 791, "value": 0.14062222222222223}

:::MLPv0.5.0 ssd 1541710953.641016245 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 792, "value": 0.1408}

:::MLPv0.5.0 ssd 1541710953.734946251 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 793, "value": 0.14097777777777779}

:::MLPv0.5.0 ssd 1541710953.829436064 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 794, "value": 0.14115555555555556}

:::MLPv0.5.0 ssd 1541710953.925217867 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 795, "value": 0.14133333333333334}

:::MLPv0.5.0 ssd 1541710954.019972563 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 796, "value": 0.14151111111111112}

:::MLPv0.5.0 ssd 1541710954.113306284 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 797, "value": 0.1416888888888889}

:::MLPv0.5.0 ssd 1541710954.208371639 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 798, "value": 0.14186666666666667}

:::MLPv0.5.0 ssd 1541710954.302442312 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 799, "value": 0.14204444444444445}

:::MLPv0.5.0 ssd 1541710954.395859241 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 800, "value": 0.14222222222222222}
Iteration:    800, Loss function: 4.831, Average Loss: 3.899, avg. samples / sec: 21634.37
Iteration:    800, Loss function: 5.562, Average Loss: 3.907, avg. samples / sec: 21628.31
Iteration:    800, Loss function: 5.029, Average Loss: 3.904, avg. samples / sec: 21626.69
Iteration:    800, Loss function: 5.228, Average Loss: 3.905, avg. samples / sec: 21621.20
Iteration:    800, Loss function: 5.055, Average Loss: 3.905, avg. samples / sec: 21627.70
Iteration:    800, Loss function: 4.927, Average Loss: 3.894, avg. samples / sec: 21624.97
Iteration:    800, Loss function: 5.246, Average Loss: 3.902, avg. samples / sec: 21626.47
Iteration:    800, Loss function: 5.300, Average Loss: 3.906, avg. samples / sec: 21628.26

:::MLPv0.5.0 ssd 1541710954.490136862 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 801, "value": 0.1424}

:::MLPv0.5.0 ssd 1541710954.583909750 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 802, "value": 0.14257777777777778}

:::MLPv0.5.0 ssd 1541710954.677004337 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 803, "value": 0.14275555555555555}

:::MLPv0.5.0 ssd 1541710954.774028778 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 804, "value": 0.14293333333333333}

:::MLPv0.5.0 ssd 1541710954.867939234 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 805, "value": 0.1431111111111111}

:::MLPv0.5.0 ssd 1541710954.964493275 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 806, "value": 0.14328888888888888}

:::MLPv0.5.0 ssd 1541710955.058715582 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 807, "value": 0.14346666666666666}

:::MLPv0.5.0 ssd 1541710955.152633190 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 808, "value": 0.14364444444444444}

:::MLPv0.5.0 ssd 1541710955.243335485 (train.py:553) train_epoch: 14

:::MLPv0.5.0 ssd 1541710955.249145508 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 809, "value": 0.14382222222222224}

:::MLPv0.5.0 ssd 1541710955.344087124 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 810, "value": 0.14400000000000002}

:::MLPv0.5.0 ssd 1541710955.438773155 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 811, "value": 0.1441777777777778}

:::MLPv0.5.0 ssd 1541710955.537349224 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 812, "value": 0.14435555555555557}

:::MLPv0.5.0 ssd 1541710955.632921696 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 813, "value": 0.14453333333333335}

:::MLPv0.5.0 ssd 1541710955.727100849 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 814, "value": 0.14471111111111112}

:::MLPv0.5.0 ssd 1541710955.822201014 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 815, "value": 0.1448888888888889}

:::MLPv0.5.0 ssd 1541710955.916595221 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 816, "value": 0.14506666666666668}

:::MLPv0.5.0 ssd 1541710956.010829926 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 817, "value": 0.14524444444444445}

:::MLPv0.5.0 ssd 1541710956.104501724 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 818, "value": 0.14542222222222223}

:::MLPv0.5.0 ssd 1541710956.199291229 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 819, "value": 0.1456}

:::MLPv0.5.0 ssd 1541710956.293467999 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 820, "value": 0.14577777777777778}
Iteration:    820, Loss function: 5.460, Average Loss: 3.935, avg. samples / sec: 21585.23
Iteration:    820, Loss function: 5.367, Average Loss: 3.937, avg. samples / sec: 21587.84
Iteration:    820, Loss function: 5.831, Average Loss: 3.939, avg. samples / sec: 21585.73
Iteration:    820, Loss function: 4.982, Average Loss: 3.935, avg. samples / sec: 21588.72
Iteration:    820, Loss function: 5.184, Average Loss: 3.933, avg. samples / sec: 21590.55
Iteration:    820, Loss function: 4.725, Average Loss: 3.936, avg. samples / sec: 21585.21
Iteration:    820, Loss function: 5.685, Average Loss: 3.925, avg. samples / sec: 21587.68
Iteration:    820, Loss function: 5.288, Average Loss: 3.938, avg. samples / sec: 21587.01

:::MLPv0.5.0 ssd 1541710956.388452530 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 821, "value": 0.14595555555555556}

:::MLPv0.5.0 ssd 1541710956.483175755 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 822, "value": 0.14613333333333334}

:::MLPv0.5.0 ssd 1541710956.577805042 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 823, "value": 0.14631111111111111}

:::MLPv0.5.0 ssd 1541710956.671880245 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 824, "value": 0.1464888888888889}

:::MLPv0.5.0 ssd 1541710956.766145706 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 825, "value": 0.14666666666666667}

:::MLPv0.5.0 ssd 1541710956.860144615 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 826, "value": 0.14684444444444444}

:::MLPv0.5.0 ssd 1541710956.956156492 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 827, "value": 0.14702222222222222}

:::MLPv0.5.0 ssd 1541710957.050113678 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 828, "value": 0.1472}

:::MLPv0.5.0 ssd 1541710957.147368431 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 829, "value": 0.14737777777777777}

:::MLPv0.5.0 ssd 1541710957.241598368 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 830, "value": 0.14755555555555555}

:::MLPv0.5.0 ssd 1541710957.336175680 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 831, "value": 0.14773333333333333}

:::MLPv0.5.0 ssd 1541710957.429967880 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 832, "value": 0.1479111111111111}

:::MLPv0.5.0 ssd 1541710957.524873257 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 833, "value": 0.14808888888888888}

:::MLPv0.5.0 ssd 1541710957.620706320 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 834, "value": 0.14826666666666666}

:::MLPv0.5.0 ssd 1541710957.715071917 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 835, "value": 0.14844444444444443}

:::MLPv0.5.0 ssd 1541710957.809275627 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 836, "value": 0.1486222222222222}

:::MLPv0.5.0 ssd 1541710957.903460026 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 837, "value": 0.14880000000000002}

:::MLPv0.5.0 ssd 1541710958.000233412 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 838, "value": 0.1489777777777778}

:::MLPv0.5.0 ssd 1541710958.093330622 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 839, "value": 0.14915555555555557}

:::MLPv0.5.0 ssd 1541710958.187086582 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 840, "value": 0.14933333333333335}
Iteration:    840, Loss function: 5.088, Average Loss: 3.947, avg. samples / sec: 21639.50
Iteration:    840, Loss function: 5.490, Average Loss: 3.962, avg. samples / sec: 21632.13
Iteration:    840, Loss function: 4.696, Average Loss: 3.960, avg. samples / sec: 21630.99
Iteration:    840, Loss function: 5.162, Average Loss: 3.957, avg. samples / sec: 21628.58
Iteration:    840, Loss function: 4.597, Average Loss: 3.953, avg. samples / sec: 21627.59
Iteration:    840, Loss function: 4.740, Average Loss: 3.958, avg. samples / sec: 21627.64
Iteration:    840, Loss function: 4.839, Average Loss: 3.955, avg. samples / sec: 21618.96
Iteration:    840, Loss function: 4.754, Average Loss: 3.958, avg. samples / sec: 21624.00

:::MLPv0.5.0 ssd 1541710958.282521486 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 841, "value": 0.14951111111111112}

:::MLPv0.5.0 ssd 1541710958.376392365 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 842, "value": 0.1496888888888889}

:::MLPv0.5.0 ssd 1541710958.470811844 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 843, "value": 0.14986666666666668}

:::MLPv0.5.0 ssd 1541710958.566141367 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 844, "value": 0.15004444444444445}

:::MLPv0.5.0 ssd 1541710958.660716295 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 845, "value": 0.15022222222222223}

:::MLPv0.5.0 ssd 1541710958.753784895 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 846, "value": 0.1504}

:::MLPv0.5.0 ssd 1541710958.848184586 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 847, "value": 0.15057777777777778}

:::MLPv0.5.0 ssd 1541710958.943140507 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 848, "value": 0.15075555555555556}

:::MLPv0.5.0 ssd 1541710959.037253618 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 849, "value": 0.15093333333333334}

:::MLPv0.5.0 ssd 1541710959.133290052 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 850, "value": 0.1511111111111111}

:::MLPv0.5.0 ssd 1541710959.228310585 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 851, "value": 0.1512888888888889}

:::MLPv0.5.0 ssd 1541710959.324607849 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 852, "value": 0.15146666666666667}

:::MLPv0.5.0 ssd 1541710959.418741941 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 853, "value": 0.15164444444444444}

:::MLPv0.5.0 ssd 1541710959.514286757 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 854, "value": 0.15182222222222222}

:::MLPv0.5.0 ssd 1541710959.609286547 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 855, "value": 0.152}

:::MLPv0.5.0 ssd 1541710959.705896378 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 856, "value": 0.15217777777777777}

:::MLPv0.5.0 ssd 1541710959.801159620 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 857, "value": 0.15235555555555555}

:::MLPv0.5.0 ssd 1541710959.897850275 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 858, "value": 0.15253333333333333}

:::MLPv0.5.0 ssd 1541710959.993092299 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 859, "value": 0.1527111111111111}

:::MLPv0.5.0 ssd 1541710960.088902950 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 860, "value": 0.15288888888888888}
Iteration:    860, Loss function: 5.485, Average Loss: 3.976, avg. samples / sec: 21551.03
Iteration:    860, Loss function: 5.030, Average Loss: 3.972, avg. samples / sec: 21545.90
Iteration:    860, Loss function: 5.663, Average Loss: 3.982, avg. samples / sec: 21536.11
Iteration:    860, Loss function: 5.213, Average Loss: 3.982, avg. samples / sec: 21533.34
Iteration:    860, Loss function: 5.456, Average Loss: 3.980, avg. samples / sec: 21545.66
Iteration:    860, Loss function: 4.964, Average Loss: 3.970, avg. samples / sec: 21529.46
Iteration:    860, Loss function: 5.433, Average Loss: 3.979, avg. samples / sec: 21536.57
Iteration:    860, Loss function: 5.291, Average Loss: 3.983, avg. samples / sec: 21540.96

:::MLPv0.5.0 ssd 1541710960.183061600 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 861, "value": 0.15306666666666666}

:::MLPv0.5.0 ssd 1541710960.277276516 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 862, "value": 0.15324444444444446}

:::MLPv0.5.0 ssd 1541710960.371537685 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 863, "value": 0.15342222222222224}

:::MLPv0.5.0 ssd 1541710960.468172073 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 864, "value": 0.15360000000000001}

:::MLPv0.5.0 ssd 1541710960.562300682 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 865, "value": 0.1537777777777778}

:::MLPv0.5.0 ssd 1541710960.656644821 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 866, "value": 0.15395555555555557}

:::MLPv0.5.0 ssd 1541710960.746634007 (train.py:553) train_epoch: 15

:::MLPv0.5.0 ssd 1541710960.752225399 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 867, "value": 0.15413333333333334}

:::MLPv0.5.0 ssd 1541710960.846900940 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 868, "value": 0.15431111111111112}

:::MLPv0.5.0 ssd 1541710960.941390514 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 869, "value": 0.1544888888888889}

:::MLPv0.5.0 ssd 1541710961.034447670 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 870, "value": 0.15466666666666667}

:::MLPv0.5.0 ssd 1541710961.128623724 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 871, "value": 0.15484444444444445}

:::MLPv0.5.0 ssd 1541710961.222416878 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 872, "value": 0.15502222222222223}

:::MLPv0.5.0 ssd 1541710961.315990925 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 873, "value": 0.1552}

:::MLPv0.5.0 ssd 1541710961.415049553 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 874, "value": 0.15537777777777778}

:::MLPv0.5.0 ssd 1541710961.509521961 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 875, "value": 0.15555555555555556}

:::MLPv0.5.0 ssd 1541710961.603289843 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 876, "value": 0.15573333333333333}

:::MLPv0.5.0 ssd 1541710961.700045824 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 877, "value": 0.1559111111111111}

:::MLPv0.5.0 ssd 1541710961.794968605 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 878, "value": 0.1560888888888889}

:::MLPv0.5.0 ssd 1541710961.889671564 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 879, "value": 0.15626666666666666}

:::MLPv0.5.0 ssd 1541710961.983861446 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 880, "value": 0.15644444444444444}
Iteration:    880, Loss function: 4.218, Average Loss: 4.001, avg. samples / sec: 21621.83
Iteration:    880, Loss function: 5.123, Average Loss: 4.000, avg. samples / sec: 21626.95
Iteration:    880, Loss function: 4.951, Average Loss: 4.003, avg. samples / sec: 21622.18
Iteration:    880, Loss function: 4.915, Average Loss: 4.003, avg. samples / sec: 21621.37
Iteration:    880, Loss function: 4.945, Average Loss: 3.998, avg. samples / sec: 21616.83
Iteration:    880, Loss function: 5.524, Average Loss: 3.995, avg. samples / sec: 21621.71
Iteration:    880, Loss function: 4.788, Average Loss: 3.996, avg. samples / sec: 21612.35
Iteration:    880, Loss function: 4.951, Average Loss: 4.005, avg. samples / sec: 21614.39

:::MLPv0.5.0 ssd 1541710962.077804565 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 881, "value": 0.15662222222222222}

:::MLPv0.5.0 ssd 1541710962.171767473 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 882, "value": 0.1568}

:::MLPv0.5.0 ssd 1541710962.265604973 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 883, "value": 0.15697777777777777}

:::MLPv0.5.0 ssd 1541710962.361299515 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 884, "value": 0.15715555555555555}

:::MLPv0.5.0 ssd 1541710962.455456257 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 885, "value": 0.15733333333333333}

:::MLPv0.5.0 ssd 1541710962.551565170 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 886, "value": 0.1575111111111111}

:::MLPv0.5.0 ssd 1541710962.646058559 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 887, "value": 0.15768888888888888}

:::MLPv0.5.0 ssd 1541710962.742750645 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 888, "value": 0.15786666666666668}

:::MLPv0.5.0 ssd 1541710962.837311506 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 889, "value": 0.15804444444444446}

:::MLPv0.5.0 ssd 1541710962.931473732 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 890, "value": 0.15822222222222224}

:::MLPv0.5.0 ssd 1541710963.026998520 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 891, "value": 0.1584}

:::MLPv0.5.0 ssd 1541710963.122407913 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 892, "value": 0.1585777777777778}

:::MLPv0.5.0 ssd 1541710963.216698408 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 893, "value": 0.15875555555555557}

:::MLPv0.5.0 ssd 1541710963.311327457 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 894, "value": 0.15893333333333334}

:::MLPv0.5.0 ssd 1541710963.406972408 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 895, "value": 0.15911111111111112}

:::MLPv0.5.0 ssd 1541710963.500923634 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 896, "value": 0.1592888888888889}

:::MLPv0.5.0 ssd 1541710963.594898701 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 897, "value": 0.15946666666666667}

:::MLPv0.5.0 ssd 1541710963.690349340 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 898, "value": 0.15964444444444445}

:::MLPv0.5.0 ssd 1541710963.784668684 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 899, "value": 0.15982222222222223}
Iteration:    900, Loss function: 4.450, Average Loss: 4.019, avg. samples / sec: 21621.90
Iteration:    900, Loss function: 4.990, Average Loss: 4.021, avg. samples / sec: 21623.20
Iteration:    900, Loss function: 4.760, Average Loss: 4.017, avg. samples / sec: 21620.77
Iteration:    900, Loss function: 4.441, Average Loss: 4.020, avg. samples / sec: 21620.26
Iteration:    900, Loss function: 4.860, Average Loss: 4.018, avg. samples / sec: 21617.66
Iteration:    900, Loss function: 5.153, Average Loss: 4.024, avg. samples / sec: 21632.65
Iteration:    900, Loss function: 5.189, Average Loss: 4.014, avg. samples / sec: 21620.68
Iteration:    900, Loss function: 4.751, Average Loss: 4.012, avg. samples / sec: 21615.37
Iteration:    920, Loss function: 5.499, Average Loss: 4.045, avg. samples / sec: 21794.27
Iteration:    920, Loss function: 5.458, Average Loss: 4.035, avg. samples / sec: 21804.78
Iteration:    920, Loss function: 5.318, Average Loss: 4.049, avg. samples / sec: 21792.29
Iteration:    920, Loss function: 5.951, Average Loss: 4.048, avg. samples / sec: 21792.38
Iteration:    920, Loss function: 5.870, Average Loss: 4.044, avg. samples / sec: 21790.27
Iteration:    920, Loss function: 5.407, Average Loss: 4.046, avg. samples / sec: 21790.59
Iteration:    920, Loss function: 5.437, Average Loss: 4.053, avg. samples / sec: 21789.05
Iteration:    920, Loss function: 5.223, Average Loss: 4.042, avg. samples / sec: 21791.59

:::MLPv0.5.0 ssd 1541710966.221999168 (train.py:553) train_epoch: 16
Iteration:    940, Loss function: 4.471, Average Loss: 4.071, avg. samples / sec: 21905.31
Iteration:    940, Loss function: 4.907, Average Loss: 4.066, avg. samples / sec: 21907.73
Iteration:    940, Loss function: 5.799, Average Loss: 4.073, avg. samples / sec: 21905.98
Iteration:    940, Loss function: 4.851, Average Loss: 4.068, avg. samples / sec: 21904.02
Iteration:    940, Loss function: 4.937, Average Loss: 4.066, avg. samples / sec: 21894.41
Iteration:    940, Loss function: 5.700, Average Loss: 4.058, avg. samples / sec: 21898.71
Iteration:    940, Loss function: 4.994, Average Loss: 4.066, avg. samples / sec: 21902.27
Iteration:    940, Loss function: 4.820, Average Loss: 4.063, avg. samples / sec: 21904.31
Iteration:    960, Loss function: 4.551, Average Loss: 4.082, avg. samples / sec: 21842.53
Iteration:    960, Loss function: 5.054, Average Loss: 4.090, avg. samples / sec: 21835.36
Iteration:    960, Loss function: 5.043, Average Loss: 4.086, avg. samples / sec: 21833.31
Iteration:    960, Loss function: 5.030, Average Loss: 4.082, avg. samples / sec: 21830.80
Iteration:    960, Loss function: 4.903, Average Loss: 4.080, avg. samples / sec: 21833.72
Iteration:    960, Loss function: 4.780, Average Loss: 4.075, avg. samples / sec: 21832.77
Iteration:    960, Loss function: 5.050, Average Loss: 4.082, avg. samples / sec: 21833.63
Iteration:    960, Loss function: 5.138, Average Loss: 4.086, avg. samples / sec: 21818.43
Iteration:    980, Loss function: 5.231, Average Loss: 4.100, avg. samples / sec: 21694.20
Iteration:    980, Loss function: 5.108, Average Loss: 4.098, avg. samples / sec: 21686.55
Iteration:    980, Loss function: 5.077, Average Loss: 4.097, avg. samples / sec: 21680.08
Iteration:    980, Loss function: 5.074, Average Loss: 4.097, avg. samples / sec: 21680.13
Iteration:    980, Loss function: 4.665, Average Loss: 4.105, avg. samples / sec: 21676.29
Iteration:    980, Loss function: 5.203, Average Loss: 4.099, avg. samples / sec: 21678.55
Iteration:    980, Loss function: 4.805, Average Loss: 4.089, avg. samples / sec: 21677.22
Iteration:    980, Loss function: 5.083, Average Loss: 4.094, avg. samples / sec: 21654.42

:::MLPv0.5.0 ssd 1541710971.579563379 (train.py:553) train_epoch: 17
Iteration:   1000, Loss function: 5.154, Average Loss: 4.114, avg. samples / sec: 21734.03
Iteration:   1000, Loss function: 5.062, Average Loss: 4.115, avg. samples / sec: 21734.24
Iteration:   1000, Loss function: 4.711, Average Loss: 4.114, avg. samples / sec: 21737.69
Iteration:   1000, Loss function: 4.708, Average Loss: 4.114, avg. samples / sec: 21735.49
Iteration:   1000, Loss function: 4.463, Average Loss: 4.104, avg. samples / sec: 21739.88
Iteration:   1000, Loss function: 4.918, Average Loss: 4.112, avg. samples / sec: 21734.02
Iteration:   1000, Loss function: 5.096, Average Loss: 4.110, avg. samples / sec: 21749.62
Iteration:   1000, Loss function: 5.590, Average Loss: 4.120, avg. samples / sec: 21724.35
Iteration:   1020, Loss function: 4.822, Average Loss: 4.130, avg. samples / sec: 21889.23
Iteration:   1020, Loss function: 4.417, Average Loss: 4.128, avg. samples / sec: 21889.16
Iteration:   1020, Loss function: 4.113, Average Loss: 4.125, avg. samples / sec: 21893.32
Iteration:   1020, Loss function: 5.434, Average Loss: 4.130, avg. samples / sec: 21886.12
Iteration:   1020, Loss function: 4.430, Average Loss: 4.126, avg. samples / sec: 21885.69
Iteration:   1020, Loss function: 4.587, Average Loss: 4.119, avg. samples / sec: 21884.62
Iteration:   1020, Loss function: 4.416, Average Loss: 4.126, avg. samples / sec: 21881.58
Iteration:   1020, Loss function: 4.939, Average Loss: 4.132, avg. samples / sec: 21893.28

:::MLPv0.5.0 ssd 1541710977.019081831 (train.py:553) train_epoch: 18
Iteration:   1040, Loss function: 4.984, Average Loss: 4.143, avg. samples / sec: 21861.25
Iteration:   1040, Loss function: 4.659, Average Loss: 4.138, avg. samples / sec: 21863.21
Iteration:   1040, Loss function: 4.710, Average Loss: 4.129, avg. samples / sec: 21862.88
Iteration:   1040, Loss function: 4.809, Average Loss: 4.137, avg. samples / sec: 21858.22
Iteration:   1040, Loss function: 4.220, Average Loss: 4.134, avg. samples / sec: 21863.68
Iteration:   1040, Loss function: 4.782, Average Loss: 4.144, avg. samples / sec: 21858.94
Iteration:   1040, Loss function: 4.067, Average Loss: 4.140, avg. samples / sec: 21861.77
Iteration:   1040, Loss function: 5.058, Average Loss: 4.138, avg. samples / sec: 21849.80
Iteration:   1060, Loss function: 4.799, Average Loss: 4.155, avg. samples / sec: 21814.28
Iteration:   1060, Loss function: 4.751, Average Loss: 4.148, avg. samples / sec: 21820.92
Iteration:   1060, Loss function: 4.676, Average Loss: 4.148, avg. samples / sec: 21821.23
Iteration:   1060, Loss function: 4.321, Average Loss: 4.143, avg. samples / sec: 21821.12
Iteration:   1060, Loss function: 4.650, Average Loss: 4.154, avg. samples / sec: 21819.84
Iteration:   1060, Loss function: 4.551, Average Loss: 4.151, avg. samples / sec: 21821.52
Iteration:   1060, Loss function: 5.009, Average Loss: 4.150, avg. samples / sec: 21820.10
Iteration:   1060, Loss function: 5.986, Average Loss: 4.142, avg. samples / sec: 21809.73
Iteration:   1080, Loss function: 4.966, Average Loss: 4.159, avg. samples / sec: 21823.04
Iteration:   1080, Loss function: 4.509, Average Loss: 4.164, avg. samples / sec: 21821.63
Iteration:   1080, Loss function: 4.829, Average Loss: 4.157, avg. samples / sec: 21832.22
Iteration:   1080, Loss function: 4.396, Average Loss: 4.155, avg. samples / sec: 21820.34
Iteration:   1080, Loss function: 4.456, Average Loss: 4.160, avg. samples / sec: 21824.02
Iteration:   1080, Loss function: 4.377, Average Loss: 4.158, avg. samples / sec: 21818.57
Iteration:   1080, Loss function: 5.065, Average Loss: 4.168, avg. samples / sec: 21815.24
Iteration:   1080, Loss function: 4.631, Average Loss: 4.162, avg. samples / sec: 21810.30

:::MLPv0.5.0 ssd 1541710982.460440874 (train.py:553) train_epoch: 19
Iteration:   1100, Loss function: 4.333, Average Loss: 4.175, avg. samples / sec: 21855.87
Iteration:   1100, Loss function: 4.492, Average Loss: 4.178, avg. samples / sec: 21866.92
Iteration:   1100, Loss function: 4.642, Average Loss: 4.168, avg. samples / sec: 21858.66
Iteration:   1100, Loss function: 4.857, Average Loss: 4.172, avg. samples / sec: 21852.23
Iteration:   1100, Loss function: 4.781, Average Loss: 4.172, avg. samples / sec: 21853.34
Iteration:   1100, Loss function: 4.437, Average Loss: 4.173, avg. samples / sec: 21864.65
Iteration:   1100, Loss function: 4.504, Average Loss: 4.167, avg. samples / sec: 21849.04
Iteration:   1100, Loss function: 5.056, Average Loss: 4.165, avg. samples / sec: 21849.81
Iteration:   1120, Loss function: 4.440, Average Loss: 4.183, avg. samples / sec: 21808.83
Iteration:   1120, Loss function: 4.452, Average Loss: 4.179, avg. samples / sec: 21815.09
Iteration:   1120, Loss function: 5.190, Average Loss: 4.183, avg. samples / sec: 21812.75
Iteration:   1120, Loss function: 5.258, Average Loss: 4.181, avg. samples / sec: 21804.74
Iteration:   1120, Loss function: 4.771, Average Loss: 4.177, avg. samples / sec: 21802.57
Iteration:   1120, Loss function: 4.816, Average Loss: 4.174, avg. samples / sec: 21809.99
Iteration:   1120, Loss function: 4.966, Average Loss: 4.186, avg. samples / sec: 21800.45
Iteration:   1120, Loss function: 4.758, Average Loss: 4.175, avg. samples / sec: 21797.55
Iteration:   1140, Loss function: 4.636, Average Loss: 4.190, avg. samples / sec: 21828.46
Iteration:   1140, Loss function: 5.090, Average Loss: 4.191, avg. samples / sec: 21820.01
Iteration:   1140, Loss function: 4.075, Average Loss: 4.191, avg. samples / sec: 21823.21
Iteration:   1140, Loss function: 4.846, Average Loss: 4.185, avg. samples / sec: 21825.60
Iteration:   1140, Loss function: 4.557, Average Loss: 4.196, avg. samples / sec: 21824.71
Iteration:   1140, Loss function: 4.658, Average Loss: 4.188, avg. samples / sec: 21835.15
Iteration:   1140, Loss function: 4.926, Average Loss: 4.185, avg. samples / sec: 21820.36
Iteration:   1140, Loss function: 4.084, Average Loss: 4.194, avg. samples / sec: 21805.02

:::MLPv0.5.0 ssd 1541710987.901715994 (train.py:553) train_epoch: 20
Iteration:   1160, Loss function: 4.533, Average Loss: 4.200, avg. samples / sec: 21853.68
Iteration:   1160, Loss function: 4.928, Average Loss: 4.202, avg. samples / sec: 21854.02
Iteration:   1160, Loss function: 4.948, Average Loss: 4.209, avg. samples / sec: 21858.52
Iteration:   1160, Loss function: 4.827, Average Loss: 4.203, avg. samples / sec: 21867.03
Iteration:   1160, Loss function: 4.681, Average Loss: 4.199, avg. samples / sec: 21855.94
Iteration:   1160, Loss function: 4.755, Average Loss: 4.195, avg. samples / sec: 21852.57
Iteration:   1160, Loss function: 5.178, Average Loss: 4.202, avg. samples / sec: 21849.06
Iteration:   1160, Loss function: 4.215, Average Loss: 4.195, avg. samples / sec: 21855.79
Iteration:   1180, Loss function: 4.602, Average Loss: 4.211, avg. samples / sec: 21858.24
Iteration:   1180, Loss function: 4.007, Average Loss: 4.204, avg. samples / sec: 21855.57
Iteration:   1180, Loss function: 4.751, Average Loss: 4.202, avg. samples / sec: 21860.66
Iteration:   1180, Loss function: 4.066, Average Loss: 4.209, avg. samples / sec: 21858.97
Iteration:   1180, Loss function: 3.823, Average Loss: 4.201, avg. samples / sec: 21860.52
Iteration:   1180, Loss function: 3.895, Average Loss: 4.204, avg. samples / sec: 21855.05
Iteration:   1180, Loss function: 4.134, Average Loss: 4.215, avg. samples / sec: 21851.51
Iteration:   1180, Loss function: 4.421, Average Loss: 4.208, avg. samples / sec: 21847.46
Iteration:   1200, Loss function: 4.013, Average Loss: 4.214, avg. samples / sec: 21851.41
Iteration:   1200, Loss function: 4.852, Average Loss: 4.217, avg. samples / sec: 21840.89
Iteration:   1200, Loss function: 4.028, Average Loss: 4.219, avg. samples / sec: 21845.05
Iteration:   1200, Loss function: 4.568, Average Loss: 4.206, avg. samples / sec: 21841.44
Iteration:   1200, Loss function: 4.269, Average Loss: 4.205, avg. samples / sec: 21838.30
Iteration:   1200, Loss function: 4.620, Average Loss: 4.214, avg. samples / sec: 21841.67
Iteration:   1200, Loss function: 4.565, Average Loss: 4.209, avg. samples / sec: 21841.16
Iteration:   1200, Loss function: 4.526, Average Loss: 4.210, avg. samples / sec: 21830.78

:::MLPv0.5.0 ssd 1541710993.245321989 (train.py:553) train_epoch: 21
Iteration:   1220, Loss function: 5.117, Average Loss: 4.223, avg. samples / sec: 21758.21
Iteration:   1220, Loss function: 4.680, Average Loss: 4.222, avg. samples / sec: 21758.87
Iteration:   1220, Loss function: 4.608, Average Loss: 4.213, avg. samples / sec: 21768.74
Iteration:   1220, Loss function: 4.368, Average Loss: 4.212, avg. samples / sec: 21756.00
Iteration:   1220, Loss function: 4.663, Average Loss: 4.213, avg. samples / sec: 21757.85
Iteration:   1220, Loss function: 3.914, Average Loss: 4.221, avg. samples / sec: 21755.19
Iteration:   1220, Loss function: 4.690, Average Loss: 4.223, avg. samples / sec: 21751.86
Iteration:   1220, Loss function: 4.024, Average Loss: 4.215, avg. samples / sec: 21749.77
Iteration:   1240, Loss function: 4.524, Average Loss: 4.221, avg. samples / sec: 21713.34
Iteration:   1240, Loss function: 4.673, Average Loss: 4.229, avg. samples / sec: 21705.24
Iteration:   1240, Loss function: 4.546, Average Loss: 4.217, avg. samples / sec: 21709.55
Iteration:   1240, Loss function: 4.895, Average Loss: 4.229, avg. samples / sec: 21707.77
Iteration:   1240, Loss function: 4.335, Average Loss: 4.222, avg. samples / sec: 21714.28
Iteration:   1240, Loss function: 4.319, Average Loss: 4.229, avg. samples / sec: 21709.76
Iteration:   1240, Loss function: 4.819, Average Loss: 4.218, avg. samples / sec: 21700.56
Iteration:   1240, Loss function: 5.327, Average Loss: 4.232, avg. samples / sec: 21698.21
Iteration:   1260, Loss function: 4.696, Average Loss: 4.236, avg. samples / sec: 21761.81
Iteration:   1260, Loss function: 4.324, Average Loss: 4.239, avg. samples / sec: 21759.09
Iteration:   1260, Loss function: 4.975, Average Loss: 4.229, avg. samples / sec: 21746.01
Iteration:   1260, Loss function: 4.991, Average Loss: 4.235, avg. samples / sec: 21753.45
Iteration:   1260, Loss function: 4.495, Average Loss: 4.223, avg. samples / sec: 21749.82
Iteration:   1260, Loss function: 4.420, Average Loss: 4.226, avg. samples / sec: 21751.77
Iteration:   1260, Loss function: 4.817, Average Loss: 4.236, avg. samples / sec: 21745.99
Iteration:   1260, Loss function: 4.778, Average Loss: 4.227, avg. samples / sec: 21739.88

:::MLPv0.5.0 ssd 1541710998.721389771 (train.py:553) train_epoch: 22
Iteration:   1280, Loss function: 4.446, Average Loss: 4.242, avg. samples / sec: 21680.93
Iteration:   1280, Loss function: 3.843, Average Loss: 4.240, avg. samples / sec: 21681.38
Iteration:   1280, Loss function: 4.549, Average Loss: 4.232, avg. samples / sec: 21679.65
Iteration:   1280, Loss function: 4.980, Average Loss: 4.236, avg. samples / sec: 21675.94
Iteration:   1280, Loss function: 4.593, Average Loss: 4.246, avg. samples / sec: 21670.37
Iteration:   1280, Loss function: 4.430, Average Loss: 4.241, avg. samples / sec: 21665.68
Iteration:   1280, Loss function: 5.241, Average Loss: 4.235, avg. samples / sec: 21687.59
Iteration:   1280, Loss function: 4.421, Average Loss: 4.228, avg. samples / sec: 21671.02
Iteration:   1300, Loss function: 4.942, Average Loss: 4.232, avg. samples / sec: 21833.57
Iteration:   1300, Loss function: 4.578, Average Loss: 4.247, avg. samples / sec: 21822.72
Iteration:   1300, Loss function: 4.673, Average Loss: 4.242, avg. samples / sec: 21820.73
Iteration:   1300, Loss function: 4.149, Average Loss: 4.241, avg. samples / sec: 21821.89
Iteration:   1300, Loss function: 4.054, Average Loss: 4.247, avg. samples / sec: 21820.97
Iteration:   1300, Loss function: 4.126, Average Loss: 4.236, avg. samples / sec: 21815.39
Iteration:   1300, Loss function: 4.267, Average Loss: 4.246, avg. samples / sec: 21814.51
Iteration:   1300, Loss function: 4.353, Average Loss: 4.250, avg. samples / sec: 21818.93
Iteration:   1320, Loss function: 4.724, Average Loss: 4.240, avg. samples / sec: 21876.84
Iteration:   1320, Loss function: 4.673, Average Loss: 4.241, avg. samples / sec: 21864.41
Iteration:   1320, Loss function: 4.401, Average Loss: 4.252, avg. samples / sec: 21864.20
Iteration:   1320, Loss function: 4.653, Average Loss: 4.244, avg. samples / sec: 21869.20
Iteration:   1320, Loss function: 4.743, Average Loss: 4.256, avg. samples / sec: 21871.43
Iteration:   1320, Loss function: 4.350, Average Loss: 4.251, avg. samples / sec: 21869.93
Iteration:   1320, Loss function: 4.620, Average Loss: 4.252, avg. samples / sec: 21868.10
Iteration:   1320, Loss function: 4.974, Average Loss: 4.247, avg. samples / sec: 21865.60

:::MLPv0.5.0 ssd 1541711004.166396618 (train.py:553) train_epoch: 23
Iteration:   1340, Loss function: 4.433, Average Loss: 4.257, avg. samples / sec: 21807.65
Iteration:   1340, Loss function: 4.382, Average Loss: 4.250, avg. samples / sec: 21812.24
Iteration:   1340, Loss function: 4.777, Average Loss: 4.257, avg. samples / sec: 21811.38
Iteration:   1340, Loss function: 4.458, Average Loss: 4.255, avg. samples / sec: 21811.25
Iteration:   1340, Loss function: 4.160, Average Loss: 4.249, avg. samples / sec: 21809.95
Iteration:   1340, Loss function: 4.163, Average Loss: 4.258, avg. samples / sec: 21807.52
Iteration:   1340, Loss function: 4.625, Average Loss: 4.248, avg. samples / sec: 21801.47
Iteration:   1340, Loss function: 5.041, Average Loss: 4.246, avg. samples / sec: 21802.02
Iteration:   1360, Loss function: 4.183, Average Loss: 4.257, avg. samples / sec: 21807.09
Iteration:   1360, Loss function: 4.025, Average Loss: 4.256, avg. samples / sec: 21806.98
Iteration:   1360, Loss function: 4.828, Average Loss: 4.250, avg. samples / sec: 21809.52
Iteration:   1360, Loss function: 4.858, Average Loss: 4.255, avg. samples / sec: 21805.47
Iteration:   1360, Loss function: 4.800, Average Loss: 4.260, avg. samples / sec: 21806.58
Iteration:   1360, Loss function: 4.753, Average Loss: 4.252, avg. samples / sec: 21806.95
Iteration:   1360, Loss function: 4.131, Average Loss: 4.259, avg. samples / sec: 21797.98
Iteration:   1360, Loss function: 4.739, Average Loss: 4.254, avg. samples / sec: 21794.76
Iteration:   1380, Loss function: 4.910, Average Loss: 4.261, avg. samples / sec: 21782.66
Iteration:   1380, Loss function: 4.899, Average Loss: 4.260, avg. samples / sec: 21786.14
Iteration:   1380, Loss function: 4.368, Average Loss: 4.255, avg. samples / sec: 21777.63
Iteration:   1380, Loss function: 3.931, Average Loss: 4.257, avg. samples / sec: 21785.06
Iteration:   1380, Loss function: 4.070, Average Loss: 4.259, avg. samples / sec: 21779.60
Iteration:   1380, Loss function: 4.301, Average Loss: 4.261, avg. samples / sec: 21774.44
Iteration:   1380, Loss function: 4.610, Average Loss: 4.266, avg. samples / sec: 21773.38
Iteration:   1380, Loss function: 4.645, Average Loss: 4.257, avg. samples / sec: 21770.47

:::MLPv0.5.0 ssd 1541711009.620904446 (train.py:553) train_epoch: 24
Iteration:   1400, Loss function: 4.811, Average Loss: 4.263, avg. samples / sec: 21805.46
Iteration:   1400, Loss function: 4.723, Average Loss: 4.265, avg. samples / sec: 21800.92
Iteration:   1400, Loss function: 4.517, Average Loss: 4.260, avg. samples / sec: 21802.27
Iteration:   1400, Loss function: 4.177, Average Loss: 4.270, avg. samples / sec: 21808.07
Iteration:   1400, Loss function: 4.059, Average Loss: 4.263, avg. samples / sec: 21802.54
Iteration:   1400, Loss function: 4.566, Average Loss: 4.262, avg. samples / sec: 21810.50
Iteration:   1400, Loss function: 4.497, Average Loss: 4.261, avg. samples / sec: 21798.45
Iteration:   1400, Loss function: 4.657, Average Loss: 4.262, avg. samples / sec: 21798.15
Iteration:   1420, Loss function: 3.996, Average Loss: 4.264, avg. samples / sec: 21834.70
Iteration:   1420, Loss function: 5.166, Average Loss: 4.273, avg. samples / sec: 21826.67
Iteration:   1420, Loss function: 5.169, Average Loss: 4.269, avg. samples / sec: 21831.31
Iteration:   1420, Loss function: 5.051, Average Loss: 4.270, avg. samples / sec: 21829.86
Iteration:   1420, Loss function: 5.298, Average Loss: 4.269, avg. samples / sec: 21818.36
Iteration:   1420, Loss function: 5.192, Average Loss: 4.267, avg. samples / sec: 21827.18
Iteration:   1420, Loss function: 4.638, Average Loss: 4.274, avg. samples / sec: 21823.52
Iteration:   1420, Loss function: 5.017, Average Loss: 4.271, avg. samples / sec: 21824.16
Iteration:   1440, Loss function: 3.963, Average Loss: 4.276, avg. samples / sec: 21825.53
Iteration:   1440, Loss function: 4.538, Average Loss: 4.277, avg. samples / sec: 21832.97
Iteration:   1440, Loss function: 4.181, Average Loss: 4.276, avg. samples / sec: 21830.51
Iteration:   1440, Loss function: 5.048, Average Loss: 4.270, avg. samples / sec: 21821.45
Iteration:   1440, Loss function: 4.521, Average Loss: 4.276, avg. samples / sec: 21826.75
Iteration:   1440, Loss function: 4.404, Average Loss: 4.276, avg. samples / sec: 21828.53
Iteration:   1440, Loss function: 4.223, Average Loss: 4.272, avg. samples / sec: 21825.37
Iteration:   1440, Loss function: 4.525, Average Loss: 4.274, avg. samples / sec: 21820.24

:::MLPv0.5.0 ssd 1541711014.964730740 (train.py:553) train_epoch: 25
Iteration:   1460, Loss function: 4.931, Average Loss: 4.278, avg. samples / sec: 21908.88
Iteration:   1460, Loss function: 4.078, Average Loss: 4.277, avg. samples / sec: 21901.10
Iteration:   1460, Loss function: 5.002, Average Loss: 4.277, avg. samples / sec: 21905.33
Iteration:   1460, Loss function: 4.748, Average Loss: 4.274, avg. samples / sec: 21905.46
Iteration:   1460, Loss function: 4.274, Average Loss: 4.279, avg. samples / sec: 21897.09
Iteration:   1460, Loss function: 4.611, Average Loss: 4.278, avg. samples / sec: 21905.52
Iteration:   1460, Loss function: 4.997, Average Loss: 4.278, avg. samples / sec: 21901.77
Iteration:   1460, Loss function: 4.777, Average Loss: 4.270, avg. samples / sec: 21895.46
Iteration:   1480, Loss function: 4.298, Average Loss: 4.285, avg. samples / sec: 21839.94
Iteration:   1480, Loss function: 4.526, Average Loss: 4.282, avg. samples / sec: 21828.82
Iteration:   1480, Loss function: 4.451, Average Loss: 4.283, avg. samples / sec: 21836.56
Iteration:   1480, Loss function: 4.446, Average Loss: 4.282, avg. samples / sec: 21839.37
Iteration:   1480, Loss function: 3.765, Average Loss: 4.275, avg. samples / sec: 21837.46
Iteration:   1480, Loss function: 4.376, Average Loss: 4.284, avg. samples / sec: 21832.98
Iteration:   1480, Loss function: 4.482, Average Loss: 4.285, avg. samples / sec: 21832.19
Iteration:   1480, Loss function: 4.020, Average Loss: 4.276, avg. samples / sec: 21829.49
Iteration:   1500, Loss function: 4.451, Average Loss: 4.287, avg. samples / sec: 21921.19
Iteration:   1500, Loss function: 4.154, Average Loss: 4.275, avg. samples / sec: 21924.88
Iteration:   1500, Loss function: 4.086, Average Loss: 4.281, avg. samples / sec: 21916.92
Iteration:   1500, Loss function: 4.329, Average Loss: 4.284, avg. samples / sec: 21916.35
Iteration:   1500, Loss function: 4.082, Average Loss: 4.283, avg. samples / sec: 21914.63
Iteration:   1500, Loss function: 4.480, Average Loss: 4.285, avg. samples / sec: 21918.58
Iteration:   1500, Loss function: 4.225, Average Loss: 4.277, avg. samples / sec: 21920.63
Iteration:   1500, Loss function: 4.288, Average Loss: 4.286, avg. samples / sec: 21914.46

:::MLPv0.5.0 ssd 1541711020.394185305 (train.py:553) train_epoch: 26
Iteration:   1520, Loss function: 3.721, Average Loss: 4.284, avg. samples / sec: 21837.17
Iteration:   1520, Loss function: 4.064, Average Loss: 4.287, avg. samples / sec: 21825.19
Iteration:   1520, Loss function: 4.373, Average Loss: 4.283, avg. samples / sec: 21825.59
Iteration:   1520, Loss function: 4.270, Average Loss: 4.287, avg. samples / sec: 21829.26
Iteration:   1520, Loss function: 4.409, Average Loss: 4.285, avg. samples / sec: 21826.12
Iteration:   1520, Loss function: 3.992, Average Loss: 4.283, avg. samples / sec: 21823.26
Iteration:   1520, Loss function: 4.179, Average Loss: 4.278, avg. samples / sec: 21827.55
Iteration:   1520, Loss function: 4.264, Average Loss: 4.276, avg. samples / sec: 21815.97
Iteration:   1540, Loss function: 3.574, Average Loss: 4.278, avg. samples / sec: 21847.88
Iteration:   1540, Loss function: 4.201, Average Loss: 4.284, avg. samples / sec: 21836.18
Iteration:   1540, Loss function: 3.758, Average Loss: 4.282, avg. samples / sec: 21842.43
Iteration:   1540, Loss function: 4.111, Average Loss: 4.287, avg. samples / sec: 21838.57
Iteration:   1540, Loss function: 3.867, Average Loss: 4.283, avg. samples / sec: 21836.88
Iteration:   1540, Loss function: 4.817, Average Loss: 4.290, avg. samples / sec: 21836.76
Iteration:   1540, Loss function: 4.339, Average Loss: 4.281, avg. samples / sec: 21839.01
Iteration:   1540, Loss function: 4.172, Average Loss: 4.288, avg. samples / sec: 21832.38

:::MLPv0.5.0 ssd 1541711025.834192753 (train.py:553) train_epoch: 27
Iteration:   1560, Loss function: 4.614, Average Loss: 4.290, avg. samples / sec: 21834.59
Iteration:   1560, Loss function: 3.966, Average Loss: 4.282, avg. samples / sec: 21826.61
Iteration:   1560, Loss function: 4.350, Average Loss: 4.286, avg. samples / sec: 21830.08
Iteration:   1560, Loss function: 3.979, Average Loss: 4.280, avg. samples / sec: 21824.29
Iteration:   1560, Loss function: 4.382, Average Loss: 4.279, avg. samples / sec: 21825.23
Iteration:   1560, Loss function: 4.126, Average Loss: 4.282, avg. samples / sec: 21822.31
Iteration:   1560, Loss function: 3.973, Average Loss: 4.279, avg. samples / sec: 21813.02
Iteration:   1560, Loss function: 4.432, Average Loss: 4.287, avg. samples / sec: 21810.94
Iteration:   1580, Loss function: 4.485, Average Loss: 4.285, avg. samples / sec: 21870.79
Iteration:   1580, Loss function: 3.910, Average Loss: 4.278, avg. samples / sec: 21865.54
Iteration:   1580, Loss function: 4.347, Average Loss: 4.281, avg. samples / sec: 21863.97
Iteration:   1580, Loss function: 4.206, Average Loss: 4.292, avg. samples / sec: 21854.97
Iteration:   1580, Loss function: 4.611, Average Loss: 4.288, avg. samples / sec: 21875.71
Iteration:   1580, Loss function: 4.261, Average Loss: 4.288, avg. samples / sec: 21857.99
Iteration:   1580, Loss function: 4.703, Average Loss: 4.283, avg. samples / sec: 21863.41
Iteration:   1580, Loss function: 4.874, Average Loss: 4.280, avg. samples / sec: 21831.08
Iteration:   1600, Loss function: 4.208, Average Loss: 4.281, avg. samples / sec: 21911.86
Iteration:   1600, Loss function: 4.285, Average Loss: 4.283, avg. samples / sec: 21861.48
Iteration:   1600, Loss function: 4.403, Average Loss: 4.284, avg. samples / sec: 21871.40
Iteration:   1600, Loss function: 3.450, Average Loss: 4.288, avg. samples / sec: 21868.70
Iteration:   1600, Loss function: 4.492, Average Loss: 4.290, avg. samples / sec: 21864.56
Iteration:   1600, Loss function: 4.414, Average Loss: 4.276, avg. samples / sec: 21864.74
Iteration:   1600, Loss function: 3.681, Average Loss: 4.279, avg. samples / sec: 21862.57
Iteration:   1600, Loss function: 4.251, Average Loss: 4.288, avg. samples / sec: 21862.06

:::MLPv0.5.0 ssd 1541711031.267976284 (train.py:553) train_epoch: 28
Iteration:   1620, Loss function: 4.012, Average Loss: 4.280, avg. samples / sec: 21837.99
Iteration:   1620, Loss function: 3.685, Average Loss: 4.291, avg. samples / sec: 21839.61
Iteration:   1620, Loss function: 4.222, Average Loss: 4.284, avg. samples / sec: 21833.08
Iteration:   1620, Loss function: 3.916, Average Loss: 4.279, avg. samples / sec: 21838.55
Iteration:   1620, Loss function: 3.945, Average Loss: 4.284, avg. samples / sec: 21834.51
Iteration:   1620, Loss function: 4.029, Average Loss: 4.289, avg. samples / sec: 21831.19
Iteration:   1620, Loss function: 4.657, Average Loss: 4.288, avg. samples / sec: 21836.49
Iteration:   1620, Loss function: 4.226, Average Loss: 4.278, avg. samples / sec: 21828.52
Iteration:   1640, Loss function: 3.686, Average Loss: 4.278, avg. samples / sec: 21865.38
Iteration:   1640, Loss function: 4.335, Average Loss: 4.278, avg. samples / sec: 21873.09
Iteration:   1640, Loss function: 4.312, Average Loss: 4.278, avg. samples / sec: 21864.95
Iteration:   1640, Loss function: 4.474, Average Loss: 4.283, avg. samples / sec: 21860.56
Iteration:   1640, Loss function: 4.432, Average Loss: 4.290, avg. samples / sec: 21859.67
Iteration:   1640, Loss function: 4.662, Average Loss: 4.286, avg. samples / sec: 21869.36
Iteration:   1640, Loss function: 4.476, Average Loss: 4.288, avg. samples / sec: 21866.97
Iteration:   1640, Loss function: 4.146, Average Loss: 4.282, avg. samples / sec: 21860.02
Iteration:   1660, Loss function: 4.030, Average Loss: 4.283, avg. samples / sec: 21768.18
Iteration:   1660, Loss function: 3.609, Average Loss: 4.279, avg. samples / sec: 21766.03
Iteration:   1660, Loss function: 3.942, Average Loss: 4.279, avg. samples / sec: 21759.00
Iteration:   1660, Loss function: 4.715, Average Loss: 4.288, avg. samples / sec: 21760.10
Iteration:   1660, Loss function: 4.573, Average Loss: 4.288, avg. samples / sec: 21759.28
Iteration:   1660, Loss function: 3.985, Average Loss: 4.280, avg. samples / sec: 21755.20
Iteration:   1660, Loss function: 4.195, Average Loss: 4.277, avg. samples / sec: 21743.90
Iteration:   1660, Loss function: 4.237, Average Loss: 4.290, avg. samples / sec: 21754.27

:::MLPv0.5.0 ssd 1541711036.625487566 (train.py:553) train_epoch: 29
Iteration:   1680, Loss function: 3.942, Average Loss: 4.274, avg. samples / sec: 21741.50
Iteration:   1680, Loss function: 4.062, Average Loss: 4.279, avg. samples / sec: 21738.56
Iteration:   1680, Loss function: 4.502, Average Loss: 4.287, avg. samples / sec: 21734.54
Iteration:   1680, Loss function: 4.924, Average Loss: 4.283, avg. samples / sec: 21731.57
Iteration:   1680, Loss function: 4.504, Average Loss: 4.290, avg. samples / sec: 21739.84
Iteration:   1680, Loss function: 4.264, Average Loss: 4.281, avg. samples / sec: 21724.11
Iteration:   1680, Loss function: 4.458, Average Loss: 4.289, avg. samples / sec: 21729.39
Iteration:   1680, Loss function: 4.424, Average Loss: 4.275, avg. samples / sec: 21728.21
Iteration:   1700, Loss function: 4.357, Average Loss: 4.285, avg. samples / sec: 21793.34
Iteration:   1700, Loss function: 4.143, Average Loss: 4.274, avg. samples / sec: 21798.69
Iteration:   1700, Loss function: 4.483, Average Loss: 4.289, avg. samples / sec: 21791.95
Iteration:   1700, Loss function: 4.146, Average Loss: 4.282, avg. samples / sec: 21791.67
Iteration:   1700, Loss function: 4.441, Average Loss: 4.287, avg. samples / sec: 21796.37
Iteration:   1700, Loss function: 4.735, Average Loss: 4.278, avg. samples / sec: 21789.76
Iteration:   1700, Loss function: 4.677, Average Loss: 4.279, avg. samples / sec: 21786.75
Iteration:   1700, Loss function: 4.157, Average Loss: 4.274, avg. samples / sec: 21745.67
Iteration:   1720, Loss function: 4.143, Average Loss: 4.287, avg. samples / sec: 21924.62
Iteration:   1720, Loss function: 3.991, Average Loss: 4.290, avg. samples / sec: 21923.97
Iteration:   1720, Loss function: 3.975, Average Loss: 4.287, avg. samples / sec: 21921.73
Iteration:   1720, Loss function: 4.577, Average Loss: 4.276, avg. samples / sec: 21921.97
Iteration:   1720, Loss function: 4.910, Average Loss: 4.283, avg. samples / sec: 21923.32
Iteration:   1720, Loss function: 4.498, Average Loss: 4.280, avg. samples / sec: 21923.59
Iteration:   1720, Loss function: 4.173, Average Loss: 4.273, avg. samples / sec: 21961.29
Iteration:   1720, Loss function: 4.409, Average Loss: 4.274, avg. samples / sec: 21916.84

:::MLPv0.5.0 ssd 1541711042.065624475 (train.py:553) train_epoch: 30
Iteration:   1740, Loss function: 4.591, Average Loss: 4.290, avg. samples / sec: 21864.56
Iteration:   1740, Loss function: 3.976, Average Loss: 4.273, avg. samples / sec: 21869.29
Iteration:   1740, Loss function: 4.356, Average Loss: 4.283, avg. samples / sec: 21864.46
Iteration:   1740, Loss function: 4.148, Average Loss: 4.278, avg. samples / sec: 21865.60
Iteration:   1740, Loss function: 4.155, Average Loss: 4.287, avg. samples / sec: 21863.23
Iteration:   1740, Loss function: 4.309, Average Loss: 4.280, avg. samples / sec: 21860.86
Iteration:   1740, Loss function: 3.929, Average Loss: 4.271, avg. samples / sec: 21867.51
Iteration:   1740, Loss function: 4.161, Average Loss: 4.284, avg. samples / sec: 21853.38
Iteration:   1760, Loss function: 4.003, Average Loss: 4.288, avg. samples / sec: 21841.98
Iteration:   1760, Loss function: 4.375, Average Loss: 4.271, avg. samples / sec: 21843.90
Iteration:   1760, Loss function: 4.338, Average Loss: 4.285, avg. samples / sec: 21841.17
Iteration:   1760, Loss function: 3.687, Average Loss: 4.281, avg. samples / sec: 21839.43
Iteration:   1760, Loss function: 3.897, Average Loss: 4.278, avg. samples / sec: 21842.73
Iteration:   1760, Loss function: 4.032, Average Loss: 4.281, avg. samples / sec: 21846.21
Iteration:   1760, Loss function: 4.053, Average Loss: 4.276, avg. samples / sec: 21833.55
Iteration:   1760, Loss function: 4.094, Average Loss: 4.269, avg. samples / sec: 21837.91
Iteration:   1780, Loss function: 4.351, Average Loss: 4.271, avg. samples / sec: 21648.06
Iteration:   1780, Loss function: 4.270, Average Loss: 4.269, avg. samples / sec: 21655.11
Iteration:   1780, Loss function: 4.663, Average Loss: 4.281, avg. samples / sec: 21648.91
Iteration:   1780, Loss function: 4.191, Average Loss: 4.285, avg. samples / sec: 21643.96
Iteration:   1780, Loss function: 3.965, Average Loss: 4.276, avg. samples / sec: 21650.62
Iteration:   1780, Loss function: 4.632, Average Loss: 4.280, avg. samples / sec: 21642.21
Iteration:   1780, Loss function: 4.818, Average Loss: 4.287, avg. samples / sec: 21635.83
Iteration:   1780, Loss function: 4.441, Average Loss: 4.277, avg. samples / sec: 21639.41

:::MLPv0.5.0 ssd 1541711047.518767595 (train.py:553) train_epoch: 31
Iteration:   1800, Loss function: 3.992, Average Loss: 4.279, avg. samples / sec: 21868.91
Iteration:   1800, Loss function: 4.362, Average Loss: 4.281, avg. samples / sec: 21872.12
Iteration:   1800, Loss function: 4.344, Average Loss: 4.272, avg. samples / sec: 21861.10
Iteration:   1800, Loss function: 3.807, Average Loss: 4.275, avg. samples / sec: 21866.18
Iteration:   1800, Loss function: 3.685, Average Loss: 4.269, avg. samples / sec: 21860.62
Iteration:   1800, Loss function: 4.452, Average Loss: 4.287, avg. samples / sec: 21867.79
Iteration:   1800, Loss function: 3.824, Average Loss: 4.278, avg. samples / sec: 21867.17
Iteration:   1800, Loss function: 4.318, Average Loss: 4.284, avg. samples / sec: 21858.89
Iteration:   1820, Loss function: 4.434, Average Loss: 4.287, avg. samples / sec: 21839.85
Iteration:   1820, Loss function: 4.273, Average Loss: 4.280, avg. samples / sec: 21831.00
Iteration:   1820, Loss function: 4.045, Average Loss: 4.273, avg. samples / sec: 21835.08
Iteration:   1820, Loss function: 4.990, Average Loss: 4.274, avg. samples / sec: 21829.95
Iteration:   1820, Loss function: 4.416, Average Loss: 4.285, avg. samples / sec: 21841.05
Iteration:   1820, Loss function: 4.877, Average Loss: 4.270, avg. samples / sec: 21833.38
Iteration:   1820, Loss function: 4.037, Average Loss: 4.279, avg. samples / sec: 21835.27
Iteration:   1820, Loss function: 4.302, Average Loss: 4.282, avg. samples / sec: 21823.93
Iteration:   1840, Loss function: 4.402, Average Loss: 4.287, avg. samples / sec: 21803.29
Iteration:   1840, Loss function: 4.031, Average Loss: 4.283, avg. samples / sec: 21810.94
Iteration:   1840, Loss function: 4.746, Average Loss: 4.280, avg. samples / sec: 21801.91
Iteration:   1840, Loss function: 4.399, Average Loss: 4.284, avg. samples / sec: 21799.21
Iteration:   1840, Loss function: 4.148, Average Loss: 4.275, avg. samples / sec: 21799.30
Iteration:   1840, Loss function: 4.349, Average Loss: 4.272, avg. samples / sec: 21801.51
Iteration:   1840, Loss function: 4.210, Average Loss: 4.289, avg. samples / sec: 21799.46
Iteration:   1840, Loss function: 3.904, Average Loss: 4.281, avg. samples / sec: 21795.65

:::MLPv0.5.0 ssd 1541711052.959286928 (train.py:553) train_epoch: 32
Iteration:   1860, Loss function: 3.656, Average Loss: 4.282, avg. samples / sec: 21839.95
Iteration:   1860, Loss function: 3.932, Average Loss: 4.277, avg. samples / sec: 21852.02
Iteration:   1860, Loss function: 3.949, Average Loss: 4.268, avg. samples / sec: 21843.95
Iteration:   1860, Loss function: 3.993, Average Loss: 4.281, avg. samples / sec: 21837.15
Iteration:   1860, Loss function: 4.204, Average Loss: 4.272, avg. samples / sec: 21841.15
Iteration:   1860, Loss function: 4.175, Average Loss: 4.284, avg. samples / sec: 21839.96
Iteration:   1860, Loss function: 3.972, Average Loss: 4.277, avg. samples / sec: 21837.38
Iteration:   1860, Loss function: 4.248, Average Loss: 4.280, avg. samples / sec: 21834.91

































































:::MLPv0.5.0 ssd 1541711055.497454643 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711055.498287916 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711055.499038458 (train.py:220) eval_start: 32
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.29 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.29 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.29 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.29 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.29 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.29 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.29 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 6.29 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(264992, 7)
Loading and preparing results...
(264992, 7)
Converting ndarray to lists...
0/264992
0/264992
(264992, 7)
Converting ndarray to lists...
0/264992
(264992, 7)
0/264992
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(264992, 7)
Converting ndarray to lists...
0/264992
Converting ndarray to lists...
Loading and preparing results...
(264992, 7)
(264992, 7)
0/264992
Converting ndarray to lists...
0/264992
(264992, 7)
0/264992
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(264992, 7)
(264992, 7)
Converting ndarray to lists...
0/264992
0/264992
(264992, 7)
0/264992
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(264992, 7)
0/264992
Converting ndarray to lists...
Converting ndarray to lists...
(264992, 7)
(264992, 7)
0/264992
0/264992
Loading and preparing results...
Converting ndarray to lists...
(264992, 7)
0/264992
Loading and preparing results...
Converting ndarray to lists...
(264992, 7)
0/264992
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(264992, 7)
Converting ndarray to lists...
(264992, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/264992
(264992, 7)
0/264992
(264992, 7)
Converting ndarray to lists...
0/264992
Loading and preparing results...
(264992, 7)
Converting ndarray to lists...
Loading and preparing results...
0/264992
Loading and preparing results...
Loading and preparing results...
(264992, 7)
Loading and preparing results...
0/264992
(264992, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/264992
Converting ndarray to lists...
0/264992
(264992, 7)
Loading and preparing results...
Converting ndarray to lists...
0/264992
Converting ndarray to lists...
(264992, 7)
Loading and preparing results...
(264992, 7)
Loading and preparing results...
Loading and preparing results...
(264992, 7)
(264992, 7)
0/264992
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/264992
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/264992
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/264992
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(264992, 7)
(264992, 7)
(264992, 7)
(264992, 7)
Converting ndarray to lists...
(264992, 7)
Loading and preparing results...
0/264992
(264992, 7)
(264992, 7)
Converting ndarray to lists...
(264992, 7)
Converting ndarray to lists...
(264992, 7)
Loading and preparing results...
(264992, 7)
0/264992
Loading and preparing results...
Converting ndarray to lists...
(264992, 7)
(264992, 7)
(264992, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/264992
Loading and preparing results...
0/264992
(264992, 7)
(264992, 7)
0/264992
0/264992
0/264992
0/264992
(264992, 7)
Loading and preparing results...
0/264992
0/264992
0/264992
Loading and preparing results...
Converting ndarray to lists...
(264992, 7)
(264992, 7)
0/264992
0/264992
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/264992
Converting ndarray to lists...
0/264992
Loading and preparing results...
Loading and preparing results...
(264992, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/264992
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(264992, 7)
Converting ndarray to lists...
(264992, 7)
Converting ndarray to lists...
(264992, 7)
0/264992
0/264992
(264992, 7)
0/264992
(264992, 7)
(264992, 7)
0/264992
0/264992
Converting ndarray to lists...
0/264992
Converting ndarray to lists...
0/264992
Loading and preparing results...
(264992, 7)
0/264992
(264992, 7)
Loading and preparing results...
(264992, 7)
Converting ndarray to lists...
(264992, 7)
Loading and preparing results...
0/264992
0/264992
0/264992
0/264992
Converting ndarray to lists...
(264992, 7)
Loading and preparing results...
Converting ndarray to lists...
0/264992
0/264992
Converting ndarray to lists...
Converting ndarray to lists...
(264992, 7)
(264992, 7)
(264992, 7)
0/264992
(264992, 7)
0/264992
0/264992
0/264992
Loading and preparing results...
Converting ndarray to lists...
(264992, 7)
Loading and preparing results...
0/264992
Converting ndarray to lists...
(264992, 7)
0/264992
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
DONE (t=1.79s)
DONE (t=1.79s)
creating index...
creating index...
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.83s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=1.89s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
DONE (t=1.91s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
DONE (t=3.34s).
Accumulating evaluation results...
DONE (t=3.35s).
Accumulating evaluation results...
DONE (t=3.35s).
Accumulating evaluation results...
DONE (t=3.38s).
Accumulating evaluation results...
DONE (t=3.39s).
Accumulating evaluation results...
DONE (t=3.40s).
Accumulating evaluation results...
DONE (t=3.38s).
Accumulating evaluation results...
DONE (t=3.46s).
Accumulating evaluation results...
DONE (t=0.98s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
DONE (t=1.02s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.153
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.065
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.14229 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.153
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
DONE (t=1.04s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.065
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.14229 AP goal: 0.21200
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.142
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.153
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.153
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.153
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.065
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.14229 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.065
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.14229 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.065
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.14229 AP goal: 0.21200
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.153
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.065
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.14229 AP goal: 0.21200
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.153
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.065
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.14229 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.153
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.065
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.270
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.369
Current AP: 0.14229 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711068.325484037 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711068.326444387 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.14229290287010982}

:::MLPv0.5.0 ssd 1541711068.327231884 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.14229290287010982}

:::MLPv0.5.0 ssd 1541711068.328017712 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711068.328804970 (train.py:338) eval_stop: 32
Iteration:   1880, Loss function: 4.329, Average Loss: 4.269, avg. samples / sec: 2694.55
Iteration:   1880, Loss function: 4.584, Average Loss: 4.282, avg. samples / sec: 2694.52
Iteration:   1880, Loss function: 4.425, Average Loss: 4.277, avg. samples / sec: 2694.47
Iteration:   1880, Loss function: 4.055, Average Loss: 4.273, avg. samples / sec: 2694.41
Iteration:   1880, Loss function: 3.905, Average Loss: 4.265, avg. samples / sec: 2694.40
Iteration:   1880, Loss function: 4.278, Average Loss: 4.273, avg. samples / sec: 2694.49
Iteration:   1880, Loss function: 4.339, Average Loss: 4.276, avg. samples / sec: 2694.51
Iteration:   1880, Loss function: 4.383, Average Loss: 4.279, avg. samples / sec: 2694.31
Iteration:   1900, Loss function: 4.323, Average Loss: 4.263, avg. samples / sec: 21805.03
Iteration:   1900, Loss function: 3.828, Average Loss: 4.267, avg. samples / sec: 21791.88
Iteration:   1900, Loss function: 4.448, Average Loss: 4.280, avg. samples / sec: 21794.33
Iteration:   1900, Loss function: 4.449, Average Loss: 4.274, avg. samples / sec: 21799.42
Iteration:   1900, Loss function: 4.345, Average Loss: 4.273, avg. samples / sec: 21795.49
Iteration:   1900, Loss function: 3.927, Average Loss: 4.277, avg. samples / sec: 21798.16
Iteration:   1900, Loss function: 4.714, Average Loss: 4.273, avg. samples / sec: 21791.47
Iteration:   1900, Loss function: 4.727, Average Loss: 4.271, avg. samples / sec: 21790.12

:::MLPv0.5.0 ssd 1541711071.633391619 (train.py:553) train_epoch: 33
Iteration:   1920, Loss function: 4.224, Average Loss: 4.268, avg. samples / sec: 21931.95
Iteration:   1920, Loss function: 4.683, Average Loss: 4.275, avg. samples / sec: 21933.94
Iteration:   1920, Loss function: 3.993, Average Loss: 4.283, avg. samples / sec: 21932.06
Iteration:   1920, Loss function: 4.128, Average Loss: 4.275, avg. samples / sec: 21934.33
Iteration:   1920, Loss function: 4.614, Average Loss: 4.281, avg. samples / sec: 21933.16
Iteration:   1920, Loss function: 4.393, Average Loss: 4.269, avg. samples / sec: 21929.84
Iteration:   1920, Loss function: 3.793, Average Loss: 4.276, avg. samples / sec: 21935.02
Iteration:   1920, Loss function: 4.025, Average Loss: 4.274, avg. samples / sec: 21928.31
Iteration:   1940, Loss function: 4.285, Average Loss: 4.265, avg. samples / sec: 21884.23
Iteration:   1940, Loss function: 3.859, Average Loss: 4.272, avg. samples / sec: 21899.61
Iteration:   1940, Loss function: 3.794, Average Loss: 4.273, avg. samples / sec: 21885.64
Iteration:   1940, Loss function: 4.134, Average Loss: 4.274, avg. samples / sec: 21884.57
Iteration:   1940, Loss function: 4.406, Average Loss: 4.280, avg. samples / sec: 21885.89
Iteration:   1940, Loss function: 4.066, Average Loss: 4.281, avg. samples / sec: 21882.13
Iteration:   1940, Loss function: 3.465, Average Loss: 4.266, avg. samples / sec: 21882.52
Iteration:   1940, Loss function: 4.136, Average Loss: 4.275, avg. samples / sec: 21881.40
Iteration:   1960, Loss function: 3.808, Average Loss: 4.268, avg. samples / sec: 21984.00
Iteration:   1960, Loss function: 3.934, Average Loss: 4.278, avg. samples / sec: 21981.07
Iteration:   1960, Loss function: 4.043, Average Loss: 4.269, avg. samples / sec: 21973.70
Iteration:   1960, Loss function: 3.776, Average Loss: 4.278, avg. samples / sec: 21973.81
Iteration:   1960, Loss function: 4.057, Average Loss: 4.262, avg. samples / sec: 21967.73
Iteration:   1960, Loss function: 3.807, Average Loss: 4.273, avg. samples / sec: 21977.73
Iteration:   1960, Loss function: 3.794, Average Loss: 4.268, avg. samples / sec: 21971.54
Iteration:   1960, Loss function: 4.461, Average Loss: 4.263, avg. samples / sec: 21972.18

:::MLPv0.5.0 ssd 1541711077.050637722 (train.py:553) train_epoch: 34
Iteration:   1980, Loss function: 4.010, Average Loss: 4.275, avg. samples / sec: 21837.88
Iteration:   1980, Loss function: 4.051, Average Loss: 4.261, avg. samples / sec: 21838.42
Iteration:   1980, Loss function: 4.377, Average Loss: 4.267, avg. samples / sec: 21831.08
Iteration:   1980, Loss function: 4.682, Average Loss: 4.272, avg. samples / sec: 21833.79
Iteration:   1980, Loss function: 4.143, Average Loss: 4.264, avg. samples / sec: 21832.98
Iteration:   1980, Loss function: 4.091, Average Loss: 4.266, avg. samples / sec: 21824.28
Iteration:   1980, Loss function: 4.022, Average Loss: 4.259, avg. samples / sec: 21829.45
Iteration:   1980, Loss function: 4.320, Average Loss: 4.279, avg. samples / sec: 21820.38
Iteration:   2000, Loss function: 4.242, Average Loss: 4.275, avg. samples / sec: 21742.83
Iteration:   2000, Loss function: 4.445, Average Loss: 4.269, avg. samples / sec: 21747.26
Iteration:   2000, Loss function: 3.736, Average Loss: 4.260, avg. samples / sec: 21747.57
Iteration:   2000, Loss function: 4.254, Average Loss: 4.274, avg. samples / sec: 21747.51
Iteration:   2000, Loss function: 4.044, Average Loss: 4.260, avg. samples / sec: 21748.54
Iteration:   2000, Loss function: 4.326, Average Loss: 4.263, avg. samples / sec: 21746.93
Iteration:   2000, Loss function: 3.949, Average Loss: 4.279, avg. samples / sec: 21750.82
Iteration:   2000, Loss function: 3.965, Average Loss: 4.267, avg. samples / sec: 21745.09
Iteration:   2020, Loss function: 3.817, Average Loss: 4.271, avg. samples / sec: 21845.33
Iteration:   2020, Loss function: 3.994, Average Loss: 4.264, avg. samples / sec: 21836.70
Iteration:   2020, Loss function: 3.987, Average Loss: 4.266, avg. samples / sec: 21844.79
Iteration:   2020, Loss function: 4.003, Average Loss: 4.270, avg. samples / sec: 21836.21
Iteration:   2020, Loss function: 4.109, Average Loss: 4.259, avg. samples / sec: 21839.23
Iteration:   2020, Loss function: 3.988, Average Loss: 4.257, avg. samples / sec: 21836.38
Iteration:   2020, Loss function: 4.154, Average Loss: 4.276, avg. samples / sec: 21838.31
Iteration:   2020, Loss function: 4.340, Average Loss: 4.259, avg. samples / sec: 21824.75

:::MLPv0.5.0 ssd 1541711082.500658512 (train.py:553) train_epoch: 35
Iteration:   2040, Loss function: 4.290, Average Loss: 4.268, avg. samples / sec: 21813.87
Iteration:   2040, Loss function: 3.900, Average Loss: 4.262, avg. samples / sec: 21817.50
Iteration:   2040, Loss function: 4.077, Average Loss: 4.253, avg. samples / sec: 21819.75
Iteration:   2040, Loss function: 4.120, Average Loss: 4.262, avg. samples / sec: 21813.75
Iteration:   2040, Loss function: 4.300, Average Loss: 4.266, avg. samples / sec: 21813.95
Iteration:   2040, Loss function: 4.272, Average Loss: 4.272, avg. samples / sec: 21816.44
Iteration:   2040, Loss function: 3.779, Average Loss: 4.255, avg. samples / sec: 21820.61
Iteration:   2040, Loss function: 4.143, Average Loss: 4.255, avg. samples / sec: 21807.72
Iteration:   2060, Loss function: 4.383, Average Loss: 4.264, avg. samples / sec: 21814.06
Iteration:   2060, Loss function: 3.956, Average Loss: 4.259, avg. samples / sec: 21824.54
Iteration:   2060, Loss function: 4.211, Average Loss: 4.251, avg. samples / sec: 21824.77
Iteration:   2060, Loss function: 4.127, Average Loss: 4.270, avg. samples / sec: 21819.97
Iteration:   2060, Loss function: 4.066, Average Loss: 4.261, avg. samples / sec: 21817.32
Iteration:   2060, Loss function: 4.722, Average Loss: 4.252, avg. samples / sec: 21820.64
Iteration:   2060, Loss function: 4.252, Average Loss: 4.249, avg. samples / sec: 21812.53
Iteration:   2060, Loss function: 5.225, Average Loss: 4.258, avg. samples / sec: 21811.66

:::MLPv0.5.0 ssd 1541711087.946316957 (train.py:553) train_epoch: 36
Iteration:   2080, Loss function: 3.684, Average Loss: 4.255, avg. samples / sec: 21796.36
Iteration:   2080, Loss function: 3.948, Average Loss: 4.250, avg. samples / sec: 21787.08
Iteration:   2080, Loss function: 4.158, Average Loss: 4.265, avg. samples / sec: 21780.63
Iteration:   2080, Loss function: 3.941, Average Loss: 4.257, avg. samples / sec: 21785.86
Iteration:   2080, Loss function: 4.694, Average Loss: 4.253, avg. samples / sec: 21785.26
Iteration:   2080, Loss function: 3.474, Average Loss: 4.260, avg. samples / sec: 21776.42
Iteration:   2080, Loss function: 3.751, Average Loss: 4.262, avg. samples / sec: 21783.54
Iteration:   2080, Loss function: 4.070, Average Loss: 4.270, avg. samples / sec: 21775.39
Iteration:   2100, Loss function: 4.177, Average Loss: 4.263, avg. samples / sec: 21949.80
Iteration:   2100, Loss function: 3.631, Average Loss: 4.263, avg. samples / sec: 21959.71
Iteration:   2100, Loss function: 4.202, Average Loss: 4.257, avg. samples / sec: 21950.18
Iteration:   2100, Loss function: 4.094, Average Loss: 4.259, avg. samples / sec: 21949.89
Iteration:   2100, Loss function: 3.611, Average Loss: 4.243, avg. samples / sec: 21940.27
Iteration:   2100, Loss function: 3.981, Average Loss: 4.254, avg. samples / sec: 21942.77
Iteration:   2100, Loss function: 4.057, Average Loss: 4.252, avg. samples / sec: 21931.41
Iteration:   2100, Loss function: 4.052, Average Loss: 4.247, avg. samples / sec: 21933.56
Iteration:   2120, Loss function: 3.508, Average Loss: 4.252, avg. samples / sec: 21828.21
Iteration:   2120, Loss function: 4.333, Average Loss: 4.259, avg. samples / sec: 21822.38
Iteration:   2120, Loss function: 4.269, Average Loss: 4.261, avg. samples / sec: 21822.04
Iteration:   2120, Loss function: 4.822, Average Loss: 4.247, avg. samples / sec: 21832.12
Iteration:   2120, Loss function: 3.639, Average Loss: 4.243, avg. samples / sec: 21839.50
Iteration:   2120, Loss function: 4.302, Average Loss: 4.253, avg. samples / sec: 21828.78
Iteration:   2120, Loss function: 3.844, Average Loss: 4.255, avg. samples / sec: 21822.67
Iteration:   2120, Loss function: 4.174, Average Loss: 4.240, avg. samples / sec: 21822.82

:::MLPv0.5.0 ssd 1541711093.377089262 (train.py:553) train_epoch: 37
Iteration:   2140, Loss function: 4.191, Average Loss: 4.261, avg. samples / sec: 21867.20
Iteration:   2140, Loss function: 4.356, Average Loss: 4.264, avg. samples / sec: 21865.96
Iteration:   2140, Loss function: 3.983, Average Loss: 4.245, avg. samples / sec: 21868.23
Iteration:   2140, Loss function: 3.953, Average Loss: 4.247, avg. samples / sec: 21866.41
Iteration:   2140, Loss function: 3.844, Average Loss: 4.255, avg. samples / sec: 21865.46
Iteration:   2140, Loss function: 4.198, Average Loss: 4.258, avg. samples / sec: 21862.41
Iteration:   2140, Loss function: 4.207, Average Loss: 4.254, avg. samples / sec: 21855.02
Iteration:   2140, Loss function: 3.912, Average Loss: 4.242, avg. samples / sec: 21857.25
Iteration:   2160, Loss function: 4.177, Average Loss: 4.256, avg. samples / sec: 21890.28
Iteration:   2160, Loss function: 3.858, Average Loss: 4.243, avg. samples / sec: 21893.16
Iteration:   2160, Loss function: 4.078, Average Loss: 4.258, avg. samples / sec: 21891.56
Iteration:   2160, Loss function: 4.216, Average Loss: 4.252, avg. samples / sec: 21897.69
Iteration:   2160, Loss function: 3.448, Average Loss: 4.242, avg. samples / sec: 21889.36
Iteration:   2160, Loss function: 4.638, Average Loss: 4.251, avg. samples / sec: 21897.36
Iteration:   2160, Loss function: 3.850, Average Loss: 4.236, avg. samples / sec: 21901.52
Iteration:   2160, Loss function: 3.597, Average Loss: 4.247, avg. samples / sec: 21891.63
Iteration:   2180, Loss function: 4.503, Average Loss: 4.253, avg. samples / sec: 21779.02
Iteration:   2180, Loss function: 3.759, Average Loss: 4.251, avg. samples / sec: 21777.57
Iteration:   2180, Loss function: 4.350, Average Loss: 4.248, avg. samples / sec: 21780.83
Iteration:   2180, Loss function: 4.391, Average Loss: 4.250, avg. samples / sec: 21779.62
Iteration:   2180, Loss function: 3.661, Average Loss: 4.236, avg. samples / sec: 21777.10
Iteration:   2180, Loss function: 4.010, Average Loss: 4.238, avg. samples / sec: 21774.14
Iteration:   2180, Loss function: 3.590, Average Loss: 4.230, avg. samples / sec: 21778.30
Iteration:   2180, Loss function: 3.874, Average Loss: 4.241, avg. samples / sec: 21772.99

:::MLPv0.5.0 ssd 1541711098.719957590 (train.py:553) train_epoch: 38
Iteration:   2200, Loss function: 4.178, Average Loss: 4.247, avg. samples / sec: 21888.22
Iteration:   2200, Loss function: 4.094, Average Loss: 4.247, avg. samples / sec: 21886.48
Iteration:   2200, Loss function: 3.839, Average Loss: 4.235, avg. samples / sec: 21887.53
Iteration:   2200, Loss function: 4.525, Average Loss: 4.225, avg. samples / sec: 21884.37
Iteration:   2200, Loss function: 3.741, Average Loss: 4.236, avg. samples / sec: 21883.30
Iteration:   2200, Loss function: 4.190, Average Loss: 4.246, avg. samples / sec: 21880.00
Iteration:   2200, Loss function: 4.426, Average Loss: 4.248, avg. samples / sec: 21879.31
Iteration:   2200, Loss function: 4.057, Average Loss: 4.239, avg. samples / sec: 21888.93
Iteration:   2220, Loss function: 4.384, Average Loss: 4.232, avg. samples / sec: 21771.94
Iteration:   2220, Loss function: 3.892, Average Loss: 4.243, avg. samples / sec: 21767.97
Iteration:   2220, Loss function: 3.928, Average Loss: 4.245, avg. samples / sec: 21775.88
Iteration:   2220, Loss function: 3.876, Average Loss: 4.241, avg. samples / sec: 21764.27
Iteration:   2220, Loss function: 3.515, Average Loss: 4.243, avg. samples / sec: 21772.94
Iteration:   2220, Loss function: 3.878, Average Loss: 4.232, avg. samples / sec: 21770.78
Iteration:   2220, Loss function: 4.006, Average Loss: 4.238, avg. samples / sec: 21770.25
Iteration:   2220, Loss function: 4.688, Average Loss: 4.222, avg. samples / sec: 21765.26
Iteration:   2240, Loss function: 4.350, Average Loss: 4.240, avg. samples / sec: 21859.39
Iteration:   2240, Loss function: 4.351, Average Loss: 4.228, avg. samples / sec: 21857.16
Iteration:   2240, Loss function: 4.302, Average Loss: 4.241, avg. samples / sec: 21852.65
Iteration:   2240, Loss function: 3.991, Average Loss: 4.242, avg. samples / sec: 21850.55
Iteration:   2240, Loss function: 3.938, Average Loss: 4.229, avg. samples / sec: 21854.45
Iteration:   2240, Loss function: 4.142, Average Loss: 4.234, avg. samples / sec: 21853.72
Iteration:   2240, Loss function: 4.377, Average Loss: 4.218, avg. samples / sec: 21856.68
Iteration:   2240, Loss function: 4.584, Average Loss: 4.237, avg. samples / sec: 21848.42

:::MLPv0.5.0 ssd 1541711104.162281513 (train.py:553) train_epoch: 39
Iteration:   2260, Loss function: 4.162, Average Loss: 4.241, avg. samples / sec: 21876.51
Iteration:   2260, Loss function: 3.961, Average Loss: 4.238, avg. samples / sec: 21883.15
Iteration:   2260, Loss function: 3.847, Average Loss: 4.238, avg. samples / sec: 21880.46
Iteration:   2260, Loss function: 3.595, Average Loss: 4.242, avg. samples / sec: 21878.44
Iteration:   2260, Loss function: 4.109, Average Loss: 4.229, avg. samples / sec: 21869.60
Iteration:   2260, Loss function: 4.202, Average Loss: 4.235, avg. samples / sec: 21877.85
Iteration:   2260, Loss function: 3.917, Average Loss: 4.230, avg. samples / sec: 21874.56
Iteration:   2260, Loss function: 3.896, Average Loss: 4.219, avg. samples / sec: 21875.31
Iteration:   2280, Loss function: 4.531, Average Loss: 4.233, avg. samples / sec: 21859.22
Iteration:   2280, Loss function: 4.314, Average Loss: 4.232, avg. samples / sec: 21863.44
Iteration:   2280, Loss function: 3.966, Average Loss: 4.226, avg. samples / sec: 21861.58
Iteration:   2280, Loss function: 3.511, Average Loss: 4.234, avg. samples / sec: 21855.45
Iteration:   2280, Loss function: 4.408, Average Loss: 4.239, avg. samples / sec: 21855.20
Iteration:   2280, Loss function: 4.491, Average Loss: 4.229, avg. samples / sec: 21853.40
Iteration:   2280, Loss function: 4.218, Average Loss: 4.234, avg. samples / sec: 21850.85
Iteration:   2280, Loss function: 3.990, Average Loss: 4.214, avg. samples / sec: 21858.93
Iteration:   2300, Loss function: 4.184, Average Loss: 4.225, avg. samples / sec: 21836.79
Iteration:   2300, Loss function: 3.330, Average Loss: 4.235, avg. samples / sec: 21838.25
Iteration:   2300, Loss function: 4.019, Average Loss: 4.231, avg. samples / sec: 21825.46
Iteration:   2300, Loss function: 3.588, Average Loss: 4.231, avg. samples / sec: 21832.67
Iteration:   2300, Loss function: 4.355, Average Loss: 4.230, avg. samples / sec: 21835.81
Iteration:   2300, Loss function: 4.345, Average Loss: 4.233, avg. samples / sec: 21829.01
Iteration:   2300, Loss function: 4.182, Average Loss: 4.230, avg. samples / sec: 21819.69
Iteration:   2300, Loss function: 4.227, Average Loss: 4.213, avg. samples / sec: 21826.50

:::MLPv0.5.0 ssd 1541711109.595700026 (train.py:553) train_epoch: 40
Iteration:   2320, Loss function: 3.849, Average Loss: 4.230, avg. samples / sec: 21923.22
Iteration:   2320, Loss function: 4.561, Average Loss: 4.237, avg. samples / sec: 21919.65
Iteration:   2320, Loss function: 4.465, Average Loss: 4.212, avg. samples / sec: 21932.14
Iteration:   2320, Loss function: 4.704, Average Loss: 4.232, avg. samples / sec: 21922.64
Iteration:   2320, Loss function: 4.404, Average Loss: 4.226, avg. samples / sec: 21916.12
Iteration:   2320, Loss function: 4.328, Average Loss: 4.234, avg. samples / sec: 21927.72
Iteration:   2320, Loss function: 4.454, Average Loss: 4.231, avg. samples / sec: 21927.83
Iteration:   2320, Loss function: 4.166, Average Loss: 4.231, avg. samples / sec: 21912.88
Iteration:   2340, Loss function: 3.775, Average Loss: 4.227, avg. samples / sec: 21914.10
Iteration:   2340, Loss function: 4.356, Average Loss: 4.231, avg. samples / sec: 21907.41
Iteration:   2340, Loss function: 4.101, Average Loss: 4.229, avg. samples / sec: 21898.73
Iteration:   2340, Loss function: 4.400, Average Loss: 4.234, avg. samples / sec: 21900.77
Iteration:   2340, Loss function: 4.154, Average Loss: 4.224, avg. samples / sec: 21903.45
Iteration:   2340, Loss function: 3.880, Average Loss: 4.210, avg. samples / sec: 21900.50
Iteration:   2340, Loss function: 3.406, Average Loss: 4.226, avg. samples / sec: 21909.14
Iteration:   2340, Loss function: 3.376, Average Loss: 4.227, avg. samples / sec: 21897.22
Iteration:   2360, Loss function: 4.151, Average Loss: 4.223, avg. samples / sec: 21853.26
Iteration:   2360, Loss function: 3.396, Average Loss: 4.224, avg. samples / sec: 21840.00
Iteration:   2360, Loss function: 4.049, Average Loss: 4.228, avg. samples / sec: 21840.84
Iteration:   2360, Loss function: 4.266, Average Loss: 4.224, avg. samples / sec: 21830.70
Iteration:   2360, Loss function: 4.605, Average Loss: 4.219, avg. samples / sec: 21839.36
Iteration:   2360, Loss function: 3.681, Average Loss: 4.221, avg. samples / sec: 21838.84
Iteration:   2360, Loss function: 4.259, Average Loss: 4.206, avg. samples / sec: 21835.26
Iteration:   2360, Loss function: 4.664, Average Loss: 4.227, avg. samples / sec: 21830.77

:::MLPv0.5.0 ssd 1541711115.025793314 (train.py:553) train_epoch: 41
Iteration:   2380, Loss function: 3.516, Average Loss: 4.216, avg. samples / sec: 21804.25
Iteration:   2380, Loss function: 3.739, Average Loss: 4.221, avg. samples / sec: 21796.55
Iteration:   2380, Loss function: 4.006, Average Loss: 4.220, avg. samples / sec: 21800.22
Iteration:   2380, Loss function: 3.994, Average Loss: 4.223, avg. samples / sec: 21796.91
Iteration:   2380, Loss function: 3.939, Average Loss: 4.202, avg. samples / sec: 21801.83
Iteration:   2380, Loss function: 4.248, Average Loss: 4.216, avg. samples / sec: 21795.31
Iteration:   2380, Loss function: 3.365, Average Loss: 4.219, avg. samples / sec: 21784.66
Iteration:   2380, Loss function: 4.208, Average Loss: 4.221, avg. samples / sec: 21800.59
Iteration:   2400, Loss function: 3.930, Average Loss: 4.216, avg. samples / sec: 21891.19
Iteration:   2400, Loss function: 4.125, Average Loss: 4.215, avg. samples / sec: 21896.73
Iteration:   2400, Loss function: 3.824, Average Loss: 4.212, avg. samples / sec: 21893.83
Iteration:   2400, Loss function: 4.015, Average Loss: 4.217, avg. samples / sec: 21887.60
Iteration:   2400, Loss function: 4.113, Average Loss: 4.221, avg. samples / sec: 21890.96
Iteration:   2400, Loss function: 3.615, Average Loss: 4.212, avg. samples / sec: 21886.14
Iteration:   2400, Loss function: 3.893, Average Loss: 4.215, avg. samples / sec: 21890.84
Iteration:   2400, Loss function: 4.126, Average Loss: 4.195, avg. samples / sec: 21888.48
Iteration:   2420, Loss function: 3.699, Average Loss: 4.208, avg. samples / sec: 21901.16
Iteration:   2420, Loss function: 4.274, Average Loss: 4.209, avg. samples / sec: 21902.12
Iteration:   2420, Loss function: 4.007, Average Loss: 4.210, avg. samples / sec: 21900.44
Iteration:   2420, Loss function: 4.323, Average Loss: 4.209, avg. samples / sec: 21901.60
Iteration:   2420, Loss function: 4.301, Average Loss: 4.205, avg. samples / sec: 21898.48
Iteration:   2420, Loss function: 4.479, Average Loss: 4.211, avg. samples / sec: 21897.26
Iteration:   2420, Loss function: 4.468, Average Loss: 4.220, avg. samples / sec: 21893.39
Iteration:   2420, Loss function: 3.735, Average Loss: 4.191, avg. samples / sec: 21891.31

:::MLPv0.5.0 ssd 1541711120.364725351 (train.py:553) train_epoch: 42
Iteration:   2440, Loss function: 3.562, Average Loss: 4.203, avg. samples / sec: 21769.77
Iteration:   2440, Loss function: 3.461, Average Loss: 4.207, avg. samples / sec: 21766.75
Iteration:   2440, Loss function: 4.539, Average Loss: 4.215, avg. samples / sec: 21772.16
Iteration:   2440, Loss function: 3.826, Average Loss: 4.201, avg. samples / sec: 21765.92
Iteration:   2440, Loss function: 4.249, Average Loss: 4.208, avg. samples / sec: 21761.06
Iteration:   2440, Loss function: 4.504, Average Loss: 4.208, avg. samples / sec: 21766.15
Iteration:   2440, Loss function: 3.826, Average Loss: 4.206, avg. samples / sec: 21753.60
Iteration:   2440, Loss function: 3.892, Average Loss: 4.188, avg. samples / sec: 21769.13
Iteration:   2460, Loss function: 3.026, Average Loss: 4.198, avg. samples / sec: 21845.23
Iteration:   2460, Loss function: 4.596, Average Loss: 4.204, avg. samples / sec: 21856.72
Iteration:   2460, Loss function: 4.121, Average Loss: 4.205, avg. samples / sec: 21852.19
Iteration:   2460, Loss function: 3.265, Average Loss: 4.197, avg. samples / sec: 21849.75
Iteration:   2460, Loss function: 3.887, Average Loss: 4.203, avg. samples / sec: 21851.97
Iteration:   2460, Loss function: 4.378, Average Loss: 4.203, avg. samples / sec: 21843.84
Iteration:   2460, Loss function: 4.140, Average Loss: 4.210, avg. samples / sec: 21842.59
Iteration:   2460, Loss function: 3.694, Average Loss: 4.182, avg. samples / sec: 21852.89
Iteration:   2480, Loss function: 4.072, Average Loss: 4.201, avg. samples / sec: 21865.38
Iteration:   2480, Loss function: 3.865, Average Loss: 4.201, avg. samples / sec: 21863.08
Iteration:   2480, Loss function: 3.667, Average Loss: 4.194, avg. samples / sec: 21854.40
Iteration:   2480, Loss function: 3.861, Average Loss: 4.194, avg. samples / sec: 21859.11
Iteration:   2480, Loss function: 3.967, Average Loss: 4.201, avg. samples / sec: 21861.45
Iteration:   2480, Loss function: 4.064, Average Loss: 4.180, avg. samples / sec: 21861.22
Iteration:   2480, Loss function: 3.843, Average Loss: 4.208, avg. samples / sec: 21860.31
Iteration:   2480, Loss function: 4.294, Average Loss: 4.204, avg. samples / sec: 21847.00

:::MLPv0.5.0 ssd 1541711125.809844017 (train.py:553) train_epoch: 43
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1

:::MLPv0.5.0 ssd 1541711127.309506655 (train.py:578) opt_learning_rate: 0.016
Iteration:   2500, Loss function: 4.032, Average Loss: 4.198, avg. samples / sec: 21861.31
Iteration:   2500, Loss function: 4.090, Average Loss: 4.206, avg. samples / sec: 21867.29
Iteration:   2500, Loss function: 3.809, Average Loss: 4.177, avg. samples / sec: 21865.99
Iteration:   2500, Loss function: 4.355, Average Loss: 4.191, avg. samples / sec: 21860.60
Iteration:   2500, Loss function: 3.851, Average Loss: 4.188, avg. samples / sec: 21860.36
Iteration:   2500, Loss function: 4.014, Average Loss: 4.199, avg. samples / sec: 21861.86
Iteration:   2500, Loss function: 4.342, Average Loss: 4.197, avg. samples / sec: 21858.64
Iteration:   2500, Loss function: 4.118, Average Loss: 4.202, avg. samples / sec: 21868.91

































































:::MLPv0.5.0 ssd 1541711127.401520729 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711127.402389526 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711127.403145313 (train.py:220) eval_start: 43
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.93 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.93 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.93 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.93 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.93 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.93 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.93 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.93 s
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(316740, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/316740
(316740, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/316740
(316740, 7)
(316740, 7)
0/316740
0/316740
Loading and preparing results...
Converting ndarray to lists...
(316740, 7)
Loading and preparing results...
0/316740
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(316740, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/316740
(316740, 7)
(316740, 7)
0/316740
0/316740
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(316740, 7)
(316740, 7)
(316740, 7)
Converting ndarray to lists...
0/316740
0/316740
0/316740
(316740, 7)
0/316740
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(316740, 7)
(316740, 7)
Converting ndarray to lists...
0/316740
0/316740
(316740, 7)
0/316740
Loading and preparing results...
Converting ndarray to lists...
(316740, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(316740, 7)
Loading and preparing results...
(316740, 7)
Converting ndarray to lists...
0/316740
Converting ndarray to lists...
0/316740
0/316740
Converting ndarray to lists...
(316740, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(316740, 7)
Converting ndarray to lists...
(316740, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(316740, 7)
0/316740
Converting ndarray to lists...
Loading and preparing results...
(316740, 7)
(316740, 7)
(316740, 7)
Converting ndarray to lists...
(316740, 7)
0/316740
0/316740
(316740, 7)
(316740, 7)
(316740, 7)
Converting ndarray to lists...
(316740, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
0/316740
(316740, 7)
0/316740
(316740, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(316740, 7)
0/316740
Loading and preparing results...
0/316740
Converting ndarray to lists...
Loading and preparing results...
0/316740
0/316740
(316740, 7)
0/316740
0/316740
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/316740
0/316740
0/316740
(316740, 7)
(316740, 7)
Converting ndarray to lists...
Loading and preparing results...
(316740, 7)
0/316740
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/316740
(316740, 7)
0/316740
Converting ndarray to lists...
(316740, 7)
0/316740
Loading and preparing results...
(316740, 7)
0/316740
Loading and preparing results...
Loading and preparing results...
0/316740
(316740, 7)
Loading and preparing results...
0/316740
(316740, 7)
Converting ndarray to lists...
0/316740
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(316740, 7)
Loading and preparing results...
0/316740
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(316740, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/316740
(316740, 7)
(316740, 7)
(316740, 7)
Loading and preparing results...
Loading and preparing results...
(316740, 7)
Converting ndarray to lists...
0/316740
Converting ndarray to lists...
0/316740
(316740, 7)
0/316740
Converting ndarray to lists...
Converting ndarray to lists...
0/316740
(316740, 7)
Converting ndarray to lists...
0/316740
(316740, 7)
Converting ndarray to lists...
(316740, 7)
Loading and preparing results...
0/316740
0/316740
(316740, 7)
0/316740
(316740, 7)
(316740, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/316740
Converting ndarray to lists...
0/316740
0/316740
(316740, 7)
(316740, 7)
Converting ndarray to lists...
0/316740
0/316740
0/316740
Loading and preparing results...
0/316740
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(316740, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(316740, 7)
(316740, 7)
(316740, 7)
(316740, 7)
(316740, 7)
0/316740
(316740, 7)
0/316740
0/316740
0/316740
0/316740
0/316740
0/316740
DONE (t=1.89s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
DONE (t=2.00s)
creating index...
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
index created!
DONE (t=2.03s)
creating index...
DONE (t=2.05s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.23s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.56s).
Accumulating evaluation results...
DONE (t=3.59s).
Accumulating evaluation results...
DONE (t=3.60s).
Accumulating evaluation results...
DONE (t=3.60s).
Accumulating evaluation results...
DONE (t=3.62s).
Accumulating evaluation results...
DONE (t=3.66s).
Accumulating evaluation results...
DONE (t=3.59s).
Accumulating evaluation results...
DONE (t=3.58s).
Accumulating evaluation results...
DONE (t=1.15s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.284
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.070
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381
Current AP: 0.14468 AP goal: 0.21200
DONE (t=1.17s).
DONE (t=1.17s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
DONE (t=1.17s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.284
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.284
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.284
DONE (t=1.17s).
DONE (t=1.18s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.284
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.284
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.070
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381
Current AP: 0.14468 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.070
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381
Current AP: 0.14468 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.224
DONE (t=1.18s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.070
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381
Current AP: 0.14468 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.284
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.070
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381
Current AP: 0.14468 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.070
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381
Current AP: 0.14468 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.070
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381
Current AP: 0.14468 AP goal: 0.21200
DONE (t=1.15s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.284
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.136
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.166
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.070
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.381
Current AP: 0.14468 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711139.566889524 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711139.567812681 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.14468055853698575}

:::MLPv0.5.0 ssd 1541711139.568601608 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.14468055853698575}

:::MLPv0.5.0 ssd 1541711139.569397926 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711139.570196152 (train.py:338) eval_stop: 43
Iteration:   2520, Loss function: 2.659, Average Loss: 4.177, avg. samples / sec: 2820.58
Iteration:   2520, Loss function: 3.927, Average Loss: 4.189, avg. samples / sec: 2820.53
Iteration:   2520, Loss function: 3.108, Average Loss: 4.193, avg. samples / sec: 2820.57
Iteration:   2520, Loss function: 3.504, Average Loss: 4.189, avg. samples / sec: 2820.47
Iteration:   2520, Loss function: 3.527, Average Loss: 4.167, avg. samples / sec: 2820.40
Iteration:   2520, Loss function: 3.453, Average Loss: 4.195, avg. samples / sec: 2820.40
Iteration:   2520, Loss function: 4.020, Average Loss: 4.188, avg. samples / sec: 2820.29
Iteration:   2520, Loss function: 3.688, Average Loss: 4.180, avg. samples / sec: 2820.37
Iteration:   2540, Loss function: 3.293, Average Loss: 4.174, avg. samples / sec: 21928.94
Iteration:   2540, Loss function: 2.697, Average Loss: 4.173, avg. samples / sec: 21922.20
Iteration:   2540, Loss function: 2.846, Average Loss: 4.175, avg. samples / sec: 21924.37
Iteration:   2540, Loss function: 3.141, Average Loss: 4.164, avg. samples / sec: 21915.72
Iteration:   2540, Loss function: 3.616, Average Loss: 4.182, avg. samples / sec: 21925.31
Iteration:   2540, Loss function: 3.510, Average Loss: 4.180, avg. samples / sec: 21920.92
Iteration:   2540, Loss function: 3.318, Average Loss: 4.168, avg. samples / sec: 21928.81
Iteration:   2540, Loss function: 3.762, Average Loss: 4.154, avg. samples / sec: 21913.94

:::MLPv0.5.0 ssd 1541711143.883870602 (train.py:553) train_epoch: 44
Iteration:   2560, Loss function: 3.626, Average Loss: 4.168, avg. samples / sec: 21909.82
Iteration:   2560, Loss function: 2.991, Average Loss: 4.166, avg. samples / sec: 21903.27
Iteration:   2560, Loss function: 4.030, Average Loss: 4.161, avg. samples / sec: 21901.06
Iteration:   2560, Loss function: 3.043, Average Loss: 4.159, avg. samples / sec: 21901.47
Iteration:   2560, Loss function: 3.410, Average Loss: 4.164, avg. samples / sec: 21902.92
Iteration:   2560, Loss function: 3.420, Average Loss: 4.154, avg. samples / sec: 21901.15
Iteration:   2560, Loss function: 3.582, Average Loss: 4.141, avg. samples / sec: 21907.50
Iteration:   2560, Loss function: 3.693, Average Loss: 4.153, avg. samples / sec: 21895.30
Iteration:   2580, Loss function: 3.487, Average Loss: 4.152, avg. samples / sec: 21938.21
Iteration:   2580, Loss function: 3.386, Average Loss: 4.150, avg. samples / sec: 21940.45
Iteration:   2580, Loss function: 3.971, Average Loss: 4.127, avg. samples / sec: 21945.69
Iteration:   2580, Loss function: 3.323, Average Loss: 4.141, avg. samples / sec: 21937.56
Iteration:   2580, Loss function: 3.216, Average Loss: 4.150, avg. samples / sec: 21936.75
Iteration:   2580, Loss function: 3.826, Average Loss: 4.149, avg. samples / sec: 21936.67
Iteration:   2580, Loss function: 3.103, Average Loss: 4.139, avg. samples / sec: 21943.32
Iteration:   2580, Loss function: 3.443, Average Loss: 4.140, avg. samples / sec: 21936.70

:::MLPv0.5.0 ssd 1541711149.305918932 (train.py:553) train_epoch: 45
Iteration:   2600, Loss function: 3.164, Average Loss: 4.137, avg. samples / sec: 21873.65
Iteration:   2600, Loss function: 3.380, Average Loss: 4.135, avg. samples / sec: 21875.30
Iteration:   2600, Loss function: 3.071, Average Loss: 4.121, avg. samples / sec: 21880.45
Iteration:   2600, Loss function: 3.734, Average Loss: 4.125, avg. samples / sec: 21876.49
Iteration:   2600, Loss function: 3.338, Average Loss: 4.131, avg. samples / sec: 21877.40
Iteration:   2600, Loss function: 3.512, Average Loss: 4.125, avg. samples / sec: 21876.77
Iteration:   2600, Loss function: 3.059, Average Loss: 4.111, avg. samples / sec: 21872.01
Iteration:   2600, Loss function: 3.550, Average Loss: 4.134, avg. samples / sec: 21865.66
Iteration:   2620, Loss function: 3.241, Average Loss: 4.120, avg. samples / sec: 21880.97
Iteration:   2620, Loss function: 3.402, Average Loss: 4.119, avg. samples / sec: 21882.98
Iteration:   2620, Loss function: 3.021, Average Loss: 4.115, avg. samples / sec: 21883.93
Iteration:   2620, Loss function: 3.081, Average Loss: 4.110, avg. samples / sec: 21886.80
Iteration:   2620, Loss function: 3.199, Average Loss: 4.107, avg. samples / sec: 21882.32
Iteration:   2620, Loss function: 3.343, Average Loss: 4.118, avg. samples / sec: 21891.82
Iteration:   2620, Loss function: 3.323, Average Loss: 4.106, avg. samples / sec: 21878.85
Iteration:   2620, Loss function: 3.303, Average Loss: 4.094, avg. samples / sec: 21880.09
Iteration:   2640, Loss function: 3.025, Average Loss: 4.103, avg. samples / sec: 21895.07
Iteration:   2640, Loss function: 3.326, Average Loss: 4.092, avg. samples / sec: 21895.29
Iteration:   2640, Loss function: 3.579, Average Loss: 4.094, avg. samples / sec: 21892.74
Iteration:   2640, Loss function: 3.520, Average Loss: 4.104, avg. samples / sec: 21890.00
Iteration:   2640, Loss function: 4.056, Average Loss: 4.103, avg. samples / sec: 21893.21
Iteration:   2640, Loss function: 3.291, Average Loss: 4.101, avg. samples / sec: 21889.97
Iteration:   2640, Loss function: 3.268, Average Loss: 4.089, avg. samples / sec: 21892.96
Iteration:   2640, Loss function: 3.456, Average Loss: 4.077, avg. samples / sec: 21894.24

:::MLPv0.5.0 ssd 1541711154.641723156 (train.py:553) train_epoch: 46
Iteration:   2660, Loss function: 3.427, Average Loss: 4.076, avg. samples / sec: 21880.94
Iteration:   2660, Loss function: 2.912, Average Loss: 4.086, avg. samples / sec: 21873.14
Iteration:   2660, Loss function: 2.809, Average Loss: 4.084, avg. samples / sec: 21877.06
Iteration:   2660, Loss function: 3.965, Average Loss: 4.082, avg. samples / sec: 21873.25
Iteration:   2660, Loss function: 3.525, Average Loss: 4.077, avg. samples / sec: 21871.35
Iteration:   2660, Loss function: 3.712, Average Loss: 4.089, avg. samples / sec: 21869.27
Iteration:   2660, Loss function: 3.528, Average Loss: 4.062, avg. samples / sec: 21873.27
Iteration:   2660, Loss function: 3.556, Average Loss: 4.091, avg. samples / sec: 21867.82
Iteration:   2680, Loss function: 2.909, Average Loss: 4.047, avg. samples / sec: 21866.73
Iteration:   2680, Loss function: 3.246, Average Loss: 4.069, avg. samples / sec: 21859.11
Iteration:   2680, Loss function: 3.269, Average Loss: 4.074, avg. samples / sec: 21865.05
Iteration:   2680, Loss function: 3.700, Average Loss: 4.060, avg. samples / sec: 21848.42
Iteration:   2680, Loss function: 3.141, Average Loss: 4.072, avg. samples / sec: 21854.12
Iteration:   2680, Loss function: 3.747, Average Loss: 4.072, avg. samples / sec: 21861.43
Iteration:   2680, Loss function: 3.361, Average Loss: 4.068, avg. samples / sec: 21855.16
Iteration:   2680, Loss function: 3.472, Average Loss: 4.063, avg. samples / sec: 21858.44
Iteration:   2700, Loss function: 3.639, Average Loss: 4.056, avg. samples / sec: 21826.36
Iteration:   2700, Loss function: 3.689, Average Loss: 4.046, avg. samples / sec: 21825.05
Iteration:   2700, Loss function: 3.090, Average Loss: 4.056, avg. samples / sec: 21826.36
Iteration:   2700, Loss function: 3.284, Average Loss: 4.048, avg. samples / sec: 21826.27
Iteration:   2700, Loss function: 3.381, Average Loss: 4.056, avg. samples / sec: 21815.62
Iteration:   2700, Loss function: 3.926, Average Loss: 4.057, avg. samples / sec: 21816.47
Iteration:   2700, Loss function: 3.390, Average Loss: 4.036, avg. samples / sec: 21808.93
Iteration:   2700, Loss function: 3.183, Average Loss: 4.054, avg. samples / sec: 21812.83

:::MLPv0.5.0 ssd 1541711160.076743126 (train.py:553) train_epoch: 47
Iteration:   2720, Loss function: 3.370, Average Loss: 4.042, avg. samples / sec: 21904.21
Iteration:   2720, Loss function: 3.658, Average Loss: 4.030, avg. samples / sec: 21901.20
Iteration:   2720, Loss function: 3.580, Average Loss: 4.043, avg. samples / sec: 21908.99
Iteration:   2720, Loss function: 3.571, Average Loss: 4.034, avg. samples / sec: 21901.66
Iteration:   2720, Loss function: 3.437, Average Loss: 4.022, avg. samples / sec: 21907.63
Iteration:   2720, Loss function: 3.077, Average Loss: 4.039, avg. samples / sec: 21912.44
Iteration:   2720, Loss function: 3.361, Average Loss: 4.041, avg. samples / sec: 21900.55
Iteration:   2720, Loss function: 3.543, Average Loss: 4.043, avg. samples / sec: 21889.88
Iteration:   2740, Loss function: 3.330, Average Loss: 4.016, avg. samples / sec: 21870.22
Iteration:   2740, Loss function: 3.466, Average Loss: 4.029, avg. samples / sec: 21878.64
Iteration:   2740, Loss function: 3.173, Average Loss: 4.022, avg. samples / sec: 21871.53
Iteration:   2740, Loss function: 3.479, Average Loss: 4.009, avg. samples / sec: 21871.45
Iteration:   2740, Loss function: 2.683, Average Loss: 4.020, avg. samples / sec: 21866.50
Iteration:   2740, Loss function: 3.238, Average Loss: 4.027, avg. samples / sec: 21862.26
Iteration:   2740, Loss function: 3.569, Average Loss: 4.027, avg. samples / sec: 21868.03
Iteration:   2740, Loss function: 3.677, Average Loss: 4.028, avg. samples / sec: 21857.55
Iteration:   2760, Loss function: 2.927, Average Loss: 4.001, avg. samples / sec: 21888.60
Iteration:   2760, Loss function: 3.648, Average Loss: 4.011, avg. samples / sec: 21897.10
Iteration:   2760, Loss function: 3.083, Average Loss: 4.013, avg. samples / sec: 21897.45
Iteration:   2760, Loss function: 3.546, Average Loss: 4.013, avg. samples / sec: 21890.22
Iteration:   2760, Loss function: 2.715, Average Loss: 4.006, avg. samples / sec: 21886.62
Iteration:   2760, Loss function: 3.533, Average Loss: 3.997, avg. samples / sec: 21886.76
Iteration:   2760, Loss function: 3.324, Average Loss: 4.007, avg. samples / sec: 21887.07
Iteration:   2760, Loss function: 3.533, Average Loss: 4.014, avg. samples / sec: 21881.50

:::MLPv0.5.0 ssd 1541711165.505799532 (train.py:553) train_epoch: 48
Iteration:   2780, Loss function: 3.190, Average Loss: 3.985, avg. samples / sec: 21777.85
Iteration:   2780, Loss function: 3.062, Average Loss: 3.999, avg. samples / sec: 21782.97
Iteration:   2780, Loss function: 3.029, Average Loss: 3.995, avg. samples / sec: 21779.70
Iteration:   2780, Loss function: 3.065, Average Loss: 3.998, avg. samples / sec: 21784.37
Iteration:   2780, Loss function: 3.147, Average Loss: 3.998, avg. samples / sec: 21778.38
Iteration:   2780, Loss function: 3.430, Average Loss: 3.993, avg. samples / sec: 21779.00
Iteration:   2780, Loss function: 3.078, Average Loss: 3.992, avg. samples / sec: 21774.69
Iteration:   2780, Loss function: 3.105, Average Loss: 3.985, avg. samples / sec: 21774.14
Iteration:   2800, Loss function: 3.632, Average Loss: 3.982, avg. samples / sec: 21823.70
Iteration:   2800, Loss function: 3.133, Average Loss: 3.972, avg. samples / sec: 21822.52
Iteration:   2800, Loss function: 2.885, Average Loss: 3.983, avg. samples / sec: 21822.14
Iteration:   2800, Loss function: 3.151, Average Loss: 3.980, avg. samples / sec: 21824.58
Iteration:   2800, Loss function: 2.925, Average Loss: 3.982, avg. samples / sec: 21822.85
Iteration:   2800, Loss function: 3.256, Average Loss: 3.971, avg. samples / sec: 21827.02
Iteration:   2800, Loss function: 3.331, Average Loss: 3.984, avg. samples / sec: 21817.11
Iteration:   2800, Loss function: 3.788, Average Loss: 3.977, avg. samples / sec: 21822.20

































































:::MLPv0.5.0 ssd 1541711169.264981508 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711169.265854359 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711169.266644001 (train.py:220) eval_start: 48
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 46
Predicting Ended, total time: 4.99 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.99 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.99 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.99 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.99 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.99 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.99 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.99 s
Loading and preparing results...
Converting ndarray to lists...
(310699, 7)
0/310699
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(310699, 7)
Converting ndarray to lists...
0/310699
(310699, 7)
0/310699
Loading and preparing results...
Converting ndarray to lists...
(310699, 7)
0/310699
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(310699, 7)
(310699, 7)
Converting ndarray to lists...
Loading and preparing results...
0/310699
0/310699
(310699, 7)
Converting ndarray to lists...
0/310699
(310699, 7)
0/310699
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(310699, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/310699
(310699, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/310699
(310699, 7)
(310699, 7)
0/310699
0/310699
Loading and preparing results...
Converting ndarray to lists...
(310699, 7)
0/310699
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(310699, 7)
(310699, 7)
0/310699
0/310699
Loading and preparing results...
Converting ndarray to lists...
(310699, 7)
0/310699
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(310699, 7)
(310699, 7)
0/310699
0/310699
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(310699, 7)
(310699, 7)
0/310699
Loading and preparing results...
Loading and preparing results...
0/310699
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(310699, 7)
Converting ndarray to lists...
(310699, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/310699
Loading and preparing results...
(310699, 7)
(310699, 7)
0/310699
(310699, 7)
0/310699
Converting ndarray to lists...
Loading and preparing results...
0/310699
Converting ndarray to lists...
Converting ndarray to lists...
(310699, 7)
Loading and preparing results...
0/310699
(310699, 7)
Converting ndarray to lists...
(310699, 7)
0/310699
(310699, 7)
0/310699
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/310699
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(310699, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(310699, 7)
Converting ndarray to lists...
Loading and preparing results...
(310699, 7)
(310699, 7)
0/310699
0/310699
(310699, 7)
(310699, 7)
0/310699
Converting ndarray to lists...
(310699, 7)
0/310699
Loading and preparing results...
Converting ndarray to lists...
0/310699
Converting ndarray to lists...
0/310699
(310699, 7)
(310699, 7)
0/310699
0/310699
(310699, 7)
Loading and preparing results...
Loading and preparing results...
0/310699
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(310699, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/310699
0/310699
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(310699, 7)
(310699, 7)
(310699, 7)
Converting ndarray to lists...
(310699, 7)
(310699, 7)
(310699, 7)
(310699, 7)
Converting ndarray to lists...
0/310699
(310699, 7)
(310699, 7)
(310699, 7)
0/310699
0/310699
Loading and preparing results...
Converting ndarray to lists...
(310699, 7)
0/310699
0/310699
0/310699
0/310699
0/310699
0/310699
0/310699
0/310699
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(310699, 7)
(310699, 7)
Converting ndarray to lists...
0/310699
0/310699
(310699, 7)
Loading and preparing results...
0/310699
Converting ndarray to lists...
Loading and preparing results...
(310699, 7)
Converting ndarray to lists...
0/310699
(310699, 7)
0/310699
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(310699, 7)
(310699, 7)
Converting ndarray to lists...
(310699, 7)
0/310699
0/310699
0/310699
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(310699, 7)
(310699, 7)
0/310699
0/310699
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(310699, 7)
0/310699
Converting ndarray to lists...
Converting ndarray to lists...
0/310699
(310699, 7)
(310699, 7)
0/310699
0/310699
DONE (t=1.54s)
creating index...
DONE (t=1.56s)
creating index...
index created!
index created!
DONE (t=1.87s)
creating index...
DONE (t=1.88s)
DONE (t=1.88s)
creating index...
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.91s)
creating index...
DONE (t=1.91s)
creating index...
DONE (t=1.91s)
creating index...
DONE (t=1.91s)
creating index...
DONE (t=1.91s)
creating index...
DONE (t=1.92s)
creating index...
DONE (t=1.92s)
creating index...
DONE (t=1.92s)
creating index...
DONE (t=1.92s)
DONE (t=1.92s)
creating index...
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.98s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=2.08s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=2.09s)
creating index...
index created!
index created!
index created!
index created!
index created!
DONE (t=2.11s)
creating index...
DONE (t=2.13s)
creating index...
index created!
DONE (t=2.13s)
creating index...
DONE (t=2.13s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.22s)
creating index...
index created!
index created!
DONE (t=2.23s)
creating index...
DONE (t=2.24s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
DONE (t=2.35s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
DONE (t=3.64s).
Accumulating evaluation results...
DONE (t=3.61s).
Accumulating evaluation results...
DONE (t=3.66s).
Accumulating evaluation results...
DONE (t=3.69s).
Accumulating evaluation results...
DONE (t=3.68s).
Accumulating evaluation results...
DONE (t=3.64s).
Accumulating evaluation results...
DONE (t=3.63s).
Accumulating evaluation results...
DONE (t=3.63s).
Accumulating evaluation results...
DONE (t=1.14s).
DONE (t=1.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
DONE (t=1.09s).
DONE (t=1.14s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.344
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21161 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.344
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21161 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.344
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21161 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.344
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21161 AP goal: 0.21200
DONE (t=1.13s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
DONE (t=1.17s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.344
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21161 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.344
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21161 AP goal: 0.21200
DONE (t=1.12s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
DONE (t=1.13s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.344
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21161 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.344
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499
Current AP: 0.21161 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711181.588140488 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711181.589114904 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.21160835178191123}

:::MLPv0.5.0 ssd 1541711181.589908838 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.21160835178191123}

:::MLPv0.5.0 ssd 1541711181.590695620 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711181.591507435 (train.py:338) eval_stop: 48
Iteration:   2820, Loss function: 3.494, Average Loss: 3.968, avg. samples / sec: 2791.00
Iteration:   2820, Loss function: 2.788, Average Loss: 3.956, avg. samples / sec: 2790.90
Iteration:   2820, Loss function: 3.132, Average Loss: 3.960, avg. samples / sec: 2790.79
Iteration:   2820, Loss function: 3.039, Average Loss: 3.968, avg. samples / sec: 2790.70
Iteration:   2820, Loss function: 3.743, Average Loss: 3.966, avg. samples / sec: 2790.77
Iteration:   2820, Loss function: 3.264, Average Loss: 3.969, avg. samples / sec: 2790.76
Iteration:   2820, Loss function: 2.942, Average Loss: 3.960, avg. samples / sec: 2790.83
Iteration:   2820, Loss function: 3.778, Average Loss: 3.968, avg. samples / sec: 2790.61

:::MLPv0.5.0 ssd 1541711183.748423338 (train.py:553) train_epoch: 49
Iteration:   2840, Loss function: 3.060, Average Loss: 3.951, avg. samples / sec: 21967.11
Iteration:   2840, Loss function: 2.968, Average Loss: 3.944, avg. samples / sec: 21960.29
Iteration:   2840, Loss function: 2.653, Average Loss: 3.953, avg. samples / sec: 21968.23
Iteration:   2840, Loss function: 3.069, Average Loss: 3.943, avg. samples / sec: 21955.47
Iteration:   2840, Loss function: 3.171, Average Loss: 3.952, avg. samples / sec: 21961.51
Iteration:   2840, Loss function: 2.996, Average Loss: 3.953, avg. samples / sec: 21951.35
Iteration:   2840, Loss function: 3.100, Average Loss: 3.945, avg. samples / sec: 21960.37
Iteration:   2840, Loss function: 3.127, Average Loss: 3.956, avg. samples / sec: 21959.43
Iteration:   2860, Loss function: 3.015, Average Loss: 3.940, avg. samples / sec: 21805.41
Iteration:   2860, Loss function: 2.904, Average Loss: 3.932, avg. samples / sec: 21803.66
Iteration:   2860, Loss function: 3.348, Average Loss: 3.943, avg. samples / sec: 21801.41
Iteration:   2860, Loss function: 2.970, Average Loss: 3.935, avg. samples / sec: 21792.11
Iteration:   2860, Loss function: 3.194, Average Loss: 3.939, avg. samples / sec: 21795.91
Iteration:   2860, Loss function: 3.196, Average Loss: 3.928, avg. samples / sec: 21792.19
Iteration:   2860, Loss function: 2.947, Average Loss: 3.926, avg. samples / sec: 21789.94
Iteration:   2860, Loss function: 2.947, Average Loss: 3.939, avg. samples / sec: 21788.68
Iteration:   2880, Loss function: 3.330, Average Loss: 3.915, avg. samples / sec: 21891.41
Iteration:   2880, Loss function: 3.362, Average Loss: 3.918, avg. samples / sec: 21881.99
Iteration:   2880, Loss function: 3.443, Average Loss: 3.927, avg. samples / sec: 21885.71
Iteration:   2880, Loss function: 3.181, Average Loss: 3.925, avg. samples / sec: 21890.34
Iteration:   2880, Loss function: 3.670, Average Loss: 3.931, avg. samples / sec: 21882.43
Iteration:   2880, Loss function: 3.567, Average Loss: 3.926, avg. samples / sec: 21874.68
Iteration:   2880, Loss function: 3.038, Average Loss: 3.911, avg. samples / sec: 21889.76
Iteration:   2880, Loss function: 3.287, Average Loss: 3.920, avg. samples / sec: 21872.27

:::MLPv0.5.0 ssd 1541711189.085323572 (train.py:553) train_epoch: 50
Iteration:   2900, Loss function: 3.662, Average Loss: 3.901, avg. samples / sec: 21929.60
Iteration:   2900, Loss function: 2.764, Average Loss: 3.911, avg. samples / sec: 21930.74
Iteration:   2900, Loss function: 2.816, Average Loss: 3.906, avg. samples / sec: 21939.65
Iteration:   2900, Loss function: 3.146, Average Loss: 3.896, avg. samples / sec: 21929.25
Iteration:   2900, Loss function: 2.672, Average Loss: 3.910, avg. samples / sec: 21927.85
Iteration:   2900, Loss function: 3.513, Average Loss: 3.912, avg. samples / sec: 21929.32
Iteration:   2900, Loss function: 3.067, Average Loss: 3.906, avg. samples / sec: 21925.68
Iteration:   2900, Loss function: 2.789, Average Loss: 3.918, avg. samples / sec: 21926.91
Iteration:   2920, Loss function: 3.606, Average Loss: 3.888, avg. samples / sec: 21939.84
Iteration:   2920, Loss function: 3.155, Average Loss: 3.904, avg. samples / sec: 21945.96
Iteration:   2920, Loss function: 2.996, Average Loss: 3.893, avg. samples / sec: 21939.96
Iteration:   2920, Loss function: 3.652, Average Loss: 3.897, avg. samples / sec: 21936.73
Iteration:   2920, Loss function: 3.639, Average Loss: 3.900, avg. samples / sec: 21938.94
Iteration:   2920, Loss function: 3.620, Average Loss: 3.893, avg. samples / sec: 21937.84
Iteration:   2920, Loss function: 3.307, Average Loss: 3.882, avg. samples / sec: 21934.50
Iteration:   2920, Loss function: 3.174, Average Loss: 3.899, avg. samples / sec: 21932.85
Iteration:   2940, Loss function: 2.945, Average Loss: 3.873, avg. samples / sec: 21938.83
Iteration:   2940, Loss function: 3.334, Average Loss: 3.888, avg. samples / sec: 21940.24
Iteration:   2940, Loss function: 3.088, Average Loss: 3.886, avg. samples / sec: 21952.02
Iteration:   2940, Loss function: 3.268, Average Loss: 3.880, avg. samples / sec: 21938.60
Iteration:   2940, Loss function: 3.013, Average Loss: 3.867, avg. samples / sec: 21942.06
Iteration:   2940, Loss function: 3.045, Average Loss: 3.880, avg. samples / sec: 21936.45
Iteration:   2940, Loss function: 3.449, Average Loss: 3.883, avg. samples / sec: 21933.14
Iteration:   2940, Loss function: 3.313, Average Loss: 3.888, avg. samples / sec: 21934.33

:::MLPv0.5.0 ssd 1541711194.502254248 (train.py:553) train_epoch: 51
Iteration:   2960, Loss function: 3.132, Average Loss: 3.866, avg. samples / sec: 21884.49
Iteration:   2960, Loss function: 3.386, Average Loss: 3.874, avg. samples / sec: 21875.80
Iteration:   2960, Loss function: 3.029, Average Loss: 3.868, avg. samples / sec: 21886.42
Iteration:   2960, Loss function: 3.177, Average Loss: 3.870, avg. samples / sec: 21880.87
Iteration:   2960, Loss function: 3.151, Average Loss: 3.860, avg. samples / sec: 21865.96
Iteration:   2960, Loss function: 3.132, Average Loss: 3.853, avg. samples / sec: 21875.55
Iteration:   2960, Loss function: 3.262, Average Loss: 3.872, avg. samples / sec: 21867.03
Iteration:   2960, Loss function: 3.206, Average Loss: 3.874, avg. samples / sec: 21877.26
Iteration:   2980, Loss function: 3.665, Average Loss: 3.862, avg. samples / sec: 21816.39
Iteration:   2980, Loss function: 3.473, Average Loss: 3.844, avg. samples / sec: 21821.97
Iteration:   2980, Loss function: 3.029, Average Loss: 3.860, avg. samples / sec: 21820.02
Iteration:   2980, Loss function: 2.963, Average Loss: 3.850, avg. samples / sec: 21806.71
Iteration:   2980, Loss function: 3.008, Average Loss: 3.856, avg. samples / sec: 21816.45
Iteration:   2980, Loss function: 3.095, Average Loss: 3.854, avg. samples / sec: 21809.79
Iteration:   2980, Loss function: 3.082, Average Loss: 3.836, avg. samples / sec: 21815.74
Iteration:   2980, Loss function: 3.605, Average Loss: 3.859, avg. samples / sec: 21817.40
Iteration:   3000, Loss function: 3.328, Average Loss: 3.850, avg. samples / sec: 21865.94
Iteration:   3000, Loss function: 3.173, Average Loss: 3.839, avg. samples / sec: 21872.08
Iteration:   3000, Loss function: 3.045, Average Loss: 3.844, avg. samples / sec: 21868.78
Iteration:   3000, Loss function: 2.608, Average Loss: 3.839, avg. samples / sec: 21866.68
Iteration:   3000, Loss function: 3.216, Average Loss: 3.824, avg. samples / sec: 21867.95
Iteration:   3000, Loss function: 3.295, Average Loss: 3.846, avg. samples / sec: 21863.64
Iteration:   3000, Loss function: 2.983, Average Loss: 3.846, avg. samples / sec: 21866.96
Iteration:   3000, Loss function: 2.889, Average Loss: 3.832, avg. samples / sec: 21854.33

:::MLPv0.5.0 ssd 1541711199.940047264 (train.py:553) train_epoch: 52
Iteration:   3020, Loss function: 3.027, Average Loss: 3.825, avg. samples / sec: 21883.68
Iteration:   3020, Loss function: 3.237, Average Loss: 3.818, avg. samples / sec: 21890.81
Iteration:   3020, Loss function: 3.171, Average Loss: 3.826, avg. samples / sec: 21884.83
Iteration:   3020, Loss function: 2.444, Average Loss: 3.831, avg. samples / sec: 21886.23
Iteration:   3020, Loss function: 3.300, Average Loss: 3.830, avg. samples / sec: 21881.17
Iteration:   3020, Loss function: 3.273, Average Loss: 3.837, avg. samples / sec: 21875.27
Iteration:   3020, Loss function: 3.690, Average Loss: 3.813, avg. samples / sec: 21880.22
Iteration:   3020, Loss function: 3.350, Average Loss: 3.831, avg. samples / sec: 21881.99
Iteration:   3040, Loss function: 3.549, Average Loss: 3.812, avg. samples / sec: 21895.17
Iteration:   3040, Loss function: 3.526, Average Loss: 3.816, avg. samples / sec: 21900.91
Iteration:   3040, Loss function: 2.930, Average Loss: 3.820, avg. samples / sec: 21900.86
Iteration:   3040, Loss function: 3.071, Average Loss: 3.800, avg. samples / sec: 21900.58
Iteration:   3040, Loss function: 3.059, Average Loss: 3.814, avg. samples / sec: 21892.48
Iteration:   3040, Loss function: 3.134, Average Loss: 3.819, avg. samples / sec: 21893.36
Iteration:   3040, Loss function: 3.147, Average Loss: 3.825, avg. samples / sec: 21893.00
Iteration:   3040, Loss function: 2.803, Average Loss: 3.804, avg. samples / sec: 21888.13
Iteration:   3060, Loss function: 3.494, Average Loss: 3.800, avg. samples / sec: 21884.23
Iteration:   3060, Loss function: 2.847, Average Loss: 3.799, avg. samples / sec: 21890.01
Iteration:   3060, Loss function: 3.077, Average Loss: 3.805, avg. samples / sec: 21884.81
Iteration:   3060, Loss function: 3.023, Average Loss: 3.813, avg. samples / sec: 21890.38
Iteration:   3060, Loss function: 4.008, Average Loss: 3.791, avg. samples / sec: 21891.55
Iteration:   3060, Loss function: 3.212, Average Loss: 3.786, avg. samples / sec: 21881.43
Iteration:   3060, Loss function: 2.824, Average Loss: 3.806, avg. samples / sec: 21886.25
Iteration:   3060, Loss function: 2.911, Average Loss: 3.806, avg. samples / sec: 21875.00

:::MLPv0.5.0 ssd 1541711205.368454456 (train.py:553) train_epoch: 53
Iteration:   3080, Loss function: 3.167, Average Loss: 3.778, avg. samples / sec: 21832.11
Iteration:   3080, Loss function: 2.997, Average Loss: 3.789, avg. samples / sec: 21825.35
Iteration:   3080, Loss function: 3.083, Average Loss: 3.793, avg. samples / sec: 21832.03
Iteration:   3080, Loss function: 3.218, Average Loss: 3.791, avg. samples / sec: 21823.93
Iteration:   3080, Loss function: 3.048, Average Loss: 3.786, avg. samples / sec: 21820.35
Iteration:   3080, Loss function: 3.183, Average Loss: 3.798, avg. samples / sec: 21821.23
Iteration:   3080, Loss function: 3.041, Average Loss: 3.773, avg. samples / sec: 21821.54
Iteration:   3080, Loss function: 3.548, Average Loss: 3.793, avg. samples / sec: 21797.63
Iteration:   3100, Loss function: 3.345, Average Loss: 3.765, avg. samples / sec: 21896.05
Iteration:   3100, Loss function: 3.366, Average Loss: 3.778, avg. samples / sec: 21896.17
Iteration:   3100, Loss function: 2.929, Average Loss: 3.777, avg. samples / sec: 21896.08
Iteration:   3100, Loss function: 3.117, Average Loss: 3.773, avg. samples / sec: 21896.07
Iteration:   3100, Loss function: 2.955, Average Loss: 3.785, avg. samples / sec: 21898.60
Iteration:   3100, Loss function: 3.308, Average Loss: 3.780, avg. samples / sec: 21892.00
Iteration:   3100, Loss function: 2.900, Average Loss: 3.779, avg. samples / sec: 21921.22
Iteration:   3100, Loss function: 3.241, Average Loss: 3.759, avg. samples / sec: 21897.38

:::MLPv0.5.0 ssd 1541711210.708098888 (train.py:553) train_epoch: 54
Iteration:   3120, Loss function: 2.995, Average Loss: 3.754, avg. samples / sec: 21836.43
Iteration:   3120, Loss function: 2.303, Average Loss: 3.747, avg. samples / sec: 21847.82
Iteration:   3120, Loss function: 3.222, Average Loss: 3.773, avg. samples / sec: 21838.52
Iteration:   3120, Loss function: 3.024, Average Loss: 3.768, avg. samples / sec: 21839.09
Iteration:   3120, Loss function: 3.048, Average Loss: 3.765, avg. samples / sec: 21830.10
Iteration:   3120, Loss function: 3.360, Average Loss: 3.757, avg. samples / sec: 21834.00
Iteration:   3120, Loss function: 3.027, Average Loss: 3.763, avg. samples / sec: 21829.27
Iteration:   3120, Loss function: 3.348, Average Loss: 3.766, avg. samples / sec: 21830.12
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2

:::MLPv0.5.0 ssd 1541711211.273553371 (train.py:586) opt_learning_rate: 0.0016

































































:::MLPv0.5.0 ssd 1541711211.365242481 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711211.366055727 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711211.366785526 (train.py:220) eval_start: 54
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.94 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.94 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.94 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.94 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.94 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.94 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.94 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.94 s
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(294107, 7)
(294107, 7)
0/294107
0/294107
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(294107, 7)
Converting ndarray to lists...
0/294107
Loading and preparing results...
(294107, 7)
Loading and preparing results...
Loading and preparing results...
0/294107
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(294107, 7)
0/294107
(294107, 7)
Loading and preparing results...
0/294107
Converting ndarray to lists...
(294107, 7)
(294107, 7)
0/294107
0/294107
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(294107, 7)
(294107, 7)
Converting ndarray to lists...
(294107, 7)
Converting ndarray to lists...
0/294107
Loading and preparing results...
(294107, 7)
0/294107
Converting ndarray to lists...
Loading and preparing results...
0/294107
(294107, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(294107, 7)
(294107, 7)
Converting ndarray to lists...
0/294107
0/294107
Converting ndarray to lists...
Loading and preparing results...
0/294107
(294107, 7)
Converting ndarray to lists...
0/294107
(294107, 7)
(294107, 7)
Converting ndarray to lists...
(294107, 7)
Loading and preparing results...
(294107, 7)
0/294107
Loading and preparing results...
0/294107
0/294107
0/294107
0/294107
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(294107, 7)
Converting ndarray to lists...
(294107, 7)
0/294107
(294107, 7)
0/294107
(294107, 7)
Converting ndarray to lists...
Loading and preparing results...
(294107, 7)
Loading and preparing results...
0/294107
0/294107
0/294107
Converting ndarray to lists...
Converting ndarray to lists...
(294107, 7)
Loading and preparing results...
(294107, 7)
0/294107
0/294107
Converting ndarray to lists...
(294107, 7)
0/294107
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(294107, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/294107
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(294107, 7)
(294107, 7)
(294107, 7)
(294107, 7)
(294107, 7)
0/294107
0/294107
Converting ndarray to lists...
0/294107
(294107, 7)
0/294107
0/294107
Loading and preparing results...
0/294107
Converting ndarray to lists...
(294107, 7)
0/294107
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(294107, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/294107
(294107, 7)
Loading and preparing results...
(294107, 7)
Converting ndarray to lists...
Loading and preparing results...
0/294107
Loading and preparing results...
0/294107
Converting ndarray to lists...
(294107, 7)
Converting ndarray to lists...
0/294107
Converting ndarray to lists...
(294107, 7)
(294107, 7)
(294107, 7)
0/294107
0/294107
0/294107
Loading and preparing results...
Converting ndarray to lists...
(294107, 7)
0/294107
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(294107, 7)
(294107, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/294107
0/294107
(294107, 7)
(294107, 7)
0/294107
0/294107
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(294107, 7)
(294107, 7)
(294107, 7)
(294107, 7)
0/294107
0/294107
0/294107
0/294107
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(294107, 7)
Loading and preparing results...
(294107, 7)
Loading and preparing results...
(294107, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/294107
Converting ndarray to lists...
0/294107
Converting ndarray to lists...
0/294107
(294107, 7)
(294107, 7)
0/294107
(294107, 7)
(294107, 7)
0/294107
0/294107
0/294107
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(294107, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/294107
(294107, 7)
(294107, 7)
0/294107
0/294107
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(294107, 7)
Converting ndarray to lists...
(294107, 7)
0/294107
0/294107
DONE (t=1.53s)
creating index...
DONE (t=1.56s)
creating index...
index created!
index created!
DONE (t=1.71s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.84s)
creating index...
index created!
DONE (t=1.85s)
creating index...
index created!
index created!
index created!
index created!
index created!
DONE (t=1.89s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=1.91s)
creating index...
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
DONE (t=1.95s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
index created!
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.04s)
creating index...
index created!
DONE (t=2.04s)
creating index...
DONE (t=2.04s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.05s)
creating index...
index created!
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.07s)
creating index...
DONE (t=2.08s)
creating index...
index created!
DONE (t=2.08s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.09s)
creating index...
DONE (t=2.09s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
index created!
index created!
DONE (t=2.11s)
creating index...
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=3.51s).
Accumulating evaluation results...
DONE (t=3.54s).
Accumulating evaluation results...
DONE (t=3.52s).
Accumulating evaluation results...
DONE (t=3.55s).
Accumulating evaluation results...
DONE (t=3.52s).
Accumulating evaluation results...
DONE (t=3.51s).
Accumulating evaluation results...
DONE (t=3.62s).
Accumulating evaluation results...
DONE (t=3.56s).
Accumulating evaluation results...
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.225
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.342
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21245 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
DONE (t=1.09s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.225
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.342
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21245 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.225
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.342
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21245 AP goal: 0.21200
DONE (t=1.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.225
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.342
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21245 AP goal: 0.21200
DONE (t=1.09s).
DONE (t=1.09s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.225
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.225
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.342
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21245 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.342
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21245 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.225
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.225
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.339
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.342
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21245 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.303
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.317
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.342
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.494
Current AP: 0.21245 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711223.312617302 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711223.313582897 (train.py:333) eval_accuracy: {"epoch": 54, "value": 0.21244958564247396}

:::MLPv0.5.0 ssd 1541711223.314368963 (train.py:336) eval_iteration_accuracy: {"epoch": 54, "value": 0.21244958564247396}

:::MLPv0.5.0 ssd 1541711223.315147161 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711223.315922022 (train.py:338) eval_stop: 54

:::MLPv0.5.0 ssd 1541711224.371865034 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1541711224.372616529 (train.py:707) run_final
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2018-11-08 09:07:09 PM
RESULT,OBJECT_DETECTION,,404,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:09 PM
RESULT,OBJECT_DETECTION,,404,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:09 PM
RESULT,OBJECT_DETECTION,,404,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:09 PM
RESULT,OBJECT_DETECTION,,404,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:09 PM
RESULT,OBJECT_DETECTION,,404,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:09 PM
RESULT,OBJECT_DETECTION,,404,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:09 PM
RESULT,OBJECT_DETECTION,,404,nvidia,2018-11-08 09:00:25 PM
ENDING TIMING RUN AT 2018-11-08 09:07:09 PM
RESULT,OBJECT_DETECTION,,404,nvidia,2018-11-08 09:00:25 PM
