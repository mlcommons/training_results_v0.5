Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3

:::MLPv0.5.0 ssd 1541710826.707458973 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710826.714167118 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710826.719208717 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710826.741303205 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710826.761885881 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710826.765388489 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710826.766335726 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541710826.771736145 (<string>:1) run_clear_caches
Launching on node sc-sdgx-323
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-324
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-810
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-323
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-816
+ srun -N 1 -n 1 -w sc-sdgx-323 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=0 --master_addr=172.22.0.124 --master_port=4242' -e SLURM_JOB_ID=155378 -e SLURM_NTASKS_PER_NODE=8 cont_155378 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-324
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-817
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-810
+ srun -N 1 -n 1 -w sc-sdgx-324 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=1 --master_addr=172.22.0.124 --master_port=4242' -e SLURM_JOB_ID=155378 -e SLURM_NTASKS_PER_NODE=8 cont_155378 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-818
+ srun -N 1 -n 1 -w sc-sdgx-810 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=2 --master_addr=172.22.0.124 --master_port=4242' -e SLURM_JOB_ID=155378 -e SLURM_NTASKS_PER_NODE=8 cont_155378 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-816
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-819
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-817
+ srun -N 1 -n 1 -w sc-sdgx-816 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=3 --master_addr=172.22.0.124 --master_port=4242' -e SLURM_JOB_ID=155378 -e SLURM_NTASKS_PER_NODE=8 cont_155378 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node sc-sdgx-838
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-818
+ srun -N 1 -n 1 -w sc-sdgx-817 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=4 --master_addr=172.22.0.124 --master_port=4242' -e SLURM_JOB_ID=155378 -e SLURM_NTASKS_PER_NODE=8 cont_155378 ./run_and_time.sh
+ pids+=($!)
+ set +x
+ srun -N 1 -n 1 -w sc-sdgx-818 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=5 --master_addr=172.22.0.124 --master_port=4242' -e SLURM_JOB_ID=155378 -e SLURM_NTASKS_PER_NODE=8 cont_155378 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-838
+ srun -N 1 -n 1 -w sc-sdgx-838 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=7 --master_addr=172.22.0.124 --master_port=4242' -e SLURM_JOB_ID=155378 -e SLURM_NTASKS_PER_NODE=8 cont_155378 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-819
+ srun -N 1 -n 1 -w sc-sdgx-819 docker exec -e DGXSYSTEM=DGX1_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=6 --master_addr=172.22.0.124 --master_port=4242' -e SLURM_JOB_ID=155378 -e SLURM_NTASKS_PER_NODE=8 cont_155378 ./run_and_time.sh
Run vars: id 155378 gpus 8 mparams  --nnodes=8 --node_rank=0 --master_addr=172.22.0.124 --master_port=4242
Run vars: id 155378 gpus 8 mparams  --nnodes=8 --node_rank=3 --master_addr=172.22.0.124 --master_port=4242
Run vars: id 155378 gpus 8 mparams  --nnodes=8 --node_rank=1 --master_addr=172.22.0.124 --master_port=4242
Run vars: id 155378 gpus 8 mparams  --nnodes=8 --node_rank=4 --master_addr=172.22.0.124 --master_port=4242
Run vars: id 155378 gpus 8 mparams  --nnodes=8 --node_rank=7 --master_addr=172.22.0.124 --master_port=4242
Run vars: id 155378 gpus 8 mparams  --nnodes=8 --node_rank=6 --master_addr=172.22.0.124 --master_port=4242
Run vars: id 155378 gpus 8 mparams  --nnodes=8 --node_rank=5 --master_addr=172.22.0.124 --master_port=4242
Run vars: id 155378 gpus 8 mparams  --nnodes=8 --node_rank=2 --master_addr=172.22.0.124 --master_port=4242
STARTING TIMING RUN AT 2018-11-08 09:00:27 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=0 --master_addr=172.22.0.124 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:27 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=3 --master_addr=172.22.0.124 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:27 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=1 --master_addr=172.22.0.124 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:27 PM
running benchmark
STARTING TIMING RUN AT 2018-11-08 09:00:27 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ echo 'running benchmark'
+ DATASET_DIR=/data/coco2017
+ export DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ DATASET_DIR=/data/coco2017
+ TORCH_MODEL_ZOO=/data/torchvision
+ export TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=7 --master_addr=172.22.0.124 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=4 --master_addr=172.22.0.124 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:27 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
STARTING TIMING RUN AT 2018-11-08 09:00:27 PM
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=5 --master_addr=172.22.0.124 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=6 --master_addr=172.22.0.124 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-08 09:00:27 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 --nnodes=8 --node_rank=2 --master_addr=172.22.0.124 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
0 Using seed = 2821529286
1 Using seed = 2821529287
3 Using seed = 2821529289
5 Using seed = 2821529291
7 Using seed = 2821529293
4 Using seed = 2821529290
8 Using seed = 2821529294
10 Using seed = 2821529296
9 Using seed = 2821529295
11 Using seed = 2821529297
13 Using seed = 2821529299
14 Using seed = 2821529300
15 Using seed = 2821529301
12 Using seed = 2821529298
19 Using seed = 2821529305
16 Using seed = 2821529302
18 Using seed = 2821529304
17 Using seed = 2821529303
23 Using seed = 2821529309
21 Using seed = 2821529307
20 Using seed = 2821529306
22 Using seed = 2821529308
24 Using seed = 2821529310
26 Using seed = 2821529312
25 Using seed = 2821529311
27 Using seed = 2821529313
29 Using seed = 2821529315
31 Using seed = 2821529317
30 Using seed = 2821529316
28 Using seed = 2821529314
39 Using seed = 2821529325
37 Using seed = 2821529323
38 Using seed = 2821529324
36 Using seed = 2821529322
33 Using seed = 2821529319
32 Using seed = 2821529318
34 Using seed = 2821529320
35 Using seed = 2821529321
45 Using seed = 2821529331
46 Using seed = 2821529332
44 Using seed = 2821529330
47 Using seed = 2821529333
43 Using seed = 2821529329
41 Using seed = 2821529327
42 Using seed = 2821529328
40 Using seed = 2821529326
52 Using seed = 2821529338
54 Using seed = 2821529340
53 Using seed = 2821529339
55 Using seed = 2821529341
51 Using seed = 2821529337
49 Using seed = 2821529335
50 Using seed = 2821529336
48 Using seed = 2821529334
58 Using seed = 2821529344
59 Using seed = 2821529345
57 Using seed = 2821529343
56 Using seed = 2821529342
63 Using seed = 2821529349
62 Using seed = 2821529348
61 Using seed = 2821529347
60 Using seed = 2821529346
6 Using seed = 2821529292
2 Using seed = 2821529288

:::MLPv0.5.0 ssd 1541710839.310066700 (train.py:371) run_start

:::MLPv0.5.0 ssd 1541710839.313417435 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1541710839.328189135 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1541710839.343186378 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1541710839.344016552 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1541710839.398703337 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1541710839.422776222 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1541710839.442722082 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1541710839.462655306 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1541710839.482651949 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1541710839.484651089 (train.py:382) input_size: 300
loading annotations into memory...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.47s)
creating index...
Done (t=0.48s)
creating index...
index created!
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
index created!
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
index created!
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
Done (t=0.49s)
creating index...
creating index...
index created!
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.49s)
creating index...
Done (t=0.50s)
creating index...
index created!
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
index created!
index created!
index created!
index created!
index created!
Done (t=0.51s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
time_check a: 1541710840.450773001
time_check a: 1541710840.454235315
time_check a: 1541710840.454727888
time_check a: 1541710840.455459356
time_check a: 1541710840.456538677
time_check a: 1541710840.457627058
time_check a: 1541710840.461033106
time_check a: 1541710840.465434551
time_check b: 1541710864.075463057
time_check b: 1541710864.085582733
time_check b: 1541710864.115521908
time_check b: 1541710864.155652285
time_check b: 1541710864.193122625
time_check b: 1541710864.236361742
time_check b: 1541710864.529075146
time_check b: 1541710864.797918797

:::MLPv0.5.0 ssd 1541710865.944556713 (train.py:413) input_order

:::MLPv0.5.0 ssd 1541710865.951800585 (train.py:414) input_batch_size: 32

:::MLPv0.5.0 ssd 1541710870.129812479 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710870.130698681 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710870.160063505 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1541710871.191702127 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1541710871.192495346 (train.py:477) opt_learning_rate: 0.16

:::MLPv0.5.0 ssd 1541710871.193214178 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1541710871.193906069 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1541710871.194609642 (train.py:483) opt_learning_rate_warmup_steps: 900

:::MLPv0.5.0 ssd 1541710875.368169308 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710875.369016647 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710875.398322344 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss

:::MLPv0.5.0 ssd 1541710878.222493410 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1541710878.223282337 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1541710878.226508617 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 21.984, Average Loss: 0.022, avg. samples / sec: 14889.43
Iteration:      0, Loss function: 22.340, Average Loss: 0.022, avg. samples / sec: 18164.76
Iteration:      0, Loss function: 22.199, Average Loss: 0.022, avg. samples / sec: 16155.97
Iteration:      0, Loss function: 22.343, Average Loss: 0.022, avg. samples / sec: 17930.10
Iteration:      0, Loss function: 22.420, Average Loss: 0.022, avg. samples / sec: 14925.67
Iteration:      0, Loss function: 22.014, Average Loss: 0.022, avg. samples / sec: 32599.74
Iteration:      0, Loss function: 21.854, Average Loss: 0.022, avg. samples / sec: 24694.36
Iteration:      0, Loss function: 21.397, Average Loss: 0.021, avg. samples / sec: 31248.17

:::MLPv0.5.0 ssd 1541710880.127051830 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.0001777777777777767}

:::MLPv0.5.0 ssd 1541710880.357973337 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0003555555555555534}

:::MLPv0.5.0 ssd 1541710880.469220400 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0005333333333333301}

:::MLPv0.5.0 ssd 1541710880.599488497 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0007111111111111068}

:::MLPv0.5.0 ssd 1541710880.704548120 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0008888888888888835}

:::MLPv0.5.0 ssd 1541710880.813546181 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0010666666666666602}

:::MLPv0.5.0 ssd 1541710880.919963360 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.001244444444444437}

:::MLPv0.5.0 ssd 1541710881.029624462 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0014222222222222136}

:::MLPv0.5.0 ssd 1541710881.130521297 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0015999999999999903}

:::MLPv0.5.0 ssd 1541710881.237492800 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.001777777777777767}

:::MLPv0.5.0 ssd 1541710881.342967749 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0019555555555555437}

:::MLPv0.5.0 ssd 1541710881.448350906 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.0021333333333333204}

:::MLPv0.5.0 ssd 1541710881.557448864 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.002311111111111097}

:::MLPv0.5.0 ssd 1541710881.664510012 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.002488888888888874}

:::MLPv0.5.0 ssd 1541710881.767341852 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.0026666666666666505}

:::MLPv0.5.0 ssd 1541710881.877999067 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.0028444444444444272}

:::MLPv0.5.0 ssd 1541710881.983357430 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.0030222222222222317}

:::MLPv0.5.0 ssd 1541710882.088575602 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.0032000000000000084}

:::MLPv0.5.0 ssd 1541710882.203952551 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.003377777777777785}

:::MLPv0.5.0 ssd 1541710882.302946806 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.003555555555555562}
Iteration:     20, Loss function: 20.752, Average Loss: 0.440, avg. samples / sec: 10080.21
Iteration:     20, Loss function: 21.276, Average Loss: 0.439, avg. samples / sec: 10070.90
Iteration:     20, Loss function: 20.928, Average Loss: 0.437, avg. samples / sec: 10075.07
Iteration:     20, Loss function: 20.859, Average Loss: 0.438, avg. samples / sec: 10078.36
Iteration:     20, Loss function: 19.805, Average Loss: 0.442, avg. samples / sec: 10076.23
Iteration:     20, Loss function: 20.535, Average Loss: 0.438, avg. samples / sec: 10076.04
Iteration:     20, Loss function: 20.694, Average Loss: 0.437, avg. samples / sec: 10079.12
Iteration:     20, Loss function: 20.435, Average Loss: 0.439, avg. samples / sec: 10074.11

:::MLPv0.5.0 ssd 1541710882.413430691 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.0037333333333333385}

:::MLPv0.5.0 ssd 1541710882.512436628 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.003911111111111115}

:::MLPv0.5.0 ssd 1541710882.610782623 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.004088888888888892}

:::MLPv0.5.0 ssd 1541710882.708383322 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.004266666666666669}

:::MLPv0.5.0 ssd 1541710882.816761255 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.004444444444444445}

:::MLPv0.5.0 ssd 1541710882.923581123 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.004622222222222222}

:::MLPv0.5.0 ssd 1541710883.020249128 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.004799999999999999}

:::MLPv0.5.0 ssd 1541710883.121445417 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.004977777777777775}

:::MLPv0.5.0 ssd 1541710883.225940943 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.005155555555555552}

:::MLPv0.5.0 ssd 1541710883.324500084 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.005333333333333329}

:::MLPv0.5.0 ssd 1541710883.424495935 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.0055111111111111055}

:::MLPv0.5.0 ssd 1541710883.526549578 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.005688888888888882}

:::MLPv0.5.0 ssd 1541710883.626940250 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.005866666666666659}

:::MLPv0.5.0 ssd 1541710883.728307009 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.006044444444444436}

:::MLPv0.5.0 ssd 1541710883.833232164 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.006222222222222212}

:::MLPv0.5.0 ssd 1541710883.937489748 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.006399999999999989}

:::MLPv0.5.0 ssd 1541710884.037826300 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.006577777777777766}

:::MLPv0.5.0 ssd 1541710884.139260054 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.0067555555555555424}

:::MLPv0.5.0 ssd 1541710884.237345219 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.006933333333333319}

:::MLPv0.5.0 ssd 1541710884.343591213 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.007111111111111096}
Iteration:     40, Loss function: 16.473, Average Loss: 0.809, avg. samples / sec: 20100.21
Iteration:     40, Loss function: 16.722, Average Loss: 0.814, avg. samples / sec: 20120.78
Iteration:     40, Loss function: 16.156, Average Loss: 0.811, avg. samples / sec: 20085.89
Iteration:     40, Loss function: 15.919, Average Loss: 0.814, avg. samples / sec: 20091.15
Iteration:     40, Loss function: 16.372, Average Loss: 0.812, avg. samples / sec: 20076.71
Iteration:     40, Loss function: 16.299, Average Loss: 0.813, avg. samples / sec: 20053.05
Iteration:     40, Loss function: 15.942, Average Loss: 0.814, avg. samples / sec: 20038.51
Iteration:     40, Loss function: 16.259, Average Loss: 0.809, avg. samples / sec: 20045.08

:::MLPv0.5.0 ssd 1541710884.449448109 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.0072888888888888725}

:::MLPv0.5.0 ssd 1541710884.546526432 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.007466666666666649}

:::MLPv0.5.0 ssd 1541710884.648745298 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.007644444444444454}

:::MLPv0.5.0 ssd 1541710884.749511242 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.00782222222222223}

:::MLPv0.5.0 ssd 1541710884.846684456 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.008000000000000007}

:::MLPv0.5.0 ssd 1541710884.947466135 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.008177777777777784}

:::MLPv0.5.0 ssd 1541710885.047978401 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.00835555555555556}

:::MLPv0.5.0 ssd 1541710885.146762848 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.008533333333333337}

:::MLPv0.5.0 ssd 1541710885.246700048 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.008711111111111114}

:::MLPv0.5.0 ssd 1541710885.350693703 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.00888888888888889}

:::MLPv0.5.0 ssd 1541710885.453497887 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.009066666666666667}

:::MLPv0.5.0 ssd 1541710885.551186323 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.009244444444444444}

:::MLPv0.5.0 ssd 1541710885.649853468 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.00942222222222222}

:::MLPv0.5.0 ssd 1541710885.748355389 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.009599999999999997}

:::MLPv0.5.0 ssd 1541710885.845770597 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.009777777777777774}

:::MLPv0.5.0 ssd 1541710885.944754601 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.00995555555555555}

:::MLPv0.5.0 ssd 1541710886.056198120 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.010133333333333328}

:::MLPv0.5.0 ssd 1541710886.148664474 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1541710886.153909683 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 58, "value": 0.010311111111111104}

:::MLPv0.5.0 ssd 1541710886.249282598 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 59, "value": 0.010488888888888881}

:::MLPv0.5.0 ssd 1541710886.347933769 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 60, "value": 0.010666666666666658}
Iteration:     60, Loss function: 11.521, Average Loss: 1.045, avg. samples / sec: 20498.88
Iteration:     60, Loss function: 10.776, Average Loss: 1.049, avg. samples / sec: 20471.21
Iteration:     60, Loss function: 11.426, Average Loss: 1.044, avg. samples / sec: 20436.94
Iteration:     60, Loss function: 11.558, Average Loss: 1.044, avg. samples / sec: 20443.65
Iteration:     60, Loss function: 11.159, Average Loss: 1.048, avg. samples / sec: 20464.68
Iteration:     60, Loss function: 10.668, Average Loss: 1.048, avg. samples / sec: 20435.83
Iteration:     60, Loss function: 10.823, Average Loss: 1.051, avg. samples / sec: 20426.24
Iteration:     60, Loss function: 10.385, Average Loss: 1.045, avg. samples / sec: 20429.76

:::MLPv0.5.0 ssd 1541710886.446260691 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 61, "value": 0.010844444444444434}

:::MLPv0.5.0 ssd 1541710886.546909332 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 62, "value": 0.011022222222222211}

:::MLPv0.5.0 ssd 1541710886.644314289 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 63, "value": 0.011199999999999988}

:::MLPv0.5.0 ssd 1541710886.745587111 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 64, "value": 0.011377777777777764}

:::MLPv0.5.0 ssd 1541710886.842659950 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 65, "value": 0.011555555555555541}

:::MLPv0.5.0 ssd 1541710886.939150572 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 66, "value": 0.011733333333333318}

:::MLPv0.5.0 ssd 1541710887.036501646 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 67, "value": 0.011911111111111095}

:::MLPv0.5.0 ssd 1541710887.138128996 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 68, "value": 0.012088888888888899}

:::MLPv0.5.0 ssd 1541710887.235317230 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 69, "value": 0.012266666666666676}

:::MLPv0.5.0 ssd 1541710887.333284855 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 70, "value": 0.012444444444444452}

:::MLPv0.5.0 ssd 1541710887.429940224 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 71, "value": 0.012622222222222229}

:::MLPv0.5.0 ssd 1541710887.536825657 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 72, "value": 0.012800000000000006}

:::MLPv0.5.0 ssd 1541710887.638315678 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 73, "value": 0.012977777777777783}

:::MLPv0.5.0 ssd 1541710887.740633965 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 74, "value": 0.01315555555555556}

:::MLPv0.5.0 ssd 1541710887.838866472 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 75, "value": 0.013333333333333336}

:::MLPv0.5.0 ssd 1541710887.935465813 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 76, "value": 0.013511111111111113}

:::MLPv0.5.0 ssd 1541710888.040283680 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 77, "value": 0.01368888888888889}

:::MLPv0.5.0 ssd 1541710888.151960850 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 78, "value": 0.013866666666666666}

:::MLPv0.5.0 ssd 1541710888.247557163 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 79, "value": 0.014044444444444443}

:::MLPv0.5.0 ssd 1541710888.344475508 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 80, "value": 0.01422222222222222}
Iteration:     80, Loss function: 10.375, Average Loss: 1.243, avg. samples / sec: 20519.89
Iteration:     80, Loss function: 9.923, Average Loss: 1.246, avg. samples / sec: 20513.54
Iteration:     80, Loss function: 10.319, Average Loss: 1.251, avg. samples / sec: 20527.42
Iteration:     80, Loss function: 10.224, Average Loss: 1.246, avg. samples / sec: 20511.44
Iteration:     80, Loss function: 9.459, Average Loss: 1.243, avg. samples / sec: 20511.10
Iteration:     80, Loss function: 10.043, Average Loss: 1.250, avg. samples / sec: 20513.02
Iteration:     80, Loss function: 10.328, Average Loss: 1.248, avg. samples / sec: 20517.07
Iteration:     80, Loss function: 9.905, Average Loss: 1.243, avg. samples / sec: 20523.02

:::MLPv0.5.0 ssd 1541710888.438710451 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 81, "value": 0.014399999999999996}

:::MLPv0.5.0 ssd 1541710888.534241199 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 82, "value": 0.014577777777777773}

:::MLPv0.5.0 ssd 1541710888.632909775 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 83, "value": 0.01475555555555555}

:::MLPv0.5.0 ssd 1541710888.728672981 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 84, "value": 0.014933333333333326}

:::MLPv0.5.0 ssd 1541710888.826228619 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 85, "value": 0.015111111111111103}

:::MLPv0.5.0 ssd 1541710888.921428442 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 86, "value": 0.01528888888888888}

:::MLPv0.5.0 ssd 1541710889.017308712 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 87, "value": 0.015466666666666656}

:::MLPv0.5.0 ssd 1541710889.115560293 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 88, "value": 0.015644444444444433}

:::MLPv0.5.0 ssd 1541710889.218454361 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 89, "value": 0.01582222222222221}

:::MLPv0.5.0 ssd 1541710889.315163136 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 90, "value": 0.015999999999999986}

:::MLPv0.5.0 ssd 1541710889.413386345 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 91, "value": 0.016177777777777763}

:::MLPv0.5.0 ssd 1541710889.509026051 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 92, "value": 0.01635555555555554}

:::MLPv0.5.0 ssd 1541710889.604615688 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 93, "value": 0.016533333333333317}

:::MLPv0.5.0 ssd 1541710889.708541870 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 94, "value": 0.01671111111111112}

:::MLPv0.5.0 ssd 1541710889.805668831 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 95, "value": 0.016888888888888898}

:::MLPv0.5.0 ssd 1541710889.903219938 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 96, "value": 0.017066666666666674}

:::MLPv0.5.0 ssd 1541710889.999671221 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 97, "value": 0.01724444444444445}

:::MLPv0.5.0 ssd 1541710890.096165180 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 98, "value": 0.017422222222222228}

:::MLPv0.5.0 ssd 1541710890.198007107 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 99, "value": 0.017600000000000005}

:::MLPv0.5.0 ssd 1541710890.294404030 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 100, "value": 0.01777777777777778}
Iteration:    100, Loss function: 9.596, Average Loss: 1.409, avg. samples / sec: 21013.23
Iteration:    100, Loss function: 8.820, Average Loss: 1.411, avg. samples / sec: 21004.98
Iteration:    100, Loss function: 9.089, Average Loss: 1.409, avg. samples / sec: 20997.19
Iteration:    100, Loss function: 9.341, Average Loss: 1.414, avg. samples / sec: 21008.90
Iteration:    100, Loss function: 9.218, Average Loss: 1.409, avg. samples / sec: 21026.73
Iteration:    100, Loss function: 8.852, Average Loss: 1.414, avg. samples / sec: 20997.02
Iteration:    100, Loss function: 9.209, Average Loss: 1.411, avg. samples / sec: 20996.00
Iteration:    100, Loss function: 8.918, Average Loss: 1.413, avg. samples / sec: 20988.11

:::MLPv0.5.0 ssd 1541710890.395318031 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 101, "value": 0.017955555555555558}

:::MLPv0.5.0 ssd 1541710890.492352009 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 102, "value": 0.018133333333333335}

:::MLPv0.5.0 ssd 1541710890.593834162 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 103, "value": 0.01831111111111111}

:::MLPv0.5.0 ssd 1541710890.690544605 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 104, "value": 0.018488888888888888}

:::MLPv0.5.0 ssd 1541710890.787041426 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 105, "value": 0.018666666666666665}

:::MLPv0.5.0 ssd 1541710890.893898964 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 106, "value": 0.01884444444444444}

:::MLPv0.5.0 ssd 1541710890.990154505 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 107, "value": 0.019022222222222218}

:::MLPv0.5.0 ssd 1541710891.095753193 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 108, "value": 0.019199999999999995}

:::MLPv0.5.0 ssd 1541710891.191457748 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 109, "value": 0.01937777777777777}

:::MLPv0.5.0 ssd 1541710891.286216736 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 110, "value": 0.019555555555555548}

:::MLPv0.5.0 ssd 1541710891.382450342 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 111, "value": 0.019733333333333325}

:::MLPv0.5.0 ssd 1541710891.478008032 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 112, "value": 0.0199111111111111}

:::MLPv0.5.0 ssd 1541710891.574017048 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 113, "value": 0.02008888888888888}

:::MLPv0.5.0 ssd 1541710891.670075655 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 114, "value": 0.020266666666666655}

:::MLPv0.5.0 ssd 1541710891.778254032 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 115, "value": 0.020444444444444432}

:::MLPv0.5.0 ssd 1541710891.871425867 (train.py:553) train_epoch: 2

:::MLPv0.5.0 ssd 1541710891.876908064 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 116, "value": 0.02062222222222221}

:::MLPv0.5.0 ssd 1541710891.979452372 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 117, "value": 0.020799999999999985}

:::MLPv0.5.0 ssd 1541710892.076766491 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 118, "value": 0.020977777777777762}

:::MLPv0.5.0 ssd 1541710892.174219847 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 119, "value": 0.02115555555555554}

:::MLPv0.5.0 ssd 1541710892.273968935 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 120, "value": 0.021333333333333343}
Iteration:    120, Loss function: 8.734, Average Loss: 1.563, avg. samples / sec: 20707.02
Iteration:    120, Loss function: 9.113, Average Loss: 1.560, avg. samples / sec: 20700.34
Iteration:    120, Loss function: 8.717, Average Loss: 1.562, avg. samples / sec: 20696.83
Iteration:    120, Loss function: 8.466, Average Loss: 1.558, avg. samples / sec: 20690.23
Iteration:    120, Loss function: 8.522, Average Loss: 1.558, avg. samples / sec: 20698.44
Iteration:    120, Loss function: 8.868, Average Loss: 1.563, avg. samples / sec: 20710.59
Iteration:    120, Loss function: 9.043, Average Loss: 1.564, avg. samples / sec: 20695.27
Iteration:    120, Loss function: 8.762, Average Loss: 1.566, avg. samples / sec: 20689.00

:::MLPv0.5.0 ssd 1541710892.370179892 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 121, "value": 0.02151111111111112}

:::MLPv0.5.0 ssd 1541710892.466421843 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 122, "value": 0.021688888888888896}

:::MLPv0.5.0 ssd 1541710892.562364101 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 123, "value": 0.021866666666666673}

:::MLPv0.5.0 ssd 1541710892.657428265 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 124, "value": 0.02204444444444445}

:::MLPv0.5.0 ssd 1541710892.754827738 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 125, "value": 0.022222222222222227}

:::MLPv0.5.0 ssd 1541710892.850203037 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 126, "value": 0.022400000000000003}

:::MLPv0.5.0 ssd 1541710892.947641611 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 127, "value": 0.02257777777777778}

:::MLPv0.5.0 ssd 1541710893.047272682 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 128, "value": 0.022755555555555557}

:::MLPv0.5.0 ssd 1541710893.149211645 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 129, "value": 0.022933333333333333}

:::MLPv0.5.0 ssd 1541710893.244832039 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 130, "value": 0.02311111111111111}

:::MLPv0.5.0 ssd 1541710893.342997313 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 131, "value": 0.023288888888888887}

:::MLPv0.5.0 ssd 1541710893.442497969 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 132, "value": 0.023466666666666663}

:::MLPv0.5.0 ssd 1541710893.542624950 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 133, "value": 0.02364444444444444}

:::MLPv0.5.0 ssd 1541710893.647445202 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 134, "value": 0.023822222222222217}

:::MLPv0.5.0 ssd 1541710893.743930101 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 135, "value": 0.023999999999999994}

:::MLPv0.5.0 ssd 1541710893.840594292 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 136, "value": 0.02417777777777777}

:::MLPv0.5.0 ssd 1541710893.936120272 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 137, "value": 0.024355555555555547}

:::MLPv0.5.0 ssd 1541710894.031478167 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 138, "value": 0.024533333333333324}

:::MLPv0.5.0 ssd 1541710894.128387213 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 139, "value": 0.0247111111111111}

:::MLPv0.5.0 ssd 1541710894.227614403 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 140, "value": 0.024888888888888877}
Iteration:    140, Loss function: 8.227, Average Loss: 1.699, avg. samples / sec: 20967.55
Iteration:    140, Loss function: 9.521, Average Loss: 1.700, avg. samples / sec: 20963.78
Iteration:    140, Loss function: 8.859, Average Loss: 1.700, avg. samples / sec: 20961.02
Iteration:    140, Loss function: 8.476, Average Loss: 1.705, avg. samples / sec: 20967.56
Iteration:    140, Loss function: 8.646, Average Loss: 1.697, avg. samples / sec: 20964.55
Iteration:    140, Loss function: 9.263, Average Loss: 1.701, avg. samples / sec: 20955.71
Iteration:    140, Loss function: 8.704, Average Loss: 1.704, avg. samples / sec: 20951.99
Iteration:    140, Loss function: 8.254, Average Loss: 1.704, avg. samples / sec: 20950.05

:::MLPv0.5.0 ssd 1541710894.324640989 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 141, "value": 0.025066666666666654}

:::MLPv0.5.0 ssd 1541710894.421597719 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 142, "value": 0.02524444444444443}

:::MLPv0.5.0 ssd 1541710894.518465281 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 143, "value": 0.025422222222222207}

:::MLPv0.5.0 ssd 1541710894.616470337 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 144, "value": 0.025599999999999984}

:::MLPv0.5.0 ssd 1541710894.712624788 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 145, "value": 0.02577777777777779}

:::MLPv0.5.0 ssd 1541710894.808209181 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 146, "value": 0.025955555555555565}

:::MLPv0.5.0 ssd 1541710894.915219545 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 147, "value": 0.026133333333333342}

:::MLPv0.5.0 ssd 1541710895.010846376 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 148, "value": 0.02631111111111112}

:::MLPv0.5.0 ssd 1541710895.110218287 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 149, "value": 0.026488888888888895}

:::MLPv0.5.0 ssd 1541710895.205276728 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 150, "value": 0.026666666666666672}

:::MLPv0.5.0 ssd 1541710895.300020695 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 151, "value": 0.02684444444444445}

:::MLPv0.5.0 ssd 1541710895.396539688 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 152, "value": 0.027022222222222225}

:::MLPv0.5.0 ssd 1541710895.496829033 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 153, "value": 0.027200000000000002}

:::MLPv0.5.0 ssd 1541710895.592992544 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 154, "value": 0.02737777777777778}

:::MLPv0.5.0 ssd 1541710895.689041853 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 155, "value": 0.027555555555555555}

:::MLPv0.5.0 ssd 1541710895.784934282 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 156, "value": 0.027733333333333332}

:::MLPv0.5.0 ssd 1541710895.886526346 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 157, "value": 0.02791111111111111}

:::MLPv0.5.0 ssd 1541710895.981751442 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 158, "value": 0.028088888888888885}

:::MLPv0.5.0 ssd 1541710896.078993082 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 159, "value": 0.028266666666666662}

:::MLPv0.5.0 ssd 1541710896.173983335 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 160, "value": 0.02844444444444444}
Iteration:    160, Loss function: 7.934, Average Loss: 1.835, avg. samples / sec: 21061.00
Iteration:    160, Loss function: 8.035, Average Loss: 1.836, avg. samples / sec: 21054.79
Iteration:    160, Loss function: 8.340, Average Loss: 1.836, avg. samples / sec: 21046.58
Iteration:    160, Loss function: 8.463, Average Loss: 1.832, avg. samples / sec: 21048.02
Iteration:    160, Loss function: 8.694, Average Loss: 1.841, avg. samples / sec: 21067.86
Iteration:    160, Loss function: 8.019, Average Loss: 1.835, avg. samples / sec: 21043.37
Iteration:    160, Loss function: 8.888, Average Loss: 1.840, avg. samples / sec: 21046.13
Iteration:    160, Loss function: 8.188, Average Loss: 1.839, avg. samples / sec: 21064.17

:::MLPv0.5.0 ssd 1541710896.270421505 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 161, "value": 0.028622222222222216}

:::MLPv0.5.0 ssd 1541710896.365834951 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 162, "value": 0.028799999999999992}

:::MLPv0.5.0 ssd 1541710896.470189333 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 163, "value": 0.02897777777777777}

:::MLPv0.5.0 ssd 1541710896.566080093 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 164, "value": 0.029155555555555546}

:::MLPv0.5.0 ssd 1541710896.673969984 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 165, "value": 0.029333333333333322}

:::MLPv0.5.0 ssd 1541710896.774516106 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 166, "value": 0.0295111111111111}

:::MLPv0.5.0 ssd 1541710896.868905067 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 167, "value": 0.029688888888888876}

:::MLPv0.5.0 ssd 1541710896.964169741 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 168, "value": 0.029866666666666652}

:::MLPv0.5.0 ssd 1541710897.060484171 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 169, "value": 0.03004444444444443}

:::MLPv0.5.0 ssd 1541710897.160074949 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 170, "value": 0.030222222222222206}

:::MLPv0.5.0 ssd 1541710897.256035566 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 171, "value": 0.03040000000000001}

:::MLPv0.5.0 ssd 1541710897.360615253 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 172, "value": 0.030577777777777787}

:::MLPv0.5.0 ssd 1541710897.455647469 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 173, "value": 0.030755555555555564}

:::MLPv0.5.0 ssd 1541710897.548174381 (train.py:553) train_epoch: 3

:::MLPv0.5.0 ssd 1541710897.553520918 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 174, "value": 0.03093333333333334}

:::MLPv0.5.0 ssd 1541710897.653574705 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 175, "value": 0.031111111111111117}

:::MLPv0.5.0 ssd 1541710897.749169111 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 176, "value": 0.031288888888888894}

:::MLPv0.5.0 ssd 1541710897.844591141 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 177, "value": 0.03146666666666667}

:::MLPv0.5.0 ssd 1541710897.939114332 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 178, "value": 0.03164444444444445}

:::MLPv0.5.0 ssd 1541710898.035066843 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 179, "value": 0.031822222222222224}

:::MLPv0.5.0 ssd 1541710898.130717039 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 180, "value": 0.032}
Iteration:    180, Loss function: 7.741, Average Loss: 1.963, avg. samples / sec: 20935.39
Iteration:    180, Loss function: 8.407, Average Loss: 1.964, avg. samples / sec: 20935.36
Iteration:    180, Loss function: 7.996, Average Loss: 1.961, avg. samples / sec: 20931.24
Iteration:    180, Loss function: 8.144, Average Loss: 1.967, avg. samples / sec: 20934.90
Iteration:    180, Loss function: 8.144, Average Loss: 1.960, avg. samples / sec: 20925.22
Iteration:    180, Loss function: 8.117, Average Loss: 1.967, avg. samples / sec: 20925.64
Iteration:    180, Loss function: 8.152, Average Loss: 1.968, avg. samples / sec: 20916.43
Iteration:    180, Loss function: 8.074, Average Loss: 1.961, avg. samples / sec: 20904.53

:::MLPv0.5.0 ssd 1541710898.229121208 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 181, "value": 0.03217777777777778}

:::MLPv0.5.0 ssd 1541710898.322453499 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 182, "value": 0.032355555555555554}

:::MLPv0.5.0 ssd 1541710898.418939114 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 183, "value": 0.03253333333333333}

:::MLPv0.5.0 ssd 1541710898.518358469 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 184, "value": 0.03271111111111111}

:::MLPv0.5.0 ssd 1541710898.616385937 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 185, "value": 0.032888888888888884}

:::MLPv0.5.0 ssd 1541710898.712244272 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 186, "value": 0.03306666666666666}

:::MLPv0.5.0 ssd 1541710898.807524681 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 187, "value": 0.03324444444444444}

:::MLPv0.5.0 ssd 1541710898.905597925 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 188, "value": 0.033422222222222214}

:::MLPv0.5.0 ssd 1541710899.001233578 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 189, "value": 0.03359999999999999}

:::MLPv0.5.0 ssd 1541710899.095013142 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 190, "value": 0.03377777777777777}

:::MLPv0.5.0 ssd 1541710899.192057133 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 191, "value": 0.033955555555555544}

:::MLPv0.5.0 ssd 1541710899.288126707 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 192, "value": 0.03413333333333332}

:::MLPv0.5.0 ssd 1541710899.387452126 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 193, "value": 0.0343111111111111}

:::MLPv0.5.0 ssd 1541710899.483186007 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 194, "value": 0.034488888888888874}

:::MLPv0.5.0 ssd 1541710899.587144375 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 195, "value": 0.03466666666666665}

:::MLPv0.5.0 ssd 1541710899.685001373 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 196, "value": 0.03484444444444443}

:::MLPv0.5.0 ssd 1541710899.780213356 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 197, "value": 0.03502222222222222}

:::MLPv0.5.0 ssd 1541710899.874759197 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 198, "value": 0.035199999999999995}

:::MLPv0.5.0 ssd 1541710899.971673965 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 199, "value": 0.03537777777777777}

:::MLPv0.5.0 ssd 1541710900.067478418 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 200, "value": 0.03555555555555555}
Iteration:    200, Loss function: 7.618, Average Loss: 2.081, avg. samples / sec: 21149.17
Iteration:    200, Loss function: 6.937, Average Loss: 2.078, avg. samples / sec: 21155.35
Iteration:    200, Loss function: 7.944, Average Loss: 2.085, avg. samples / sec: 21172.26
Iteration:    200, Loss function: 8.138, Average Loss: 2.080, avg. samples / sec: 21149.91
Iteration:    200, Loss function: 7.773, Average Loss: 2.077, avg. samples / sec: 21158.47
Iteration:    200, Loss function: 7.665, Average Loss: 2.084, avg. samples / sec: 21160.80
Iteration:    200, Loss function: 7.548, Average Loss: 2.078, avg. samples / sec: 21168.90
Iteration:    200, Loss function: 7.291, Average Loss: 2.082, avg. samples / sec: 21150.12

:::MLPv0.5.0 ssd 1541710900.165770531 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 201, "value": 0.035733333333333325}

:::MLPv0.5.0 ssd 1541710900.262012482 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 202, "value": 0.0359111111111111}

:::MLPv0.5.0 ssd 1541710900.360638857 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 203, "value": 0.03608888888888889}

:::MLPv0.5.0 ssd 1541710900.454897642 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 204, "value": 0.03626666666666667}

:::MLPv0.5.0 ssd 1541710900.549100399 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 205, "value": 0.036444444444444446}

:::MLPv0.5.0 ssd 1541710900.643669844 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 206, "value": 0.03662222222222222}

:::MLPv0.5.0 ssd 1541710900.738818407 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 207, "value": 0.0368}

:::MLPv0.5.0 ssd 1541710900.832690001 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 208, "value": 0.036977777777777776}

:::MLPv0.5.0 ssd 1541710900.928205252 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 209, "value": 0.03715555555555555}

:::MLPv0.5.0 ssd 1541710901.025022745 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 210, "value": 0.03733333333333333}

:::MLPv0.5.0 ssd 1541710901.121829271 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 211, "value": 0.037511111111111106}

:::MLPv0.5.0 ssd 1541710901.216224909 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 212, "value": 0.03768888888888888}

:::MLPv0.5.0 ssd 1541710901.311888695 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 213, "value": 0.03786666666666666}

:::MLPv0.5.0 ssd 1541710901.407190084 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 214, "value": 0.038044444444444436}

:::MLPv0.5.0 ssd 1541710901.502661467 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 215, "value": 0.03822222222222221}

:::MLPv0.5.0 ssd 1541710901.596488714 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 216, "value": 0.038400000000000004}

:::MLPv0.5.0 ssd 1541710901.692092419 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 217, "value": 0.03857777777777778}

:::MLPv0.5.0 ssd 1541710901.785931349 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 218, "value": 0.03875555555555556}

:::MLPv0.5.0 ssd 1541710901.882300854 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 219, "value": 0.038933333333333334}

:::MLPv0.5.0 ssd 1541710901.976099730 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 220, "value": 0.03911111111111111}
Iteration:    220, Loss function: 8.256, Average Loss: 2.196, avg. samples / sec: 21460.32
Iteration:    220, Loss function: 8.489, Average Loss: 2.200, avg. samples / sec: 21461.34
Iteration:    220, Loss function: 7.946, Average Loss: 2.195, avg. samples / sec: 21460.98
Iteration:    220, Loss function: 7.805, Average Loss: 2.193, avg. samples / sec: 21462.40
Iteration:    220, Loss function: 8.092, Average Loss: 2.196, avg. samples / sec: 21453.95
Iteration:    220, Loss function: 8.082, Average Loss: 2.199, avg. samples / sec: 21459.12
Iteration:    220, Loss function: 8.166, Average Loss: 2.192, avg. samples / sec: 21456.17
Iteration:    220, Loss function: 8.136, Average Loss: 2.198, avg. samples / sec: 21449.82

:::MLPv0.5.0 ssd 1541710902.069687605 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 221, "value": 0.03928888888888889}

:::MLPv0.5.0 ssd 1541710902.166426897 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 222, "value": 0.039466666666666664}

:::MLPv0.5.0 ssd 1541710902.261164188 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 223, "value": 0.03964444444444444}

:::MLPv0.5.0 ssd 1541710902.354296923 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 224, "value": 0.03982222222222222}

:::MLPv0.5.0 ssd 1541710902.457559347 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 225, "value": 0.039999999999999994}

:::MLPv0.5.0 ssd 1541710902.552144289 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 226, "value": 0.04017777777777777}

:::MLPv0.5.0 ssd 1541710902.646196365 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 227, "value": 0.04035555555555555}

:::MLPv0.5.0 ssd 1541710902.740324974 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 228, "value": 0.04053333333333334}

:::MLPv0.5.0 ssd 1541710902.834492683 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 229, "value": 0.040711111111111115}

:::MLPv0.5.0 ssd 1541710902.929271460 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 230, "value": 0.04088888888888889}

:::MLPv0.5.0 ssd 1541710903.025507927 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 231, "value": 0.04106666666666667}

:::MLPv0.5.0 ssd 1541710903.116969585 (train.py:553) train_epoch: 4

:::MLPv0.5.0 ssd 1541710903.122235298 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 232, "value": 0.041244444444444445}

:::MLPv0.5.0 ssd 1541710903.217424870 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 233, "value": 0.04142222222222222}

:::MLPv0.5.0 ssd 1541710903.311714649 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 234, "value": 0.0416}

:::MLPv0.5.0 ssd 1541710903.407316923 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 235, "value": 0.041777777777777775}

:::MLPv0.5.0 ssd 1541710903.502219677 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 236, "value": 0.04195555555555555}

:::MLPv0.5.0 ssd 1541710903.595973492 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 237, "value": 0.04213333333333333}

:::MLPv0.5.0 ssd 1541710903.692643166 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 238, "value": 0.042311111111111105}

:::MLPv0.5.0 ssd 1541710903.789955616 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 239, "value": 0.04248888888888888}

:::MLPv0.5.0 ssd 1541710903.885204554 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 240, "value": 0.04266666666666666}
Iteration:    240, Loss function: 7.852, Average Loss: 2.303, avg. samples / sec: 21462.21
Iteration:    240, Loss function: 7.136, Average Loss: 2.309, avg. samples / sec: 21464.19
Iteration:    240, Loss function: 7.656, Average Loss: 2.302, avg. samples / sec: 21452.35
Iteration:    240, Loss function: 7.174, Average Loss: 2.306, avg. samples / sec: 21457.70
Iteration:    240, Loss function: 7.216, Average Loss: 2.304, avg. samples / sec: 21448.08
Iteration:    240, Loss function: 7.442, Average Loss: 2.305, avg. samples / sec: 21465.58
Iteration:    240, Loss function: 7.526, Average Loss: 2.308, avg. samples / sec: 21436.28
Iteration:    240, Loss function: 7.950, Average Loss: 2.300, avg. samples / sec: 21402.12

:::MLPv0.5.0 ssd 1541710903.982176065 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 241, "value": 0.04284444444444445}

:::MLPv0.5.0 ssd 1541710904.078698397 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 242, "value": 0.043022222222222226}

:::MLPv0.5.0 ssd 1541710904.173811436 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 243, "value": 0.0432}

:::MLPv0.5.0 ssd 1541710904.268426418 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 244, "value": 0.04337777777777778}

:::MLPv0.5.0 ssd 1541710904.364749670 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 245, "value": 0.043555555555555556}

:::MLPv0.5.0 ssd 1541710904.460078001 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 246, "value": 0.04373333333333333}

:::MLPv0.5.0 ssd 1541710904.554121733 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 247, "value": 0.04391111111111111}

:::MLPv0.5.0 ssd 1541710904.647194147 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 248, "value": 0.044088888888888886}

:::MLPv0.5.0 ssd 1541710904.743390799 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 249, "value": 0.04426666666666666}

:::MLPv0.5.0 ssd 1541710904.839109659 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 250, "value": 0.04444444444444444}

:::MLPv0.5.0 ssd 1541710904.933969736 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 251, "value": 0.044622222222222216}

:::MLPv0.5.0 ssd 1541710905.028682709 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 252, "value": 0.04479999999999999}

:::MLPv0.5.0 ssd 1541710905.122390509 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 253, "value": 0.04497777777777777}

:::MLPv0.5.0 ssd 1541710905.216842175 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 254, "value": 0.04515555555555556}

:::MLPv0.5.0 ssd 1541710905.310477257 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 255, "value": 0.04533333333333334}

:::MLPv0.5.0 ssd 1541710905.405945539 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 256, "value": 0.04551111111111111}

:::MLPv0.5.0 ssd 1541710905.499642134 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 257, "value": 0.04568888888888889}

:::MLPv0.5.0 ssd 1541710905.594555378 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 258, "value": 0.04586666666666667}

:::MLPv0.5.0 ssd 1541710905.689443827 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 259, "value": 0.04604444444444444}

:::MLPv0.5.0 ssd 1541710905.786701918 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 260, "value": 0.04622222222222222}
Iteration:    260, Loss function: 7.508, Average Loss: 2.403, avg. samples / sec: 21541.51
Iteration:    260, Loss function: 7.428, Average Loss: 2.405, avg. samples / sec: 21541.44
Iteration:    260, Loss function: 7.177, Average Loss: 2.406, avg. samples / sec: 21536.06
Iteration:    260, Loss function: 7.324, Average Loss: 2.406, avg. samples / sec: 21554.25
Iteration:    260, Loss function: 7.187, Average Loss: 2.401, avg. samples / sec: 21537.54
Iteration:    260, Loss function: 7.458, Average Loss: 2.400, avg. samples / sec: 21597.50
Iteration:    260, Loss function: 7.133, Average Loss: 2.405, avg. samples / sec: 21545.48
Iteration:    260, Loss function: 7.131, Average Loss: 2.404, avg. samples / sec: 21543.03

:::MLPv0.5.0 ssd 1541710905.881375790 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 261, "value": 0.0464}

:::MLPv0.5.0 ssd 1541710905.975713253 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 262, "value": 0.046577777777777774}

:::MLPv0.5.0 ssd 1541710906.069042921 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 263, "value": 0.04675555555555555}

:::MLPv0.5.0 ssd 1541710906.167208433 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 264, "value": 0.04693333333333333}

:::MLPv0.5.0 ssd 1541710906.263270378 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 265, "value": 0.047111111111111104}

:::MLPv0.5.0 ssd 1541710906.358423710 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 266, "value": 0.04728888888888888}

:::MLPv0.5.0 ssd 1541710906.451402903 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 267, "value": 0.04746666666666667}

:::MLPv0.5.0 ssd 1541710906.547245026 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 268, "value": 0.04764444444444445}

:::MLPv0.5.0 ssd 1541710906.645414829 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 269, "value": 0.047822222222222224}

:::MLPv0.5.0 ssd 1541710906.742204189 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 270, "value": 0.048}

:::MLPv0.5.0 ssd 1541710906.837995768 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 271, "value": 0.04817777777777778}

:::MLPv0.5.0 ssd 1541710906.932664633 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 272, "value": 0.048355555555555554}

:::MLPv0.5.0 ssd 1541710907.027762890 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 273, "value": 0.04853333333333333}

:::MLPv0.5.0 ssd 1541710907.125426292 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 274, "value": 0.04871111111111111}

:::MLPv0.5.0 ssd 1541710907.220517635 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 275, "value": 0.048888888888888885}

:::MLPv0.5.0 ssd 1541710907.315129519 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 276, "value": 0.04906666666666666}

:::MLPv0.5.0 ssd 1541710907.411798954 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 277, "value": 0.04924444444444444}

:::MLPv0.5.0 ssd 1541710907.508571625 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 278, "value": 0.049422222222222215}

:::MLPv0.5.0 ssd 1541710907.604489326 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 279, "value": 0.04959999999999999}

:::MLPv0.5.0 ssd 1541710907.701515436 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 280, "value": 0.04977777777777778}
Iteration:    280, Loss function: 7.643, Average Loss: 2.496, avg. samples / sec: 21400.34
Iteration:    280, Loss function: 7.288, Average Loss: 2.501, avg. samples / sec: 21388.38
Iteration:    280, Loss function: 7.519, Average Loss: 2.499, avg. samples / sec: 21395.08
Iteration:    280, Loss function: 7.352, Average Loss: 2.501, avg. samples / sec: 21391.53
Iteration:    280, Loss function: 7.342, Average Loss: 2.501, avg. samples / sec: 21388.03
Iteration:    280, Loss function: 7.149, Average Loss: 2.495, avg. samples / sec: 21373.99
Iteration:    280, Loss function: 7.188, Average Loss: 2.500, avg. samples / sec: 21370.48
Iteration:    280, Loss function: 7.149, Average Loss: 2.502, avg. samples / sec: 21291.75

:::MLPv0.5.0 ssd 1541710907.801144123 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 281, "value": 0.04995555555555556}

:::MLPv0.5.0 ssd 1541710907.899247408 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 282, "value": 0.050133333333333335}

:::MLPv0.5.0 ssd 1541710907.994754076 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 283, "value": 0.05031111111111111}

:::MLPv0.5.0 ssd 1541710908.093683720 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 284, "value": 0.05048888888888889}

:::MLPv0.5.0 ssd 1541710908.189115763 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 285, "value": 0.050666666666666665}

:::MLPv0.5.0 ssd 1541710908.282911301 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 286, "value": 0.05084444444444444}

:::MLPv0.5.0 ssd 1541710908.382963657 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 287, "value": 0.05102222222222222}

:::MLPv0.5.0 ssd 1541710908.477496862 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 288, "value": 0.051199999999999996}

:::MLPv0.5.0 ssd 1541710908.568929911 (train.py:553) train_epoch: 5

:::MLPv0.5.0 ssd 1541710908.574116707 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 289, "value": 0.05137777777777777}

:::MLPv0.5.0 ssd 1541710908.668179750 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 290, "value": 0.05155555555555555}

:::MLPv0.5.0 ssd 1541710908.762777328 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 291, "value": 0.051733333333333326}

:::MLPv0.5.0 ssd 1541710908.857687235 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 292, "value": 0.0519111111111111}

:::MLPv0.5.0 ssd 1541710908.953293562 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 293, "value": 0.05208888888888889}

:::MLPv0.5.0 ssd 1541710909.047216654 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 294, "value": 0.05226666666666667}

:::MLPv0.5.0 ssd 1541710909.140851021 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 295, "value": 0.052444444444444446}

:::MLPv0.5.0 ssd 1541710909.233807802 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 296, "value": 0.05262222222222222}

:::MLPv0.5.0 ssd 1541710909.329854012 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 297, "value": 0.0528}

:::MLPv0.5.0 ssd 1541710909.424960613 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 298, "value": 0.052977777777777776}

:::MLPv0.5.0 ssd 1541710909.519990444 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 299, "value": 0.05315555555555555}

:::MLPv0.5.0 ssd 1541710909.618824244 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 300, "value": 0.05333333333333333}
Iteration:    300, Loss function: 7.160, Average Loss: 2.594, avg. samples / sec: 21473.11
Iteration:    300, Loss function: 6.665, Average Loss: 2.592, avg. samples / sec: 21364.67
Iteration:    300, Loss function: 7.546, Average Loss: 2.594, avg. samples / sec: 21367.57
Iteration:    300, Loss function: 7.331, Average Loss: 2.592, avg. samples / sec: 21392.84
Iteration:    300, Loss function: 7.307, Average Loss: 2.588, avg. samples / sec: 21358.91
Iteration:    300, Loss function: 6.822, Average Loss: 2.594, avg. samples / sec: 21360.77
Iteration:    300, Loss function: 7.024, Average Loss: 2.590, avg. samples / sec: 21382.09
Iteration:    300, Loss function: 6.817, Average Loss: 2.591, avg. samples / sec: 21366.55

:::MLPv0.5.0 ssd 1541710909.715610266 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 301, "value": 0.053511111111111107}

:::MLPv0.5.0 ssd 1541710909.810241938 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 302, "value": 0.05368888888888888}

:::MLPv0.5.0 ssd 1541710909.906774044 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 303, "value": 0.05386666666666666}

:::MLPv0.5.0 ssd 1541710910.002760887 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 304, "value": 0.05404444444444444}

:::MLPv0.5.0 ssd 1541710910.098212004 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 305, "value": 0.05422222222222223}

:::MLPv0.5.0 ssd 1541710910.193935871 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 306, "value": 0.054400000000000004}

:::MLPv0.5.0 ssd 1541710910.291296005 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 307, "value": 0.05457777777777778}

:::MLPv0.5.0 ssd 1541710910.388620138 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 308, "value": 0.05475555555555556}

:::MLPv0.5.0 ssd 1541710910.484324932 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 309, "value": 0.054933333333333334}

:::MLPv0.5.0 ssd 1541710910.580656290 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 310, "value": 0.05511111111111111}

:::MLPv0.5.0 ssd 1541710910.676917553 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 311, "value": 0.05528888888888889}

:::MLPv0.5.0 ssd 1541710910.771412849 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 312, "value": 0.055466666666666664}

:::MLPv0.5.0 ssd 1541710910.867230415 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 313, "value": 0.05564444444444444}

:::MLPv0.5.0 ssd 1541710910.962789297 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 314, "value": 0.05582222222222222}

:::MLPv0.5.0 ssd 1541710911.057554960 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 315, "value": 0.055999999999999994}

:::MLPv0.5.0 ssd 1541710911.151418924 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 316, "value": 0.05617777777777777}

:::MLPv0.5.0 ssd 1541710911.245592833 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 317, "value": 0.05635555555555555}

:::MLPv0.5.0 ssd 1541710911.339161396 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 318, "value": 0.05653333333333334}

:::MLPv0.5.0 ssd 1541710911.434403896 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 319, "value": 0.056711111111111115}

:::MLPv0.5.0 ssd 1541710911.528123617 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 320, "value": 0.05688888888888889}
Iteration:    320, Loss function: 6.747, Average Loss: 2.679, avg. samples / sec: 21454.55
Iteration:    320, Loss function: 6.916, Average Loss: 2.681, avg. samples / sec: 21449.61
Iteration:    320, Loss function: 7.313, Average Loss: 2.675, avg. samples / sec: 21450.64
Iteration:    320, Loss function: 6.812, Average Loss: 2.683, avg. samples / sec: 21451.38
Iteration:    320, Loss function: 6.740, Average Loss: 2.680, avg. samples / sec: 21444.07
Iteration:    320, Loss function: 6.437, Average Loss: 2.677, avg. samples / sec: 21453.36
Iteration:    320, Loss function: 7.056, Average Loss: 2.684, avg. samples / sec: 21440.75
Iteration:    320, Loss function: 6.792, Average Loss: 2.681, avg. samples / sec: 21432.67

:::MLPv0.5.0 ssd 1541710911.623559475 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 321, "value": 0.05706666666666667}

:::MLPv0.5.0 ssd 1541710911.718028069 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 322, "value": 0.057244444444444445}

:::MLPv0.5.0 ssd 1541710911.813271046 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 323, "value": 0.05742222222222222}

:::MLPv0.5.0 ssd 1541710911.909638643 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 324, "value": 0.0576}

:::MLPv0.5.0 ssd 1541710912.006366968 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 325, "value": 0.057777777777777775}

:::MLPv0.5.0 ssd 1541710912.101254940 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 326, "value": 0.05795555555555555}

:::MLPv0.5.0 ssd 1541710912.195685148 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 327, "value": 0.05813333333333333}

:::MLPv0.5.0 ssd 1541710912.291208744 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 328, "value": 0.058311111111111105}

:::MLPv0.5.0 ssd 1541710912.385506153 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 329, "value": 0.05848888888888888}

:::MLPv0.5.0 ssd 1541710912.481809139 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 330, "value": 0.05866666666666666}

:::MLPv0.5.0 ssd 1541710912.581414461 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 331, "value": 0.05884444444444445}

:::MLPv0.5.0 ssd 1541710912.675414801 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 332, "value": 0.059022222222222226}

:::MLPv0.5.0 ssd 1541710912.771597862 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 333, "value": 0.0592}

:::MLPv0.5.0 ssd 1541710912.865818262 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 334, "value": 0.05937777777777778}

:::MLPv0.5.0 ssd 1541710912.959170580 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 335, "value": 0.059555555555555556}

:::MLPv0.5.0 ssd 1541710913.053660393 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 336, "value": 0.05973333333333333}

:::MLPv0.5.0 ssd 1541710913.147975445 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 337, "value": 0.05991111111111111}

:::MLPv0.5.0 ssd 1541710913.241931438 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 338, "value": 0.060088888888888886}

:::MLPv0.5.0 ssd 1541710913.335253000 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 339, "value": 0.06026666666666666}

:::MLPv0.5.0 ssd 1541710913.429788828 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 340, "value": 0.06044444444444444}
Iteration:    340, Loss function: 7.443, Average Loss: 2.766, avg. samples / sec: 21545.64
Iteration:    340, Loss function: 6.682, Average Loss: 2.763, avg. samples / sec: 21549.41
Iteration:    340, Loss function: 7.438, Average Loss: 2.762, avg. samples / sec: 21561.88
Iteration:    340, Loss function: 6.819, Average Loss: 2.765, avg. samples / sec: 21531.86
Iteration:    340, Loss function: 7.479, Average Loss: 2.761, avg. samples / sec: 21537.20
Iteration:    340, Loss function: 7.633, Average Loss: 2.760, avg. samples / sec: 21540.44
Iteration:    340, Loss function: 7.200, Average Loss: 2.763, avg. samples / sec: 21525.17
Iteration:    340, Loss function: 6.698, Average Loss: 2.764, avg. samples / sec: 21528.04

:::MLPv0.5.0 ssd 1541710913.523985863 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 341, "value": 0.060622222222222216}

:::MLPv0.5.0 ssd 1541710913.619314194 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 342, "value": 0.06079999999999999}

:::MLPv0.5.0 ssd 1541710913.715773582 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 343, "value": 0.06097777777777777}

:::MLPv0.5.0 ssd 1541710913.810144424 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 344, "value": 0.06115555555555556}

:::MLPv0.5.0 ssd 1541710913.908488512 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 345, "value": 0.06133333333333334}

:::MLPv0.5.0 ssd 1541710914.002192974 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 346, "value": 0.061511111111111114}

:::MLPv0.5.0 ssd 1541710914.093260527 (train.py:553) train_epoch: 6

:::MLPv0.5.0 ssd 1541710914.098478317 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 347, "value": 0.06168888888888889}

:::MLPv0.5.0 ssd 1541710914.193706036 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 348, "value": 0.06186666666666667}

:::MLPv0.5.0 ssd 1541710914.287966728 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 349, "value": 0.062044444444444444}

:::MLPv0.5.0 ssd 1541710914.381756783 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 350, "value": 0.06222222222222222}

:::MLPv0.5.0 ssd 1541710914.477423191 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 351, "value": 0.0624}

:::MLPv0.5.0 ssd 1541710914.573340178 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 352, "value": 0.06257777777777777}

:::MLPv0.5.0 ssd 1541710914.667735815 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 353, "value": 0.06275555555555555}

:::MLPv0.5.0 ssd 1541710914.761833668 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 354, "value": 0.06293333333333333}

:::MLPv0.5.0 ssd 1541710914.856194973 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 355, "value": 0.0631111111111111}

:::MLPv0.5.0 ssd 1541710914.953871250 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 356, "value": 0.0632888888888889}

:::MLPv0.5.0 ssd 1541710915.046881676 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 357, "value": 0.06346666666666667}

:::MLPv0.5.0 ssd 1541710915.143703461 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 358, "value": 0.06364444444444445}

:::MLPv0.5.0 ssd 1541710915.240773201 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 359, "value": 0.06382222222222222}

:::MLPv0.5.0 ssd 1541710915.334833622 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 360, "value": 0.064}
Iteration:    360, Loss function: 6.535, Average Loss: 2.848, avg. samples / sec: 21508.16
Iteration:    360, Loss function: 6.654, Average Loss: 2.844, avg. samples / sec: 21528.25
Iteration:    360, Loss function: 6.388, Average Loss: 2.847, avg. samples / sec: 21508.35
Iteration:    360, Loss function: 6.666, Average Loss: 2.841, avg. samples / sec: 21500.41
Iteration:    360, Loss function: 6.107, Average Loss: 2.845, avg. samples / sec: 21514.84
Iteration:    360, Loss function: 6.825, Average Loss: 2.845, avg. samples / sec: 21504.17
Iteration:    360, Loss function: 6.457, Average Loss: 2.842, avg. samples / sec: 21505.22
Iteration:    360, Loss function: 6.707, Average Loss: 2.841, avg. samples / sec: 21502.60

:::MLPv0.5.0 ssd 1541710915.428807497 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 361, "value": 0.06417777777777778}

:::MLPv0.5.0 ssd 1541710915.525842667 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 362, "value": 0.06435555555555555}

:::MLPv0.5.0 ssd 1541710915.619336367 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 363, "value": 0.06453333333333333}

:::MLPv0.5.0 ssd 1541710915.715456724 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 364, "value": 0.06471111111111111}

:::MLPv0.5.0 ssd 1541710915.809109926 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 365, "value": 0.06488888888888888}

:::MLPv0.5.0 ssd 1541710915.904376030 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 366, "value": 0.06506666666666666}

:::MLPv0.5.0 ssd 1541710916.002001762 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 367, "value": 0.06524444444444444}

:::MLPv0.5.0 ssd 1541710916.097028494 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 368, "value": 0.06542222222222221}

:::MLPv0.5.0 ssd 1541710916.190557003 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 369, "value": 0.0656}

:::MLPv0.5.0 ssd 1541710916.283457756 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 370, "value": 0.06577777777777778}

:::MLPv0.5.0 ssd 1541710916.377844334 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 371, "value": 0.06595555555555556}

:::MLPv0.5.0 ssd 1541710916.470770597 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 372, "value": 0.06613333333333334}

:::MLPv0.5.0 ssd 1541710916.566143513 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 373, "value": 0.06631111111111111}

:::MLPv0.5.0 ssd 1541710916.660386086 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 374, "value": 0.06648888888888889}

:::MLPv0.5.0 ssd 1541710916.753838778 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 375, "value": 0.06666666666666667}

:::MLPv0.5.0 ssd 1541710916.847602606 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 376, "value": 0.06684444444444444}

:::MLPv0.5.0 ssd 1541710916.945761681 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 377, "value": 0.06702222222222222}

:::MLPv0.5.0 ssd 1541710917.046344995 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 378, "value": 0.0672}

:::MLPv0.5.0 ssd 1541710917.141964197 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 379, "value": 0.06737777777777777}

:::MLPv0.5.0 ssd 1541710917.235761166 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 380, "value": 0.06755555555555555}
Iteration:    380, Loss function: 6.610, Average Loss: 2.916, avg. samples / sec: 21551.60
Iteration:    380, Loss function: 6.273, Average Loss: 2.922, avg. samples / sec: 21546.29
Iteration:    380, Loss function: 6.714, Average Loss: 2.920, avg. samples / sec: 21549.89
Iteration:    380, Loss function: 6.643, Average Loss: 2.921, avg. samples / sec: 21545.24
Iteration:    380, Loss function: 7.071, Average Loss: 2.917, avg. samples / sec: 21551.71
Iteration:    380, Loss function: 7.121, Average Loss: 2.916, avg. samples / sec: 21545.87
Iteration:    380, Loss function: 6.443, Average Loss: 2.921, avg. samples / sec: 21543.41
Iteration:    380, Loss function: 6.484, Average Loss: 2.914, avg. samples / sec: 21542.38

:::MLPv0.5.0 ssd 1541710917.329258919 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 381, "value": 0.06773333333333333}

:::MLPv0.5.0 ssd 1541710917.424684763 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 382, "value": 0.06791111111111112}

:::MLPv0.5.0 ssd 1541710917.519485474 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 383, "value": 0.0680888888888889}

:::MLPv0.5.0 ssd 1541710917.617963314 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 384, "value": 0.06826666666666667}

:::MLPv0.5.0 ssd 1541710917.711784124 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 385, "value": 0.06844444444444445}

:::MLPv0.5.0 ssd 1541710917.805626154 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 386, "value": 0.06862222222222222}

:::MLPv0.5.0 ssd 1541710917.899747372 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 387, "value": 0.0688}

:::MLPv0.5.0 ssd 1541710917.992576599 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 388, "value": 0.06897777777777778}

:::MLPv0.5.0 ssd 1541710918.086925030 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 389, "value": 0.06915555555555555}

:::MLPv0.5.0 ssd 1541710918.182766676 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 390, "value": 0.06933333333333333}

:::MLPv0.5.0 ssd 1541710918.278874397 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 391, "value": 0.0695111111111111}

:::MLPv0.5.0 ssd 1541710918.372756958 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 392, "value": 0.06968888888888888}

:::MLPv0.5.0 ssd 1541710918.467951059 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 393, "value": 0.06986666666666666}

:::MLPv0.5.0 ssd 1541710918.562670231 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 394, "value": 0.07004444444444444}

:::MLPv0.5.0 ssd 1541710918.657280922 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 395, "value": 0.07022222222222223}

:::MLPv0.5.0 ssd 1541710918.751674652 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 396, "value": 0.0704}

:::MLPv0.5.0 ssd 1541710918.850062847 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 397, "value": 0.07057777777777778}

:::MLPv0.5.0 ssd 1541710918.944277048 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 398, "value": 0.07075555555555556}

:::MLPv0.5.0 ssd 1541710919.038394690 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 399, "value": 0.07093333333333333}

:::MLPv0.5.0 ssd 1541710919.135092735 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 400, "value": 0.07111111111111111}
Iteration:    400, Loss function: 6.568, Average Loss: 2.990, avg. samples / sec: 21578.74
Iteration:    400, Loss function: 6.368, Average Loss: 2.994, avg. samples / sec: 21574.19
Iteration:    400, Loss function: 6.816, Average Loss: 2.990, avg. samples / sec: 21564.91
Iteration:    400, Loss function: 6.725, Average Loss: 2.989, avg. samples / sec: 21571.28
Iteration:    400, Loss function: 6.875, Average Loss: 2.997, avg. samples / sec: 21556.40
Iteration:    400, Loss function: 6.007, Average Loss: 2.995, avg. samples / sec: 21567.40
Iteration:    400, Loss function: 6.926, Average Loss: 2.995, avg. samples / sec: 21559.50
Iteration:    400, Loss function: 6.806, Average Loss: 2.991, avg. samples / sec: 21564.89

:::MLPv0.5.0 ssd 1541710919.229798079 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 401, "value": 0.07128888888888889}

:::MLPv0.5.0 ssd 1541710919.323365688 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 402, "value": 0.07146666666666666}

:::MLPv0.5.0 ssd 1541710919.419095993 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 403, "value": 0.07164444444444444}

:::MLPv0.5.0 ssd 1541710919.514508009 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 404, "value": 0.07182222222222222}

:::MLPv0.5.0 ssd 1541710919.604079247 (train.py:553) train_epoch: 7

:::MLPv0.5.0 ssd 1541710919.610368729 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 405, "value": 0.072}

:::MLPv0.5.0 ssd 1541710919.704202175 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 406, "value": 0.07217777777777777}

:::MLPv0.5.0 ssd 1541710919.797999382 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 407, "value": 0.07235555555555555}

:::MLPv0.5.0 ssd 1541710919.893202543 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 408, "value": 0.07253333333333334}

:::MLPv0.5.0 ssd 1541710919.985988855 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 409, "value": 0.07271111111111112}

:::MLPv0.5.0 ssd 1541710920.079499960 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 410, "value": 0.07288888888888889}

:::MLPv0.5.0 ssd 1541710920.173008204 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 411, "value": 0.07306666666666667}

:::MLPv0.5.0 ssd 1541710920.267178535 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 412, "value": 0.07324444444444445}

:::MLPv0.5.0 ssd 1541710920.362854242 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 413, "value": 0.07342222222222222}

:::MLPv0.5.0 ssd 1541710920.457051277 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 414, "value": 0.0736}

:::MLPv0.5.0 ssd 1541710920.551832438 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 415, "value": 0.07377777777777778}

:::MLPv0.5.0 ssd 1541710920.646783590 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 416, "value": 0.07395555555555555}

:::MLPv0.5.0 ssd 1541710920.741061926 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 417, "value": 0.07413333333333333}

:::MLPv0.5.0 ssd 1541710920.838696957 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 418, "value": 0.0743111111111111}

:::MLPv0.5.0 ssd 1541710920.933992863 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 419, "value": 0.07448888888888888}

:::MLPv0.5.0 ssd 1541710921.029324770 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 420, "value": 0.07466666666666666}
Iteration:    420, Loss function: 6.251, Average Loss: 3.057, avg. samples / sec: 21621.08
Iteration:    420, Loss function: 6.287, Average Loss: 3.056, avg. samples / sec: 21629.37
Iteration:    420, Loss function: 5.932, Average Loss: 3.059, avg. samples / sec: 21620.70
Iteration:    420, Loss function: 6.232, Average Loss: 3.061, avg. samples / sec: 21617.40
Iteration:    420, Loss function: 6.662, Average Loss: 3.061, avg. samples / sec: 21631.39
Iteration:    420, Loss function: 6.218, Average Loss: 3.064, avg. samples / sec: 21618.93
Iteration:    420, Loss function: 6.360, Average Loss: 3.063, avg. samples / sec: 21618.22
Iteration:    420, Loss function: 6.168, Average Loss: 3.059, avg. samples / sec: 21619.53

:::MLPv0.5.0 ssd 1541710921.126007080 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 421, "value": 0.07484444444444445}

:::MLPv0.5.0 ssd 1541710921.219370604 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 422, "value": 0.07502222222222223}

:::MLPv0.5.0 ssd 1541710921.313350677 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 423, "value": 0.0752}

:::MLPv0.5.0 ssd 1541710921.407736540 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 424, "value": 0.07537777777777778}

:::MLPv0.5.0 ssd 1541710921.501264095 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 425, "value": 0.07555555555555556}

:::MLPv0.5.0 ssd 1541710921.598879576 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 426, "value": 0.07573333333333333}

:::MLPv0.5.0 ssd 1541710921.692790270 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 427, "value": 0.07591111111111111}

:::MLPv0.5.0 ssd 1541710921.788117409 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 428, "value": 0.07608888888888889}

:::MLPv0.5.0 ssd 1541710921.883059740 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 429, "value": 0.07626666666666666}

:::MLPv0.5.0 ssd 1541710921.977288246 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 430, "value": 0.07644444444444444}

:::MLPv0.5.0 ssd 1541710922.071649313 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 431, "value": 0.07662222222222222}

:::MLPv0.5.0 ssd 1541710922.165297270 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 432, "value": 0.0768}

:::MLPv0.5.0 ssd 1541710922.259670258 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 433, "value": 0.07697777777777778}

:::MLPv0.5.0 ssd 1541710922.352719069 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 434, "value": 0.07715555555555556}

:::MLPv0.5.0 ssd 1541710922.446385384 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 435, "value": 0.07733333333333334}

:::MLPv0.5.0 ssd 1541710922.539880037 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 436, "value": 0.07751111111111111}

:::MLPv0.5.0 ssd 1541710922.635602236 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 437, "value": 0.07768888888888889}

:::MLPv0.5.0 ssd 1541710922.729792118 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 438, "value": 0.07786666666666667}

:::MLPv0.5.0 ssd 1541710922.824952602 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 439, "value": 0.07804444444444444}

:::MLPv0.5.0 ssd 1541710922.921681166 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 440, "value": 0.07822222222222222}
Iteration:    440, Loss function: 6.825, Average Loss: 3.128, avg. samples / sec: 21648.52
Iteration:    440, Loss function: 5.954, Average Loss: 3.129, avg. samples / sec: 21649.06
Iteration:    440, Loss function: 6.297, Average Loss: 3.127, avg. samples / sec: 21642.97
Iteration:    440, Loss function: 6.655, Average Loss: 3.132, avg. samples / sec: 21653.77
Iteration:    440, Loss function: 6.660, Average Loss: 3.124, avg. samples / sec: 21641.17
Iteration:    440, Loss function: 6.990, Average Loss: 3.132, avg. samples / sec: 21641.59
Iteration:    440, Loss function: 6.424, Average Loss: 3.133, avg. samples / sec: 21650.88
Iteration:    440, Loss function: 7.438, Average Loss: 3.130, avg. samples / sec: 21645.04

:::MLPv0.5.0 ssd 1541710923.017517090 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 441, "value": 0.0784}

:::MLPv0.5.0 ssd 1541710923.111288071 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 442, "value": 0.07857777777777777}

:::MLPv0.5.0 ssd 1541710923.206712723 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 443, "value": 0.07875555555555555}

:::MLPv0.5.0 ssd 1541710923.300867319 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 444, "value": 0.07893333333333333}

:::MLPv0.5.0 ssd 1541710923.398880482 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 445, "value": 0.0791111111111111}

:::MLPv0.5.0 ssd 1541710923.496779203 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 446, "value": 0.0792888888888889}

:::MLPv0.5.0 ssd 1541710923.591903687 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 447, "value": 0.07946666666666667}

:::MLPv0.5.0 ssd 1541710923.685286999 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 448, "value": 0.07964444444444445}

:::MLPv0.5.0 ssd 1541710923.779262066 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 449, "value": 0.07982222222222222}

:::MLPv0.5.0 ssd 1541710923.873984337 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 450, "value": 0.08}

:::MLPv0.5.0 ssd 1541710923.968342781 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 451, "value": 0.08017777777777778}

:::MLPv0.5.0 ssd 1541710924.063121557 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 452, "value": 0.08035555555555556}

:::MLPv0.5.0 ssd 1541710924.156714916 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 453, "value": 0.08053333333333333}

:::MLPv0.5.0 ssd 1541710924.251071215 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 454, "value": 0.08071111111111111}

:::MLPv0.5.0 ssd 1541710924.345236063 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 455, "value": 0.08088888888888889}

:::MLPv0.5.0 ssd 1541710924.441159725 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 456, "value": 0.08106666666666666}

:::MLPv0.5.0 ssd 1541710924.535532713 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 457, "value": 0.08124444444444444}

:::MLPv0.5.0 ssd 1541710924.632258892 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 458, "value": 0.08142222222222222}

:::MLPv0.5.0 ssd 1541710924.727477551 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 459, "value": 0.0816}

:::MLPv0.5.0 ssd 1541710924.825386047 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 460, "value": 0.08177777777777778}
Iteration:    460, Loss function: 6.402, Average Loss: 3.193, avg. samples / sec: 21516.39
Iteration:    460, Loss function: 6.857, Average Loss: 3.195, avg. samples / sec: 21523.24
Iteration:    460, Loss function: 6.257, Average Loss: 3.190, avg. samples / sec: 21513.59
Iteration:    460, Loss function: 5.887, Average Loss: 3.192, avg. samples / sec: 21529.15
Iteration:    460, Loss function: 6.447, Average Loss: 3.196, avg. samples / sec: 21521.60
Iteration:    460, Loss function: 5.665, Average Loss: 3.189, avg. samples / sec: 21516.75
Iteration:    460, Loss function: 5.777, Average Loss: 3.190, avg. samples / sec: 21512.80
Iteration:    460, Loss function: 6.668, Average Loss: 3.196, avg. samples / sec: 21514.30

:::MLPv0.5.0 ssd 1541710924.919105053 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 461, "value": 0.08195555555555556}

:::MLPv0.5.0 ssd 1541710925.013797045 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 462, "value": 0.08213333333333334}

:::MLPv0.5.0 ssd 1541710925.104774237 (train.py:553) train_epoch: 8

:::MLPv0.5.0 ssd 1541710925.110169172 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 463, "value": 0.08231111111111111}

:::MLPv0.5.0 ssd 1541710925.205919981 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 464, "value": 0.08248888888888889}

:::MLPv0.5.0 ssd 1541710925.299619913 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 465, "value": 0.08266666666666667}

:::MLPv0.5.0 ssd 1541710925.394058704 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 466, "value": 0.08284444444444444}

:::MLPv0.5.0 ssd 1541710925.488407850 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 467, "value": 0.08302222222222222}

:::MLPv0.5.0 ssd 1541710925.581922770 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 468, "value": 0.0832}

:::MLPv0.5.0 ssd 1541710925.676293373 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 469, "value": 0.08337777777777777}

:::MLPv0.5.0 ssd 1541710925.770481825 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 470, "value": 0.08355555555555555}

:::MLPv0.5.0 ssd 1541710925.867426872 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 471, "value": 0.08373333333333333}

:::MLPv0.5.0 ssd 1541710925.961938858 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 472, "value": 0.08391111111111112}

:::MLPv0.5.0 ssd 1541710926.056596994 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 473, "value": 0.0840888888888889}

:::MLPv0.5.0 ssd 1541710926.152708292 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 474, "value": 0.08426666666666667}

:::MLPv0.5.0 ssd 1541710926.247294664 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 475, "value": 0.08444444444444445}

:::MLPv0.5.0 ssd 1541710926.343056679 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 476, "value": 0.08462222222222222}

:::MLPv0.5.0 ssd 1541710926.437051535 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 477, "value": 0.0848}

:::MLPv0.5.0 ssd 1541710926.534568310 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 478, "value": 0.08497777777777778}

:::MLPv0.5.0 ssd 1541710926.629018307 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 479, "value": 0.08515555555555555}

:::MLPv0.5.0 ssd 1541710926.723104239 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 480, "value": 0.08533333333333333}
Iteration:    480, Loss function: 6.113, Average Loss: 3.251, avg. samples / sec: 21589.12
Iteration:    480, Loss function: 5.645, Average Loss: 3.247, avg. samples / sec: 21594.16
Iteration:    480, Loss function: 5.766, Average Loss: 3.254, avg. samples / sec: 21593.09
Iteration:    480, Loss function: 5.920, Average Loss: 3.249, avg. samples / sec: 21589.39
Iteration:    480, Loss function: 5.823, Average Loss: 3.253, avg. samples / sec: 21583.38
Iteration:    480, Loss function: 6.394, Average Loss: 3.247, avg. samples / sec: 21589.90
Iteration:    480, Loss function: 5.463, Average Loss: 3.248, avg. samples / sec: 21580.88
Iteration:    480, Loss function: 5.721, Average Loss: 3.247, avg. samples / sec: 21572.80

:::MLPv0.5.0 ssd 1541710926.817067623 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 481, "value": 0.08551111111111111}

:::MLPv0.5.0 ssd 1541710926.911135197 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 482, "value": 0.08568888888888888}

:::MLPv0.5.0 ssd 1541710927.006470442 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 483, "value": 0.08586666666666666}

:::MLPv0.5.0 ssd 1541710927.100584269 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 484, "value": 0.08604444444444445}

:::MLPv0.5.0 ssd 1541710927.194737911 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 485, "value": 0.08622222222222223}

:::MLPv0.5.0 ssd 1541710927.288953066 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 486, "value": 0.0864}

:::MLPv0.5.0 ssd 1541710927.383244514 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 487, "value": 0.08657777777777778}

:::MLPv0.5.0 ssd 1541710927.476774454 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 488, "value": 0.08675555555555556}

:::MLPv0.5.0 ssd 1541710927.570514441 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 489, "value": 0.08693333333333333}

:::MLPv0.5.0 ssd 1541710927.668811083 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 490, "value": 0.08711111111111111}

:::MLPv0.5.0 ssd 1541710927.763139248 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 491, "value": 0.08728888888888889}

:::MLPv0.5.0 ssd 1541710927.857661724 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 492, "value": 0.08746666666666666}

:::MLPv0.5.0 ssd 1541710927.951683283 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 493, "value": 0.08764444444444444}

:::MLPv0.5.0 ssd 1541710928.048688889 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 494, "value": 0.08782222222222222}

:::MLPv0.5.0 ssd 1541710928.143615246 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 495, "value": 0.088}

:::MLPv0.5.0 ssd 1541710928.237771273 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 496, "value": 0.08817777777777777}

:::MLPv0.5.0 ssd 1541710928.333594799 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 497, "value": 0.08835555555555556}

:::MLPv0.5.0 ssd 1541710928.428254128 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 498, "value": 0.08853333333333334}

:::MLPv0.5.0 ssd 1541710928.522555113 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 499, "value": 0.08871111111111112}

:::MLPv0.5.0 ssd 1541710928.617993116 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 500, "value": 0.08888888888888889}
Iteration:    500, Loss function: 6.749, Average Loss: 3.309, avg. samples / sec: 21624.05
Iteration:    500, Loss function: 6.432, Average Loss: 3.303, avg. samples / sec: 21616.07
Iteration:    500, Loss function: 6.346, Average Loss: 3.304, avg. samples / sec: 21608.50
Iteration:    500, Loss function: 6.186, Average Loss: 3.301, avg. samples / sec: 21616.21
Iteration:    500, Loss function: 6.207, Average Loss: 3.305, avg. samples / sec: 21611.49
Iteration:    500, Loss function: 6.917, Average Loss: 3.303, avg. samples / sec: 21605.98
Iteration:    500, Loss function: 6.356, Average Loss: 3.304, avg. samples / sec: 21617.73
Iteration:    500, Loss function: 6.111, Average Loss: 3.307, avg. samples / sec: 21577.37

:::MLPv0.5.0 ssd 1541710928.713849306 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 501, "value": 0.08906666666666667}

:::MLPv0.5.0 ssd 1541710928.807797194 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 502, "value": 0.08924444444444445}

:::MLPv0.5.0 ssd 1541710928.903769016 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 503, "value": 0.08942222222222222}

:::MLPv0.5.0 ssd 1541710928.998502493 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 504, "value": 0.0896}

:::MLPv0.5.0 ssd 1541710929.093412638 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 505, "value": 0.08977777777777778}

:::MLPv0.5.0 ssd 1541710929.187861443 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 506, "value": 0.08995555555555555}

:::MLPv0.5.0 ssd 1541710929.282588243 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 507, "value": 0.09013333333333333}

:::MLPv0.5.0 ssd 1541710929.377056837 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 508, "value": 0.0903111111111111}

:::MLPv0.5.0 ssd 1541710929.475423336 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 509, "value": 0.09048888888888888}

:::MLPv0.5.0 ssd 1541710929.569587946 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 510, "value": 0.09066666666666667}

:::MLPv0.5.0 ssd 1541710929.663087606 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 511, "value": 0.09084444444444445}

:::MLPv0.5.0 ssd 1541710929.757516384 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 512, "value": 0.09102222222222223}

:::MLPv0.5.0 ssd 1541710929.853494167 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 513, "value": 0.0912}

:::MLPv0.5.0 ssd 1541710929.948155642 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 514, "value": 0.09137777777777778}

:::MLPv0.5.0 ssd 1541710930.044504166 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 515, "value": 0.09155555555555556}

:::MLPv0.5.0 ssd 1541710930.139121056 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 516, "value": 0.09173333333333333}

:::MLPv0.5.0 ssd 1541710930.233326435 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 517, "value": 0.09191111111111111}

:::MLPv0.5.0 ssd 1541710930.327827930 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 518, "value": 0.09208888888888889}

:::MLPv0.5.0 ssd 1541710930.421757936 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 519, "value": 0.09226666666666666}

:::MLPv0.5.0 ssd 1541710930.512573957 (train.py:553) train_epoch: 9

:::MLPv0.5.0 ssd 1541710930.518014669 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 520, "value": 0.09244444444444444}
Iteration:    520, Loss function: 5.987, Average Loss: 3.370, avg. samples / sec: 21555.86
Iteration:    520, Loss function: 5.865, Average Loss: 3.368, avg. samples / sec: 21560.57
Iteration:    520, Loss function: 5.708, Average Loss: 3.367, avg. samples / sec: 21562.21
Iteration:    520, Loss function: 6.442, Average Loss: 3.367, avg. samples / sec: 21566.57
Iteration:    520, Loss function: 6.172, Average Loss: 3.373, avg. samples / sec: 21596.21
Iteration:    520, Loss function: 6.392, Average Loss: 3.369, avg. samples / sec: 21561.34
Iteration:    520, Loss function: 5.627, Average Loss: 3.367, avg. samples / sec: 21554.24
Iteration:    520, Loss function: 5.837, Average Loss: 3.369, avg. samples / sec: 21561.96

:::MLPv0.5.0 ssd 1541710930.612204313 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 521, "value": 0.09262222222222222}

:::MLPv0.5.0 ssd 1541710930.706883430 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 522, "value": 0.0928}

:::MLPv0.5.0 ssd 1541710930.801613569 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 523, "value": 0.09297777777777778}

:::MLPv0.5.0 ssd 1541710930.895555019 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 524, "value": 0.09315555555555556}

:::MLPv0.5.0 ssd 1541710930.990158558 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 525, "value": 0.09333333333333334}

:::MLPv0.5.0 ssd 1541710931.085649014 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 526, "value": 0.09351111111111111}

:::MLPv0.5.0 ssd 1541710931.181398630 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 527, "value": 0.09368888888888889}

:::MLPv0.5.0 ssd 1541710931.276739597 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 528, "value": 0.09386666666666667}

:::MLPv0.5.0 ssd 1541710931.371958971 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 529, "value": 0.09404444444444444}

:::MLPv0.5.0 ssd 1541710931.466702700 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 530, "value": 0.09422222222222222}

:::MLPv0.5.0 ssd 1541710931.560723543 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 531, "value": 0.0944}

:::MLPv0.5.0 ssd 1541710931.655067682 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 532, "value": 0.09457777777777777}

:::MLPv0.5.0 ssd 1541710931.749236822 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 533, "value": 0.09475555555555555}

:::MLPv0.5.0 ssd 1541710931.843336821 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 534, "value": 0.09493333333333333}

:::MLPv0.5.0 ssd 1541710931.939345598 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 535, "value": 0.0951111111111111}

:::MLPv0.5.0 ssd 1541710932.037550449 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 536, "value": 0.0952888888888889}

:::MLPv0.5.0 ssd 1541710932.133543968 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 537, "value": 0.09546666666666667}

:::MLPv0.5.0 ssd 1541710932.228741169 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 538, "value": 0.09564444444444445}

:::MLPv0.5.0 ssd 1541710932.322827578 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 539, "value": 0.09582222222222223}

:::MLPv0.5.0 ssd 1541710932.416422606 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 540, "value": 0.096}
Iteration:    540, Loss function: 5.463, Average Loss: 3.416, avg. samples / sec: 21581.84
Iteration:    540, Loss function: 5.820, Average Loss: 3.419, avg. samples / sec: 21581.22
Iteration:    540, Loss function: 6.022, Average Loss: 3.419, avg. samples / sec: 21576.01
Iteration:    540, Loss function: 5.766, Average Loss: 3.420, avg. samples / sec: 21579.98
Iteration:    540, Loss function: 5.976, Average Loss: 3.419, avg. samples / sec: 21584.13
Iteration:    540, Loss function: 5.884, Average Loss: 3.423, avg. samples / sec: 21574.48
Iteration:    540, Loss function: 6.180, Average Loss: 3.418, avg. samples / sec: 21569.25
Iteration:    540, Loss function: 6.357, Average Loss: 3.422, avg. samples / sec: 21570.67

:::MLPv0.5.0 ssd 1541710932.510344982 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 541, "value": 0.09617777777777778}

:::MLPv0.5.0 ssd 1541710932.603811502 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 542, "value": 0.09635555555555556}

:::MLPv0.5.0 ssd 1541710932.699056149 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 543, "value": 0.09653333333333333}

:::MLPv0.5.0 ssd 1541710932.796301842 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 544, "value": 0.09671111111111111}

:::MLPv0.5.0 ssd 1541710932.890351295 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 545, "value": 0.09688888888888889}

:::MLPv0.5.0 ssd 1541710932.984520197 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 546, "value": 0.09706666666666666}

:::MLPv0.5.0 ssd 1541710933.079721212 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 547, "value": 0.09724444444444444}

:::MLPv0.5.0 ssd 1541710933.174876213 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 548, "value": 0.09742222222222222}

:::MLPv0.5.0 ssd 1541710933.271874905 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 549, "value": 0.09759999999999999}

:::MLPv0.5.0 ssd 1541710933.366750240 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 550, "value": 0.09777777777777777}

:::MLPv0.5.0 ssd 1541710933.461827755 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 551, "value": 0.09795555555555555}

:::MLPv0.5.0 ssd 1541710933.555458069 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 552, "value": 0.09813333333333334}

:::MLPv0.5.0 ssd 1541710933.649784803 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 553, "value": 0.09831111111111111}

:::MLPv0.5.0 ssd 1541710933.744241238 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 554, "value": 0.09848888888888889}

:::MLPv0.5.0 ssd 1541710933.839360952 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 555, "value": 0.09866666666666667}

:::MLPv0.5.0 ssd 1541710933.933549643 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 556, "value": 0.09884444444444444}

:::MLPv0.5.0 ssd 1541710934.029480219 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 557, "value": 0.09902222222222222}

:::MLPv0.5.0 ssd 1541710934.123498917 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 558, "value": 0.09920000000000001}

:::MLPv0.5.0 ssd 1541710934.218334198 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 559, "value": 0.09937777777777779}

:::MLPv0.5.0 ssd 1541710934.314531565 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 560, "value": 0.09955555555555556}
Iteration:    560, Loss function: 5.582, Average Loss: 3.466, avg. samples / sec: 21580.25
Iteration:    560, Loss function: 5.856, Average Loss: 3.473, avg. samples / sec: 21583.79
Iteration:    560, Loss function: 5.857, Average Loss: 3.466, avg. samples / sec: 21574.24
Iteration:    560, Loss function: 6.048, Average Loss: 3.469, avg. samples / sec: 21594.27
Iteration:    560, Loss function: 5.537, Average Loss: 3.470, avg. samples / sec: 21574.16
Iteration:    560, Loss function: 5.949, Average Loss: 3.468, avg. samples / sec: 21582.33
Iteration:    560, Loss function: 5.956, Average Loss: 3.466, avg. samples / sec: 21570.83
Iteration:    560, Loss function: 5.392, Average Loss: 3.466, avg. samples / sec: 21571.23

:::MLPv0.5.0 ssd 1541710934.408343315 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 561, "value": 0.09973333333333334}

:::MLPv0.5.0 ssd 1541710934.503694057 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 562, "value": 0.09991111111111112}

:::MLPv0.5.0 ssd 1541710934.598191977 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 563, "value": 0.1000888888888889}

:::MLPv0.5.0 ssd 1541710934.691606045 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 564, "value": 0.10026666666666667}

:::MLPv0.5.0 ssd 1541710934.785643578 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 565, "value": 0.10044444444444445}

:::MLPv0.5.0 ssd 1541710934.880422354 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 566, "value": 0.10062222222222222}

:::MLPv0.5.0 ssd 1541710934.975814342 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 567, "value": 0.1008}

:::MLPv0.5.0 ssd 1541710935.072310686 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 568, "value": 0.10097777777777778}

:::MLPv0.5.0 ssd 1541710935.167267084 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 569, "value": 0.10115555555555555}

:::MLPv0.5.0 ssd 1541710935.262811422 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 570, "value": 0.10133333333333333}

:::MLPv0.5.0 ssd 1541710935.355905771 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 571, "value": 0.10151111111111111}

:::MLPv0.5.0 ssd 1541710935.450014114 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 572, "value": 0.10168888888888888}

:::MLPv0.5.0 ssd 1541710935.543399334 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 573, "value": 0.10186666666666666}

:::MLPv0.5.0 ssd 1541710935.636991739 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 574, "value": 0.10204444444444444}

:::MLPv0.5.0 ssd 1541710935.731238604 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 575, "value": 0.10222222222222221}

:::MLPv0.5.0 ssd 1541710935.828302383 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 576, "value": 0.10239999999999999}

:::MLPv0.5.0 ssd 1541710935.922109127 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 577, "value": 0.10257777777777778}

:::MLPv0.5.0 ssd 1541710936.012540102 (train.py:553) train_epoch: 10

:::MLPv0.5.0 ssd 1541710936.017905712 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 578, "value": 0.10275555555555556}

:::MLPv0.5.0 ssd 1541710936.111547947 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 579, "value": 0.10293333333333334}

:::MLPv0.5.0 ssd 1541710936.206269026 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 580, "value": 0.10311111111111111}
Iteration:    580, Loss function: 5.211, Average Loss: 3.515, avg. samples / sec: 21651.85
Iteration:    580, Loss function: 5.334, Average Loss: 3.508, avg. samples / sec: 21655.51
Iteration:    580, Loss function: 5.928, Average Loss: 3.508, avg. samples / sec: 21653.07
Iteration:    580, Loss function: 5.482, Average Loss: 3.508, avg. samples / sec: 21646.02
Iteration:    580, Loss function: 6.249, Average Loss: 3.508, avg. samples / sec: 21645.86
Iteration:    580, Loss function: 5.950, Average Loss: 3.512, avg. samples / sec: 21650.12
Iteration:    580, Loss function: 6.137, Average Loss: 3.508, avg. samples / sec: 21648.40
Iteration:    580, Loss function: 5.393, Average Loss: 3.511, avg. samples / sec: 21594.92

:::MLPv0.5.0 ssd 1541710936.306756258 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 581, "value": 0.10328888888888889}

:::MLPv0.5.0 ssd 1541710936.400707245 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 582, "value": 0.10346666666666667}

:::MLPv0.5.0 ssd 1541710936.495995283 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 583, "value": 0.10364444444444444}

:::MLPv0.5.0 ssd 1541710936.589401245 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 584, "value": 0.10382222222222223}

:::MLPv0.5.0 ssd 1541710936.683906794 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 585, "value": 0.10400000000000001}

:::MLPv0.5.0 ssd 1541710936.777648926 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 586, "value": 0.10417777777777779}

:::MLPv0.5.0 ssd 1541710936.872591019 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 587, "value": 0.10435555555555556}

:::MLPv0.5.0 ssd 1541710936.966880322 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 588, "value": 0.10453333333333334}

:::MLPv0.5.0 ssd 1541710937.065310478 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 589, "value": 0.10471111111111112}

:::MLPv0.5.0 ssd 1541710937.160754919 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 590, "value": 0.10488888888888889}

:::MLPv0.5.0 ssd 1541710937.254792213 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 591, "value": 0.10506666666666667}

:::MLPv0.5.0 ssd 1541710937.348972559 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 592, "value": 0.10524444444444445}

:::MLPv0.5.0 ssd 1541710937.445422888 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 593, "value": 0.10542222222222222}

:::MLPv0.5.0 ssd 1541710937.540086985 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 594, "value": 0.1056}

:::MLPv0.5.0 ssd 1541710937.634074450 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 595, "value": 0.10577777777777778}

:::MLPv0.5.0 ssd 1541710937.728085756 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 596, "value": 0.10595555555555555}

:::MLPv0.5.0 ssd 1541710937.822439432 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 597, "value": 0.10613333333333333}

:::MLPv0.5.0 ssd 1541710937.916275501 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 598, "value": 0.1063111111111111}

:::MLPv0.5.0 ssd 1541710938.010075569 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 599, "value": 0.10648888888888888}

:::MLPv0.5.0 ssd 1541710938.110028982 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 600, "value": 0.10666666666666666}
Iteration:    600, Loss function: 5.947, Average Loss: 3.554, avg. samples / sec: 21527.33
Iteration:    600, Loss function: 5.973, Average Loss: 3.557, avg. samples / sec: 21518.43
Iteration:    600, Loss function: 5.557, Average Loss: 3.555, avg. samples / sec: 21520.29
Iteration:    600, Loss function: 5.679, Average Loss: 3.554, avg. samples / sec: 21521.49
Iteration:    600, Loss function: 6.212, Average Loss: 3.556, avg. samples / sec: 21517.03
Iteration:    600, Loss function: 5.546, Average Loss: 3.563, avg. samples / sec: 21511.35
Iteration:    600, Loss function: 6.033, Average Loss: 3.561, avg. samples / sec: 21511.62
Iteration:    600, Loss function: 5.784, Average Loss: 3.555, avg. samples / sec: 21561.70

:::MLPv0.5.0 ssd 1541710938.205817699 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 601, "value": 0.10684444444444444}

:::MLPv0.5.0 ssd 1541710938.299456835 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 602, "value": 0.10702222222222221}

:::MLPv0.5.0 ssd 1541710938.392583132 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 603, "value": 0.1072}

:::MLPv0.5.0 ssd 1541710938.487796545 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 604, "value": 0.10737777777777778}

:::MLPv0.5.0 ssd 1541710938.582055330 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 605, "value": 0.10755555555555556}

:::MLPv0.5.0 ssd 1541710938.679580927 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 606, "value": 0.10773333333333333}

:::MLPv0.5.0 ssd 1541710938.773261786 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 607, "value": 0.10791111111111111}

:::MLPv0.5.0 ssd 1541710938.868310690 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 608, "value": 0.10808888888888889}

:::MLPv0.5.0 ssd 1541710938.962726116 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 609, "value": 0.10826666666666668}

:::MLPv0.5.0 ssd 1541710939.056091309 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 610, "value": 0.10844444444444445}

:::MLPv0.5.0 ssd 1541710939.151742935 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 611, "value": 0.10862222222222223}

:::MLPv0.5.0 ssd 1541710939.246983051 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 612, "value": 0.10880000000000001}

:::MLPv0.5.0 ssd 1541710939.342806339 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 613, "value": 0.10897777777777778}

:::MLPv0.5.0 ssd 1541710939.436721802 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 614, "value": 0.10915555555555556}

:::MLPv0.5.0 ssd 1541710939.530576229 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 615, "value": 0.10933333333333334}

:::MLPv0.5.0 ssd 1541710939.624920130 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 616, "value": 0.10951111111111111}

:::MLPv0.5.0 ssd 1541710939.719222069 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 617, "value": 0.10968888888888889}

:::MLPv0.5.0 ssd 1541710939.812939882 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 618, "value": 0.10986666666666667}

:::MLPv0.5.0 ssd 1541710939.907397985 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 619, "value": 0.11004444444444444}

:::MLPv0.5.0 ssd 1541710940.001146078 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 620, "value": 0.11022222222222222}
Iteration:    620, Loss function: 5.232, Average Loss: 3.596, avg. samples / sec: 21673.58
Iteration:    620, Loss function: 5.051, Average Loss: 3.596, avg. samples / sec: 21669.50
Iteration:    620, Loss function: 4.856, Average Loss: 3.595, avg. samples / sec: 21668.20
Iteration:    620, Loss function: 5.397, Average Loss: 3.595, avg. samples / sec: 21668.55
Iteration:    620, Loss function: 4.637, Average Loss: 3.594, avg. samples / sec: 21653.68
Iteration:    620, Loss function: 4.691, Average Loss: 3.600, avg. samples / sec: 21673.44
Iteration:    620, Loss function: 5.579, Average Loss: 3.604, avg. samples / sec: 21661.83
Iteration:    620, Loss function: 5.426, Average Loss: 3.594, avg. samples / sec: 21658.83

:::MLPv0.5.0 ssd 1541710940.095203876 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 621, "value": 0.1104}

:::MLPv0.5.0 ssd 1541710940.188947439 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 622, "value": 0.11057777777777777}

:::MLPv0.5.0 ssd 1541710940.282405376 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 623, "value": 0.11075555555555555}

:::MLPv0.5.0 ssd 1541710940.375262022 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 624, "value": 0.11093333333333333}

:::MLPv0.5.0 ssd 1541710940.469498873 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 625, "value": 0.1111111111111111}

:::MLPv0.5.0 ssd 1541710940.562486887 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 626, "value": 0.11128888888888888}

:::MLPv0.5.0 ssd 1541710940.655712843 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 627, "value": 0.11146666666666666}

:::MLPv0.5.0 ssd 1541710940.749378443 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 628, "value": 0.11164444444444445}

:::MLPv0.5.0 ssd 1541710940.843200445 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 629, "value": 0.11182222222222223}

:::MLPv0.5.0 ssd 1541710940.937132359 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 630, "value": 0.112}

:::MLPv0.5.0 ssd 1541710941.032094955 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 631, "value": 0.11217777777777778}

:::MLPv0.5.0 ssd 1541710941.126669884 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 632, "value": 0.11235555555555556}

:::MLPv0.5.0 ssd 1541710941.221204281 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 633, "value": 0.11253333333333333}

:::MLPv0.5.0 ssd 1541710941.315589666 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 634, "value": 0.11271111111111111}

:::MLPv0.5.0 ssd 1541710941.409294367 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 635, "value": 0.1128888888888889}

:::MLPv0.5.0 ssd 1541710941.498811722 (train.py:553) train_epoch: 11

:::MLPv0.5.0 ssd 1541710941.504101276 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 636, "value": 0.11306666666666668}

:::MLPv0.5.0 ssd 1541710941.598922491 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 637, "value": 0.11324444444444445}

:::MLPv0.5.0 ssd 1541710941.695925713 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 638, "value": 0.11342222222222223}

:::MLPv0.5.0 ssd 1541710941.789773226 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 639, "value": 0.1136}

:::MLPv0.5.0 ssd 1541710941.884287596 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 640, "value": 0.11377777777777778}
Iteration:    640, Loss function: 5.511, Average Loss: 3.635, avg. samples / sec: 21750.08
Iteration:    640, Loss function: 4.477, Average Loss: 3.631, avg. samples / sec: 21753.07
Iteration:    640, Loss function: 5.712, Average Loss: 3.632, avg. samples / sec: 21749.67
Iteration:    640, Loss function: 5.279, Average Loss: 3.635, avg. samples / sec: 21748.00
Iteration:    640, Loss function: 5.674, Average Loss: 3.636, avg. samples / sec: 21750.51
Iteration:    640, Loss function: 5.186, Average Loss: 3.634, avg. samples / sec: 21739.07
Iteration:    640, Loss function: 6.265, Average Loss: 3.641, avg. samples / sec: 21750.20
Iteration:    640, Loss function: 5.247, Average Loss: 3.631, avg. samples / sec: 21759.29

:::MLPv0.5.0 ssd 1541710941.979141951 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 641, "value": 0.11395555555555556}

:::MLPv0.5.0 ssd 1541710942.073487759 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 642, "value": 0.11413333333333334}

:::MLPv0.5.0 ssd 1541710942.168422937 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 643, "value": 0.11431111111111111}

:::MLPv0.5.0 ssd 1541710942.262413263 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 644, "value": 0.11448888888888889}

:::MLPv0.5.0 ssd 1541710942.356106520 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 645, "value": 0.11466666666666667}

:::MLPv0.5.0 ssd 1541710942.449824333 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 646, "value": 0.11484444444444444}

:::MLPv0.5.0 ssd 1541710942.545324564 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 647, "value": 0.11502222222222222}

:::MLPv0.5.0 ssd 1541710942.639061689 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 648, "value": 0.1152}

:::MLPv0.5.0 ssd 1541710942.733255386 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 649, "value": 0.11537777777777777}

:::MLPv0.5.0 ssd 1541710942.827575445 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 650, "value": 0.11555555555555555}

:::MLPv0.5.0 ssd 1541710942.920945406 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 651, "value": 0.11573333333333333}

:::MLPv0.5.0 ssd 1541710943.018719912 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 652, "value": 0.1159111111111111}

:::MLPv0.5.0 ssd 1541710943.111865520 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 653, "value": 0.11608888888888888}

:::MLPv0.5.0 ssd 1541710943.206836939 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 654, "value": 0.11626666666666667}

:::MLPv0.5.0 ssd 1541710943.300967216 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 655, "value": 0.11644444444444445}

:::MLPv0.5.0 ssd 1541710943.394019604 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 656, "value": 0.11662222222222222}

:::MLPv0.5.0 ssd 1541710943.487818480 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 657, "value": 0.1168}

:::MLPv0.5.0 ssd 1541710943.581342220 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 658, "value": 0.11697777777777778}

:::MLPv0.5.0 ssd 1541710943.675828457 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 659, "value": 0.11715555555555555}

:::MLPv0.5.0 ssd 1541710943.769011259 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 660, "value": 0.11733333333333333}
Iteration:    660, Loss function: 5.814, Average Loss: 3.672, avg. samples / sec: 21733.59
Iteration:    660, Loss function: 5.893, Average Loss: 3.668, avg. samples / sec: 21734.81
Iteration:    660, Loss function: 5.657, Average Loss: 3.670, avg. samples / sec: 21731.92
Iteration:    660, Loss function: 5.060, Average Loss: 3.676, avg. samples / sec: 21741.84
Iteration:    660, Loss function: 5.610, Average Loss: 3.674, avg. samples / sec: 21738.47
Iteration:    660, Loss function: 5.338, Average Loss: 3.672, avg. samples / sec: 21730.33
Iteration:    660, Loss function: 5.434, Average Loss: 3.673, avg. samples / sec: 21722.31
Iteration:    660, Loss function: 5.405, Average Loss: 3.669, avg. samples / sec: 21729.48

:::MLPv0.5.0 ssd 1541710943.864375591 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 661, "value": 0.11751111111111112}

:::MLPv0.5.0 ssd 1541710943.958173752 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 662, "value": 0.1176888888888889}

:::MLPv0.5.0 ssd 1541710944.052153587 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 663, "value": 0.11786666666666668}

:::MLPv0.5.0 ssd 1541710944.149342060 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 664, "value": 0.11804444444444445}

:::MLPv0.5.0 ssd 1541710944.244970798 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 665, "value": 0.11822222222222223}

:::MLPv0.5.0 ssd 1541710944.339729548 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 666, "value": 0.1184}

:::MLPv0.5.0 ssd 1541710944.433095217 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 667, "value": 0.11857777777777778}

:::MLPv0.5.0 ssd 1541710944.527184010 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 668, "value": 0.11875555555555556}

:::MLPv0.5.0 ssd 1541710944.621948004 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 669, "value": 0.11893333333333334}

:::MLPv0.5.0 ssd 1541710944.715523481 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 670, "value": 0.11911111111111111}

:::MLPv0.5.0 ssd 1541710944.809031248 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 671, "value": 0.11928888888888889}

:::MLPv0.5.0 ssd 1541710944.902258396 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 672, "value": 0.11946666666666667}

:::MLPv0.5.0 ssd 1541710944.995590925 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 673, "value": 0.11964444444444444}

:::MLPv0.5.0 ssd 1541710945.092740536 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 674, "value": 0.11982222222222222}

:::MLPv0.5.0 ssd 1541710945.187487125 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 675, "value": 0.12}

:::MLPv0.5.0 ssd 1541710945.282015800 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 676, "value": 0.12017777777777777}

:::MLPv0.5.0 ssd 1541710945.377831459 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 677, "value": 0.12035555555555555}

:::MLPv0.5.0 ssd 1541710945.472109318 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 678, "value": 0.12053333333333333}

:::MLPv0.5.0 ssd 1541710945.565931082 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 679, "value": 0.1207111111111111}

:::MLPv0.5.0 ssd 1541710945.660442352 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 680, "value": 0.12088888888888889}
Iteration:    680, Loss function: 5.136, Average Loss: 3.703, avg. samples / sec: 21658.38
Iteration:    680, Loss function: 5.766, Average Loss: 3.710, avg. samples / sec: 21660.47
Iteration:    680, Loss function: 4.902, Average Loss: 3.704, avg. samples / sec: 21655.02
Iteration:    680, Loss function: 5.199, Average Loss: 3.703, avg. samples / sec: 21657.06
Iteration:    680, Loss function: 4.955, Average Loss: 3.708, avg. samples / sec: 21655.65
Iteration:    680, Loss function: 5.029, Average Loss: 3.706, avg. samples / sec: 21664.06
Iteration:    680, Loss function: 5.911, Average Loss: 3.706, avg. samples / sec: 21647.55
Iteration:    680, Loss function: 5.035, Average Loss: 3.703, avg. samples / sec: 21656.87

:::MLPv0.5.0 ssd 1541710945.754064798 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 681, "value": 0.12106666666666667}

:::MLPv0.5.0 ssd 1541710945.850380898 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 682, "value": 0.12124444444444445}

:::MLPv0.5.0 ssd 1541710945.944938660 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 683, "value": 0.12142222222222222}

:::MLPv0.5.0 ssd 1541710946.039260864 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 684, "value": 0.1216}

:::MLPv0.5.0 ssd 1541710946.136456013 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 685, "value": 0.12177777777777778}

:::MLPv0.5.0 ssd 1541710946.234319448 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 686, "value": 0.12195555555555557}

:::MLPv0.5.0 ssd 1541710946.329135895 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 687, "value": 0.12213333333333334}

:::MLPv0.5.0 ssd 1541710946.424640179 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 688, "value": 0.12231111111111112}

:::MLPv0.5.0 ssd 1541710946.520729542 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 689, "value": 0.1224888888888889}

:::MLPv0.5.0 ssd 1541710946.617167950 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 690, "value": 0.12266666666666667}

:::MLPv0.5.0 ssd 1541710946.711878300 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 691, "value": 0.12284444444444445}

:::MLPv0.5.0 ssd 1541710946.805777311 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 692, "value": 0.12302222222222223}

:::MLPv0.5.0 ssd 1541710946.901844501 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 693, "value": 0.1232}

:::MLPv0.5.0 ssd 1541710946.994864702 (train.py:553) train_epoch: 12

:::MLPv0.5.0 ssd 1541710947.000308275 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 694, "value": 0.12337777777777778}

:::MLPv0.5.0 ssd 1541710947.098777294 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 695, "value": 0.12355555555555556}

:::MLPv0.5.0 ssd 1541710947.194726706 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 696, "value": 0.12373333333333333}

:::MLPv0.5.0 ssd 1541710947.287946939 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 697, "value": 0.12391111111111111}

:::MLPv0.5.0 ssd 1541710947.382237911 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 698, "value": 0.12408888888888889}

:::MLPv0.5.0 ssd 1541710947.476417303 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 699, "value": 0.12426666666666666}

:::MLPv0.5.0 ssd 1541710947.570629597 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 700, "value": 0.12444444444444444}
Iteration:    700, Loss function: 5.471, Average Loss: 3.736, avg. samples / sec: 21451.25
Iteration:    700, Loss function: 4.951, Average Loss: 3.733, avg. samples / sec: 21446.27
Iteration:    700, Loss function: 5.226, Average Loss: 3.740, avg. samples / sec: 21444.47
Iteration:    700, Loss function: 5.023, Average Loss: 3.740, avg. samples / sec: 21451.50
Iteration:    700, Loss function: 5.560, Average Loss: 3.738, avg. samples / sec: 21453.27
Iteration:    700, Loss function: 4.762, Average Loss: 3.735, avg. samples / sec: 21439.16
Iteration:    700, Loss function: 5.357, Average Loss: 3.738, avg. samples / sec: 21444.44
Iteration:    700, Loss function: 4.834, Average Loss: 3.734, avg. samples / sec: 21443.65

:::MLPv0.5.0 ssd 1541710947.665431738 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 701, "value": 0.12462222222222222}

:::MLPv0.5.0 ssd 1541710947.759899139 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 702, "value": 0.1248}

:::MLPv0.5.0 ssd 1541710947.853992939 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 703, "value": 0.12497777777777777}

:::MLPv0.5.0 ssd 1541710947.948254108 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 704, "value": 0.12515555555555555}

:::MLPv0.5.0 ssd 1541710948.046270370 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 705, "value": 0.12533333333333335}

:::MLPv0.5.0 ssd 1541710948.140546322 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 706, "value": 0.12551111111111113}

:::MLPv0.5.0 ssd 1541710948.234062433 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 707, "value": 0.1256888888888889}

:::MLPv0.5.0 ssd 1541710948.327946901 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 708, "value": 0.12586666666666668}

:::MLPv0.5.0 ssd 1541710948.423924685 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 709, "value": 0.12604444444444446}

:::MLPv0.5.0 ssd 1541710948.518894434 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 710, "value": 0.12622222222222224}

:::MLPv0.5.0 ssd 1541710948.612012386 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 711, "value": 0.1264}

:::MLPv0.5.0 ssd 1541710948.706786633 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 712, "value": 0.1265777777777778}

:::MLPv0.5.0 ssd 1541710948.801100731 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 713, "value": 0.12675555555555557}

:::MLPv0.5.0 ssd 1541710948.895329475 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 714, "value": 0.12693333333333334}

:::MLPv0.5.0 ssd 1541710948.989230394 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 715, "value": 0.12711111111111112}

:::MLPv0.5.0 ssd 1541710949.082862139 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 716, "value": 0.1272888888888889}

:::MLPv0.5.0 ssd 1541710949.180940628 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 717, "value": 0.12746666666666667}

:::MLPv0.5.0 ssd 1541710949.276314735 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 718, "value": 0.12764444444444445}

:::MLPv0.5.0 ssd 1541710949.369926453 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 719, "value": 0.12782222222222223}

:::MLPv0.5.0 ssd 1541710949.465946674 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 720, "value": 0.128}
Iteration:    720, Loss function: 5.006, Average Loss: 3.770, avg. samples / sec: 21614.54
Iteration:    720, Loss function: 5.407, Average Loss: 3.763, avg. samples / sec: 21610.90
Iteration:    720, Loss function: 5.610, Average Loss: 3.767, avg. samples / sec: 21606.31
Iteration:    720, Loss function: 5.350, Average Loss: 3.771, avg. samples / sec: 21610.83
Iteration:    720, Loss function: 5.130, Average Loss: 3.768, avg. samples / sec: 21615.63
Iteration:    720, Loss function: 6.211, Average Loss: 3.766, avg. samples / sec: 21615.35
Iteration:    720, Loss function: 5.013, Average Loss: 3.767, avg. samples / sec: 21604.79
Iteration:    720, Loss function: 5.541, Average Loss: 3.765, avg. samples / sec: 21602.98

:::MLPv0.5.0 ssd 1541710949.559497118 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 721, "value": 0.12817777777777778}

:::MLPv0.5.0 ssd 1541710949.654951572 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 722, "value": 0.12835555555555556}

:::MLPv0.5.0 ssd 1541710949.748533964 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 723, "value": 0.12853333333333333}

:::MLPv0.5.0 ssd 1541710949.842327118 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 724, "value": 0.1287111111111111}

:::MLPv0.5.0 ssd 1541710949.936260462 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 725, "value": 0.1288888888888889}

:::MLPv0.5.0 ssd 1541710950.030050278 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 726, "value": 0.12906666666666666}

:::MLPv0.5.0 ssd 1541710950.123411417 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 727, "value": 0.12924444444444444}

:::MLPv0.5.0 ssd 1541710950.219533205 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 728, "value": 0.12942222222222222}

:::MLPv0.5.0 ssd 1541710950.313601971 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 729, "value": 0.1296}

:::MLPv0.5.0 ssd 1541710950.407931566 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 730, "value": 0.12977777777777777}

:::MLPv0.5.0 ssd 1541710950.502759457 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 731, "value": 0.12995555555555555}

:::MLPv0.5.0 ssd 1541710950.596273661 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 732, "value": 0.13013333333333332}

:::MLPv0.5.0 ssd 1541710950.689501524 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 733, "value": 0.1303111111111111}

:::MLPv0.5.0 ssd 1541710950.783129692 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 734, "value": 0.13048888888888888}

:::MLPv0.5.0 ssd 1541710950.876269817 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 735, "value": 0.13066666666666665}

:::MLPv0.5.0 ssd 1541710950.970852137 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 736, "value": 0.13084444444444446}

:::MLPv0.5.0 ssd 1541710951.065070868 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 737, "value": 0.13102222222222223}

:::MLPv0.5.0 ssd 1541710951.158583403 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 738, "value": 0.1312}

:::MLPv0.5.0 ssd 1541710951.251767874 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 739, "value": 0.1313777777777778}

:::MLPv0.5.0 ssd 1541710951.346211910 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 740, "value": 0.13155555555555556}
Iteration:    740, Loss function: 5.297, Average Loss: 3.801, avg. samples / sec: 21781.76
Iteration:    740, Loss function: 5.906, Average Loss: 3.794, avg. samples / sec: 21781.13
Iteration:    740, Loss function: 5.069, Average Loss: 3.799, avg. samples / sec: 21787.19
Iteration:    740, Loss function: 5.164, Average Loss: 3.798, avg. samples / sec: 21781.72
Iteration:    740, Loss function: 5.625, Average Loss: 3.800, avg. samples / sec: 21783.91
Iteration:    740, Loss function: 5.181, Average Loss: 3.796, avg. samples / sec: 21804.22
Iteration:    740, Loss function: 5.505, Average Loss: 3.804, avg. samples / sec: 21779.32
Iteration:    740, Loss function: 5.116, Average Loss: 3.800, avg. samples / sec: 21790.93

:::MLPv0.5.0 ssd 1541710951.439851522 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 741, "value": 0.13173333333333334}

:::MLPv0.5.0 ssd 1541710951.533118010 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 742, "value": 0.13191111111111112}

:::MLPv0.5.0 ssd 1541710951.626777887 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 743, "value": 0.1320888888888889}

:::MLPv0.5.0 ssd 1541710951.721898317 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 744, "value": 0.13226666666666667}

:::MLPv0.5.0 ssd 1541710951.818337202 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 745, "value": 0.13244444444444445}

:::MLPv0.5.0 ssd 1541710951.912612915 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 746, "value": 0.13262222222222222}

:::MLPv0.5.0 ssd 1541710952.006680250 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 747, "value": 0.1328}

:::MLPv0.5.0 ssd 1541710952.101617336 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 748, "value": 0.13297777777777778}

:::MLPv0.5.0 ssd 1541710952.195213795 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 749, "value": 0.13315555555555555}

:::MLPv0.5.0 ssd 1541710952.289109230 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 750, "value": 0.13333333333333333}

:::MLPv0.5.0 ssd 1541710952.379063129 (train.py:553) train_epoch: 13

:::MLPv0.5.0 ssd 1541710952.384994030 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 751, "value": 0.1335111111111111}

:::MLPv0.5.0 ssd 1541710952.479235649 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 752, "value": 0.13368888888888888}

:::MLPv0.5.0 ssd 1541710952.576073170 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 753, "value": 0.13386666666666666}

:::MLPv0.5.0 ssd 1541710952.669652700 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 754, "value": 0.13404444444444444}

:::MLPv0.5.0 ssd 1541710952.763676405 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 755, "value": 0.13422222222222221}

:::MLPv0.5.0 ssd 1541710952.857318401 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 756, "value": 0.1344}

:::MLPv0.5.0 ssd 1541710952.951648235 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 757, "value": 0.13457777777777777}

:::MLPv0.5.0 ssd 1541710953.046623468 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 758, "value": 0.13475555555555557}

:::MLPv0.5.0 ssd 1541710953.140868902 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 759, "value": 0.13493333333333335}

:::MLPv0.5.0 ssd 1541710953.235673904 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 760, "value": 0.13511111111111113}
Iteration:    760, Loss function: 4.927, Average Loss: 3.828, avg. samples / sec: 21681.44
Iteration:    760, Loss function: 5.201, Average Loss: 3.828, avg. samples / sec: 21679.44
Iteration:    760, Loss function: 5.386, Average Loss: 3.828, avg. samples / sec: 21676.35
Iteration:    760, Loss function: 5.423, Average Loss: 3.832, avg. samples / sec: 21678.62
Iteration:    760, Loss function: 5.714, Average Loss: 3.824, avg. samples / sec: 21671.06
Iteration:    760, Loss function: 5.470, Average Loss: 3.827, avg. samples / sec: 21672.17
Iteration:    760, Loss function: 5.894, Average Loss: 3.830, avg. samples / sec: 21671.30
Iteration:    760, Loss function: 4.953, Average Loss: 3.825, avg. samples / sec: 21667.32

:::MLPv0.5.0 ssd 1541710953.330291033 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 761, "value": 0.1352888888888889}

:::MLPv0.5.0 ssd 1541710953.424162865 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 762, "value": 0.13546666666666668}

:::MLPv0.5.0 ssd 1541710953.517900229 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 763, "value": 0.13564444444444446}

:::MLPv0.5.0 ssd 1541710953.612345457 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 764, "value": 0.13582222222222223}

:::MLPv0.5.0 ssd 1541710953.708145380 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 765, "value": 0.136}

:::MLPv0.5.0 ssd 1541710953.801871300 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 766, "value": 0.1361777777777778}

:::MLPv0.5.0 ssd 1541710953.895278454 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 767, "value": 0.13635555555555556}

:::MLPv0.5.0 ssd 1541710953.992081881 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 768, "value": 0.13653333333333334}

:::MLPv0.5.0 ssd 1541710954.086364269 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 769, "value": 0.13671111111111112}

:::MLPv0.5.0 ssd 1541710954.183733463 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 770, "value": 0.1368888888888889}

:::MLPv0.5.0 ssd 1541710954.279651403 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 771, "value": 0.13706666666666667}

:::MLPv0.5.0 ssd 1541710954.372974634 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 772, "value": 0.13724444444444445}

:::MLPv0.5.0 ssd 1541710954.467560530 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 773, "value": 0.13742222222222222}

:::MLPv0.5.0 ssd 1541710954.561751366 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 774, "value": 0.1376}

:::MLPv0.5.0 ssd 1541710954.656372547 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 775, "value": 0.13777777777777778}

:::MLPv0.5.0 ssd 1541710954.751772404 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 776, "value": 0.13795555555555555}

:::MLPv0.5.0 ssd 1541710954.849702835 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 777, "value": 0.13813333333333333}

:::MLPv0.5.0 ssd 1541710954.944365978 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 778, "value": 0.1383111111111111}

:::MLPv0.5.0 ssd 1541710955.038487434 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 779, "value": 0.13848888888888888}

:::MLPv0.5.0 ssd 1541710955.134215593 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 780, "value": 0.13866666666666666}
Iteration:    780, Loss function: 5.192, Average Loss: 3.854, avg. samples / sec: 21575.11
Iteration:    780, Loss function: 5.407, Average Loss: 3.854, avg. samples / sec: 21573.80
Iteration:    780, Loss function: 5.356, Average Loss: 3.856, avg. samples / sec: 21574.11
Iteration:    780, Loss function: 5.768, Average Loss: 3.856, avg. samples / sec: 21582.66
Iteration:    780, Loss function: 5.350, Average Loss: 3.858, avg. samples / sec: 21583.30
Iteration:    780, Loss function: 5.951, Average Loss: 3.858, avg. samples / sec: 21569.58
Iteration:    780, Loss function: 5.278, Average Loss: 3.853, avg. samples / sec: 21578.07
Iteration:    780, Loss function: 5.423, Average Loss: 3.850, avg. samples / sec: 21565.09

:::MLPv0.5.0 ssd 1541710955.231446505 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 781, "value": 0.13884444444444444}

:::MLPv0.5.0 ssd 1541710955.325085402 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 782, "value": 0.1390222222222222}

:::MLPv0.5.0 ssd 1541710955.418699503 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 783, "value": 0.1392}

:::MLPv0.5.0 ssd 1541710955.512798786 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 784, "value": 0.13937777777777777}

:::MLPv0.5.0 ssd 1541710955.609655857 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 785, "value": 0.13955555555555554}

:::MLPv0.5.0 ssd 1541710955.703680515 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 786, "value": 0.13973333333333332}

:::MLPv0.5.0 ssd 1541710955.799173832 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 787, "value": 0.13991111111111112}

:::MLPv0.5.0 ssd 1541710955.892665148 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 788, "value": 0.1400888888888889}

:::MLPv0.5.0 ssd 1541710955.987380505 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 789, "value": 0.14026666666666668}

:::MLPv0.5.0 ssd 1541710956.081035852 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 790, "value": 0.14044444444444446}

:::MLPv0.5.0 ssd 1541710956.176496744 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 791, "value": 0.14062222222222223}

:::MLPv0.5.0 ssd 1541710956.271549225 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 792, "value": 0.1408}

:::MLPv0.5.0 ssd 1541710956.366150379 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 793, "value": 0.14097777777777779}

:::MLPv0.5.0 ssd 1541710956.461776495 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 794, "value": 0.14115555555555556}

:::MLPv0.5.0 ssd 1541710956.558444262 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 795, "value": 0.14133333333333334}

:::MLPv0.5.0 ssd 1541710956.653647900 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 796, "value": 0.14151111111111112}

:::MLPv0.5.0 ssd 1541710956.749115229 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 797, "value": 0.1416888888888889}

:::MLPv0.5.0 ssd 1541710956.844774485 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 798, "value": 0.14186666666666667}

:::MLPv0.5.0 ssd 1541710956.939131498 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 799, "value": 0.14204444444444445}

:::MLPv0.5.0 ssd 1541710957.033660412 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 800, "value": 0.14222222222222222}
Iteration:    800, Loss function: 5.338, Average Loss: 3.875, avg. samples / sec: 21582.72
Iteration:    800, Loss function: 4.747, Average Loss: 3.878, avg. samples / sec: 21565.32
Iteration:    800, Loss function: 5.280, Average Loss: 3.878, avg. samples / sec: 21559.86
Iteration:    800, Loss function: 5.484, Average Loss: 3.883, avg. samples / sec: 21563.31
Iteration:    800, Loss function: 5.930, Average Loss: 3.881, avg. samples / sec: 21572.95
Iteration:    800, Loss function: 5.158, Average Loss: 3.883, avg. samples / sec: 21563.33
Iteration:    800, Loss function: 5.166, Average Loss: 3.880, avg. samples / sec: 21560.50
Iteration:    800, Loss function: 5.245, Average Loss: 3.883, avg. samples / sec: 21568.85

:::MLPv0.5.0 ssd 1541710957.132022619 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 801, "value": 0.1424}

:::MLPv0.5.0 ssd 1541710957.226724863 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 802, "value": 0.14257777777777778}

:::MLPv0.5.0 ssd 1541710957.321220398 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 803, "value": 0.14275555555555555}

:::MLPv0.5.0 ssd 1541710957.415334225 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 804, "value": 0.14293333333333333}

:::MLPv0.5.0 ssd 1541710957.509998560 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 805, "value": 0.1431111111111111}

:::MLPv0.5.0 ssd 1541710957.603457451 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 806, "value": 0.14328888888888888}

:::MLPv0.5.0 ssd 1541710957.697448492 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 807, "value": 0.14346666666666666}

:::MLPv0.5.0 ssd 1541710957.791406870 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 808, "value": 0.14364444444444444}

:::MLPv0.5.0 ssd 1541710957.882543564 (train.py:553) train_epoch: 14

:::MLPv0.5.0 ssd 1541710957.888269663 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 809, "value": 0.14382222222222224}

:::MLPv0.5.0 ssd 1541710957.994779825 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 810, "value": 0.14400000000000002}

:::MLPv0.5.0 ssd 1541710958.089072943 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 811, "value": 0.1441777777777778}

:::MLPv0.5.0 ssd 1541710958.184219122 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 812, "value": 0.14435555555555557}

:::MLPv0.5.0 ssd 1541710958.278519154 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 813, "value": 0.14453333333333335}

:::MLPv0.5.0 ssd 1541710958.372946501 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 814, "value": 0.14471111111111112}

:::MLPv0.5.0 ssd 1541710958.469768763 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 815, "value": 0.1448888888888889}

:::MLPv0.5.0 ssd 1541710958.563423395 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 816, "value": 0.14506666666666668}

:::MLPv0.5.0 ssd 1541710958.658680439 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 817, "value": 0.14524444444444445}

:::MLPv0.5.0 ssd 1541710958.752790928 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 818, "value": 0.14542222222222223}

:::MLPv0.5.0 ssd 1541710958.846904755 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 819, "value": 0.1456}

:::MLPv0.5.0 ssd 1541710958.940687418 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 820, "value": 0.14577777777777778}
Iteration:    820, Loss function: 5.210, Average Loss: 3.910, avg. samples / sec: 21488.54
Iteration:    820, Loss function: 5.182, Average Loss: 3.915, avg. samples / sec: 21476.52
Iteration:    820, Loss function: 4.774, Average Loss: 3.913, avg. samples / sec: 21478.84
Iteration:    820, Loss function: 5.335, Average Loss: 3.906, avg. samples / sec: 21472.86
Iteration:    820, Loss function: 5.914, Average Loss: 3.910, avg. samples / sec: 21478.84
Iteration:    820, Loss function: 4.824, Average Loss: 3.910, avg. samples / sec: 21475.97
Iteration:    820, Loss function: 5.515, Average Loss: 3.903, avg. samples / sec: 21468.93
Iteration:    820, Loss function: 5.378, Average Loss: 3.906, avg. samples / sec: 21473.21

:::MLPv0.5.0 ssd 1541710959.036740065 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 821, "value": 0.14595555555555556}

:::MLPv0.5.0 ssd 1541710959.130402088 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 822, "value": 0.14613333333333334}

:::MLPv0.5.0 ssd 1541710959.224210739 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 823, "value": 0.14631111111111111}

:::MLPv0.5.0 ssd 1541710959.317884684 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 824, "value": 0.1464888888888889}

:::MLPv0.5.0 ssd 1541710959.413028717 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 825, "value": 0.14666666666666667}

:::MLPv0.5.0 ssd 1541710959.508501530 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 826, "value": 0.14684444444444444}

:::MLPv0.5.0 ssd 1541710959.601759672 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 827, "value": 0.14702222222222222}

:::MLPv0.5.0 ssd 1541710959.695698500 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 828, "value": 0.1472}

:::MLPv0.5.0 ssd 1541710959.790082932 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 829, "value": 0.14737777777777777}

:::MLPv0.5.0 ssd 1541710959.883410692 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 830, "value": 0.14755555555555555}

:::MLPv0.5.0 ssd 1541710959.978236914 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 831, "value": 0.14773333333333333}

:::MLPv0.5.0 ssd 1541710960.072814226 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 832, "value": 0.1479111111111111}

:::MLPv0.5.0 ssd 1541710960.165912151 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 833, "value": 0.14808888888888888}

:::MLPv0.5.0 ssd 1541710960.259732962 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 834, "value": 0.14826666666666666}

:::MLPv0.5.0 ssd 1541710960.356158257 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 835, "value": 0.14844444444444443}

:::MLPv0.5.0 ssd 1541710960.451116323 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 836, "value": 0.1486222222222222}

:::MLPv0.5.0 ssd 1541710960.546501875 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 837, "value": 0.14880000000000002}

:::MLPv0.5.0 ssd 1541710960.640561581 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 838, "value": 0.1489777777777778}

:::MLPv0.5.0 ssd 1541710960.734442711 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 839, "value": 0.14915555555555557}

:::MLPv0.5.0 ssd 1541710960.828967571 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 840, "value": 0.14933333333333335}
Iteration:    840, Loss function: 4.597, Average Loss: 3.927, avg. samples / sec: 21703.12
Iteration:    840, Loss function: 4.492, Average Loss: 3.937, avg. samples / sec: 21697.43
Iteration:    840, Loss function: 4.941, Average Loss: 3.934, avg. samples / sec: 21698.58
Iteration:    840, Loss function: 5.593, Average Loss: 3.933, avg. samples / sec: 21684.33
Iteration:    840, Loss function: 4.992, Average Loss: 3.927, avg. samples / sec: 21694.44
Iteration:    840, Loss function: 5.461, Average Loss: 3.934, avg. samples / sec: 21694.58
Iteration:    840, Loss function: 4.678, Average Loss: 3.935, avg. samples / sec: 21692.01
Iteration:    840, Loss function: 4.866, Average Loss: 3.926, avg. samples / sec: 21693.50

:::MLPv0.5.0 ssd 1541710960.922911167 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 841, "value": 0.14951111111111112}

:::MLPv0.5.0 ssd 1541710961.016469955 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 842, "value": 0.1496888888888889}

:::MLPv0.5.0 ssd 1541710961.110146761 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 843, "value": 0.14986666666666668}

:::MLPv0.5.0 ssd 1541710961.204584599 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 844, "value": 0.15004444444444445}

:::MLPv0.5.0 ssd 1541710961.298581362 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 845, "value": 0.15022222222222223}

:::MLPv0.5.0 ssd 1541710961.392973423 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 846, "value": 0.1504}

:::MLPv0.5.0 ssd 1541710961.487415552 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 847, "value": 0.15057777777777778}

:::MLPv0.5.0 ssd 1541710961.581633568 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 848, "value": 0.15075555555555556}

:::MLPv0.5.0 ssd 1541710961.678309202 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 849, "value": 0.15093333333333334}

:::MLPv0.5.0 ssd 1541710961.773949862 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 850, "value": 0.1511111111111111}

:::MLPv0.5.0 ssd 1541710961.871042013 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 851, "value": 0.1512888888888889}

:::MLPv0.5.0 ssd 1541710961.964123487 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 852, "value": 0.15146666666666667}

:::MLPv0.5.0 ssd 1541710962.057266235 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 853, "value": 0.15164444444444444}

:::MLPv0.5.0 ssd 1541710962.151638746 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 854, "value": 0.15182222222222222}

:::MLPv0.5.0 ssd 1541710962.246148586 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 855, "value": 0.152}

:::MLPv0.5.0 ssd 1541710962.340922356 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 856, "value": 0.15217777777777777}

:::MLPv0.5.0 ssd 1541710962.435253620 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 857, "value": 0.15235555555555555}

:::MLPv0.5.0 ssd 1541710962.530404091 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 858, "value": 0.15253333333333333}

:::MLPv0.5.0 ssd 1541710962.625831604 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 859, "value": 0.1527111111111111}

:::MLPv0.5.0 ssd 1541710962.719516277 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 860, "value": 0.15288888888888888}
Iteration:    860, Loss function: 5.481, Average Loss: 3.952, avg. samples / sec: 21677.80
Iteration:    860, Loss function: 5.093, Average Loss: 3.959, avg. samples / sec: 21673.10
Iteration:    860, Loss function: 5.610, Average Loss: 3.961, avg. samples / sec: 21664.23
Iteration:    860, Loss function: 5.118, Average Loss: 3.959, avg. samples / sec: 21666.40
Iteration:    860, Loss function: 5.048, Average Loss: 3.958, avg. samples / sec: 21662.74
Iteration:    860, Loss function: 5.849, Average Loss: 3.957, avg. samples / sec: 21657.39
Iteration:    860, Loss function: 5.001, Average Loss: 3.950, avg. samples / sec: 21648.07
Iteration:    860, Loss function: 5.100, Average Loss: 3.961, avg. samples / sec: 21645.08

:::MLPv0.5.0 ssd 1541710962.816219330 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 861, "value": 0.15306666666666666}

:::MLPv0.5.0 ssd 1541710962.910052538 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 862, "value": 0.15324444444444446}

:::MLPv0.5.0 ssd 1541710963.005245209 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 863, "value": 0.15342222222222224}

:::MLPv0.5.0 ssd 1541710963.098424673 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 864, "value": 0.15360000000000001}

:::MLPv0.5.0 ssd 1541710963.194267750 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 865, "value": 0.1537777777777778}

:::MLPv0.5.0 ssd 1541710963.288397312 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 866, "value": 0.15395555555555557}

:::MLPv0.5.0 ssd 1541710963.379019260 (train.py:553) train_epoch: 15

:::MLPv0.5.0 ssd 1541710963.384009123 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 867, "value": 0.15413333333333334}

:::MLPv0.5.0 ssd 1541710963.478208303 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 868, "value": 0.15431111111111112}

:::MLPv0.5.0 ssd 1541710963.571776867 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 869, "value": 0.1544888888888889}

:::MLPv0.5.0 ssd 1541710963.666598558 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 870, "value": 0.15466666666666667}

:::MLPv0.5.0 ssd 1541710963.774904251 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 871, "value": 0.15484444444444445}

:::MLPv0.5.0 ssd 1541710963.869140387 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 872, "value": 0.15502222222222223}

:::MLPv0.5.0 ssd 1541710963.963842630 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 873, "value": 0.1552}

:::MLPv0.5.0 ssd 1541710964.057838917 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 874, "value": 0.15537777777777778}

:::MLPv0.5.0 ssd 1541710964.152169704 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 875, "value": 0.15555555555555556}

:::MLPv0.5.0 ssd 1541710964.246852160 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 876, "value": 0.15573333333333333}

:::MLPv0.5.0 ssd 1541710964.340691805 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 877, "value": 0.1559111111111111}

:::MLPv0.5.0 ssd 1541710964.435422182 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 878, "value": 0.1560888888888889}

:::MLPv0.5.0 ssd 1541710964.528622866 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 879, "value": 0.15626666666666666}

:::MLPv0.5.0 ssd 1541710964.622338295 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 880, "value": 0.15644444444444444}
Iteration:    880, Loss function: 5.578, Average Loss: 3.979, avg. samples / sec: 21546.49
Iteration:    880, Loss function: 5.058, Average Loss: 3.978, avg. samples / sec: 21525.80
Iteration:    880, Loss function: 4.756, Average Loss: 3.986, avg. samples / sec: 21547.47
Iteration:    880, Loss function: 5.447, Average Loss: 3.986, avg. samples / sec: 21523.96
Iteration:    880, Loss function: 5.760, Average Loss: 3.985, avg. samples / sec: 21537.28
Iteration:    880, Loss function: 5.798, Average Loss: 3.987, avg. samples / sec: 21526.71
Iteration:    880, Loss function: 5.096, Average Loss: 3.984, avg. samples / sec: 21529.28
Iteration:    880, Loss function: 5.545, Average Loss: 3.988, avg. samples / sec: 21521.34

:::MLPv0.5.0 ssd 1541710964.717851877 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 881, "value": 0.15662222222222222}

:::MLPv0.5.0 ssd 1541710964.811161280 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 882, "value": 0.1568}

:::MLPv0.5.0 ssd 1541710964.904619932 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 883, "value": 0.15697777777777777}

:::MLPv0.5.0 ssd 1541710964.998374462 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 884, "value": 0.15715555555555555}

:::MLPv0.5.0 ssd 1541710965.091557980 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 885, "value": 0.15733333333333333}

:::MLPv0.5.0 ssd 1541710965.186199665 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 886, "value": 0.1575111111111111}

:::MLPv0.5.0 ssd 1541710965.281474352 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 887, "value": 0.15768888888888888}

:::MLPv0.5.0 ssd 1541710965.381989956 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 888, "value": 0.15786666666666668}

:::MLPv0.5.0 ssd 1541710965.475877285 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 889, "value": 0.15804444444444446}

:::MLPv0.5.0 ssd 1541710965.569987059 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 890, "value": 0.15822222222222224}

:::MLPv0.5.0 ssd 1541710965.664458990 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 891, "value": 0.1584}

:::MLPv0.5.0 ssd 1541710965.758145809 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 892, "value": 0.1585777777777778}

:::MLPv0.5.0 ssd 1541710965.851506233 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 893, "value": 0.15875555555555557}

:::MLPv0.5.0 ssd 1541710965.945871115 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 894, "value": 0.15893333333333334}

:::MLPv0.5.0 ssd 1541710966.039489985 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 895, "value": 0.15911111111111112}

:::MLPv0.5.0 ssd 1541710966.133644104 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 896, "value": 0.1592888888888889}

:::MLPv0.5.0 ssd 1541710966.228225708 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 897, "value": 0.15946666666666667}

:::MLPv0.5.0 ssd 1541710966.324286222 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 898, "value": 0.15964444444444445}

:::MLPv0.5.0 ssd 1541710966.418455601 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 899, "value": 0.15982222222222223}
Iteration:    900, Loss function: 4.824, Average Loss: 4.003, avg. samples / sec: 21686.95
Iteration:    900, Loss function: 5.504, Average Loss: 4.008, avg. samples / sec: 21691.99
Iteration:    900, Loss function: 4.958, Average Loss: 4.007, avg. samples / sec: 21685.32
Iteration:    900, Loss function: 5.254, Average Loss: 4.011, avg. samples / sec: 21690.65
Iteration:    900, Loss function: 5.191, Average Loss: 3.998, avg. samples / sec: 21677.00
Iteration:    900, Loss function: 5.429, Average Loss: 4.006, avg. samples / sec: 21682.64
Iteration:    900, Loss function: 4.905, Average Loss: 4.012, avg. samples / sec: 21684.29
Iteration:    900, Loss function: 5.098, Average Loss: 4.007, avg. samples / sec: 21678.10
Iteration:    920, Loss function: 5.027, Average Loss: 4.024, avg. samples / sec: 21851.35
Iteration:    920, Loss function: 4.986, Average Loss: 4.016, avg. samples / sec: 21858.68
Iteration:    920, Loss function: 4.660, Average Loss: 4.030, avg. samples / sec: 21851.85
Iteration:    920, Loss function: 5.313, Average Loss: 4.023, avg. samples / sec: 21851.46
Iteration:    920, Loss function: 5.403, Average Loss: 4.025, avg. samples / sec: 21852.87
Iteration:    920, Loss function: 4.570, Average Loss: 4.025, avg. samples / sec: 21845.69
Iteration:    920, Loss function: 5.320, Average Loss: 4.028, avg. samples / sec: 21849.36
Iteration:    920, Loss function: 5.306, Average Loss: 4.024, avg. samples / sec: 21842.28

:::MLPv0.5.0 ssd 1541710968.852104187 (train.py:553) train_epoch: 16
Iteration:    940, Loss function: 5.109, Average Loss: 4.042, avg. samples / sec: 21844.96
Iteration:    940, Loss function: 4.956, Average Loss: 4.049, avg. samples / sec: 21845.38
Iteration:    940, Loss function: 4.978, Average Loss: 4.033, avg. samples / sec: 21838.26
Iteration:    940, Loss function: 4.620, Average Loss: 4.041, avg. samples / sec: 21845.38
Iteration:    940, Loss function: 5.338, Average Loss: 4.045, avg. samples / sec: 21846.12
Iteration:    940, Loss function: 5.187, Average Loss: 4.043, avg. samples / sec: 21844.87
Iteration:    940, Loss function: 4.925, Average Loss: 4.040, avg. samples / sec: 21838.65
Iteration:    940, Loss function: 5.562, Average Loss: 4.041, avg. samples / sec: 21829.31
Iteration:    960, Loss function: 5.522, Average Loss: 4.054, avg. samples / sec: 21789.23
Iteration:    960, Loss function: 5.207, Average Loss: 4.062, avg. samples / sec: 21793.01
Iteration:    960, Loss function: 5.219, Average Loss: 4.067, avg. samples / sec: 21785.34
Iteration:    960, Loss function: 5.495, Average Loss: 4.061, avg. samples / sec: 21795.79
Iteration:    960, Loss function: 4.910, Average Loss: 4.060, avg. samples / sec: 21801.74
Iteration:    960, Loss function: 5.604, Average Loss: 4.059, avg. samples / sec: 21784.97
Iteration:    960, Loss function: 5.293, Average Loss: 4.062, avg. samples / sec: 21783.67
Iteration:    960, Loss function: 4.778, Average Loss: 4.061, avg. samples / sec: 21771.99
Iteration:    980, Loss function: 5.023, Average Loss: 4.082, avg. samples / sec: 21893.46
Iteration:    980, Loss function: 4.762, Average Loss: 4.078, avg. samples / sec: 21888.83
Iteration:    980, Loss function: 4.672, Average Loss: 4.075, avg. samples / sec: 21892.38
Iteration:    980, Loss function: 4.884, Average Loss: 4.075, avg. samples / sec: 21893.70
Iteration:    980, Loss function: 4.470, Average Loss: 4.070, avg. samples / sec: 21886.77
Iteration:    980, Loss function: 4.749, Average Loss: 4.076, avg. samples / sec: 21895.36
Iteration:    980, Loss function: 5.188, Average Loss: 4.077, avg. samples / sec: 21888.99
Iteration:    980, Loss function: 4.657, Average Loss: 4.076, avg. samples / sec: 21892.10

:::MLPv0.5.0 ssd 1541710974.195249557 (train.py:553) train_epoch: 17
Iteration:   1000, Loss function: 4.338, Average Loss: 4.081, avg. samples / sec: 21901.29
Iteration:   1000, Loss function: 4.572, Average Loss: 4.089, avg. samples / sec: 21901.68
Iteration:   1000, Loss function: 4.692, Average Loss: 4.089, avg. samples / sec: 21899.67
Iteration:   1000, Loss function: 4.253, Average Loss: 4.090, avg. samples / sec: 21902.60
Iteration:   1000, Loss function: 4.895, Average Loss: 4.088, avg. samples / sec: 21897.10
Iteration:   1000, Loss function: 4.860, Average Loss: 4.091, avg. samples / sec: 21897.37
Iteration:   1000, Loss function: 5.295, Average Loss: 4.097, avg. samples / sec: 21887.44
Iteration:   1000, Loss function: 4.651, Average Loss: 4.092, avg. samples / sec: 21891.42
Iteration:   1020, Loss function: 4.855, Average Loss: 4.100, avg. samples / sec: 21865.69
Iteration:   1020, Loss function: 4.955, Average Loss: 4.101, avg. samples / sec: 21866.66
Iteration:   1020, Loss function: 4.919, Average Loss: 4.108, avg. samples / sec: 21869.44
Iteration:   1020, Loss function: 4.560, Average Loss: 4.101, avg. samples / sec: 21860.13
Iteration:   1020, Loss function: 5.077, Average Loss: 4.101, avg. samples / sec: 21859.20
Iteration:   1020, Loss function: 5.263, Average Loss: 4.092, avg. samples / sec: 21858.70
Iteration:   1020, Loss function: 4.425, Average Loss: 4.104, avg. samples / sec: 21864.87
Iteration:   1020, Loss function: 4.947, Average Loss: 4.103, avg. samples / sec: 21860.61

:::MLPv0.5.0 ssd 1541710979.622101545 (train.py:553) train_epoch: 18
Iteration:   1040, Loss function: 4.772, Average Loss: 4.116, avg. samples / sec: 21909.24
Iteration:   1040, Loss function: 4.789, Average Loss: 4.117, avg. samples / sec: 21905.37
Iteration:   1040, Loss function: 5.186, Average Loss: 4.118, avg. samples / sec: 21909.62
Iteration:   1040, Loss function: 5.014, Average Loss: 4.121, avg. samples / sec: 21909.02
Iteration:   1040, Loss function: 4.717, Average Loss: 4.120, avg. samples / sec: 21907.98
Iteration:   1040, Loss function: 4.748, Average Loss: 4.116, avg. samples / sec: 21905.22
Iteration:   1040, Loss function: 5.051, Average Loss: 4.108, avg. samples / sec: 21903.78
Iteration:   1040, Loss function: 4.776, Average Loss: 4.120, avg. samples / sec: 21899.26
Iteration:   1060, Loss function: 4.723, Average Loss: 4.128, avg. samples / sec: 21891.15
Iteration:   1060, Loss function: 4.572, Average Loss: 4.129, avg. samples / sec: 21891.96
Iteration:   1060, Loss function: 4.608, Average Loss: 4.128, avg. samples / sec: 21896.56
Iteration:   1060, Loss function: 4.363, Average Loss: 4.119, avg. samples / sec: 21899.97
Iteration:   1060, Loss function: 4.418, Average Loss: 4.132, avg. samples / sec: 21901.66
Iteration:   1060, Loss function: 4.718, Average Loss: 4.134, avg. samples / sec: 21894.97
Iteration:   1060, Loss function: 5.103, Average Loss: 4.132, avg. samples / sec: 21891.41
Iteration:   1060, Loss function: 5.335, Average Loss: 4.134, avg. samples / sec: 21890.30
Iteration:   1080, Loss function: 4.520, Average Loss: 4.140, avg. samples / sec: 21926.46
Iteration:   1080, Loss function: 4.765, Average Loss: 4.135, avg. samples / sec: 21918.48
Iteration:   1080, Loss function: 4.776, Average Loss: 4.139, avg. samples / sec: 21919.23
Iteration:   1080, Loss function: 4.610, Average Loss: 4.143, avg. samples / sec: 21923.49
Iteration:   1080, Loss function: 4.389, Average Loss: 4.131, avg. samples / sec: 21916.44
Iteration:   1080, Loss function: 4.852, Average Loss: 4.137, avg. samples / sec: 21914.22
Iteration:   1080, Loss function: 4.850, Average Loss: 4.143, avg. samples / sec: 21914.70
Iteration:   1080, Loss function: 4.594, Average Loss: 4.144, avg. samples / sec: 21909.10

:::MLPv0.5.0 ssd 1541710985.052653551 (train.py:553) train_epoch: 19
Iteration:   1100, Loss function: 5.104, Average Loss: 4.154, avg. samples / sec: 21807.33
Iteration:   1100, Loss function: 4.583, Average Loss: 4.150, avg. samples / sec: 21786.42
Iteration:   1100, Loss function: 4.770, Average Loss: 4.149, avg. samples / sec: 21793.16
Iteration:   1100, Loss function: 4.681, Average Loss: 4.152, avg. samples / sec: 21793.71
Iteration:   1100, Loss function: 4.752, Average Loss: 4.153, avg. samples / sec: 21795.54
Iteration:   1100, Loss function: 4.044, Average Loss: 4.143, avg. samples / sec: 21787.19
Iteration:   1100, Loss function: 4.902, Average Loss: 4.144, avg. samples / sec: 21790.99
Iteration:   1100, Loss function: 4.859, Average Loss: 4.148, avg. samples / sec: 21790.40
Iteration:   1120, Loss function: 4.817, Average Loss: 4.158, avg. samples / sec: 21883.88
Iteration:   1120, Loss function: 4.241, Average Loss: 4.165, avg. samples / sec: 21885.79
Iteration:   1120, Loss function: 4.408, Average Loss: 4.151, avg. samples / sec: 21885.30
Iteration:   1120, Loss function: 4.246, Average Loss: 4.157, avg. samples / sec: 21881.19
Iteration:   1120, Loss function: 5.225, Average Loss: 4.156, avg. samples / sec: 21887.20
Iteration:   1120, Loss function: 4.430, Average Loss: 4.161, avg. samples / sec: 21874.99
Iteration:   1120, Loss function: 4.676, Average Loss: 4.160, avg. samples / sec: 21876.73
Iteration:   1120, Loss function: 4.940, Average Loss: 4.154, avg. samples / sec: 21879.78
Iteration:   1140, Loss function: 4.676, Average Loss: 4.174, avg. samples / sec: 21844.93
Iteration:   1140, Loss function: 4.520, Average Loss: 4.169, avg. samples / sec: 21846.49
Iteration:   1140, Loss function: 4.144, Average Loss: 4.164, avg. samples / sec: 21846.22
Iteration:   1140, Loss function: 4.890, Average Loss: 4.167, avg. samples / sec: 21840.95
Iteration:   1140, Loss function: 4.361, Average Loss: 4.160, avg. samples / sec: 21837.07
Iteration:   1140, Loss function: 4.042, Average Loss: 4.163, avg. samples / sec: 21841.14
Iteration:   1140, Loss function: 4.461, Average Loss: 4.170, avg. samples / sec: 21838.55
Iteration:   1140, Loss function: 4.785, Average Loss: 4.167, avg. samples / sec: 21825.80

:::MLPv0.5.0 ssd 1541710990.493000031 (train.py:553) train_epoch: 20
Iteration:   1160, Loss function: 5.162, Average Loss: 4.176, avg. samples / sec: 21837.93
Iteration:   1160, Loss function: 5.520, Average Loss: 4.171, avg. samples / sec: 21826.18
Iteration:   1160, Loss function: 4.186, Average Loss: 4.176, avg. samples / sec: 21822.75
Iteration:   1160, Loss function: 5.078, Average Loss: 4.180, avg. samples / sec: 21829.31
Iteration:   1160, Loss function: 4.354, Average Loss: 4.171, avg. samples / sec: 21816.14
Iteration:   1160, Loss function: 4.438, Average Loss: 4.170, avg. samples / sec: 21823.24
Iteration:   1160, Loss function: 4.919, Average Loss: 4.176, avg. samples / sec: 21815.59
Iteration:   1160, Loss function: 5.373, Average Loss: 4.182, avg. samples / sec: 21810.62
Iteration:   1180, Loss function: 5.469, Average Loss: 4.199, avg. samples / sec: 21804.19
Iteration:   1180, Loss function: 5.011, Average Loss: 4.199, avg. samples / sec: 21796.47
Iteration:   1180, Loss function: 5.172, Average Loss: 4.200, avg. samples / sec: 21802.13
Iteration:   1180, Loss function: 4.949, Average Loss: 4.190, avg. samples / sec: 21801.72
Iteration:   1180, Loss function: 5.494, Average Loss: 4.194, avg. samples / sec: 21794.34
Iteration:   1180, Loss function: 4.881, Average Loss: 4.198, avg. samples / sec: 21795.11
Iteration:   1180, Loss function: 4.859, Average Loss: 4.192, avg. samples / sec: 21797.16
Iteration:   1180, Loss function: 5.282, Average Loss: 4.203, avg. samples / sec: 21800.68
Iteration:   1200, Loss function: 4.412, Average Loss: 4.209, avg. samples / sec: 21769.51
Iteration:   1200, Loss function: 4.214, Average Loss: 4.204, avg. samples / sec: 21768.60
Iteration:   1200, Loss function: 4.397, Average Loss: 4.213, avg. samples / sec: 21770.77
Iteration:   1200, Loss function: 4.196, Average Loss: 4.210, avg. samples / sec: 21762.00
Iteration:   1200, Loss function: 4.440, Average Loss: 4.201, avg. samples / sec: 21767.12
Iteration:   1200, Loss function: 4.668, Average Loss: 4.200, avg. samples / sec: 21763.12
Iteration:   1200, Loss function: 4.650, Average Loss: 4.208, avg. samples / sec: 21755.47
Iteration:   1200, Loss function: 5.036, Average Loss: 4.210, avg. samples / sec: 21757.13

:::MLPv0.5.0 ssd 1541710995.848983765 (train.py:553) train_epoch: 21
Iteration:   1220, Loss function: 4.906, Average Loss: 4.217, avg. samples / sec: 21871.73
Iteration:   1220, Loss function: 4.635, Average Loss: 4.211, avg. samples / sec: 21858.74
Iteration:   1220, Loss function: 4.289, Average Loss: 4.208, avg. samples / sec: 21855.86
Iteration:   1220, Loss function: 4.350, Average Loss: 4.217, avg. samples / sec: 21852.38
Iteration:   1220, Loss function: 3.849, Average Loss: 4.219, avg. samples / sec: 21849.64
Iteration:   1220, Loss function: 4.650, Average Loss: 4.216, avg. samples / sec: 21851.22
Iteration:   1220, Loss function: 4.728, Average Loss: 4.205, avg. samples / sec: 21851.22
Iteration:   1220, Loss function: 4.808, Average Loss: 4.214, avg. samples / sec: 21848.38
Iteration:   1240, Loss function: 4.712, Average Loss: 4.223, avg. samples / sec: 21945.23
Iteration:   1240, Loss function: 4.775, Average Loss: 4.222, avg. samples / sec: 21953.13
Iteration:   1240, Loss function: 4.405, Average Loss: 4.210, avg. samples / sec: 21953.50
Iteration:   1240, Loss function: 4.678, Average Loss: 4.219, avg. samples / sec: 21954.40
Iteration:   1240, Loss function: 4.476, Average Loss: 4.224, avg. samples / sec: 21945.70
Iteration:   1240, Loss function: 4.110, Average Loss: 4.220, avg. samples / sec: 21945.51
Iteration:   1240, Loss function: 4.412, Average Loss: 4.215, avg. samples / sec: 21943.90
Iteration:   1240, Loss function: 5.021, Average Loss: 4.217, avg. samples / sec: 21932.98
Iteration:   1260, Loss function: 4.566, Average Loss: 4.228, avg. samples / sec: 21866.99
Iteration:   1260, Loss function: 4.224, Average Loss: 4.227, avg. samples / sec: 21867.16
Iteration:   1260, Loss function: 4.635, Average Loss: 4.225, avg. samples / sec: 21867.10
Iteration:   1260, Loss function: 4.518, Average Loss: 4.228, avg. samples / sec: 21856.57
Iteration:   1260, Loss function: 4.448, Average Loss: 4.218, avg. samples / sec: 21860.27
Iteration:   1260, Loss function: 4.436, Average Loss: 4.229, avg. samples / sec: 21849.29
Iteration:   1260, Loss function: 4.051, Average Loss: 4.223, avg. samples / sec: 21858.64
Iteration:   1260, Loss function: 4.918, Average Loss: 4.219, avg. samples / sec: 21859.53

:::MLPv0.5.0 ssd 1541711001.272435427 (train.py:553) train_epoch: 22
Iteration:   1280, Loss function: 4.220, Average Loss: 4.230, avg. samples / sec: 21881.35
Iteration:   1280, Loss function: 5.053, Average Loss: 4.224, avg. samples / sec: 21880.70
Iteration:   1280, Loss function: 4.913, Average Loss: 4.235, avg. samples / sec: 21874.59
Iteration:   1280, Loss function: 5.000, Average Loss: 4.236, avg. samples / sec: 21869.35
Iteration:   1280, Loss function: 4.688, Average Loss: 4.224, avg. samples / sec: 21875.40
Iteration:   1280, Loss function: 4.538, Average Loss: 4.224, avg. samples / sec: 21868.92
Iteration:   1280, Loss function: 4.710, Average Loss: 4.235, avg. samples / sec: 21869.82
Iteration:   1280, Loss function: 5.049, Average Loss: 4.234, avg. samples / sec: 21862.44
Iteration:   1300, Loss function: 4.223, Average Loss: 4.230, avg. samples / sec: 21734.70
Iteration:   1300, Loss function: 4.980, Average Loss: 4.240, avg. samples / sec: 21725.40
Iteration:   1300, Loss function: 4.619, Average Loss: 4.241, avg. samples / sec: 21733.17
Iteration:   1300, Loss function: 4.535, Average Loss: 4.242, avg. samples / sec: 21737.08
Iteration:   1300, Loss function: 3.864, Average Loss: 4.240, avg. samples / sec: 21737.15
Iteration:   1300, Loss function: 4.376, Average Loss: 4.231, avg. samples / sec: 21732.65
Iteration:   1300, Loss function: 4.054, Average Loss: 4.231, avg. samples / sec: 21729.00
Iteration:   1300, Loss function: 4.249, Average Loss: 4.238, avg. samples / sec: 21715.11
Iteration:   1320, Loss function: 4.218, Average Loss: 4.243, avg. samples / sec: 21827.51
Iteration:   1320, Loss function: 4.512, Average Loss: 4.243, avg. samples / sec: 21825.04
Iteration:   1320, Loss function: 3.825, Average Loss: 4.234, avg. samples / sec: 21817.61
Iteration:   1320, Loss function: 3.751, Average Loss: 4.243, avg. samples / sec: 21825.44
Iteration:   1320, Loss function: 4.238, Average Loss: 4.233, avg. samples / sec: 21826.75
Iteration:   1320, Loss function: 4.677, Average Loss: 4.235, avg. samples / sec: 21825.80
Iteration:   1320, Loss function: 4.720, Average Loss: 4.243, avg. samples / sec: 21834.47
Iteration:   1320, Loss function: 4.651, Average Loss: 4.244, avg. samples / sec: 21818.09

:::MLPv0.5.0 ssd 1541711006.722093582 (train.py:553) train_epoch: 23
Iteration:   1340, Loss function: 4.320, Average Loss: 4.247, avg. samples / sec: 21785.12
Iteration:   1340, Loss function: 4.139, Average Loss: 4.245, avg. samples / sec: 21779.86
Iteration:   1340, Loss function: 4.837, Average Loss: 4.239, avg. samples / sec: 21776.59
Iteration:   1340, Loss function: 4.114, Average Loss: 4.246, avg. samples / sec: 21776.62
Iteration:   1340, Loss function: 4.065, Average Loss: 4.246, avg. samples / sec: 21767.07
Iteration:   1340, Loss function: 4.366, Average Loss: 4.238, avg. samples / sec: 21773.20
Iteration:   1340, Loss function: 4.026, Average Loss: 4.238, avg. samples / sec: 21771.31
Iteration:   1340, Loss function: 4.169, Average Loss: 4.246, avg. samples / sec: 21770.94
Iteration:   1360, Loss function: 4.286, Average Loss: 4.251, avg. samples / sec: 21731.06
Iteration:   1360, Loss function: 4.863, Average Loss: 4.242, avg. samples / sec: 21734.55
Iteration:   1360, Loss function: 3.870, Average Loss: 4.247, avg. samples / sec: 21726.85
Iteration:   1360, Loss function: 4.168, Average Loss: 4.248, avg. samples / sec: 21733.26
Iteration:   1360, Loss function: 4.395, Average Loss: 4.242, avg. samples / sec: 21729.32
Iteration:   1360, Loss function: 4.951, Average Loss: 4.252, avg. samples / sec: 21730.34
Iteration:   1360, Loss function: 4.686, Average Loss: 4.248, avg. samples / sec: 21730.77
Iteration:   1360, Loss function: 4.511, Average Loss: 4.244, avg. samples / sec: 21726.47
Iteration:   1380, Loss function: 4.327, Average Loss: 4.257, avg. samples / sec: 21815.18
Iteration:   1380, Loss function: 4.034, Average Loss: 4.255, avg. samples / sec: 21822.74
Iteration:   1380, Loss function: 4.013, Average Loss: 4.252, avg. samples / sec: 21818.73
Iteration:   1380, Loss function: 4.529, Average Loss: 4.251, avg. samples / sec: 21825.41
Iteration:   1380, Loss function: 3.702, Average Loss: 4.246, avg. samples / sec: 21819.03
Iteration:   1380, Loss function: 4.102, Average Loss: 4.247, avg. samples / sec: 21815.19
Iteration:   1380, Loss function: 4.043, Average Loss: 4.250, avg. samples / sec: 21816.21
Iteration:   1380, Loss function: 4.600, Average Loss: 4.253, avg. samples / sec: 21819.82

:::MLPv0.5.0 ssd 1541711012.179762363 (train.py:553) train_epoch: 24
Iteration:   1400, Loss function: 3.921, Average Loss: 4.256, avg. samples / sec: 21778.72
Iteration:   1400, Loss function: 4.408, Average Loss: 4.260, avg. samples / sec: 21772.15
Iteration:   1400, Loss function: 4.222, Average Loss: 4.258, avg. samples / sec: 21772.85
Iteration:   1400, Loss function: 4.323, Average Loss: 4.254, avg. samples / sec: 21771.08
Iteration:   1400, Loss function: 4.803, Average Loss: 4.250, avg. samples / sec: 21770.75
Iteration:   1400, Loss function: 3.930, Average Loss: 4.255, avg. samples / sec: 21770.97
Iteration:   1400, Loss function: 4.042, Average Loss: 4.249, avg. samples / sec: 21768.48
Iteration:   1400, Loss function: 4.599, Average Loss: 4.254, avg. samples / sec: 21767.89
Iteration:   1420, Loss function: 4.635, Average Loss: 4.260, avg. samples / sec: 21940.31
Iteration:   1420, Loss function: 4.503, Average Loss: 4.264, avg. samples / sec: 21935.65
Iteration:   1420, Loss function: 4.348, Average Loss: 4.261, avg. samples / sec: 21937.25
Iteration:   1420, Loss function: 4.361, Average Loss: 4.255, avg. samples / sec: 21942.14
Iteration:   1420, Loss function: 4.246, Average Loss: 4.259, avg. samples / sec: 21943.96
Iteration:   1420, Loss function: 4.387, Average Loss: 4.252, avg. samples / sec: 21940.94
Iteration:   1420, Loss function: 4.499, Average Loss: 4.258, avg. samples / sec: 21937.40
Iteration:   1420, Loss function: 4.201, Average Loss: 4.258, avg. samples / sec: 21938.18
Iteration:   1440, Loss function: 4.151, Average Loss: 4.260, avg. samples / sec: 21820.22
Iteration:   1440, Loss function: 4.106, Average Loss: 4.263, avg. samples / sec: 21826.99
Iteration:   1440, Loss function: 3.821, Average Loss: 4.253, avg. samples / sec: 21830.14
Iteration:   1440, Loss function: 4.330, Average Loss: 4.260, avg. samples / sec: 21831.61
Iteration:   1440, Loss function: 4.177, Average Loss: 4.267, avg. samples / sec: 21821.89
Iteration:   1440, Loss function: 4.068, Average Loss: 4.257, avg. samples / sec: 21823.93
Iteration:   1440, Loss function: 4.211, Average Loss: 4.261, avg. samples / sec: 21822.69
Iteration:   1440, Loss function: 4.725, Average Loss: 4.260, avg. samples / sec: 21823.87

:::MLPv0.5.0 ssd 1541711017.523261786 (train.py:553) train_epoch: 25
Iteration:   1460, Loss function: 4.293, Average Loss: 4.269, avg. samples / sec: 21884.30
Iteration:   1460, Loss function: 4.684, Average Loss: 4.257, avg. samples / sec: 21879.76
Iteration:   1460, Loss function: 4.355, Average Loss: 4.265, avg. samples / sec: 21877.81
Iteration:   1460, Loss function: 3.995, Average Loss: 4.261, avg. samples / sec: 21876.50
Iteration:   1460, Loss function: 3.998, Average Loss: 4.266, avg. samples / sec: 21881.26
Iteration:   1460, Loss function: 3.889, Average Loss: 4.263, avg. samples / sec: 21872.34
Iteration:   1460, Loss function: 4.086, Average Loss: 4.260, avg. samples / sec: 21877.21
Iteration:   1460, Loss function: 4.307, Average Loss: 4.263, avg. samples / sec: 21875.39
Iteration:   1480, Loss function: 3.849, Average Loss: 4.263, avg. samples / sec: 21800.65
Iteration:   1480, Loss function: 4.299, Average Loss: 4.266, avg. samples / sec: 21799.33
Iteration:   1480, Loss function: 4.357, Average Loss: 4.264, avg. samples / sec: 21801.28
Iteration:   1480, Loss function: 4.414, Average Loss: 4.255, avg. samples / sec: 21794.55
Iteration:   1480, Loss function: 4.384, Average Loss: 4.269, avg. samples / sec: 21792.72
Iteration:   1480, Loss function: 4.020, Average Loss: 4.266, avg. samples / sec: 21797.01
Iteration:   1480, Loss function: 4.269, Average Loss: 4.261, avg. samples / sec: 21798.50
Iteration:   1480, Loss function: 4.214, Average Loss: 4.265, avg. samples / sec: 21802.45
Iteration:   1500, Loss function: 4.846, Average Loss: 4.268, avg. samples / sec: 21812.82
Iteration:   1500, Loss function: 4.642, Average Loss: 4.265, avg. samples / sec: 21811.89
Iteration:   1500, Loss function: 4.082, Average Loss: 4.265, avg. samples / sec: 21816.18
Iteration:   1500, Loss function: 4.312, Average Loss: 4.268, avg. samples / sec: 21812.15
Iteration:   1500, Loss function: 4.340, Average Loss: 4.264, avg. samples / sec: 21806.18
Iteration:   1500, Loss function: 4.722, Average Loss: 4.263, avg. samples / sec: 21811.12
Iteration:   1500, Loss function: 4.436, Average Loss: 4.270, avg. samples / sec: 21807.21
Iteration:   1500, Loss function: 4.274, Average Loss: 4.257, avg. samples / sec: 21805.70

:::MLPv0.5.0 ssd 1541711022.963232756 (train.py:553) train_epoch: 26
Iteration:   1520, Loss function: 4.180, Average Loss: 4.272, avg. samples / sec: 21826.87
Iteration:   1520, Loss function: 4.239, Average Loss: 4.267, avg. samples / sec: 21827.82
Iteration:   1520, Loss function: 4.453, Average Loss: 4.268, avg. samples / sec: 21831.17
Iteration:   1520, Loss function: 4.311, Average Loss: 4.269, avg. samples / sec: 21825.40
Iteration:   1520, Loss function: 4.267, Average Loss: 4.272, avg. samples / sec: 21826.36
Iteration:   1520, Loss function: 3.878, Average Loss: 4.259, avg. samples / sec: 21822.74
Iteration:   1520, Loss function: 4.165, Average Loss: 4.263, avg. samples / sec: 21815.36
Iteration:   1520, Loss function: 4.970, Average Loss: 4.265, avg. samples / sec: 21817.60
Iteration:   1540, Loss function: 4.834, Average Loss: 4.274, avg. samples / sec: 21797.00
Iteration:   1540, Loss function: 4.284, Average Loss: 4.271, avg. samples / sec: 21795.05
Iteration:   1540, Loss function: 4.508, Average Loss: 4.272, avg. samples / sec: 21792.12
Iteration:   1540, Loss function: 4.829, Average Loss: 4.272, avg. samples / sec: 21795.87
Iteration:   1540, Loss function: 5.160, Average Loss: 4.269, avg. samples / sec: 21801.91
Iteration:   1540, Loss function: 4.589, Average Loss: 4.276, avg. samples / sec: 21791.42
Iteration:   1540, Loss function: 5.138, Average Loss: 4.262, avg. samples / sec: 21796.65
Iteration:   1540, Loss function: 4.880, Average Loss: 4.266, avg. samples / sec: 21796.86

:::MLPv0.5.0 ssd 1541711028.409942389 (train.py:553) train_epoch: 27
Iteration:   1560, Loss function: 4.476, Average Loss: 4.280, avg. samples / sec: 21802.17
Iteration:   1560, Loss function: 3.882, Average Loss: 4.276, avg. samples / sec: 21798.54
Iteration:   1560, Loss function: 5.139, Average Loss: 4.278, avg. samples / sec: 21806.27
Iteration:   1560, Loss function: 4.213, Average Loss: 4.274, avg. samples / sec: 21797.62
Iteration:   1560, Loss function: 4.258, Average Loss: 4.275, avg. samples / sec: 21791.91
Iteration:   1560, Loss function: 4.048, Average Loss: 4.268, avg. samples / sec: 21802.32
Iteration:   1560, Loss function: 4.019, Average Loss: 4.272, avg. samples / sec: 21798.18
Iteration:   1560, Loss function: 4.506, Average Loss: 4.263, avg. samples / sec: 21799.29
Iteration:   1580, Loss function: 4.549, Average Loss: 4.282, avg. samples / sec: 21763.31
Iteration:   1580, Loss function: 4.667, Average Loss: 4.277, avg. samples / sec: 21773.24
Iteration:   1580, Loss function: 5.175, Average Loss: 4.277, avg. samples / sec: 21764.91
Iteration:   1580, Loss function: 4.450, Average Loss: 4.277, avg. samples / sec: 21764.19
Iteration:   1580, Loss function: 4.315, Average Loss: 4.268, avg. samples / sec: 21769.04
Iteration:   1580, Loss function: 5.511, Average Loss: 4.272, avg. samples / sec: 21769.91
Iteration:   1580, Loss function: 4.534, Average Loss: 4.275, avg. samples / sec: 21765.15
Iteration:   1580, Loss function: 4.829, Average Loss: 4.263, avg. samples / sec: 21767.26
Iteration:   1600, Loss function: 4.172, Average Loss: 4.280, avg. samples / sec: 21892.40
Iteration:   1600, Loss function: 4.223, Average Loss: 4.280, avg. samples / sec: 21889.64
Iteration:   1600, Loss function: 4.218, Average Loss: 4.278, avg. samples / sec: 21890.02
Iteration:   1600, Loss function: 3.734, Average Loss: 4.284, avg. samples / sec: 21879.48
Iteration:   1600, Loss function: 4.472, Average Loss: 4.272, avg. samples / sec: 21887.24
Iteration:   1600, Loss function: 4.595, Average Loss: 4.274, avg. samples / sec: 21886.27
Iteration:   1600, Loss function: 4.084, Average Loss: 4.278, avg. samples / sec: 21888.20
Iteration:   1600, Loss function: 4.171, Average Loss: 4.263, avg. samples / sec: 21885.58

:::MLPv0.5.0 ssd 1541711033.852981329 (train.py:553) train_epoch: 28
Iteration:   1620, Loss function: 4.627, Average Loss: 4.283, avg. samples / sec: 21805.10
Iteration:   1620, Loss function: 4.037, Average Loss: 4.279, avg. samples / sec: 21794.78
Iteration:   1620, Loss function: 4.075, Average Loss: 4.280, avg. samples / sec: 21801.34
Iteration:   1620, Loss function: 3.890, Average Loss: 4.277, avg. samples / sec: 21801.48
Iteration:   1620, Loss function: 4.316, Average Loss: 4.264, avg. samples / sec: 21805.82
Iteration:   1620, Loss function: 4.665, Average Loss: 4.276, avg. samples / sec: 21801.66
Iteration:   1620, Loss function: 3.984, Average Loss: 4.287, avg. samples / sec: 21794.22
Iteration:   1620, Loss function: 4.594, Average Loss: 4.273, avg. samples / sec: 21796.92
Iteration:   1640, Loss function: 4.439, Average Loss: 4.284, avg. samples / sec: 21817.39
Iteration:   1640, Loss function: 3.984, Average Loss: 4.277, avg. samples / sec: 21807.72
Iteration:   1640, Loss function: 4.635, Average Loss: 4.278, avg. samples / sec: 21805.31
Iteration:   1640, Loss function: 4.292, Average Loss: 4.276, avg. samples / sec: 21809.63
Iteration:   1640, Loss function: 4.684, Average Loss: 4.281, avg. samples / sec: 21798.63
Iteration:   1640, Loss function: 4.214, Average Loss: 4.269, avg. samples / sec: 21806.39
Iteration:   1640, Loss function: 4.318, Average Loss: 4.272, avg. samples / sec: 21803.27
Iteration:   1640, Loss function: 4.034, Average Loss: 4.263, avg. samples / sec: 21801.79
Iteration:   1660, Loss function: 4.151, Average Loss: 4.277, avg. samples / sec: 21842.29
Iteration:   1660, Loss function: 4.569, Average Loss: 4.280, avg. samples / sec: 21845.55
Iteration:   1660, Loss function: 4.817, Average Loss: 4.281, avg. samples / sec: 21834.10
Iteration:   1660, Loss function: 4.839, Average Loss: 4.267, avg. samples / sec: 21845.72
Iteration:   1660, Loss function: 4.757, Average Loss: 4.273, avg. samples / sec: 21837.78
Iteration:   1660, Loss function: 4.545, Average Loss: 4.273, avg. samples / sec: 21842.69
Iteration:   1660, Loss function: 5.011, Average Loss: 4.278, avg. samples / sec: 21829.41
Iteration:   1660, Loss function: 4.694, Average Loss: 4.262, avg. samples / sec: 21837.30

:::MLPv0.5.0 ssd 1541711039.201085806 (train.py:553) train_epoch: 29
Iteration:   1680, Loss function: 4.737, Average Loss: 4.269, avg. samples / sec: 21856.67
Iteration:   1680, Loss function: 4.364, Average Loss: 4.284, avg. samples / sec: 21852.31
Iteration:   1680, Loss function: 4.199, Average Loss: 4.277, avg. samples / sec: 21847.03
Iteration:   1680, Loss function: 4.463, Average Loss: 4.284, avg. samples / sec: 21863.15
Iteration:   1680, Loss function: 4.162, Average Loss: 4.273, avg. samples / sec: 21855.46
Iteration:   1680, Loss function: 4.254, Average Loss: 4.261, avg. samples / sec: 21861.72
Iteration:   1680, Loss function: 4.270, Average Loss: 4.284, avg. samples / sec: 21850.49
Iteration:   1680, Loss function: 4.099, Average Loss: 4.277, avg. samples / sec: 21850.45
Iteration:   1700, Loss function: 3.908, Average Loss: 4.274, avg. samples / sec: 21885.79
Iteration:   1700, Loss function: 4.116, Average Loss: 4.284, avg. samples / sec: 21881.73
Iteration:   1700, Loss function: 3.863, Average Loss: 4.282, avg. samples / sec: 21886.01
Iteration:   1700, Loss function: 3.951, Average Loss: 4.282, avg. samples / sec: 21880.88
Iteration:   1700, Loss function: 4.340, Average Loss: 4.271, avg. samples / sec: 21877.64
Iteration:   1700, Loss function: 4.197, Average Loss: 4.276, avg. samples / sec: 21883.07
Iteration:   1700, Loss function: 4.508, Average Loss: 4.269, avg. samples / sec: 21874.01
Iteration:   1700, Loss function: 4.614, Average Loss: 4.261, avg. samples / sec: 21875.47
Iteration:   1720, Loss function: 3.917, Average Loss: 4.278, avg. samples / sec: 21836.63
Iteration:   1720, Loss function: 4.296, Average Loss: 4.280, avg. samples / sec: 21837.36
Iteration:   1720, Loss function: 4.118, Average Loss: 4.273, avg. samples / sec: 21840.91
Iteration:   1720, Loss function: 4.226, Average Loss: 4.276, avg. samples / sec: 21828.11
Iteration:   1720, Loss function: 3.755, Average Loss: 4.259, avg. samples / sec: 21839.23
Iteration:   1720, Loss function: 4.035, Average Loss: 4.266, avg. samples / sec: 21834.73
Iteration:   1720, Loss function: 4.476, Average Loss: 4.270, avg. samples / sec: 21835.63
Iteration:   1720, Loss function: 4.577, Average Loss: 4.282, avg. samples / sec: 21818.98

:::MLPv0.5.0 ssd 1541711044.640398026 (train.py:553) train_epoch: 30
Iteration:   1740, Loss function: 4.347, Average Loss: 4.279, avg. samples / sec: 21839.47
Iteration:   1740, Loss function: 4.318, Average Loss: 4.282, avg. samples / sec: 21847.79
Iteration:   1740, Loss function: 4.090, Average Loss: 4.266, avg. samples / sec: 21837.29
Iteration:   1740, Loss function: 4.473, Average Loss: 4.258, avg. samples / sec: 21835.09
Iteration:   1740, Loss function: 4.408, Average Loss: 4.277, avg. samples / sec: 21830.70
Iteration:   1740, Loss function: 4.322, Average Loss: 4.269, avg. samples / sec: 21833.73
Iteration:   1740, Loss function: 4.534, Average Loss: 4.275, avg. samples / sec: 21824.80
Iteration:   1740, Loss function: 4.336, Average Loss: 4.277, avg. samples / sec: 21820.16
Iteration:   1760, Loss function: 4.020, Average Loss: 4.279, avg. samples / sec: 21877.87
Iteration:   1760, Loss function: 3.658, Average Loss: 4.276, avg. samples / sec: 21887.18
Iteration:   1760, Loss function: 4.197, Average Loss: 4.278, avg. samples / sec: 21889.48
Iteration:   1760, Loss function: 4.507, Average Loss: 4.283, avg. samples / sec: 21876.42
Iteration:   1760, Loss function: 4.172, Average Loss: 4.263, avg. samples / sec: 21877.41
Iteration:   1760, Loss function: 4.260, Average Loss: 4.278, avg. samples / sec: 21882.34
Iteration:   1760, Loss function: 4.080, Average Loss: 4.272, avg. samples / sec: 21876.67
Iteration:   1760, Loss function: 4.308, Average Loss: 4.259, avg. samples / sec: 21873.48
Iteration:   1780, Loss function: 4.634, Average Loss: 4.281, avg. samples / sec: 21906.02
Iteration:   1780, Loss function: 4.515, Average Loss: 4.277, avg. samples / sec: 21910.67
Iteration:   1780, Loss function: 4.408, Average Loss: 4.264, avg. samples / sec: 21908.71
Iteration:   1780, Loss function: 4.384, Average Loss: 4.276, avg. samples / sec: 21909.43
Iteration:   1780, Loss function: 4.474, Average Loss: 4.260, avg. samples / sec: 21911.07
Iteration:   1780, Loss function: 4.593, Average Loss: 4.277, avg. samples / sec: 21898.07
Iteration:   1780, Loss function: 4.490, Average Loss: 4.271, avg. samples / sec: 21910.30
Iteration:   1780, Loss function: 4.511, Average Loss: 4.283, avg. samples / sec: 21892.91

:::MLPv0.5.0 ssd 1541711050.068981171 (train.py:553) train_epoch: 31
Iteration:   1800, Loss function: 3.965, Average Loss: 4.279, avg. samples / sec: 21809.69
Iteration:   1800, Loss function: 4.211, Average Loss: 4.274, avg. samples / sec: 21807.95
Iteration:   1800, Loss function: 3.401, Average Loss: 4.279, avg. samples / sec: 21821.67
Iteration:   1800, Loss function: 4.197, Average Loss: 4.278, avg. samples / sec: 21810.20
Iteration:   1800, Loss function: 4.246, Average Loss: 4.265, avg. samples / sec: 21805.00
Iteration:   1800, Loss function: 3.914, Average Loss: 4.259, avg. samples / sec: 21806.05
Iteration:   1800, Loss function: 3.964, Average Loss: 4.275, avg. samples / sec: 21804.54
Iteration:   1800, Loss function: 4.216, Average Loss: 4.271, avg. samples / sec: 21803.35
Iteration:   1820, Loss function: 4.009, Average Loss: 4.275, avg. samples / sec: 21815.13
Iteration:   1820, Loss function: 4.618, Average Loss: 4.258, avg. samples / sec: 21816.97
Iteration:   1820, Loss function: 4.298, Average Loss: 4.278, avg. samples / sec: 21813.66
Iteration:   1820, Loss function: 4.463, Average Loss: 4.276, avg. samples / sec: 21809.48
Iteration:   1820, Loss function: 4.436, Average Loss: 4.272, avg. samples / sec: 21815.14
Iteration:   1820, Loss function: 4.015, Average Loss: 4.273, avg. samples / sec: 21803.72
Iteration:   1820, Loss function: 4.255, Average Loss: 4.261, avg. samples / sec: 21807.94
Iteration:   1820, Loss function: 4.212, Average Loss: 4.271, avg. samples / sec: 21813.67
Iteration:   1840, Loss function: 4.190, Average Loss: 4.274, avg. samples / sec: 21922.71
Iteration:   1840, Loss function: 4.032, Average Loss: 4.278, avg. samples / sec: 21919.16
Iteration:   1840, Loss function: 3.833, Average Loss: 4.271, avg. samples / sec: 21921.41
Iteration:   1840, Loss function: 3.983, Average Loss: 4.273, avg. samples / sec: 21905.29
Iteration:   1840, Loss function: 4.632, Average Loss: 4.273, avg. samples / sec: 21921.77
Iteration:   1840, Loss function: 4.431, Average Loss: 4.261, avg. samples / sec: 21914.98
Iteration:   1840, Loss function: 4.727, Average Loss: 4.262, avg. samples / sec: 21916.06
Iteration:   1840, Loss function: 4.498, Average Loss: 4.272, avg. samples / sec: 21903.58

:::MLPv0.5.0 ssd 1541711055.506273031 (train.py:553) train_epoch: 32
Iteration:   1860, Loss function: 3.940, Average Loss: 4.270, avg. samples / sec: 21843.86
Iteration:   1860, Loss function: 3.552, Average Loss: 4.258, avg. samples / sec: 21850.95
Iteration:   1860, Loss function: 3.251, Average Loss: 4.260, avg. samples / sec: 21845.84
Iteration:   1860, Loss function: 4.100, Average Loss: 4.269, avg. samples / sec: 21842.77
Iteration:   1860, Loss function: 4.180, Average Loss: 4.274, avg. samples / sec: 21838.89
Iteration:   1860, Loss function: 4.475, Average Loss: 4.269, avg. samples / sec: 21855.94
Iteration:   1860, Loss function: 4.011, Average Loss: 4.272, avg. samples / sec: 21842.38
Iteration:   1860, Loss function: 4.218, Average Loss: 4.276, avg. samples / sec: 21831.75

































































:::MLPv0.5.0 ssd 1541711058.042312384 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711058.043140650 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711058.043895483 (train.py:220) eval_start: 32
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.49 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.49 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.49 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.49 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.49 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.49 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.49 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 7.49 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(343499, 7)
(343499, 7)
(343499, 7)
0/343499
0/343499
0/343499
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(343499, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(343499, 7)
0/343499
(343499, 7)
(343499, 7)
0/343499
Converting ndarray to lists...
0/343499
0/343499
(343499, 7)
0/343499
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(343499, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/343499
Loading and preparing results...
(343499, 7)
(343499, 7)
0/343499
Converting ndarray to lists...
0/343499
(343499, 7)
0/343499
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(343499, 7)
(343499, 7)
Converting ndarray to lists...
(343499, 7)
(343499, 7)
0/343499
0/343499
0/343499
0/343499
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(343499, 7)
Converting ndarray to lists...
(343499, 7)
Converting ndarray to lists...
0/343499
(343499, 7)
0/343499
(343499, 7)
0/343499
0/343499
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(343499, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/343499
(343499, 7)
(343499, 7)
0/343499
(343499, 7)
0/343499
0/343499
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(343499, 7)
(343499, 7)
(343499, 7)
(343499, 7)
0/343499
0/343499
0/343499
0/343499
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(343499, 7)
Converting ndarray to lists...
Loading and preparing results...
0/343499
Loading and preparing results...
(343499, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(343499, 7)
(343499, 7)
Loading and preparing results...
0/343499
0/343499
Loading and preparing results...
(343499, 7)
Loading and preparing results...
Converting ndarray to lists...
0/343499
Loading and preparing results...
Converting ndarray to lists...
0/343499
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(343499, 7)
(343499, 7)
(343499, 7)
(343499, 7)
(343499, 7)
0/343499
0/343499
0/343499
0/343499
Loading and preparing results...
0/343499
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(343499, 7)
Converting ndarray to lists...
(343499, 7)
0/343499
Converting ndarray to lists...
(343499, 7)
Loading and preparing results...
0/343499
0/343499
(343499, 7)
Converting ndarray to lists...
0/343499
(343499, 7)
0/343499
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(343499, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(343499, 7)
0/343499
(343499, 7)
(343499, 7)
0/343499
0/343499
0/343499
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(343499, 7)
Converting ndarray to lists...
Loading and preparing results...
(343499, 7)
0/343499
(343499, 7)
Converting ndarray to lists...
0/343499
0/343499
(343499, 7)
0/343499
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(343499, 7)
(343499, 7)
(343499, 7)
Converting ndarray to lists...
0/343499
0/343499
0/343499
(343499, 7)
0/343499
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(343499, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/343499
(343499, 7)
(343499, 7)
Converting ndarray to lists...
0/343499
(343499, 7)
0/343499
0/343499
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(343499, 7)
0/343499
(343499, 7)
(343499, 7)
(343499, 7)
Converting ndarray to lists...
0/343499
0/343499
0/343499
(343499, 7)
0/343499
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.32s)
creating index...
DONE (t=2.32s)
creating index...
DONE (t=2.32s)
creating index...
DONE (t=2.32s)
creating index...
DONE (t=2.32s)
creating index...
DONE (t=2.32s)
creating index...
DONE (t=2.33s)
creating index...
DONE (t=2.33s)
creating index...
DONE (t=2.33s)
creating index...
DONE (t=2.33s)
creating index...
DONE (t=2.33s)
creating index...
DONE (t=2.34s)
creating index...
DONE (t=2.35s)
creating index...
DONE (t=2.36s)
creating index...
DONE (t=2.36s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=3.59s).
Accumulating evaluation results...
DONE (t=3.61s).
Accumulating evaluation results...
DONE (t=3.61s).
Accumulating evaluation results...
DONE (t=3.62s).
Accumulating evaluation results...
DONE (t=3.61s).
Accumulating evaluation results...
DONE (t=3.64s).
Accumulating evaluation results...
DONE (t=3.64s).
Accumulating evaluation results...
DONE (t=3.65s).
Accumulating evaluation results...
DONE (t=1.24s).
DONE (t=1.25s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.267
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.140
DONE (t=1.26s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.133
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.267
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.035
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.133
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.267
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.035
DONE (t=1.26s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.133
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.154
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.232
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.035
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.223
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380
Current AP: 0.14044 AP goal: 0.21200
DONE (t=1.26s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160
DONE (t=1.26s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.154
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.232
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.267
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380
Current AP: 0.14044 AP goal: 0.21200
DONE (t=1.27s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.133
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.140
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.267
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.232
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.140
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380
Current AP: 0.14044 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.035
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.267
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.133
DONE (t=1.27s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.267
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.133
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.035
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.133
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.035
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.140
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.035
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.232
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.223
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380
Current AP: 0.14044 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.267
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.154
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.232
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380
Current AP: 0.14044 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.133
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.232
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.223
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380
Current AP: 0.14044 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.035
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.232
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380
Current AP: 0.14044 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.223
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.160
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.232
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380
Current AP: 0.14044 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711073.015016556 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711073.015965700 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.14044050197611316}

:::MLPv0.5.0 ssd 1541711073.016751528 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.14044050197611316}

:::MLPv0.5.0 ssd 1541711073.017495632 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711073.018236399 (train.py:338) eval_stop: 32
Iteration:   1880, Loss function: 4.048, Average Loss: 4.266, avg. samples / sec: 2356.60
Iteration:   1880, Loss function: 4.011, Average Loss: 4.269, avg. samples / sec: 2356.62
Iteration:   1880, Loss function: 4.433, Average Loss: 4.260, avg. samples / sec: 2356.57
Iteration:   1880, Loss function: 4.053, Average Loss: 4.268, avg. samples / sec: 2356.57
Iteration:   1880, Loss function: 4.293, Average Loss: 4.273, avg. samples / sec: 2356.66
Iteration:   1880, Loss function: 4.173, Average Loss: 4.256, avg. samples / sec: 2356.55
Iteration:   1880, Loss function: 4.286, Average Loss: 4.267, avg. samples / sec: 2356.57
Iteration:   1880, Loss function: 3.554, Average Loss: 4.268, avg. samples / sec: 2356.56
Iteration:   1900, Loss function: 3.748, Average Loss: 4.260, avg. samples / sec: 21915.42
Iteration:   1900, Loss function: 4.305, Average Loss: 4.268, avg. samples / sec: 21907.46
Iteration:   1900, Loss function: 4.177, Average Loss: 4.266, avg. samples / sec: 21914.19
Iteration:   1900, Loss function: 4.026, Average Loss: 4.257, avg. samples / sec: 21911.62
Iteration:   1900, Loss function: 4.384, Average Loss: 4.269, avg. samples / sec: 21906.73
Iteration:   1900, Loss function: 4.207, Average Loss: 4.272, avg. samples / sec: 21908.52
Iteration:   1900, Loss function: 3.853, Average Loss: 4.267, avg. samples / sec: 21910.68
Iteration:   1900, Loss function: 4.086, Average Loss: 4.269, avg. samples / sec: 21903.03

:::MLPv0.5.0 ssd 1541711076.350432873 (train.py:553) train_epoch: 33
Iteration:   1920, Loss function: 4.266, Average Loss: 4.266, avg. samples / sec: 21895.93
Iteration:   1920, Loss function: 4.178, Average Loss: 4.256, avg. samples / sec: 21890.74
Iteration:   1920, Loss function: 4.949, Average Loss: 4.269, avg. samples / sec: 21888.73
Iteration:   1920, Loss function: 4.071, Average Loss: 4.259, avg. samples / sec: 21879.96
Iteration:   1920, Loss function: 3.953, Average Loss: 4.266, avg. samples / sec: 21891.13
Iteration:   1920, Loss function: 3.923, Average Loss: 4.267, avg. samples / sec: 21892.64
Iteration:   1920, Loss function: 3.643, Average Loss: 4.271, avg. samples / sec: 21886.54
Iteration:   1920, Loss function: 4.287, Average Loss: 4.266, avg. samples / sec: 21881.93
Iteration:   1940, Loss function: 4.131, Average Loss: 4.265, avg. samples / sec: 21935.65
Iteration:   1940, Loss function: 3.936, Average Loss: 4.263, avg. samples / sec: 21922.02
Iteration:   1940, Loss function: 4.151, Average Loss: 4.264, avg. samples / sec: 21929.16
Iteration:   1940, Loss function: 4.096, Average Loss: 4.254, avg. samples / sec: 21925.69
Iteration:   1940, Loss function: 4.298, Average Loss: 4.261, avg. samples / sec: 21933.52
Iteration:   1940, Loss function: 4.380, Average Loss: 4.258, avg. samples / sec: 21928.47
Iteration:   1940, Loss function: 4.505, Average Loss: 4.268, avg. samples / sec: 21929.05
Iteration:   1940, Loss function: 4.092, Average Loss: 4.261, avg. samples / sec: 21925.41
Iteration:   1960, Loss function: 4.622, Average Loss: 4.261, avg. samples / sec: 21929.41
Iteration:   1960, Loss function: 4.724, Average Loss: 4.262, avg. samples / sec: 21928.98
Iteration:   1960, Loss function: 4.745, Average Loss: 4.266, avg. samples / sec: 21920.19
Iteration:   1960, Loss function: 3.974, Average Loss: 4.253, avg. samples / sec: 21924.69
Iteration:   1960, Loss function: 4.177, Average Loss: 4.252, avg. samples / sec: 21921.75
Iteration:   1960, Loss function: 4.208, Average Loss: 4.266, avg. samples / sec: 21921.48
Iteration:   1960, Loss function: 4.118, Average Loss: 4.259, avg. samples / sec: 21920.31
Iteration:   1960, Loss function: 4.430, Average Loss: 4.259, avg. samples / sec: 21921.59

:::MLPv0.5.0 ssd 1541711081.770074606 (train.py:553) train_epoch: 34
Iteration:   1980, Loss function: 4.550, Average Loss: 4.265, avg. samples / sec: 21949.86
Iteration:   1980, Loss function: 3.750, Average Loss: 4.269, avg. samples / sec: 21955.90
Iteration:   1980, Loss function: 4.181, Average Loss: 4.251, avg. samples / sec: 21954.96
Iteration:   1980, Loss function: 4.270, Average Loss: 4.256, avg. samples / sec: 21957.40
Iteration:   1980, Loss function: 4.112, Average Loss: 4.261, avg. samples / sec: 21941.27
Iteration:   1980, Loss function: 4.336, Average Loss: 4.266, avg. samples / sec: 21954.35
Iteration:   1980, Loss function: 4.313, Average Loss: 4.253, avg. samples / sec: 21951.41
Iteration:   1980, Loss function: 3.992, Average Loss: 4.259, avg. samples / sec: 21955.07
Iteration:   2000, Loss function: 4.582, Average Loss: 4.259, avg. samples / sec: 21956.04
Iteration:   2000, Loss function: 3.954, Average Loss: 4.265, avg. samples / sec: 21944.89
Iteration:   2000, Loss function: 3.797, Average Loss: 4.245, avg. samples / sec: 21946.18
Iteration:   2000, Loss function: 3.921, Average Loss: 4.260, avg. samples / sec: 21942.84
Iteration:   2000, Loss function: 3.315, Average Loss: 4.249, avg. samples / sec: 21951.66
Iteration:   2000, Loss function: 4.008, Average Loss: 4.253, avg. samples / sec: 21948.99
Iteration:   2000, Loss function: 3.968, Average Loss: 4.264, avg. samples / sec: 21947.41
Iteration:   2000, Loss function: 4.197, Average Loss: 4.256, avg. samples / sec: 21940.62
Iteration:   2020, Loss function: 4.398, Average Loss: 4.260, avg. samples / sec: 21946.19
Iteration:   2020, Loss function: 4.278, Average Loss: 4.261, avg. samples / sec: 21938.76
Iteration:   2020, Loss function: 3.838, Average Loss: 4.258, avg. samples / sec: 21932.84
Iteration:   2020, Loss function: 4.654, Average Loss: 4.259, avg. samples / sec: 21938.00
Iteration:   2020, Loss function: 4.311, Average Loss: 4.252, avg. samples / sec: 21937.74
Iteration:   2020, Loss function: 3.798, Average Loss: 4.253, avg. samples / sec: 21940.80
Iteration:   2020, Loss function: 4.406, Average Loss: 4.241, avg. samples / sec: 21927.75
Iteration:   2020, Loss function: 4.482, Average Loss: 4.245, avg. samples / sec: 21927.26

:::MLPv0.5.0 ssd 1541711087.183994532 (train.py:553) train_epoch: 35
Iteration:   2040, Loss function: 4.170, Average Loss: 4.257, avg. samples / sec: 21947.44
Iteration:   2040, Loss function: 4.412, Average Loss: 4.257, avg. samples / sec: 21940.77
Iteration:   2040, Loss function: 4.488, Average Loss: 4.244, avg. samples / sec: 21954.43
Iteration:   2040, Loss function: 4.428, Average Loss: 4.250, avg. samples / sec: 21943.75
Iteration:   2040, Loss function: 5.155, Average Loss: 4.259, avg. samples / sec: 21940.12
Iteration:   2040, Loss function: 4.712, Average Loss: 4.260, avg. samples / sec: 21937.62
Iteration:   2040, Loss function: 4.835, Average Loss: 4.253, avg. samples / sec: 21944.43
Iteration:   2040, Loss function: 4.418, Average Loss: 4.241, avg. samples / sec: 21945.27
Iteration:   2060, Loss function: 4.031, Average Loss: 4.242, avg. samples / sec: 21867.29
Iteration:   2060, Loss function: 3.886, Average Loss: 4.251, avg. samples / sec: 21864.79
Iteration:   2060, Loss function: 4.005, Average Loss: 4.259, avg. samples / sec: 21864.64
Iteration:   2060, Loss function: 4.246, Average Loss: 4.255, avg. samples / sec: 21863.55
Iteration:   2060, Loss function: 3.767, Average Loss: 4.257, avg. samples / sec: 21853.36
Iteration:   2060, Loss function: 4.615, Average Loss: 4.257, avg. samples / sec: 21853.21
Iteration:   2060, Loss function: 5.173, Average Loss: 4.252, avg. samples / sec: 21854.47
Iteration:   2060, Loss function: 3.993, Average Loss: 4.242, avg. samples / sec: 21848.94

:::MLPv0.5.0 ssd 1541711092.607025623 (train.py:553) train_epoch: 36
Iteration:   2080, Loss function: 3.960, Average Loss: 4.254, avg. samples / sec: 21903.24
Iteration:   2080, Loss function: 4.444, Average Loss: 4.254, avg. samples / sec: 21899.31
Iteration:   2080, Loss function: 4.529, Average Loss: 4.238, avg. samples / sec: 21895.31
Iteration:   2080, Loss function: 4.145, Average Loss: 4.246, avg. samples / sec: 21899.81
Iteration:   2080, Loss function: 4.438, Average Loss: 4.248, avg. samples / sec: 21889.60
Iteration:   2080, Loss function: 4.353, Average Loss: 4.251, avg. samples / sec: 21888.62
Iteration:   2080, Loss function: 3.895, Average Loss: 4.256, avg. samples / sec: 21886.46
Iteration:   2080, Loss function: 4.154, Average Loss: 4.238, avg. samples / sec: 21891.88
Iteration:   2100, Loss function: 4.399, Average Loss: 4.252, avg. samples / sec: 21892.57
Iteration:   2100, Loss function: 4.069, Average Loss: 4.251, avg. samples / sec: 21894.66
Iteration:   2100, Loss function: 3.566, Average Loss: 4.242, avg. samples / sec: 21897.40
Iteration:   2100, Loss function: 4.331, Average Loss: 4.243, avg. samples / sec: 21897.79
Iteration:   2100, Loss function: 4.432, Average Loss: 4.238, avg. samples / sec: 21890.24
Iteration:   2100, Loss function: 4.155, Average Loss: 4.251, avg. samples / sec: 21897.00
Iteration:   2100, Loss function: 4.150, Average Loss: 4.253, avg. samples / sec: 21896.66
Iteration:   2100, Loss function: 4.206, Average Loss: 4.235, avg. samples / sec: 21895.80
Iteration:   2120, Loss function: 3.791, Average Loss: 4.249, avg. samples / sec: 21943.29
Iteration:   2120, Loss function: 4.003, Average Loss: 4.251, avg. samples / sec: 21935.95
Iteration:   2120, Loss function: 4.502, Average Loss: 4.235, avg. samples / sec: 21942.71
Iteration:   2120, Loss function: 4.262, Average Loss: 4.248, avg. samples / sec: 21945.19
Iteration:   2120, Loss function: 4.255, Average Loss: 4.237, avg. samples / sec: 21938.06
Iteration:   2120, Loss function: 3.610, Average Loss: 4.246, avg. samples / sec: 21935.69
Iteration:   2120, Loss function: 4.009, Average Loss: 4.229, avg. samples / sec: 21948.35
Iteration:   2120, Loss function: 3.827, Average Loss: 4.239, avg. samples / sec: 21930.60

:::MLPv0.5.0 ssd 1541711098.029454947 (train.py:553) train_epoch: 37
Iteration:   2140, Loss function: 4.543, Average Loss: 4.245, avg. samples / sec: 21895.36
Iteration:   2140, Loss function: 3.693, Average Loss: 4.245, avg. samples / sec: 21891.73
Iteration:   2140, Loss function: 4.147, Average Loss: 4.248, avg. samples / sec: 21889.87
Iteration:   2140, Loss function: 4.202, Average Loss: 4.233, avg. samples / sec: 21893.17
Iteration:   2140, Loss function: 4.241, Average Loss: 4.231, avg. samples / sec: 21890.51
Iteration:   2140, Loss function: 4.644, Average Loss: 4.227, avg. samples / sec: 21891.85
Iteration:   2140, Loss function: 4.261, Average Loss: 4.237, avg. samples / sec: 21893.36
Iteration:   2140, Loss function: 3.974, Average Loss: 4.243, avg. samples / sec: 21881.91
Iteration:   2160, Loss function: 3.598, Average Loss: 4.232, avg. samples / sec: 21833.98
Iteration:   2160, Loss function: 3.924, Average Loss: 4.244, avg. samples / sec: 21833.32
Iteration:   2160, Loss function: 3.413, Average Loss: 4.240, avg. samples / sec: 21829.15
Iteration:   2160, Loss function: 4.168, Average Loss: 4.239, avg. samples / sec: 21842.47
Iteration:   2160, Loss function: 4.391, Average Loss: 4.238, avg. samples / sec: 21822.14
Iteration:   2160, Loss function: 3.869, Average Loss: 4.228, avg. samples / sec: 21824.10
Iteration:   2160, Loss function: 3.560, Average Loss: 4.232, avg. samples / sec: 21831.91
Iteration:   2160, Loss function: 3.576, Average Loss: 4.223, avg. samples / sec: 21825.55
Iteration:   2180, Loss function: 3.589, Average Loss: 4.226, avg. samples / sec: 21936.09
Iteration:   2180, Loss function: 4.371, Average Loss: 4.239, avg. samples / sec: 21935.98
Iteration:   2180, Loss function: 4.102, Average Loss: 4.221, avg. samples / sec: 21946.01
Iteration:   2180, Loss function: 3.996, Average Loss: 4.236, avg. samples / sec: 21933.21
Iteration:   2180, Loss function: 3.796, Average Loss: 4.225, avg. samples / sec: 21940.43
Iteration:   2180, Loss function: 4.671, Average Loss: 4.236, avg. samples / sec: 21935.33
Iteration:   2180, Loss function: 4.181, Average Loss: 4.233, avg. samples / sec: 21937.92
Iteration:   2180, Loss function: 3.561, Average Loss: 4.240, avg. samples / sec: 21927.97

:::MLPv0.5.0 ssd 1541711103.362656593 (train.py:553) train_epoch: 38
Iteration:   2200, Loss function: 4.148, Average Loss: 4.236, avg. samples / sec: 21868.03
Iteration:   2200, Loss function: 4.333, Average Loss: 4.219, avg. samples / sec: 21867.63
Iteration:   2200, Loss function: 3.640, Average Loss: 4.223, avg. samples / sec: 21864.74
Iteration:   2200, Loss function: 3.606, Average Loss: 4.233, avg. samples / sec: 21869.33
Iteration:   2200, Loss function: 3.419, Average Loss: 4.230, avg. samples / sec: 21870.39
Iteration:   2200, Loss function: 4.538, Average Loss: 4.231, avg. samples / sec: 21871.37
Iteration:   2200, Loss function: 4.221, Average Loss: 4.223, avg. samples / sec: 21866.18
Iteration:   2200, Loss function: 4.791, Average Loss: 4.236, avg. samples / sec: 21869.59
Iteration:   2220, Loss function: 3.893, Average Loss: 4.229, avg. samples / sec: 21879.53
Iteration:   2220, Loss function: 4.190, Average Loss: 4.215, avg. samples / sec: 21874.98
Iteration:   2220, Loss function: 4.322, Average Loss: 4.230, avg. samples / sec: 21869.02
Iteration:   2220, Loss function: 4.226, Average Loss: 4.222, avg. samples / sec: 21875.58
Iteration:   2220, Loss function: 3.699, Average Loss: 4.229, avg. samples / sec: 21873.43
Iteration:   2220, Loss function: 4.210, Average Loss: 4.228, avg. samples / sec: 21867.06
Iteration:   2220, Loss function: 3.937, Average Loss: 4.227, avg. samples / sec: 21866.32
Iteration:   2220, Loss function: 4.571, Average Loss: 4.214, avg. samples / sec: 21859.54
Iteration:   2240, Loss function: 4.337, Average Loss: 4.227, avg. samples / sec: 21961.96
Iteration:   2240, Loss function: 4.190, Average Loss: 4.224, avg. samples / sec: 21969.79
Iteration:   2240, Loss function: 4.024, Average Loss: 4.229, avg. samples / sec: 21963.56
Iteration:   2240, Loss function: 4.326, Average Loss: 4.212, avg. samples / sec: 21959.11
Iteration:   2240, Loss function: 3.593, Average Loss: 4.222, avg. samples / sec: 21968.49
Iteration:   2240, Loss function: 4.004, Average Loss: 4.228, avg. samples / sec: 21965.13
Iteration:   2240, Loss function: 4.191, Average Loss: 4.218, avg. samples / sec: 21959.53
Iteration:   2240, Loss function: 4.080, Average Loss: 4.213, avg. samples / sec: 21970.86

:::MLPv0.5.0 ssd 1541711108.789747477 (train.py:553) train_epoch: 39
Iteration:   2260, Loss function: 4.200, Average Loss: 4.213, avg. samples / sec: 21883.00
Iteration:   2260, Loss function: 3.539, Average Loss: 4.226, avg. samples / sec: 21879.80
Iteration:   2260, Loss function: 4.328, Average Loss: 4.223, avg. samples / sec: 21876.09
Iteration:   2260, Loss function: 4.192, Average Loss: 4.211, avg. samples / sec: 21875.02
Iteration:   2260, Loss function: 4.282, Average Loss: 4.223, avg. samples / sec: 21872.76
Iteration:   2260, Loss function: 4.699, Average Loss: 4.226, avg. samples / sec: 21863.58
Iteration:   2260, Loss function: 3.922, Average Loss: 4.227, avg. samples / sec: 21869.19
Iteration:   2260, Loss function: 4.103, Average Loss: 4.216, avg. samples / sec: 21867.41
Iteration:   2280, Loss function: 3.404, Average Loss: 4.224, avg. samples / sec: 21810.03
Iteration:   2280, Loss function: 4.033, Average Loss: 4.211, avg. samples / sec: 21793.64
Iteration:   2280, Loss function: 3.405, Average Loss: 4.221, avg. samples / sec: 21795.25
Iteration:   2280, Loss function: 4.287, Average Loss: 4.220, avg. samples / sec: 21796.46
Iteration:   2280, Loss function: 4.059, Average Loss: 4.214, avg. samples / sec: 21806.29
Iteration:   2280, Loss function: 3.885, Average Loss: 4.226, avg. samples / sec: 21795.63
Iteration:   2280, Loss function: 3.961, Average Loss: 4.221, avg. samples / sec: 21790.63
Iteration:   2280, Loss function: 3.813, Average Loss: 4.211, avg. samples / sec: 21786.94
Iteration:   2300, Loss function: 4.190, Average Loss: 4.219, avg. samples / sec: 21815.38
Iteration:   2300, Loss function: 3.616, Average Loss: 4.216, avg. samples / sec: 21823.31
Iteration:   2300, Loss function: 4.443, Average Loss: 4.212, avg. samples / sec: 21823.67
Iteration:   2300, Loss function: 4.550, Average Loss: 4.209, avg. samples / sec: 21819.66
Iteration:   2300, Loss function: 3.639, Average Loss: 4.221, avg. samples / sec: 21823.35
Iteration:   2300, Loss function: 4.262, Average Loss: 4.206, avg. samples / sec: 21826.53
Iteration:   2300, Loss function: 3.971, Average Loss: 4.217, avg. samples / sec: 21815.95
Iteration:   2300, Loss function: 4.071, Average Loss: 4.213, avg. samples / sec: 21817.27

:::MLPv0.5.0 ssd 1541711114.234578609 (train.py:553) train_epoch: 40
Iteration:   2320, Loss function: 4.001, Average Loss: 4.216, avg. samples / sec: 21831.07
Iteration:   2320, Loss function: 3.706, Average Loss: 4.216, avg. samples / sec: 21833.23
Iteration:   2320, Loss function: 3.843, Average Loss: 4.203, avg. samples / sec: 21829.17
Iteration:   2320, Loss function: 3.940, Average Loss: 4.212, avg. samples / sec: 21822.17
Iteration:   2320, Loss function: 3.909, Average Loss: 4.215, avg. samples / sec: 21827.99
Iteration:   2320, Loss function: 4.395, Average Loss: 4.212, avg. samples / sec: 21833.71
Iteration:   2320, Loss function: 4.281, Average Loss: 4.207, avg. samples / sec: 21824.15
Iteration:   2320, Loss function: 3.722, Average Loss: 4.211, avg. samples / sec: 21816.06
Iteration:   2340, Loss function: 3.620, Average Loss: 4.209, avg. samples / sec: 21829.19
Iteration:   2340, Loss function: 4.042, Average Loss: 4.201, avg. samples / sec: 21821.77
Iteration:   2340, Loss function: 4.084, Average Loss: 4.211, avg. samples / sec: 21812.28
Iteration:   2340, Loss function: 3.874, Average Loss: 4.207, avg. samples / sec: 21820.94
Iteration:   2340, Loss function: 4.208, Average Loss: 4.214, avg. samples / sec: 21812.43
Iteration:   2340, Loss function: 3.896, Average Loss: 4.209, avg. samples / sec: 21817.75
Iteration:   2340, Loss function: 4.470, Average Loss: 4.205, avg. samples / sec: 21816.75
Iteration:   2340, Loss function: 3.820, Average Loss: 4.215, avg. samples / sec: 21810.52
Iteration:   2360, Loss function: 4.427, Average Loss: 4.209, avg. samples / sec: 21845.67
Iteration:   2360, Loss function: 3.762, Average Loss: 4.196, avg. samples / sec: 21845.56
Iteration:   2360, Loss function: 3.698, Average Loss: 4.213, avg. samples / sec: 21850.05
Iteration:   2360, Loss function: 4.136, Average Loss: 4.205, avg. samples / sec: 21841.47
Iteration:   2360, Loss function: 3.846, Average Loss: 4.206, avg. samples / sec: 21849.10
Iteration:   2360, Loss function: 4.025, Average Loss: 4.205, avg. samples / sec: 21844.04
Iteration:   2360, Loss function: 3.753, Average Loss: 4.203, avg. samples / sec: 21844.20
Iteration:   2360, Loss function: 3.661, Average Loss: 4.210, avg. samples / sec: 21845.85

:::MLPv0.5.0 ssd 1541711119.677979469 (train.py:553) train_epoch: 41
Iteration:   2380, Loss function: 4.000, Average Loss: 4.209, avg. samples / sec: 21731.64
Iteration:   2380, Loss function: 3.841, Average Loss: 4.205, avg. samples / sec: 21731.57
Iteration:   2380, Loss function: 3.821, Average Loss: 4.202, avg. samples / sec: 21734.29
Iteration:   2380, Loss function: 3.783, Average Loss: 4.200, avg. samples / sec: 21733.90
Iteration:   2380, Loss function: 3.603, Average Loss: 4.204, avg. samples / sec: 21739.69
Iteration:   2380, Loss function: 3.964, Average Loss: 4.199, avg. samples / sec: 21729.82
Iteration:   2380, Loss function: 3.695, Average Loss: 4.199, avg. samples / sec: 21727.89
Iteration:   2380, Loss function: 3.753, Average Loss: 4.194, avg. samples / sec: 21715.93
Iteration:   2400, Loss function: 4.331, Average Loss: 4.196, avg. samples / sec: 21828.53
Iteration:   2400, Loss function: 4.222, Average Loss: 4.195, avg. samples / sec: 21818.15
Iteration:   2400, Loss function: 3.677, Average Loss: 4.199, avg. samples / sec: 21818.01
Iteration:   2400, Loss function: 4.295, Average Loss: 4.203, avg. samples / sec: 21811.57
Iteration:   2400, Loss function: 3.585, Average Loss: 4.194, avg. samples / sec: 21819.40
Iteration:   2400, Loss function: 4.776, Average Loss: 4.209, avg. samples / sec: 21810.03
Iteration:   2400, Loss function: 4.079, Average Loss: 4.199, avg. samples / sec: 21812.84
Iteration:   2400, Loss function: 3.906, Average Loss: 4.189, avg. samples / sec: 21820.05
Iteration:   2420, Loss function: 4.177, Average Loss: 4.206, avg. samples / sec: 21891.25
Iteration:   2420, Loss function: 4.141, Average Loss: 4.190, avg. samples / sec: 21881.45
Iteration:   2420, Loss function: 4.650, Average Loss: 4.195, avg. samples / sec: 21891.20
Iteration:   2420, Loss function: 4.170, Average Loss: 4.196, avg. samples / sec: 21880.60
Iteration:   2420, Loss function: 4.639, Average Loss: 4.201, avg. samples / sec: 21885.93
Iteration:   2420, Loss function: 4.083, Average Loss: 4.190, avg. samples / sec: 21876.12
Iteration:   2420, Loss function: 3.815, Average Loss: 4.190, avg. samples / sec: 21880.03
Iteration:   2420, Loss function: 4.051, Average Loss: 4.187, avg. samples / sec: 21889.46

:::MLPv0.5.0 ssd 1541711125.029859066 (train.py:553) train_epoch: 42
Iteration:   2440, Loss function: 4.078, Average Loss: 4.193, avg. samples / sec: 21806.82
Iteration:   2440, Loss function: 4.697, Average Loss: 4.189, avg. samples / sec: 21803.02
Iteration:   2440, Loss function: 4.236, Average Loss: 4.189, avg. samples / sec: 21809.11
Iteration:   2440, Loss function: 4.118, Average Loss: 4.196, avg. samples / sec: 21803.50
Iteration:   2440, Loss function: 4.171, Average Loss: 4.183, avg. samples / sec: 21807.03
Iteration:   2440, Loss function: 3.825, Average Loss: 4.191, avg. samples / sec: 21802.53
Iteration:   2440, Loss function: 3.757, Average Loss: 4.205, avg. samples / sec: 21797.77
Iteration:   2440, Loss function: 3.571, Average Loss: 4.186, avg. samples / sec: 21796.08
Iteration:   2460, Loss function: 3.945, Average Loss: 4.190, avg. samples / sec: 21911.02
Iteration:   2460, Loss function: 4.111, Average Loss: 4.202, avg. samples / sec: 21917.15
Iteration:   2460, Loss function: 3.559, Average Loss: 4.188, avg. samples / sec: 21911.08
Iteration:   2460, Loss function: 3.903, Average Loss: 4.195, avg. samples / sec: 21911.41
Iteration:   2460, Loss function: 3.879, Average Loss: 4.185, avg. samples / sec: 21910.59
Iteration:   2460, Loss function: 3.607, Average Loss: 4.181, avg. samples / sec: 21906.95
Iteration:   2460, Loss function: 3.846, Average Loss: 4.181, avg. samples / sec: 21915.29
Iteration:   2460, Loss function: 4.445, Average Loss: 4.190, avg. samples / sec: 21901.94
Iteration:   2480, Loss function: 3.901, Average Loss: 4.188, avg. samples / sec: 21926.55
Iteration:   2480, Loss function: 4.566, Average Loss: 4.190, avg. samples / sec: 21923.06
Iteration:   2480, Loss function: 3.915, Average Loss: 4.200, avg. samples / sec: 21917.89
Iteration:   2480, Loss function: 4.102, Average Loss: 4.177, avg. samples / sec: 21927.44
Iteration:   2480, Loss function: 3.997, Average Loss: 4.188, avg. samples / sec: 21928.49
Iteration:   2480, Loss function: 4.214, Average Loss: 4.180, avg. samples / sec: 21925.49
Iteration:   2480, Loss function: 4.294, Average Loss: 4.183, avg. samples / sec: 21919.24
Iteration:   2480, Loss function: 3.946, Average Loss: 4.188, avg. samples / sec: 21908.37

:::MLPv0.5.0 ssd 1541711130.458112240 (train.py:553) train_epoch: 43
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1

:::MLPv0.5.0 ssd 1541711131.962466717 (train.py:578) opt_learning_rate: 0.016
Iteration:   2500, Loss function: 3.921, Average Loss: 4.180, avg. samples / sec: 21818.16
Iteration:   2500, Loss function: 4.065, Average Loss: 4.187, avg. samples / sec: 21810.75
Iteration:   2500, Loss function: 3.333, Average Loss: 4.196, avg. samples / sec: 21809.48
Iteration:   2500, Loss function: 3.974, Average Loss: 4.184, avg. samples / sec: 21803.22
Iteration:   2500, Loss function: 4.059, Average Loss: 4.184, avg. samples / sec: 21816.28
Iteration:   2500, Loss function: 3.787, Average Loss: 4.173, avg. samples / sec: 21802.52
Iteration:   2500, Loss function: 4.142, Average Loss: 4.174, avg. samples / sec: 21801.59
Iteration:   2500, Loss function: 4.403, Average Loss: 4.187, avg. samples / sec: 21758.99

































































:::MLPv0.5.0 ssd 1541711132.055196047 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711132.056026697 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711132.056778908 (train.py:220) eval_start: 43
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.36 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.36 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.36 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.36 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.36 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.36 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.36 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.36 s
Loading and preparing results...
Converting ndarray to lists...
(296124, 7)
0/296124
Loading and preparing results...
Converting ndarray to lists...
(296124, 7)
Loading and preparing results...
0/296124
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(296124, 7)
Converting ndarray to lists...
0/296124
Converting ndarray to lists...
Converting ndarray to lists...
(296124, 7)
(296124, 7)
(296124, 7)
0/296124
0/296124
0/296124
Loading and preparing results...
Converting ndarray to lists...
(296124, 7)
Loading and preparing results...
0/296124
Converting ndarray to lists...
(296124, 7)
0/296124
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(296124, 7)
(296124, 7)
0/296124
0/296124
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(296124, 7)
Loading and preparing results...
(296124, 7)
Converting ndarray to lists...
0/296124
Converting ndarray to lists...
0/296124
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(296124, 7)
(296124, 7)
(296124, 7)
0/296124
(296124, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/296124
Converting ndarray to lists...
0/296124
Converting ndarray to lists...
0/296124
Loading and preparing results...
(296124, 7)
(296124, 7)
0/296124
(296124, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/296124
Converting ndarray to lists...
0/296124
Loading and preparing results...
(296124, 7)
Converting ndarray to lists...
Loading and preparing results...
(296124, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(296124, 7)
Converting ndarray to lists...
0/296124
Converting ndarray to lists...
0/296124
Loading and preparing results...
(296124, 7)
(296124, 7)
(296124, 7)
0/296124
0/296124
Converting ndarray to lists...
0/296124
(296124, 7)
Loading and preparing results...
0/296124
Converting ndarray to lists...
(296124, 7)
Loading and preparing results...
0/296124
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(296124, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/296124
Converting ndarray to lists...
(296124, 7)
Loading and preparing results...
Converting ndarray to lists...
0/296124
(296124, 7)
Loading and preparing results...
(296124, 7)
0/296124
Loading and preparing results...
0/296124
0/296124
(296124, 7)
Converting ndarray to lists...
Loading and preparing results...
(296124, 7)
(296124, 7)
Loading and preparing results...
0/296124
(296124, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/296124
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/296124
Converting ndarray to lists...
0/296124
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(296124, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(296124, 7)
0/296124
(296124, 7)
Loading and preparing results...
Loading and preparing results...
(296124, 7)
(296124, 7)
0/296124
Converting ndarray to lists...
Converting ndarray to lists...
(296124, 7)
(296124, 7)
(296124, 7)
Loading and preparing results...
0/296124
(296124, 7)
0/296124
(296124, 7)
Converting ndarray to lists...
0/296124
0/296124
Loading and preparing results...
0/296124
Loading and preparing results...
Converting ndarray to lists...
0/296124
Loading and preparing results...
0/296124
0/296124
Converting ndarray to lists...
(296124, 7)
Loading and preparing results...
(296124, 7)
(296124, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/296124
Loading and preparing results...
0/296124
Loading and preparing results...
(296124, 7)
0/296124
Loading and preparing results...
0/296124
(296124, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(296124, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/296124
0/296124
(296124, 7)
(296124, 7)
Converting ndarray to lists...
0/296124
(296124, 7)
(296124, 7)
Loading and preparing results...
(296124, 7)
0/296124
Converting ndarray to lists...
0/296124
(296124, 7)
Converting ndarray to lists...
(296124, 7)
Loading and preparing results...
0/296124
(296124, 7)
0/296124
Converting ndarray to lists...
0/296124
(296124, 7)
0/296124
0/296124
0/296124
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(296124, 7)
Loading and preparing results...
(296124, 7)
Converting ndarray to lists...
0/296124
0/296124
Converting ndarray to lists...
(296124, 7)
(296124, 7)
0/296124
0/296124
DONE (t=1.67s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.75s)
creating index...
index created!
DONE (t=1.80s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
DONE (t=1.94s)
creating index...
index created!
DONE (t=1.95s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.02s)
creating index...
index created!
DONE (t=2.07s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.11s)
creating index...
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=3.45s).
Accumulating evaluation results...
DONE (t=3.45s).
Accumulating evaluation results...
DONE (t=3.43s).
Accumulating evaluation results...
DONE (t=3.44s).
Accumulating evaluation results...
DONE (t=3.47s).
Accumulating evaluation results...
DONE (t=3.77s).
Accumulating evaluation results...
DONE (t=3.78s).
Accumulating evaluation results...
DONE (t=3.82s).
Accumulating evaluation results...
DONE (t=1.11s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.285
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
DONE (t=1.14s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.15249 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.285
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.15249 AP goal: 0.21200
DONE (t=1.12s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.285
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
DONE (t=1.14s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.285
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.15249 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
DONE (t=1.14s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.245
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
DONE (t=1.14s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.15249 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.285
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
DONE (t=1.15s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.285
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.285
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.245
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.150
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.15249 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.245
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
DONE (t=1.14s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.15249 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.152
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.15249 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.285
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.241
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.389
Current AP: 0.15249 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711144.406735659 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711144.407665730 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.15248752504733515}

:::MLPv0.5.0 ssd 1541711144.408426285 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.15248752504733515}

:::MLPv0.5.0 ssd 1541711144.409180641 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711144.409956932 (train.py:338) eval_stop: 43
Iteration:   2520, Loss function: 3.411, Average Loss: 4.187, avg. samples / sec: 2786.70
Iteration:   2520, Loss function: 4.002, Average Loss: 4.164, avg. samples / sec: 2786.84
Iteration:   2520, Loss function: 3.481, Average Loss: 4.169, avg. samples / sec: 2786.50
Iteration:   2520, Loss function: 4.110, Average Loss: 4.174, avg. samples / sec: 2786.67
Iteration:   2520, Loss function: 3.791, Average Loss: 4.163, avg. samples / sec: 2786.74
Iteration:   2520, Loss function: 3.545, Average Loss: 4.177, avg. samples / sec: 2786.61
Iteration:   2520, Loss function: 3.366, Average Loss: 4.178, avg. samples / sec: 2786.50
Iteration:   2520, Loss function: 3.220, Average Loss: 4.177, avg. samples / sec: 2787.33
Iteration:   2540, Loss function: 3.319, Average Loss: 4.161, avg. samples / sec: 21843.25
Iteration:   2540, Loss function: 3.281, Average Loss: 4.155, avg. samples / sec: 21839.32
Iteration:   2540, Loss function: 3.389, Average Loss: 4.165, avg. samples / sec: 21840.82
Iteration:   2540, Loss function: 3.231, Average Loss: 4.150, avg. samples / sec: 21833.12
Iteration:   2540, Loss function: 3.797, Average Loss: 4.164, avg. samples / sec: 21845.47
Iteration:   2540, Loss function: 3.696, Average Loss: 4.148, avg. samples / sec: 21836.43
Iteration:   2540, Loss function: 3.152, Average Loss: 4.164, avg. samples / sec: 21838.99
Iteration:   2540, Loss function: 3.406, Average Loss: 4.174, avg. samples / sec: 21826.68

:::MLPv0.5.0 ssd 1541711148.720659256 (train.py:553) train_epoch: 44
Iteration:   2560, Loss function: 3.804, Average Loss: 4.136, avg. samples / sec: 21950.65
Iteration:   2560, Loss function: 3.040, Average Loss: 4.150, avg. samples / sec: 21949.69
Iteration:   2560, Loss function: 3.390, Average Loss: 4.146, avg. samples / sec: 21939.31
Iteration:   2560, Loss function: 3.089, Average Loss: 4.141, avg. samples / sec: 21942.23
Iteration:   2560, Loss function: 3.588, Average Loss: 4.161, avg. samples / sec: 21946.75
Iteration:   2560, Loss function: 3.355, Average Loss: 4.151, avg. samples / sec: 21939.22
Iteration:   2560, Loss function: 3.769, Average Loss: 4.150, avg. samples / sec: 21943.21
Iteration:   2560, Loss function: 4.248, Average Loss: 4.136, avg. samples / sec: 21937.18
Iteration:   2580, Loss function: 3.327, Average Loss: 4.122, avg. samples / sec: 21876.31
Iteration:   2580, Loss function: 3.529, Average Loss: 4.130, avg. samples / sec: 21878.07
Iteration:   2580, Loss function: 3.808, Average Loss: 4.135, avg. samples / sec: 21871.69
Iteration:   2580, Loss function: 3.220, Average Loss: 4.119, avg. samples / sec: 21882.30
Iteration:   2580, Loss function: 3.987, Average Loss: 4.134, avg. samples / sec: 21876.50
Iteration:   2580, Loss function: 3.082, Average Loss: 4.147, avg. samples / sec: 21873.56
Iteration:   2580, Loss function: 3.322, Average Loss: 4.125, avg. samples / sec: 21871.54
Iteration:   2580, Loss function: 3.409, Average Loss: 4.138, avg. samples / sec: 21869.30

:::MLPv0.5.0 ssd 1541711154.146360874 (train.py:553) train_epoch: 45
Iteration:   2600, Loss function: 3.545, Average Loss: 4.114, avg. samples / sec: 21852.99
Iteration:   2600, Loss function: 3.045, Average Loss: 4.104, avg. samples / sec: 21854.06
Iteration:   2600, Loss function: 3.522, Average Loss: 4.111, avg. samples / sec: 21858.12
Iteration:   2600, Loss function: 3.734, Average Loss: 4.106, avg. samples / sec: 21847.74
Iteration:   2600, Loss function: 3.149, Average Loss: 4.124, avg. samples / sec: 21861.85
Iteration:   2600, Loss function: 3.588, Average Loss: 4.118, avg. samples / sec: 21855.57
Iteration:   2600, Loss function: 3.135, Average Loss: 4.132, avg. samples / sec: 21853.29
Iteration:   2600, Loss function: 3.419, Average Loss: 4.119, avg. samples / sec: 21847.97
Iteration:   2620, Loss function: 3.297, Average Loss: 4.099, avg. samples / sec: 21885.33
Iteration:   2620, Loss function: 3.035, Average Loss: 4.113, avg. samples / sec: 21888.31
Iteration:   2620, Loss function: 3.022, Average Loss: 4.098, avg. samples / sec: 21883.06
Iteration:   2620, Loss function: 3.341, Average Loss: 4.091, avg. samples / sec: 21882.53
Iteration:   2620, Loss function: 3.380, Average Loss: 4.103, avg. samples / sec: 21887.73
Iteration:   2620, Loss function: 3.237, Average Loss: 4.104, avg. samples / sec: 21880.78
Iteration:   2620, Loss function: 3.338, Average Loss: 4.091, avg. samples / sec: 21877.44
Iteration:   2620, Loss function: 3.010, Average Loss: 4.108, avg. samples / sec: 21872.56
Iteration:   2640, Loss function: 3.057, Average Loss: 4.075, avg. samples / sec: 21877.65
Iteration:   2640, Loss function: 3.209, Average Loss: 4.082, avg. samples / sec: 21864.60
Iteration:   2640, Loss function: 3.451, Average Loss: 4.085, avg. samples / sec: 21870.62
Iteration:   2640, Loss function: 3.089, Average Loss: 4.093, avg. samples / sec: 21880.77
Iteration:   2640, Loss function: 3.304, Average Loss: 4.097, avg. samples / sec: 21864.72
Iteration:   2640, Loss function: 2.980, Average Loss: 4.086, avg. samples / sec: 21864.94
Iteration:   2640, Loss function: 3.450, Average Loss: 4.089, avg. samples / sec: 21866.87
Iteration:   2640, Loss function: 3.538, Average Loss: 4.076, avg. samples / sec: 21859.61

:::MLPv0.5.0 ssd 1541711159.483995199 (train.py:553) train_epoch: 46
Iteration:   2660, Loss function: 3.116, Average Loss: 4.061, avg. samples / sec: 21892.88
Iteration:   2660, Loss function: 3.568, Average Loss: 4.072, avg. samples / sec: 21896.88
Iteration:   2660, Loss function: 3.388, Average Loss: 4.082, avg. samples / sec: 21893.61
Iteration:   2660, Loss function: 3.273, Average Loss: 4.074, avg. samples / sec: 21895.02
Iteration:   2660, Loss function: 3.224, Average Loss: 4.075, avg. samples / sec: 21890.67
Iteration:   2660, Loss function: 3.408, Average Loss: 4.070, avg. samples / sec: 21886.10
Iteration:   2660, Loss function: 3.832, Average Loss: 4.063, avg. samples / sec: 21895.07
Iteration:   2660, Loss function: 2.686, Average Loss: 4.065, avg. samples / sec: 21864.35
Iteration:   2680, Loss function: 2.563, Average Loss: 4.058, avg. samples / sec: 21866.16
Iteration:   2680, Loss function: 2.930, Average Loss: 4.047, avg. samples / sec: 21857.23
Iteration:   2680, Loss function: 3.077, Average Loss: 4.050, avg. samples / sec: 21885.76
Iteration:   2680, Loss function: 3.444, Average Loss: 4.060, avg. samples / sec: 21856.86
Iteration:   2680, Loss function: 3.626, Average Loss: 4.065, avg. samples / sec: 21856.45
Iteration:   2680, Loss function: 3.499, Average Loss: 4.060, avg. samples / sec: 21857.19
Iteration:   2680, Loss function: 3.523, Average Loss: 4.056, avg. samples / sec: 21858.52
Iteration:   2680, Loss function: 3.659, Average Loss: 4.049, avg. samples / sec: 21857.06
Iteration:   2700, Loss function: 3.375, Average Loss: 4.034, avg. samples / sec: 21881.69
Iteration:   2700, Loss function: 3.121, Average Loss: 4.051, avg. samples / sec: 21873.14
Iteration:   2700, Loss function: 2.426, Average Loss: 4.034, avg. samples / sec: 21868.26
Iteration:   2700, Loss function: 3.473, Average Loss: 4.047, avg. samples / sec: 21870.80
Iteration:   2700, Loss function: 3.389, Average Loss: 4.044, avg. samples / sec: 21867.76
Iteration:   2700, Loss function: 3.040, Average Loss: 4.041, avg. samples / sec: 21870.57
Iteration:   2700, Loss function: 3.595, Average Loss: 4.044, avg. samples / sec: 21857.00
Iteration:   2700, Loss function: 3.229, Average Loss: 4.032, avg. samples / sec: 21860.12

:::MLPv0.5.0 ssd 1541711164.912314892 (train.py:553) train_epoch: 47
Iteration:   2720, Loss function: 3.091, Average Loss: 4.027, avg. samples / sec: 21899.21
Iteration:   2720, Loss function: 3.358, Average Loss: 4.019, avg. samples / sec: 21901.98
Iteration:   2720, Loss function: 3.228, Average Loss: 4.032, avg. samples / sec: 21895.04
Iteration:   2720, Loss function: 2.858, Average Loss: 4.019, avg. samples / sec: 21889.16
Iteration:   2720, Loss function: 3.410, Average Loss: 4.020, avg. samples / sec: 21888.20
Iteration:   2720, Loss function: 3.430, Average Loss: 4.024, avg. samples / sec: 21890.89
Iteration:   2720, Loss function: 3.304, Average Loss: 4.034, avg. samples / sec: 21876.10
Iteration:   2720, Loss function: 3.118, Average Loss: 4.027, avg. samples / sec: 21880.24
Iteration:   2740, Loss function: 3.760, Average Loss: 4.008, avg. samples / sec: 21968.31
Iteration:   2740, Loss function: 2.932, Average Loss: 4.014, avg. samples / sec: 21981.13
Iteration:   2740, Loss function: 3.321, Average Loss: 4.007, avg. samples / sec: 21959.53
Iteration:   2740, Loss function: 2.803, Average Loss: 4.020, avg. samples / sec: 21974.34
Iteration:   2740, Loss function: 3.256, Average Loss: 4.006, avg. samples / sec: 21958.92
Iteration:   2740, Loss function: 3.243, Average Loss: 4.013, avg. samples / sec: 21953.44
Iteration:   2740, Loss function: 3.458, Average Loss: 4.017, avg. samples / sec: 21952.30
Iteration:   2740, Loss function: 3.432, Average Loss: 4.007, avg. samples / sec: 21950.23
Iteration:   2760, Loss function: 3.693, Average Loss: 3.996, avg. samples / sec: 21943.48
Iteration:   2760, Loss function: 2.914, Average Loss: 3.998, avg. samples / sec: 21941.27
Iteration:   2760, Loss function: 3.271, Average Loss: 3.994, avg. samples / sec: 21934.34
Iteration:   2760, Loss function: 3.195, Average Loss: 3.994, avg. samples / sec: 21936.11
Iteration:   2760, Loss function: 3.560, Average Loss: 3.999, avg. samples / sec: 21930.87
Iteration:   2760, Loss function: 3.239, Average Loss: 4.001, avg. samples / sec: 21938.20
Iteration:   2760, Loss function: 3.159, Average Loss: 4.004, avg. samples / sec: 21932.29
Iteration:   2760, Loss function: 3.556, Average Loss: 3.992, avg. samples / sec: 21940.70

:::MLPv0.5.0 ssd 1541711170.333400726 (train.py:553) train_epoch: 48
Iteration:   2780, Loss function: 3.500, Average Loss: 3.988, avg. samples / sec: 21856.48
Iteration:   2780, Loss function: 3.148, Average Loss: 3.984, avg. samples / sec: 21849.15
Iteration:   2780, Loss function: 3.099, Average Loss: 3.983, avg. samples / sec: 21842.61
Iteration:   2780, Loss function: 3.766, Average Loss: 3.979, avg. samples / sec: 21851.38
Iteration:   2780, Loss function: 3.124, Average Loss: 3.975, avg. samples / sec: 21855.38
Iteration:   2780, Loss function: 3.595, Average Loss: 3.980, avg. samples / sec: 21842.17
Iteration:   2780, Loss function: 3.454, Average Loss: 3.986, avg. samples / sec: 21847.41
Iteration:   2780, Loss function: 3.781, Average Loss: 3.984, avg. samples / sec: 21840.15
Iteration:   2800, Loss function: 3.509, Average Loss: 3.974, avg. samples / sec: 21894.45
Iteration:   2800, Loss function: 3.672, Average Loss: 3.962, avg. samples / sec: 21893.82
Iteration:   2800, Loss function: 3.040, Average Loss: 3.968, avg. samples / sec: 21889.65
Iteration:   2800, Loss function: 2.956, Average Loss: 3.963, avg. samples / sec: 21888.58
Iteration:   2800, Loss function: 3.128, Average Loss: 3.970, avg. samples / sec: 21886.75
Iteration:   2800, Loss function: 3.169, Average Loss: 3.968, avg. samples / sec: 21899.22
Iteration:   2800, Loss function: 3.114, Average Loss: 3.968, avg. samples / sec: 21892.55
Iteration:   2800, Loss function: 2.883, Average Loss: 3.964, avg. samples / sec: 21890.90

































































:::MLPv0.5.0 ssd 1541711174.075145483 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711174.075977325 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711174.078527212 (train.py:220) eval_start: 48
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.33 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.33 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.33 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.33 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.33 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.33 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.33 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.33 s
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(300334, 7)
Loading and preparing results...
Converting ndarray to lists...
0/300334
Loading and preparing results...
Converting ndarray to lists...
(300334, 7)
Converting ndarray to lists...
0/300334
(300334, 7)
(300334, 7)
0/300334
0/300334
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(300334, 7)
Loading and preparing results...
0/300334
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(300334, 7)
(300334, 7)
0/300334
0/300334
(300334, 7)
Loading and preparing results...
0/300334
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(300334, 7)
(300334, 7)
0/300334
0/300334
Loading and preparing results...
Converting ndarray to lists...
(300334, 7)
0/300334
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(300334, 7)
Loading and preparing results...
(300334, 7)
Loading and preparing results...
0/300334
0/300334
Converting ndarray to lists...
Converting ndarray to lists...
(300334, 7)
(300334, 7)
0/300334
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(300334, 7)
Converting ndarray to lists...
0/300334
(300334, 7)
0/300334
0/300334
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(300334, 7)
(300334, 7)
0/300334
0/300334
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(300334, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(300334, 7)
0/300334
(300334, 7)
0/300334
0/300334
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(300334, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(300334, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(300334, 7)
0/300334
Converting ndarray to lists...
Loading and preparing results...
(300334, 7)
Converting ndarray to lists...
0/300334
0/300334
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(300334, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(300334, 7)
Converting ndarray to lists...
(300334, 7)
0/300334
(300334, 7)
(300334, 7)
(300334, 7)
(300334, 7)
Loading and preparing results...
Loading and preparing results...
(300334, 7)
0/300334
(300334, 7)
Loading and preparing results...
0/300334
0/300334
Converting ndarray to lists...
0/300334
0/300334
Loading and preparing results...
0/300334
Loading and preparing results...
0/300334
0/300334
(300334, 7)
Loading and preparing results...
0/300334
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/300334
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(300334, 7)
Loading and preparing results...
Converting ndarray to lists...
(300334, 7)
(300334, 7)
Loading and preparing results...
Converting ndarray to lists...
0/300334
Loading and preparing results...
0/300334
(300334, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/300334
(300334, 7)
Loading and preparing results...
Loading and preparing results...
(300334, 7)
0/300334
(300334, 7)
(300334, 7)
Loading and preparing results...
Converting ndarray to lists...
(300334, 7)
(300334, 7)
0/300334
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/300334
Converting ndarray to lists...
Converting ndarray to lists...
0/300334
(300334, 7)
0/300334
Loading and preparing results...
0/300334
Loading and preparing results...
0/300334
Loading and preparing results...
(300334, 7)
Converting ndarray to lists...
(300334, 7)
(300334, 7)
0/300334
(300334, 7)
(300334, 7)
0/300334
Loading and preparing results...
Converting ndarray to lists...
0/300334
0/300334
Converting ndarray to lists...
0/300334
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(300334, 7)
(300334, 7)
0/300334
Loading and preparing results...
(300334, 7)
Converting ndarray to lists...
0/300334
(300334, 7)
0/300334
Converting ndarray to lists...
0/300334
(300334, 7)
Loading and preparing results...
0/300334
Converting ndarray to lists...
0/300334
(300334, 7)
(300334, 7)
0/300334
Loading and preparing results...
0/300334
Converting ndarray to lists...
Loading and preparing results...
(300334, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/300334
(300334, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(300334, 7)
0/300334
(300334, 7)
(300334, 7)
0/300334
0/300334
0/300334
DONE (t=1.52s)
creating index...
DONE (t=1.53s)
creating index...
DONE (t=1.54s)
creating index...
DONE (t=1.55s)
creating index...
DONE (t=1.56s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.84s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.89s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.91s)
creating index...
DONE (t=1.91s)
creating index...
DONE (t=1.92s)
creating index...
DONE (t=1.92s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
index created!
index created!
DONE (t=1.95s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=1.98s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=2.07s)
creating index...
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.51s).
Accumulating evaluation results...
DONE (t=3.55s).
Accumulating evaluation results...
DONE (t=3.55s).
Accumulating evaluation results...
DONE (t=3.55s).
Accumulating evaluation results...
DONE (t=3.54s).
Accumulating evaluation results...
DONE (t=3.57s).
Accumulating evaluation results...
DONE (t=3.57s).
Accumulating evaluation results...
DONE (t=3.57s).
Accumulating evaluation results...
DONE (t=1.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
DONE (t=1.08s).
DONE (t=1.11s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
DONE (t=1.13s).
DONE (t=1.11s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.054
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
DONE (t=1.13s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.493
Current AP: 0.21362 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.054
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.054
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.054
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.054
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.493
Current AP: 0.21362 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.493
Current AP: 0.21362 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.493
Current AP: 0.21362 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.054
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.493
Current AP: 0.21362 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.493
Current AP: 0.21362 AP goal: 0.21200
DONE (t=1.13s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.054
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.493
Current AP: 0.21362 AP goal: 0.21200
DONE (t=1.12s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.214
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.369
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.054
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.493
Current AP: 0.21362 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711186.367085457 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711186.367974758 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.2136153477029561}

:::MLPv0.5.0 ssd 1541711186.368787289 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.2136153477029561}

:::MLPv0.5.0 ssd 1541711186.369585752 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711186.370407820 (train.py:338) eval_stop: 48

:::MLPv0.5.0 ssd 1541711187.424550056 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1541711187.425266504 (train.py:707) run_final
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2018-11-08 09:06:32 PM
RESULT,OBJECT_DETECTION,,365,nvidia,2018-11-08 09:00:27 PM
ENDING TIMING RUN AT 2018-11-08 09:06:32 PM
RESULT,OBJECT_DETECTION,,365,nvidia,2018-11-08 09:00:27 PM
ENDING TIMING RUN AT 2018-11-08 09:06:32 PM
RESULT,OBJECT_DETECTION,,365,nvidia,2018-11-08 09:00:27 PM
ENDING TIMING RUN AT 2018-11-08 09:06:32 PM
RESULT,OBJECT_DETECTION,,365,nvidia,2018-11-08 09:00:27 PM
ENDING TIMING RUN AT 2018-11-08 09:06:32 PM
RESULT,OBJECT_DETECTION,,365,nvidia,2018-11-08 09:00:27 PM
ENDING TIMING RUN AT 2018-11-08 09:06:32 PM
RESULT,OBJECT_DETECTION,,365,nvidia,2018-11-08 09:00:27 PM
ENDING TIMING RUN AT 2018-11-08 09:06:32 PM
RESULT,OBJECT_DETECTION,,365,nvidia,2018-11-08 09:00:27 PM
ENDING TIMING RUN AT 2018-11-08 09:06:32 PM
RESULT,OBJECT_DETECTION,,365,nvidia,2018-11-08 09:00:27 PM
