Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3

:::MLPv0.5.0 ssd 1541710833.702833891 (<string>:1) run_clear_caches
Launching on node sc-sdgx-362
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-362
+ srun -N 1 -n 1 -w sc-sdgx-362 docker exec -e DGXSYSTEM=DGX1 -e MULTI_NODE= -e SLURM_JOB_ID=155383 -e SLURM_NTASKS_PER_NODE=8 cont_155383 ./run_and_time.sh
Run vars: id 155383 gpus 8 mparams 
STARTING TIMING RUN AT 2018-11-08 09:00:33 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 152 --warmup 300 --nhwc --pad-input
1 Using seed = 3662338089
2 Using seed = 3662338090
3 Using seed = 3662338091
0 Using seed = 3662338088
5 Using seed = 3662338093
4 Using seed = 3662338092
7 Using seed = 3662338095
6 Using seed = 3662338094

:::MLPv0.5.0 ssd 1541710845.176418781 (train.py:371) run_start

:::MLPv0.5.0 ssd 1541710845.177283049 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1541710845.178026199 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1541710845.178719759 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1541710845.179482222 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1541710845.232390642 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1541710845.234390974 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1541710845.236229658 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1541710845.237800360 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1541710845.239235163 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1541710845.240633011 (train.py:382) input_size: 300
loading annotations into memory...
Done (t=0.47s)
creating index...
index created!
Done (t=0.50s)
creating index...
Done (t=0.53s)
index created!
creating index...
Done (t=0.53s)
creating index...
Done (t=0.53s)
creating index...
Done (t=0.53s)
creating index...
Done (t=0.53s)
creating index...
Done (t=0.54s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
time_check a: 1541710846.241136551
time_check b: 1541710869.881092787

:::MLPv0.5.0 ssd 1541710871.832475901 (train.py:413) input_order

:::MLPv0.5.0 ssd 1541710871.834782839 (train.py:414) input_batch_size: 152

:::MLPv0.5.0 ssd 1541710875.825460911 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710875.826469898 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710875.883058786 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1541710876.340595245 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1541710876.342303514 (train.py:477) opt_learning_rate: 0.095

:::MLPv0.5.0 ssd 1541710876.343927383 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1541710876.345381021 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1541710876.346502066 (train.py:483) opt_learning_rate_warmup_steps: 300

:::MLPv0.5.0 ssd 1541710880.282756567 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710880.284758091 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710880.342184544 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss

:::MLPv0.5.0 ssd 1541710884.942265034 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1541710884.942929268 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1541710884.946581602 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 22.580, Average Loss: 0.023, avg. samples / sec: 10409.27

:::MLPv0.5.0 ssd 1541710888.172814846 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.0003166666666666734}

:::MLPv0.5.0 ssd 1541710888.963570595 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0006333333333333468}

:::MLPv0.5.0 ssd 1541710889.445339441 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0009500000000000064}

:::MLPv0.5.0 ssd 1541710889.923732758 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0012666666666666798}

:::MLPv0.5.0 ssd 1541710890.475799322 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0015833333333333394}

:::MLPv0.5.0 ssd 1541710890.930180073 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0019000000000000128}

:::MLPv0.5.0 ssd 1541710891.434996843 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.0022166666666666723}

:::MLPv0.5.0 ssd 1541710891.939799070 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0025333333333333458}

:::MLPv0.5.0 ssd 1541710892.381196022 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0028500000000000053}

:::MLPv0.5.0 ssd 1541710892.841230154 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.0031666666666666787}

:::MLPv0.5.0 ssd 1541710893.342636347 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0034833333333333383}

:::MLPv0.5.0 ssd 1541710893.774845362 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.0038000000000000117}

:::MLPv0.5.0 ssd 1541710894.223344803 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.004116666666666671}

:::MLPv0.5.0 ssd 1541710894.642182589 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.004433333333333345}

:::MLPv0.5.0 ssd 1541710895.099839926 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.004750000000000004}

:::MLPv0.5.0 ssd 1541710895.550740719 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.005066666666666678}

:::MLPv0.5.0 ssd 1541710895.995661259 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.005383333333333337}

:::MLPv0.5.0 ssd 1541710896.426451921 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.005700000000000011}

:::MLPv0.5.0 ssd 1541710896.852115154 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.00601666666666667}

:::MLPv0.5.0 ssd 1541710897.271889448 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.006333333333333344}
Iteration:     20, Loss function: 20.634, Average Loss: 0.439, avg. samples / sec: 1974.46

:::MLPv0.5.0 ssd 1541710897.728492737 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.006650000000000003}

:::MLPv0.5.0 ssd 1541710898.179210901 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.0069666666666666766}

:::MLPv0.5.0 ssd 1541710898.614733934 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.007283333333333336}

:::MLPv0.5.0 ssd 1541710899.037264585 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.0076000000000000095}

:::MLPv0.5.0 ssd 1541710899.510467052 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.007916666666666669}

:::MLPv0.5.0 ssd 1541710899.960637093 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.008233333333333342}

:::MLPv0.5.0 ssd 1541710900.427088976 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.008550000000000002}

:::MLPv0.5.0 ssd 1541710900.823627949 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.008866666666666675}

:::MLPv0.5.0 ssd 1541710901.244097710 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.009183333333333335}

:::MLPv0.5.0 ssd 1541710901.714803219 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.009500000000000008}

:::MLPv0.5.0 ssd 1541710902.123567343 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.009816666666666668}

:::MLPv0.5.0 ssd 1541710902.539380789 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.010133333333333341}

:::MLPv0.5.0 ssd 1541710902.976905107 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.010450000000000001}

:::MLPv0.5.0 ssd 1541710903.424192429 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.010766666666666674}

:::MLPv0.5.0 ssd 1541710903.849520206 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.011083333333333334}

:::MLPv0.5.0 ssd 1541710904.231144190 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.011400000000000007}

:::MLPv0.5.0 ssd 1541710904.666646957 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.011716666666666667}

:::MLPv0.5.0 ssd 1541710905.068917751 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.01203333333333334}

:::MLPv0.5.0 ssd 1541710905.476404190 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.01235}

:::MLPv0.5.0 ssd 1541710905.893779516 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.012666666666666673}
Iteration:     40, Loss function: 13.496, Average Loss: 0.796, avg. samples / sec: 2814.90

:::MLPv0.5.0 ssd 1541710906.281992197 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.012983333333333333}

:::MLPv0.5.0 ssd 1541710906.661193371 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.013300000000000006}

:::MLPv0.5.0 ssd 1541710907.105463028 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.013616666666666666}

:::MLPv0.5.0 ssd 1541710907.488627911 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.01393333333333334}

:::MLPv0.5.0 ssd 1541710907.895204067 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.014250000000000013}

:::MLPv0.5.0 ssd 1541710908.266938210 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.014566666666666672}

:::MLPv0.5.0 ssd 1541710908.670426607 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.014883333333333346}

:::MLPv0.5.0 ssd 1541710909.133418083 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.015200000000000005}

:::MLPv0.5.0 ssd 1541710909.509771585 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.015516666666666679}

:::MLPv0.5.0 ssd 1541710909.874156237 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.015833333333333338}

:::MLPv0.5.0 ssd 1541710910.250421762 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.01615000000000001}

:::MLPv0.5.0 ssd 1541710910.683940887 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.01646666666666667}

:::MLPv0.5.0 ssd 1541710911.094614029 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.016783333333333345}

:::MLPv0.5.0 ssd 1541710911.508013248 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.017100000000000004}

:::MLPv0.5.0 ssd 1541710911.903183460 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.017416666666666678}

:::MLPv0.5.0 ssd 1541710912.290523052 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.017733333333333337}

:::MLPv0.5.0 ssd 1541710912.726507425 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.01805000000000001}

:::MLPv0.5.0 ssd 1541710913.086011171 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 58, "value": 0.01836666666666667}

:::MLPv0.5.0 ssd 1541710913.487760067 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 59, "value": 0.018683333333333343}

:::MLPv0.5.0 ssd 1541710913.844862461 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 60, "value": 0.019000000000000003}
Iteration:     60, Loss function: 11.590, Average Loss: 1.042, avg. samples / sec: 3061.94

:::MLPv0.5.0 ssd 1541710914.199817419 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 61, "value": 0.019316666666666676}

:::MLPv0.5.0 ssd 1541710914.603820324 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 62, "value": 0.019633333333333336}

:::MLPv0.5.0 ssd 1541710915.016126871 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 63, "value": 0.01995000000000001}

:::MLPv0.5.0 ssd 1541710915.385113001 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 64, "value": 0.02026666666666667}

:::MLPv0.5.0 ssd 1541710915.797006845 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 65, "value": 0.020583333333333342}

:::MLPv0.5.0 ssd 1541710916.173887491 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 66, "value": 0.020900000000000002}

:::MLPv0.5.0 ssd 1541710916.525032282 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 67, "value": 0.021216666666666675}

:::MLPv0.5.0 ssd 1541710916.888734579 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 68, "value": 0.021533333333333335}

:::MLPv0.5.0 ssd 1541710917.277716875 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 69, "value": 0.02185000000000001}

:::MLPv0.5.0 ssd 1541710917.670270920 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 70, "value": 0.022166666666666668}

:::MLPv0.5.0 ssd 1541710918.061354160 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 71, "value": 0.02248333333333334}

:::MLPv0.5.0 ssd 1541710918.423294067 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 72, "value": 0.0228}

:::MLPv0.5.0 ssd 1541710918.804886580 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 73, "value": 0.023116666666666674}

:::MLPv0.5.0 ssd 1541710919.159805536 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 74, "value": 0.023433333333333334}

:::MLPv0.5.0 ssd 1541710919.568069696 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 75, "value": 0.023750000000000007}

:::MLPv0.5.0 ssd 1541710919.945419073 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 76, "value": 0.024066666666666667}

:::MLPv0.5.0 ssd 1541710920.318682909 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 77, "value": 0.02438333333333334}

:::MLPv0.5.0 ssd 1541710920.689977169 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 78, "value": 0.0247}

:::MLPv0.5.0 ssd 1541710921.058273792 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 79, "value": 0.025016666666666673}

:::MLPv0.5.0 ssd 1541710921.388348818 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 80, "value": 0.025333333333333333}
Iteration:     80, Loss function: 9.915, Average Loss: 1.227, avg. samples / sec: 3217.39

:::MLPv0.5.0 ssd 1541710921.770910263 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 81, "value": 0.025650000000000006}

:::MLPv0.5.0 ssd 1541710922.165862799 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 82, "value": 0.025966666666666666}

:::MLPv0.5.0 ssd 1541710922.503925562 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 83, "value": 0.02628333333333334}

:::MLPv0.5.0 ssd 1541710922.838642597 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 84, "value": 0.0266}

:::MLPv0.5.0 ssd 1541710923.198332071 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 85, "value": 0.026916666666666672}

:::MLPv0.5.0 ssd 1541710923.561745882 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 86, "value": 0.02723333333333333}

:::MLPv0.5.0 ssd 1541710923.903940916 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 87, "value": 0.027550000000000005}

:::MLPv0.5.0 ssd 1541710924.247332811 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 88, "value": 0.02786666666666668}

:::MLPv0.5.0 ssd 1541710924.577095270 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 89, "value": 0.028183333333333338}

:::MLPv0.5.0 ssd 1541710924.910623789 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 90, "value": 0.02850000000000001}

:::MLPv0.5.0 ssd 1541710925.238959551 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 91, "value": 0.02881666666666667}

:::MLPv0.5.0 ssd 1541710925.612135887 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 92, "value": 0.029133333333333344}

:::MLPv0.5.0 ssd 1541710925.935441256 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 93, "value": 0.029450000000000004}

:::MLPv0.5.0 ssd 1541710926.294799328 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 94, "value": 0.029766666666666677}

:::MLPv0.5.0 ssd 1541710926.619082928 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 95, "value": 0.030083333333333337}

:::MLPv0.5.0 ssd 1541710926.950944662 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 96, "value": 0.03040000000000001}

:::MLPv0.5.0 ssd 1541710927.303846121 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 97, "value": 0.03071666666666667}

:::MLPv0.5.0 ssd 1541710927.594887733 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1541710927.650733232 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 98, "value": 0.031033333333333343}

:::MLPv0.5.0 ssd 1541710927.981735706 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 99, "value": 0.03135}

:::MLPv0.5.0 ssd 1541710928.304080486 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 100, "value": 0.031666666666666676}
Iteration:    100, Loss function: 8.854, Average Loss: 1.389, avg. samples / sec: 3529.69

:::MLPv0.5.0 ssd 1541710928.627674103 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 101, "value": 0.031983333333333336}

:::MLPv0.5.0 ssd 1541710929.018790245 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 102, "value": 0.03230000000000001}

:::MLPv0.5.0 ssd 1541710929.342161417 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 103, "value": 0.03261666666666667}

:::MLPv0.5.0 ssd 1541710929.660429239 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 104, "value": 0.032933333333333335}

:::MLPv0.5.0 ssd 1541710930.002962351 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 105, "value": 0.03325}

:::MLPv0.5.0 ssd 1541710930.319632053 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 106, "value": 0.03356666666666667}

:::MLPv0.5.0 ssd 1541710930.662393332 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 107, "value": 0.033883333333333335}

:::MLPv0.5.0 ssd 1541710930.992754698 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 108, "value": 0.03420000000000001}

:::MLPv0.5.0 ssd 1541710931.323631763 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 109, "value": 0.034516666666666675}

:::MLPv0.5.0 ssd 1541710931.655278683 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 110, "value": 0.03483333333333334}

:::MLPv0.5.0 ssd 1541710931.984628439 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 111, "value": 0.03515000000000001}

:::MLPv0.5.0 ssd 1541710932.343311787 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 112, "value": 0.035466666666666674}

:::MLPv0.5.0 ssd 1541710932.675780535 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 113, "value": 0.03578333333333334}

:::MLPv0.5.0 ssd 1541710933.048696518 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 114, "value": 0.03610000000000001}

:::MLPv0.5.0 ssd 1541710933.410822392 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 115, "value": 0.036416666666666674}

:::MLPv0.5.0 ssd 1541710933.755369902 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 116, "value": 0.03673333333333334}

:::MLPv0.5.0 ssd 1541710934.077876329 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 117, "value": 0.03705000000000001}

:::MLPv0.5.0 ssd 1541710934.399302006 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 118, "value": 0.03736666666666667}

:::MLPv0.5.0 ssd 1541710934.776427746 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 119, "value": 0.03768333333333334}

:::MLPv0.5.0 ssd 1541710935.110139847 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 120, "value": 0.038000000000000006}
Iteration:    120, Loss function: 8.745, Average Loss: 1.539, avg. samples / sec: 3573.32

:::MLPv0.5.0 ssd 1541710935.453809261 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 121, "value": 0.03831666666666667}

:::MLPv0.5.0 ssd 1541710935.784335613 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 122, "value": 0.03863333333333334}

:::MLPv0.5.0 ssd 1541710936.134030104 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 123, "value": 0.038950000000000005}

:::MLPv0.5.0 ssd 1541710936.483012438 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 124, "value": 0.03926666666666667}

:::MLPv0.5.0 ssd 1541710936.815221548 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 125, "value": 0.03958333333333334}

:::MLPv0.5.0 ssd 1541710937.148428917 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 126, "value": 0.039900000000000005}

:::MLPv0.5.0 ssd 1541710937.492768049 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 127, "value": 0.04021666666666667}

:::MLPv0.5.0 ssd 1541710937.821098328 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 128, "value": 0.04053333333333334}

:::MLPv0.5.0 ssd 1541710938.141581297 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 129, "value": 0.040850000000000004}

:::MLPv0.5.0 ssd 1541710938.455504656 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 130, "value": 0.04116666666666667}

:::MLPv0.5.0 ssd 1541710938.787189245 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 131, "value": 0.04148333333333334}

:::MLPv0.5.0 ssd 1541710939.101155281 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 132, "value": 0.041800000000000004}

:::MLPv0.5.0 ssd 1541710939.435490608 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 133, "value": 0.04211666666666667}

:::MLPv0.5.0 ssd 1541710939.776096582 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 134, "value": 0.04243333333333334}

:::MLPv0.5.0 ssd 1541710940.087289333 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 135, "value": 0.04275}

:::MLPv0.5.0 ssd 1541710940.431461811 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 136, "value": 0.04306666666666667}

:::MLPv0.5.0 ssd 1541710940.752818108 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 137, "value": 0.043383333333333336}

:::MLPv0.5.0 ssd 1541710941.094118118 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 138, "value": 0.0437}

:::MLPv0.5.0 ssd 1541710941.440685034 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 139, "value": 0.04401666666666667}

:::MLPv0.5.0 ssd 1541710941.757588148 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 140, "value": 0.044333333333333336}
Iteration:    140, Loss function: 8.889, Average Loss: 1.684, avg. samples / sec: 3658.35

:::MLPv0.5.0 ssd 1541710942.102614641 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 141, "value": 0.04465}

:::MLPv0.5.0 ssd 1541710942.449057579 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 142, "value": 0.04496666666666667}

:::MLPv0.5.0 ssd 1541710942.779711246 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 143, "value": 0.045283333333333335}

:::MLPv0.5.0 ssd 1541710943.105305672 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 144, "value": 0.0456}

:::MLPv0.5.0 ssd 1541710943.437510729 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 145, "value": 0.04591666666666667}

:::MLPv0.5.0 ssd 1541710943.759203672 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 146, "value": 0.046233333333333335}

:::MLPv0.5.0 ssd 1541710944.079922438 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 147, "value": 0.04655}

:::MLPv0.5.0 ssd 1541710944.408860207 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 148, "value": 0.04686666666666667}

:::MLPv0.5.0 ssd 1541710944.765055895 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 149, "value": 0.047183333333333334}

:::MLPv0.5.0 ssd 1541710945.089149952 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 150, "value": 0.0475}

:::MLPv0.5.0 ssd 1541710945.412830830 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 151, "value": 0.047816666666666674}

:::MLPv0.5.0 ssd 1541710945.735757828 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 152, "value": 0.04813333333333334}

:::MLPv0.5.0 ssd 1541710946.085092545 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 153, "value": 0.04845000000000001}

:::MLPv0.5.0 ssd 1541710946.408817768 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 154, "value": 0.04876666666666667}

:::MLPv0.5.0 ssd 1541710946.744578600 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 155, "value": 0.04908333333333334}

:::MLPv0.5.0 ssd 1541710947.065737247 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 156, "value": 0.049400000000000006}

:::MLPv0.5.0 ssd 1541710947.446487904 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 157, "value": 0.04971666666666667}

:::MLPv0.5.0 ssd 1541710947.773741007 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 158, "value": 0.05003333333333334}

:::MLPv0.5.0 ssd 1541710948.103549719 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 159, "value": 0.050350000000000006}

:::MLPv0.5.0 ssd 1541710948.440126896 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 160, "value": 0.05066666666666667}
Iteration:    160, Loss function: 8.162, Average Loss: 1.822, avg. samples / sec: 3636.58

:::MLPv0.5.0 ssd 1541710948.796717882 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 161, "value": 0.05098333333333334}

:::MLPv0.5.0 ssd 1541710949.123341799 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 162, "value": 0.051300000000000005}

:::MLPv0.5.0 ssd 1541710949.465927839 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 163, "value": 0.05161666666666667}

:::MLPv0.5.0 ssd 1541710949.788910151 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 164, "value": 0.05193333333333334}

:::MLPv0.5.0 ssd 1541710950.112012863 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 165, "value": 0.052250000000000005}

:::MLPv0.5.0 ssd 1541710950.471376657 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 166, "value": 0.05256666666666667}

:::MLPv0.5.0 ssd 1541710950.805851460 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 167, "value": 0.05288333333333334}

:::MLPv0.5.0 ssd 1541710951.120858669 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 168, "value": 0.053200000000000004}

:::MLPv0.5.0 ssd 1541710951.443472624 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 169, "value": 0.05351666666666667}

:::MLPv0.5.0 ssd 1541710951.773798227 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 170, "value": 0.05383333333333334}

:::MLPv0.5.0 ssd 1541710952.113439322 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 171, "value": 0.054150000000000004}

:::MLPv0.5.0 ssd 1541710952.437595844 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 172, "value": 0.05446666666666667}

:::MLPv0.5.0 ssd 1541710952.772887468 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 173, "value": 0.05478333333333334}

:::MLPv0.5.0 ssd 1541710953.103467703 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 174, "value": 0.0551}

:::MLPv0.5.0 ssd 1541710953.421188354 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 175, "value": 0.05541666666666667}

:::MLPv0.5.0 ssd 1541710953.762690544 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 176, "value": 0.055733333333333336}

:::MLPv0.5.0 ssd 1541710954.088884830 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 177, "value": 0.05605}

:::MLPv0.5.0 ssd 1541710954.405117035 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 178, "value": 0.05636666666666667}

:::MLPv0.5.0 ssd 1541710954.740381479 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 179, "value": 0.056683333333333336}

:::MLPv0.5.0 ssd 1541710955.067670822 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 180, "value": 0.057}
Iteration:    180, Loss function: 8.390, Average Loss: 1.950, avg. samples / sec: 3672.87

:::MLPv0.5.0 ssd 1541710955.387280703 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 181, "value": 0.05731666666666667}

:::MLPv0.5.0 ssd 1541710955.702326298 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 182, "value": 0.057633333333333335}

:::MLPv0.5.0 ssd 1541710956.026989460 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 183, "value": 0.05795}

:::MLPv0.5.0 ssd 1541710956.354396820 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 184, "value": 0.05826666666666667}

:::MLPv0.5.0 ssd 1541710956.683671474 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 185, "value": 0.058583333333333334}

:::MLPv0.5.0 ssd 1541710957.011632919 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 186, "value": 0.0589}

:::MLPv0.5.0 ssd 1541710957.336538076 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 187, "value": 0.05921666666666667}

:::MLPv0.5.0 ssd 1541710957.663238764 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 188, "value": 0.059533333333333334}

:::MLPv0.5.0 ssd 1541710957.985870361 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 189, "value": 0.05985}

:::MLPv0.5.0 ssd 1541710958.301354170 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 190, "value": 0.06016666666666667}

:::MLPv0.5.0 ssd 1541710958.648253202 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 191, "value": 0.06048333333333333}

:::MLPv0.5.0 ssd 1541710958.977478027 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 192, "value": 0.0608}

:::MLPv0.5.0 ssd 1541710959.305814028 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 193, "value": 0.061116666666666666}

:::MLPv0.5.0 ssd 1541710959.626203060 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 194, "value": 0.06143333333333334}

:::MLPv0.5.0 ssd 1541710959.919642687 (train.py:553) train_epoch: 2

:::MLPv0.5.0 ssd 1541710959.948694944 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 195, "value": 0.061750000000000006}

:::MLPv0.5.0 ssd 1541710960.267762184 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 196, "value": 0.06206666666666667}

:::MLPv0.5.0 ssd 1541710960.601413965 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 197, "value": 0.06238333333333334}

:::MLPv0.5.0 ssd 1541710960.913829565 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 198, "value": 0.0627}

:::MLPv0.5.0 ssd 1541710961.242764235 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 199, "value": 0.06301666666666667}

:::MLPv0.5.0 ssd 1541710961.566979170 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 200, "value": 0.06333333333333334}
Iteration:    200, Loss function: 7.826, Average Loss: 2.075, avg. samples / sec: 3736.14

:::MLPv0.5.0 ssd 1541710961.910103321 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 201, "value": 0.06365000000000001}

:::MLPv0.5.0 ssd 1541710962.244483948 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 202, "value": 0.06396666666666667}

:::MLPv0.5.0 ssd 1541710962.558027029 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 203, "value": 0.06428333333333333}

:::MLPv0.5.0 ssd 1541710962.893627644 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 204, "value": 0.0646}

:::MLPv0.5.0 ssd 1541710963.224641800 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 205, "value": 0.06491666666666668}

:::MLPv0.5.0 ssd 1541710963.538430929 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 206, "value": 0.06523333333333334}

:::MLPv0.5.0 ssd 1541710963.866286755 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 207, "value": 0.06555}

:::MLPv0.5.0 ssd 1541710964.200619698 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 208, "value": 0.06586666666666667}

:::MLPv0.5.0 ssd 1541710964.521038294 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 209, "value": 0.06618333333333334}

:::MLPv0.5.0 ssd 1541710964.840196133 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 210, "value": 0.0665}

:::MLPv0.5.0 ssd 1541710965.165062189 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 211, "value": 0.06681666666666666}

:::MLPv0.5.0 ssd 1541710965.493524551 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 212, "value": 0.06713333333333334}

:::MLPv0.5.0 ssd 1541710965.811621904 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 213, "value": 0.06745000000000001}

:::MLPv0.5.0 ssd 1541710966.142378092 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 214, "value": 0.06776666666666667}

:::MLPv0.5.0 ssd 1541710966.459317684 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 215, "value": 0.06808333333333333}

:::MLPv0.5.0 ssd 1541710966.787280560 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 216, "value": 0.0684}

:::MLPv0.5.0 ssd 1541710967.104369402 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 217, "value": 0.06871666666666668}

:::MLPv0.5.0 ssd 1541710967.437814951 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 218, "value": 0.06903333333333334}

:::MLPv0.5.0 ssd 1541710967.749188662 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 219, "value": 0.06935}

:::MLPv0.5.0 ssd 1541710968.069978476 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 220, "value": 0.06966666666666667}
Iteration:    220, Loss function: 7.847, Average Loss: 2.190, avg. samples / sec: 3745.07

:::MLPv0.5.0 ssd 1541710968.387480736 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 221, "value": 0.06998333333333334}

:::MLPv0.5.0 ssd 1541710968.701199532 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 222, "value": 0.0703}

:::MLPv0.5.0 ssd 1541710969.022112608 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 223, "value": 0.07061666666666666}

:::MLPv0.5.0 ssd 1541710969.337707996 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 224, "value": 0.07093333333333333}

:::MLPv0.5.0 ssd 1541710969.658890963 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 225, "value": 0.07125000000000001}

:::MLPv0.5.0 ssd 1541710969.980957270 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 226, "value": 0.07156666666666667}

:::MLPv0.5.0 ssd 1541710970.308241606 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 227, "value": 0.07188333333333334}

:::MLPv0.5.0 ssd 1541710970.625271320 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 228, "value": 0.0722}

:::MLPv0.5.0 ssd 1541710970.950121403 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 229, "value": 0.07251666666666667}

:::MLPv0.5.0 ssd 1541710971.271555424 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 230, "value": 0.07283333333333333}

:::MLPv0.5.0 ssd 1541710971.611874819 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 231, "value": 0.07315}

:::MLPv0.5.0 ssd 1541710971.921244860 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 232, "value": 0.07346666666666667}

:::MLPv0.5.0 ssd 1541710972.248358488 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 233, "value": 0.07378333333333334}

:::MLPv0.5.0 ssd 1541710972.558305979 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 234, "value": 0.0741}

:::MLPv0.5.0 ssd 1541710972.878960133 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 235, "value": 0.07441666666666667}

:::MLPv0.5.0 ssd 1541710973.187242985 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 236, "value": 0.07473333333333333}

:::MLPv0.5.0 ssd 1541710973.506971121 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 237, "value": 0.07505}

:::MLPv0.5.0 ssd 1541710973.829634905 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 238, "value": 0.07536666666666667}

:::MLPv0.5.0 ssd 1541710974.133579254 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 239, "value": 0.07568333333333334}

:::MLPv0.5.0 ssd 1541710974.453116655 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 240, "value": 0.076}
Iteration:    240, Loss function: 7.657, Average Loss: 2.299, avg. samples / sec: 3809.73

:::MLPv0.5.0 ssd 1541710974.770653009 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 241, "value": 0.07631666666666667}

:::MLPv0.5.0 ssd 1541710975.114105701 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 242, "value": 0.07663333333333333}

:::MLPv0.5.0 ssd 1541710975.434588194 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 243, "value": 0.07695}

:::MLPv0.5.0 ssd 1541710975.755527258 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 244, "value": 0.07726666666666666}

:::MLPv0.5.0 ssd 1541710976.064371109 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 245, "value": 0.07758333333333334}

:::MLPv0.5.0 ssd 1541710976.381986856 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 246, "value": 0.0779}

:::MLPv0.5.0 ssd 1541710976.708602667 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 247, "value": 0.07821666666666667}

:::MLPv0.5.0 ssd 1541710977.027497768 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 248, "value": 0.07853333333333334}

:::MLPv0.5.0 ssd 1541710977.359080791 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 249, "value": 0.07885}

:::MLPv0.5.0 ssd 1541710977.675047159 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 250, "value": 0.07916666666666666}

:::MLPv0.5.0 ssd 1541710977.989407539 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 251, "value": 0.07948333333333334}

:::MLPv0.5.0 ssd 1541710978.312749624 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 252, "value": 0.07980000000000001}

:::MLPv0.5.0 ssd 1541710978.642824411 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 253, "value": 0.08011666666666667}

:::MLPv0.5.0 ssd 1541710978.961094141 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 254, "value": 0.08043333333333333}

:::MLPv0.5.0 ssd 1541710979.271950245 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 255, "value": 0.08075}

:::MLPv0.5.0 ssd 1541710979.584195852 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 256, "value": 0.08106666666666668}

:::MLPv0.5.0 ssd 1541710979.902156115 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 257, "value": 0.08138333333333334}

:::MLPv0.5.0 ssd 1541710980.213788748 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 258, "value": 0.0817}

:::MLPv0.5.0 ssd 1541710980.526717663 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 259, "value": 0.08201666666666667}

:::MLPv0.5.0 ssd 1541710980.841347218 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 260, "value": 0.08233333333333334}
Iteration:    260, Loss function: 7.590, Average Loss: 2.403, avg. samples / sec: 3807.32

:::MLPv0.5.0 ssd 1541710981.150716305 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 261, "value": 0.08265}

:::MLPv0.5.0 ssd 1541710981.460548878 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 262, "value": 0.08296666666666666}

:::MLPv0.5.0 ssd 1541710981.772669077 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 263, "value": 0.08328333333333333}

:::MLPv0.5.0 ssd 1541710982.084033489 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 264, "value": 0.08360000000000001}

:::MLPv0.5.0 ssd 1541710982.401006222 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 265, "value": 0.08391666666666667}

:::MLPv0.5.0 ssd 1541710982.707079172 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 266, "value": 0.08423333333333334}

:::MLPv0.5.0 ssd 1541710983.032260656 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 267, "value": 0.08455}

:::MLPv0.5.0 ssd 1541710983.363786936 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 268, "value": 0.08486666666666667}

:::MLPv0.5.0 ssd 1541710983.685142517 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 269, "value": 0.08518333333333333}

:::MLPv0.5.0 ssd 1541710983.994481802 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 270, "value": 0.0855}

:::MLPv0.5.0 ssd 1541710984.301085711 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 271, "value": 0.08581666666666667}

:::MLPv0.5.0 ssd 1541710984.616475821 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 272, "value": 0.08613333333333334}

:::MLPv0.5.0 ssd 1541710984.941867113 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 273, "value": 0.08645}

:::MLPv0.5.0 ssd 1541710985.285114765 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 274, "value": 0.08676666666666667}

:::MLPv0.5.0 ssd 1541710985.596125126 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 275, "value": 0.08708333333333333}

:::MLPv0.5.0 ssd 1541710985.906920195 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 276, "value": 0.0874}

:::MLPv0.5.0 ssd 1541710986.220841169 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 277, "value": 0.08771666666666667}

:::MLPv0.5.0 ssd 1541710986.545696020 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 278, "value": 0.08803333333333334}

:::MLPv0.5.0 ssd 1541710986.863449335 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 279, "value": 0.08835}

:::MLPv0.5.0 ssd 1541710987.173794031 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 280, "value": 0.08866666666666667}
Iteration:    280, Loss function: 7.549, Average Loss: 2.504, avg. samples / sec: 3840.82

:::MLPv0.5.0 ssd 1541710987.498527765 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 281, "value": 0.08898333333333333}

:::MLPv0.5.0 ssd 1541710987.813186407 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 282, "value": 0.0893}

:::MLPv0.5.0 ssd 1541710988.145708561 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 283, "value": 0.08961666666666666}

:::MLPv0.5.0 ssd 1541710988.474481821 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 284, "value": 0.08993333333333334}

:::MLPv0.5.0 ssd 1541710988.793166876 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 285, "value": 0.09025}

:::MLPv0.5.0 ssd 1541710989.119456053 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 286, "value": 0.09056666666666667}

:::MLPv0.5.0 ssd 1541710989.438101053 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 287, "value": 0.09088333333333333}

:::MLPv0.5.0 ssd 1541710989.750608921 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 288, "value": 0.0912}

:::MLPv0.5.0 ssd 1541710990.061380386 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 289, "value": 0.09151666666666666}

:::MLPv0.5.0 ssd 1541710990.371162176 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 290, "value": 0.09183333333333334}

:::MLPv0.5.0 ssd 1541710990.690763712 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 291, "value": 0.09215}

:::MLPv0.5.0 ssd 1541710990.980008364 (train.py:553) train_epoch: 3

:::MLPv0.5.0 ssd 1541710991.010013580 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 292, "value": 0.09246666666666667}

:::MLPv0.5.0 ssd 1541710991.321151495 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 293, "value": 0.09278333333333333}

:::MLPv0.5.0 ssd 1541710991.640939951 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 294, "value": 0.0931}

:::MLPv0.5.0 ssd 1541710991.958410263 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 295, "value": 0.09341666666666666}

:::MLPv0.5.0 ssd 1541710992.269233227 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 296, "value": 0.09373333333333334}

:::MLPv0.5.0 ssd 1541710992.568092823 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 297, "value": 0.09405}

:::MLPv0.5.0 ssd 1541710992.883219004 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 298, "value": 0.09436666666666667}

:::MLPv0.5.0 ssd 1541710993.200875044 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 299, "value": 0.09468333333333333}
Iteration:    300, Loss function: 7.134, Average Loss: 2.600, avg. samples / sec: 3848.25
Iteration:    320, Loss function: 7.129, Average Loss: 2.686, avg. samples / sec: 3891.33
Iteration:    340, Loss function: 6.921, Average Loss: 2.774, avg. samples / sec: 3924.10
Iteration:    360, Loss function: 6.765, Average Loss: 2.854, avg. samples / sec: 3927.03
Iteration:    380, Loss function: 6.473, Average Loss: 2.929, avg. samples / sec: 3924.08

:::MLPv0.5.0 ssd 1541711021.412147760 (train.py:553) train_epoch: 4
Iteration:    400, Loss function: 6.286, Average Loss: 2.999, avg. samples / sec: 3943.15
Iteration:    420, Loss function: 6.492, Average Loss: 3.067, avg. samples / sec: 3914.58
Iteration:    440, Loss function: 6.357, Average Loss: 3.134, avg. samples / sec: 3957.91
Iteration:    460, Loss function: 5.904, Average Loss: 3.194, avg. samples / sec: 3972.45
Iteration:    480, Loss function: 5.878, Average Loss: 3.248, avg. samples / sec: 3959.78

:::MLPv0.5.0 ssd 1541711051.258656025 (train.py:553) train_epoch: 5
Iteration:    500, Loss function: 5.904, Average Loss: 3.303, avg. samples / sec: 3946.13
Iteration:    520, Loss function: 6.026, Average Loss: 3.358, avg. samples / sec: 3948.19
Iteration:    540, Loss function: 5.829, Average Loss: 3.406, avg. samples / sec: 3968.45
Iteration:    560, Loss function: 5.818, Average Loss: 3.457, avg. samples / sec: 3966.97
Iteration:    580, Loss function: 5.752, Average Loss: 3.500, avg. samples / sec: 3999.03

:::MLPv0.5.0 ssd 1541711080.980173349 (train.py:553) train_epoch: 6
Iteration:    600, Loss function: 5.637, Average Loss: 3.540, avg. samples / sec: 4018.33
Iteration:    620, Loss function: 5.406, Average Loss: 3.581, avg. samples / sec: 3996.78
Iteration:    640, Loss function: 5.599, Average Loss: 3.617, avg. samples / sec: 3985.96
Iteration:    660, Loss function: 5.548, Average Loss: 3.653, avg. samples / sec: 4032.89
Iteration:    680, Loss function: 5.203, Average Loss: 3.688, avg. samples / sec: 3985.02

:::MLPv0.5.0 ssd 1541711110.439443111 (train.py:553) train_epoch: 7
Iteration:    700, Loss function: 5.148, Average Loss: 3.719, avg. samples / sec: 4013.82
Iteration:    720, Loss function: 5.312, Average Loss: 3.751, avg. samples / sec: 4016.71
Iteration:    740, Loss function: 4.979, Average Loss: 3.780, avg. samples / sec: 4008.88
Iteration:    760, Loss function: 5.349, Average Loss: 3.807, avg. samples / sec: 4025.77

:::MLPv0.5.0 ssd 1541711140.146936655 (train.py:553) train_epoch: 8
Iteration:    780, Loss function: 5.166, Average Loss: 3.835, avg. samples / sec: 4000.55
Iteration:    800, Loss function: 5.130, Average Loss: 3.860, avg. samples / sec: 4029.56
Iteration:    820, Loss function: 4.630, Average Loss: 3.883, avg. samples / sec: 3974.94
Iteration:    840, Loss function: 4.857, Average Loss: 3.906, avg. samples / sec: 4007.94
Iteration:    860, Loss function: 5.196, Average Loss: 3.927, avg. samples / sec: 4025.17

:::MLPv0.5.0 ssd 1541711169.529711008 (train.py:553) train_epoch: 9
Iteration:    880, Loss function: 4.919, Average Loss: 3.950, avg. samples / sec: 4031.34
Iteration:    900, Loss function: 5.124, Average Loss: 3.969, avg. samples / sec: 4009.52
Iteration:    920, Loss function: 5.085, Average Loss: 3.988, avg. samples / sec: 4022.99
Iteration:    940, Loss function: 4.776, Average Loss: 4.004, avg. samples / sec: 4009.67
Iteration:    960, Loss function: 4.885, Average Loss: 4.022, avg. samples / sec: 3994.12

:::MLPv0.5.0 ssd 1541711198.957449198 (train.py:553) train_epoch: 10
Iteration:    980, Loss function: 4.978, Average Loss: 4.038, avg. samples / sec: 4013.71
Iteration:   1000, Loss function: 4.676, Average Loss: 4.053, avg. samples / sec: 4022.73
Iteration:   1020, Loss function: 4.603, Average Loss: 4.067, avg. samples / sec: 4048.84
Iteration:   1040, Loss function: 4.783, Average Loss: 4.081, avg. samples / sec: 4029.26
Iteration:   1060, Loss function: 4.529, Average Loss: 4.094, avg. samples / sec: 4033.16

:::MLPv0.5.0 ssd 1541711228.530563831 (train.py:553) train_epoch: 11
Iteration:   1080, Loss function: 4.737, Average Loss: 4.107, avg. samples / sec: 4025.33
Iteration:   1100, Loss function: 5.043, Average Loss: 4.119, avg. samples / sec: 4039.47
Iteration:   1120, Loss function: 4.532, Average Loss: 4.130, avg. samples / sec: 4005.81
Iteration:   1140, Loss function: 4.729, Average Loss: 4.141, avg. samples / sec: 4004.29
Iteration:   1160, Loss function: 4.703, Average Loss: 4.151, avg. samples / sec: 4037.13

:::MLPv0.5.0 ssd 1541711257.854447126 (train.py:553) train_epoch: 12
Iteration:   1180, Loss function: 4.820, Average Loss: 4.160, avg. samples / sec: 4026.39
Iteration:   1200, Loss function: 4.691, Average Loss: 4.170, avg. samples / sec: 4000.79
Iteration:   1220, Loss function: 4.538, Average Loss: 4.178, avg. samples / sec: 4018.21
Iteration:   1240, Loss function: 4.608, Average Loss: 4.185, avg. samples / sec: 4051.81
Iteration:   1260, Loss function: 4.330, Average Loss: 4.191, avg. samples / sec: 3991.21

:::MLPv0.5.0 ssd 1541711287.196275711 (train.py:553) train_epoch: 13
Iteration:   1280, Loss function: 4.398, Average Loss: 4.197, avg. samples / sec: 4032.16
Iteration:   1300, Loss function: 4.611, Average Loss: 4.204, avg. samples / sec: 4007.66
Iteration:   1320, Loss function: 4.620, Average Loss: 4.211, avg. samples / sec: 4042.17
Iteration:   1340, Loss function: 4.506, Average Loss: 4.216, avg. samples / sec: 4045.74
Iteration:   1360, Loss function: 4.714, Average Loss: 4.226, avg. samples / sec: 4038.33

:::MLPv0.5.0 ssd 1541711316.443996906 (train.py:553) train_epoch: 14
Iteration:   1380, Loss function: 4.492, Average Loss: 4.231, avg. samples / sec: 4006.74
Iteration:   1400, Loss function: 4.415, Average Loss: 4.235, avg. samples / sec: 4045.38
Iteration:   1420, Loss function: 4.100, Average Loss: 4.238, avg. samples / sec: 4045.04
Iteration:   1440, Loss function: 4.525, Average Loss: 4.242, avg. samples / sec: 4019.40

:::MLPv0.5.0 ssd 1541711346.044331551 (train.py:553) train_epoch: 15
Iteration:   1460, Loss function: 4.393, Average Loss: 4.246, avg. samples / sec: 4009.45
Iteration:   1480, Loss function: 4.304, Average Loss: 4.248, avg. samples / sec: 4050.27
Iteration:   1500, Loss function: 4.419, Average Loss: 4.251, avg. samples / sec: 4040.06
Iteration:   1520, Loss function: 4.339, Average Loss: 4.254, avg. samples / sec: 4055.76
Iteration:   1540, Loss function: 4.208, Average Loss: 4.254, avg. samples / sec: 4017.34

:::MLPv0.5.0 ssd 1541711375.251162052 (train.py:553) train_epoch: 16
Iteration:   1560, Loss function: 4.300, Average Loss: 4.256, avg. samples / sec: 4022.50
Iteration:   1580, Loss function: 4.779, Average Loss: 4.262, avg. samples / sec: 4041.19
Iteration:   1600, Loss function: 4.136, Average Loss: 4.264, avg. samples / sec: 4038.07
Iteration:   1620, Loss function: 4.293, Average Loss: 4.263, avg. samples / sec: 4009.40
Iteration:   1640, Loss function: 4.454, Average Loss: 4.265, avg. samples / sec: 4033.83

:::MLPv0.5.0 ssd 1541711404.530517817 (train.py:553) train_epoch: 17
Iteration:   1660, Loss function: 4.473, Average Loss: 4.268, avg. samples / sec: 4028.10
Iteration:   1680, Loss function: 4.303, Average Loss: 4.268, avg. samples / sec: 4058.13
Iteration:   1700, Loss function: 4.289, Average Loss: 4.268, avg. samples / sec: 4051.05
Iteration:   1720, Loss function: 4.049, Average Loss: 4.268, avg. samples / sec: 4013.49
Iteration:   1740, Loss function: 4.440, Average Loss: 4.269, avg. samples / sec: 4064.23

:::MLPv0.5.0 ssd 1541711433.708693981 (train.py:553) train_epoch: 18
Iteration:   1760, Loss function: 4.152, Average Loss: 4.269, avg. samples / sec: 4047.98
Iteration:   1780, Loss function: 4.099, Average Loss: 4.269, avg. samples / sec: 4045.50
Iteration:   1800, Loss function: 4.150, Average Loss: 4.269, avg. samples / sec: 4045.31
Iteration:   1820, Loss function: 4.448, Average Loss: 4.270, avg. samples / sec: 4061.39
Iteration:   1840, Loss function: 4.157, Average Loss: 4.268, avg. samples / sec: 4040.97

:::MLPv0.5.0 ssd 1541711463.132016420 (train.py:553) train_epoch: 19
Iteration:   1860, Loss function: 4.141, Average Loss: 4.266, avg. samples / sec: 4050.05
Iteration:   1880, Loss function: 4.092, Average Loss: 4.266, avg. samples / sec: 4047.98
Iteration:   1900, Loss function: 4.427, Average Loss: 4.267, avg. samples / sec: 4047.72
Iteration:   1920, Loss function: 4.092, Average Loss: 4.268, avg. samples / sec: 4056.03
Iteration:   1940, Loss function: 4.013, Average Loss: 4.267, avg. samples / sec: 4052.79

:::MLPv0.5.0 ssd 1541711492.249449492 (train.py:553) train_epoch: 20
Iteration:   1960, Loss function: 4.345, Average Loss: 4.265, avg. samples / sec: 4034.35
Iteration:   1980, Loss function: 4.155, Average Loss: 4.265, avg. samples / sec: 4045.82
Iteration:   2000, Loss function: 4.050, Average Loss: 4.261, avg. samples / sec: 4028.01
Iteration:   2020, Loss function: 4.256, Average Loss: 4.258, avg. samples / sec: 4030.25
Iteration:   2040, Loss function: 4.150, Average Loss: 4.257, avg. samples / sec: 4034.60

:::MLPv0.5.0 ssd 1541711521.479439974 (train.py:553) train_epoch: 21
Iteration:   2060, Loss function: 4.053, Average Loss: 4.256, avg. samples / sec: 4038.41
Iteration:   2080, Loss function: 3.957, Average Loss: 4.255, avg. samples / sec: 4038.22
Iteration:   2100, Loss function: 4.006, Average Loss: 4.253, avg. samples / sec: 4071.16
Iteration:   2120, Loss function: 3.945, Average Loss: 4.251, avg. samples / sec: 4045.74
Iteration:   2140, Loss function: 4.064, Average Loss: 4.250, avg. samples / sec: 4015.76

:::MLPv0.5.0 ssd 1541711550.973377466 (train.py:553) train_epoch: 22
Iteration:   2160, Loss function: 4.053, Average Loss: 4.247, avg. samples / sec: 4031.82
Iteration:   2180, Loss function: 4.161, Average Loss: 4.246, avg. samples / sec: 4057.37
Iteration:   2200, Loss function: 4.097, Average Loss: 4.243, avg. samples / sec: 4059.70
Iteration:   2220, Loss function: 4.032, Average Loss: 4.240, avg. samples / sec: 4026.47

:::MLPv0.5.0 ssd 1541711580.116156578 (train.py:553) train_epoch: 23
Iteration:   2240, Loss function: 4.122, Average Loss: 4.238, avg. samples / sec: 4057.06
Iteration:   2260, Loss function: 4.046, Average Loss: 4.234, avg. samples / sec: 4073.45
Iteration:   2280, Loss function: 4.145, Average Loss: 4.231, avg. samples / sec: 4044.34
Iteration:   2300, Loss function: 4.228, Average Loss: 4.228, avg. samples / sec: 4033.17
Iteration:   2320, Loss function: 4.206, Average Loss: 4.223, avg. samples / sec: 4025.45

:::MLPv0.5.0 ssd 1541711609.248783112 (train.py:553) train_epoch: 24
Iteration:   2340, Loss function: 4.260, Average Loss: 4.221, avg. samples / sec: 4064.43
Iteration:   2360, Loss function: 4.202, Average Loss: 4.220, avg. samples / sec: 4054.81
Iteration:   2380, Loss function: 4.157, Average Loss: 4.218, avg. samples / sec: 4062.24
Iteration:   2400, Loss function: 4.039, Average Loss: 4.214, avg. samples / sec: 4048.98
Iteration:   2420, Loss function: 4.050, Average Loss: 4.211, avg. samples / sec: 4033.13

:::MLPv0.5.0 ssd 1541711638.378774881 (train.py:553) train_epoch: 25
Iteration:   2440, Loss function: 4.321, Average Loss: 4.210, avg. samples / sec: 4057.90
Iteration:   2460, Loss function: 3.920, Average Loss: 4.207, avg. samples / sec: 4062.54
Iteration:   2480, Loss function: 3.956, Average Loss: 4.203, avg. samples / sec: 4051.64
Iteration:   2500, Loss function: 4.202, Average Loss: 4.200, avg. samples / sec: 4051.68
Iteration:   2520, Loss function: 4.059, Average Loss: 4.197, avg. samples / sec: 4018.13

:::MLPv0.5.0 ssd 1541711667.805052519 (train.py:553) train_epoch: 26
Iteration:   2540, Loss function: 4.183, Average Loss: 4.195, avg. samples / sec: 4058.84
Iteration:   2560, Loss function: 3.897, Average Loss: 4.190, avg. samples / sec: 4042.04
Iteration:   2580, Loss function: 3.906, Average Loss: 4.188, avg. samples / sec: 4062.61
Iteration:   2600, Loss function: 4.215, Average Loss: 4.186, avg. samples / sec: 4049.01
Iteration:   2620, Loss function: 4.158, Average Loss: 4.182, avg. samples / sec: 4014.04

:::MLPv0.5.0 ssd 1541711696.958396435 (train.py:553) train_epoch: 27
Iteration:   2640, Loss function: 3.982, Average Loss: 4.179, avg. samples / sec: 4078.47
Iteration:   2660, Loss function: 4.132, Average Loss: 4.176, avg. samples / sec: 4069.77
Iteration:   2680, Loss function: 4.053, Average Loss: 4.173, avg. samples / sec: 4040.62
Iteration:   2700, Loss function: 3.937, Average Loss: 4.170, avg. samples / sec: 4066.85
Iteration:   2720, Loss function: 4.187, Average Loss: 4.170, avg. samples / sec: 4019.30

:::MLPv0.5.0 ssd 1541711726.079421997 (train.py:553) train_epoch: 28
Iteration:   2740, Loss function: 3.986, Average Loss: 4.167, avg. samples / sec: 4042.48
Iteration:   2760, Loss function: 4.054, Average Loss: 4.163, avg. samples / sec: 4046.44
Iteration:   2780, Loss function: 3.955, Average Loss: 4.160, avg. samples / sec: 4061.07
Iteration:   2800, Loss function: 4.283, Average Loss: 4.156, avg. samples / sec: 4036.14
Iteration:   2820, Loss function: 3.991, Average Loss: 4.153, avg. samples / sec: 4026.92

:::MLPv0.5.0 ssd 1541711755.241797447 (train.py:553) train_epoch: 29
Iteration:   2840, Loss function: 4.392, Average Loss: 4.150, avg. samples / sec: 4030.05
Iteration:   2860, Loss function: 4.084, Average Loss: 4.148, avg. samples / sec: 4070.33
Iteration:   2880, Loss function: 3.943, Average Loss: 4.145, avg. samples / sec: 4064.33
Iteration:   2900, Loss function: 3.865, Average Loss: 4.140, avg. samples / sec: 4038.96

:::MLPv0.5.0 ssd 1541711784.663047314 (train.py:553) train_epoch: 30
Iteration:   2920, Loss function: 3.969, Average Loss: 4.138, avg. samples / sec: 4050.01
Iteration:   2940, Loss function: 4.198, Average Loss: 4.134, avg. samples / sec: 4070.30
Iteration:   2960, Loss function: 4.011, Average Loss: 4.132, avg. samples / sec: 4045.71
Iteration:   2980, Loss function: 4.074, Average Loss: 4.130, avg. samples / sec: 4062.08
Iteration:   3000, Loss function: 4.030, Average Loss: 4.126, avg. samples / sec: 4059.72

:::MLPv0.5.0 ssd 1541711813.736556768 (train.py:553) train_epoch: 31
Iteration:   3020, Loss function: 3.984, Average Loss: 4.121, avg. samples / sec: 4052.99
Iteration:   3040, Loss function: 4.003, Average Loss: 4.120, avg. samples / sec: 4058.65
Iteration:   3060, Loss function: 3.853, Average Loss: 4.116, avg. samples / sec: 4068.22
Iteration:   3080, Loss function: 3.761, Average Loss: 4.110, avg. samples / sec: 4044.96
Iteration:   3100, Loss function: 3.886, Average Loss: 4.107, avg. samples / sec: 4054.71

:::MLPv0.5.0 ssd 1541711842.793915510 (train.py:553) train_epoch: 32
Iteration:   3120, Loss function: 4.009, Average Loss: 4.102, avg. samples / sec: 4073.42
Iteration:   3140, Loss function: 4.184, Average Loss: 4.099, avg. samples / sec: 4061.05









:::MLPv0.5.0 ssd 1541711856.288794518 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711856.289597034 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711856.290201187 (train.py:220) eval_start: 32
Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Predicting Ended, total time: 39.37 s
Loading and preparing results...
Converting ndarray to lists...
(351158, 7)
0/351158
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(351158, 7)
(351158, 7)
Converting ndarray to lists...
(351158, 7)
(351158, 7)
Converting ndarray to lists...
0/351158
(351158, 7)
0/351158
0/351158
0/351158
(351158, 7)
0/351158
0/351158
(351158, 7)
0/351158
DONE (t=2.25s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.55s)
creating index...
DONE (t=2.56s)
creating index...
DONE (t=2.57s)
creating index...
DONE (t=2.58s)
creating index...
DONE (t=2.58s)
creating index...
DONE (t=2.59s)
creating index...
DONE (t=2.59s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=4.16s).
Accumulating evaluation results...
DONE (t=1.27s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.292
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.143
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.041
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.167
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.229
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.247
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.262
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.072
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.275
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.407
Current AP: 0.14988 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711903.587587833 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711903.588428736 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.1498762113703218}

:::MLPv0.5.0 ssd 1541711903.589125156 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.1498762113703218}

:::MLPv0.5.0 ssd 1541711903.589848757 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711903.590567589 (train.py:338) eval_stop: 32
Iteration:   3160, Loss function: 3.907, Average Loss: 4.094, avg. samples / sec: 449.46
Iteration:   3180, Loss function: 3.738, Average Loss: 4.091, avg. samples / sec: 4095.06
Iteration:   3200, Loss function: 3.938, Average Loss: 4.088, avg. samples / sec: 4063.58

:::MLPv0.5.0 ssd 1541711920.177566290 (train.py:553) train_epoch: 33
Iteration:   3220, Loss function: 3.884, Average Loss: 4.085, avg. samples / sec: 4072.74
Iteration:   3240, Loss function: 3.798, Average Loss: 4.080, avg. samples / sec: 4072.23
Iteration:   3260, Loss function: 3.834, Average Loss: 4.076, avg. samples / sec: 4075.95
Iteration:   3280, Loss function: 4.008, Average Loss: 4.074, avg. samples / sec: 4042.91
Iteration:   3300, Loss function: 4.075, Average Loss: 4.071, avg. samples / sec: 4056.45

:::MLPv0.5.0 ssd 1541711949.227905989 (train.py:553) train_epoch: 34
Iteration:   3320, Loss function: 4.129, Average Loss: 4.069, avg. samples / sec: 4051.75
Iteration:   3340, Loss function: 3.789, Average Loss: 4.066, avg. samples / sec: 4072.41
Iteration:   3360, Loss function: 4.274, Average Loss: 4.063, avg. samples / sec: 4063.06
Iteration:   3380, Loss function: 3.934, Average Loss: 4.060, avg. samples / sec: 4039.67
Iteration:   3400, Loss function: 4.107, Average Loss: 4.058, avg. samples / sec: 4059.17

:::MLPv0.5.0 ssd 1541711978.287262440 (train.py:553) train_epoch: 35
Iteration:   3420, Loss function: 4.032, Average Loss: 4.055, avg. samples / sec: 4061.46
Iteration:   3440, Loss function: 4.195, Average Loss: 4.052, avg. samples / sec: 4066.37
Iteration:   3460, Loss function: 3.777, Average Loss: 4.049, avg. samples / sec: 4035.66
Iteration:   3480, Loss function: 3.938, Average Loss: 4.046, avg. samples / sec: 4045.47
Iteration:   3500, Loss function: 3.877, Average Loss: 4.043, avg. samples / sec: 4044.58

:::MLPv0.5.0 ssd 1541712007.410969973 (train.py:553) train_epoch: 36
Iteration:   3520, Loss function: 3.959, Average Loss: 4.039, avg. samples / sec: 4056.68
Iteration:   3540, Loss function: 3.952, Average Loss: 4.036, avg. samples / sec: 4068.72
Iteration:   3560, Loss function: 3.768, Average Loss: 4.034, avg. samples / sec: 4065.14
Iteration:   3580, Loss function: 3.834, Average Loss: 4.032, avg. samples / sec: 4057.58

:::MLPv0.5.0 ssd 1541712036.760926485 (train.py:553) train_epoch: 37
Iteration:   3600, Loss function: 3.505, Average Loss: 4.028, avg. samples / sec: 4054.43
Iteration:   3620, Loss function: 3.838, Average Loss: 4.024, avg. samples / sec: 4053.61
Iteration:   3640, Loss function: 3.823, Average Loss: 4.021, avg. samples / sec: 4051.98
Iteration:   3660, Loss function: 3.807, Average Loss: 4.015, avg. samples / sec: 4052.78
Iteration:   3680, Loss function: 3.713, Average Loss: 4.012, avg. samples / sec: 4062.46

:::MLPv0.5.0 ssd 1541712065.845459938 (train.py:553) train_epoch: 38
Iteration:   3700, Loss function: 3.789, Average Loss: 4.009, avg. samples / sec: 4049.94
Iteration:   3720, Loss function: 3.807, Average Loss: 4.004, avg. samples / sec: 4075.15
Iteration:   3740, Loss function: 3.603, Average Loss: 4.002, avg. samples / sec: 4059.16
Iteration:   3760, Loss function: 3.981, Average Loss: 4.000, avg. samples / sec: 4052.83
Iteration:   3780, Loss function: 3.824, Average Loss: 3.996, avg. samples / sec: 4072.22

:::MLPv0.5.0 ssd 1541712094.868473053 (train.py:553) train_epoch: 39
Iteration:   3800, Loss function: 3.705, Average Loss: 3.992, avg. samples / sec: 4066.25
Iteration:   3820, Loss function: 3.736, Average Loss: 3.989, avg. samples / sec: 4067.23
Iteration:   3840, Loss function: 3.808, Average Loss: 3.985, avg. samples / sec: 4060.34
Iteration:   3860, Loss function: 3.684, Average Loss: 3.981, avg. samples / sec: 4068.83
Iteration:   3880, Loss function: 3.861, Average Loss: 3.979, avg. samples / sec: 4062.03

:::MLPv0.5.0 ssd 1541712124.214479208 (train.py:553) train_epoch: 40
Iteration:   3900, Loss function: 3.710, Average Loss: 3.976, avg. samples / sec: 4054.07
Iteration:   3920, Loss function: 3.662, Average Loss: 3.972, avg. samples / sec: 4061.79
Iteration:   3940, Loss function: 4.082, Average Loss: 3.969, avg. samples / sec: 4056.87
Iteration:   3960, Loss function: 4.317, Average Loss: 3.967, avg. samples / sec: 4062.08
Iteration:   3980, Loss function: 4.016, Average Loss: 3.966, avg. samples / sec: 4059.06

:::MLPv0.5.0 ssd 1541712153.263966560 (train.py:553) train_epoch: 41
Iteration:   4000, Loss function: 3.699, Average Loss: 3.963, avg. samples / sec: 4059.46
Iteration:   4020, Loss function: 3.982, Average Loss: 3.959, avg. samples / sec: 4046.60
Iteration:   4040, Loss function: 3.735, Average Loss: 3.956, avg. samples / sec: 4070.44
Iteration:   4060, Loss function: 4.031, Average Loss: 3.953, avg. samples / sec: 4062.58
Iteration:   4080, Loss function: 3.842, Average Loss: 3.950, avg. samples / sec: 4063.93

:::MLPv0.5.0 ssd 1541712182.314116240 (train.py:553) train_epoch: 42
Iteration:   4100, Loss function: 3.794, Average Loss: 3.948, avg. samples / sec: 4067.68
Iteration:   4120, Loss function: 3.828, Average Loss: 3.945, avg. samples / sec: 4050.36
Iteration:   4140, Loss function: 3.556, Average Loss: 3.942, avg. samples / sec: 4067.27
Iteration:   4160, Loss function: 3.804, Average Loss: 3.941, avg. samples / sec: 4077.75
Iteration:   4180, Loss function: 3.750, Average Loss: 3.937, avg. samples / sec: 4050.92

:::MLPv0.5.0 ssd 1541712211.332287073 (train.py:553) train_epoch: 43
Iteration:   4200, Loss function: 3.575, Average Loss: 3.934, avg. samples / sec: 4072.10
lr decay step #1

:::MLPv0.5.0 ssd 1541712219.412933350 (train.py:578) opt_learning_rate: 0.009500000000000001









:::MLPv0.5.0 ssd 1541712219.699075699 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541712219.699879408 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541712219.700606346 (train.py:220) eval_start: 43
Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3No object detected in idx: 59
Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Predicting Ended, total time: 35.09 s
Loading and preparing results...
Converting ndarray to lists...
(351808, 7)
0/351808
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(351808, 7)
(351808, 7)
(351808, 7)
Loading and preparing results...
(351808, 7)
0/351808
0/351808
0/351808
Converting ndarray to lists...
(351808, 7)
0/351808
0/351808
Converting ndarray to lists...
(351808, 7)
0/351808
Loading and preparing results...
Converting ndarray to lists...
(351808, 7)
0/351808
DONE (t=2.03s)
creating index...
DONE (t=2.07s)
creating index...
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.27s)
creating index...
DONE (t=2.34s)
creating index...
DONE (t=2.35s)
creating index...
DONE (t=2.35s)
creating index...
DONE (t=2.37s)
creating index...
DONE (t=2.39s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=3.96s).
Accumulating evaluation results...
DONE (t=1.24s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.154
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.293
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.150
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.044
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.158
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.257
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.074
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.395
Current AP: 0.15393 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541712262.570984364 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541712262.571887016 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.15392525279359315}

:::MLPv0.5.0 ssd 1541712262.572638035 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.15392525279359315}

:::MLPv0.5.0 ssd 1541712262.573384762 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541712262.574064970 (train.py:338) eval_stop: 43
Iteration:   4220, Loss function: 3.595, Average Loss: 3.931, avg. samples / sec: 492.99
Iteration:   4240, Loss function: 3.439, Average Loss: 3.922, avg. samples / sec: 4120.27
Iteration:   4260, Loss function: 3.127, Average Loss: 3.910, avg. samples / sec: 4099.83
Iteration:   4280, Loss function: 3.114, Average Loss: 3.898, avg. samples / sec: 4047.45

:::MLPv0.5.0 ssd 1541712283.887603521 (train.py:553) train_epoch: 44
Iteration:   4300, Loss function: 3.352, Average Loss: 3.885, avg. samples / sec: 4088.80
Iteration:   4320, Loss function: 3.372, Average Loss: 3.873, avg. samples / sec: 4070.65
Iteration:   4340, Loss function: 3.027, Average Loss: 3.860, avg. samples / sec: 4082.05
Iteration:   4360, Loss function: 3.479, Average Loss: 3.847, avg. samples / sec: 4079.92

:::MLPv0.5.0 ssd 1541712312.798034668 (train.py:553) train_epoch: 45
Iteration:   4380, Loss function: 3.111, Average Loss: 3.833, avg. samples / sec: 4077.71
Iteration:   4400, Loss function: 3.246, Average Loss: 3.821, avg. samples / sec: 4052.87
Iteration:   4420, Loss function: 3.047, Average Loss: 3.809, avg. samples / sec: 4065.38
Iteration:   4440, Loss function: 3.140, Average Loss: 3.796, avg. samples / sec: 4067.46
Iteration:   4460, Loss function: 3.148, Average Loss: 3.782, avg. samples / sec: 4050.00

:::MLPv0.5.0 ssd 1541712341.841816902 (train.py:553) train_epoch: 46
Iteration:   4480, Loss function: 3.096, Average Loss: 3.769, avg. samples / sec: 4066.03
Iteration:   4500, Loss function: 2.893, Average Loss: 3.757, avg. samples / sec: 4056.38
Iteration:   4520, Loss function: 3.197, Average Loss: 3.745, avg. samples / sec: 4073.30
Iteration:   4540, Loss function: 3.239, Average Loss: 3.733, avg. samples / sec: 4057.81
Iteration:   4560, Loss function: 2.988, Average Loss: 3.720, avg. samples / sec: 4057.82

:::MLPv0.5.0 ssd 1541712370.893027306 (train.py:553) train_epoch: 47
Iteration:   4580, Loss function: 2.925, Average Loss: 3.708, avg. samples / sec: 4067.80
Iteration:   4600, Loss function: 3.003, Average Loss: 3.695, avg. samples / sec: 4043.18
Iteration:   4620, Loss function: 2.870, Average Loss: 3.683, avg. samples / sec: 4066.00
Iteration:   4640, Loss function: 2.929, Average Loss: 3.670, avg. samples / sec: 4069.08
Iteration:   4660, Loss function: 3.073, Average Loss: 3.659, avg. samples / sec: 4050.27

:::MLPv0.5.0 ssd 1541712400.249294281 (train.py:553) train_epoch: 48
Iteration:   4680, Loss function: 2.901, Average Loss: 3.647, avg. samples / sec: 4062.36
Iteration:   4700, Loss function: 2.942, Average Loss: 3.635, avg. samples / sec: 4063.79
Iteration:   4720, Loss function: 2.764, Average Loss: 3.624, avg. samples / sec: 4057.65









:::MLPv0.5.0 ssd 1541712420.289638996 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541712420.290472984 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541712420.291163206 (train.py:220) eval_start: 48
Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Predicting Ended, total time: 33.91 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(315322, 7)
(315322, 7)
(315322, 7)
(315322, 7)
Converting ndarray to lists...
0/315322
0/315322
0/315322
0/315322
(315322, 7)
0/315322
(315322, 7)
0/315322
Converting ndarray to lists...
(315322, 7)
0/315322
Loading and preparing results...
Converting ndarray to lists...
(315322, 7)
0/315322
DONE (t=1.89s)
creating index...
DONE (t=1.92s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.99s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.20s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.72s).
Accumulating evaluation results...
DONE (t=1.12s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.377
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.229
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.217
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.314
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.093
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.351
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.516
Current AP: 0.21936 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541712461.442224979 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541712461.443082333 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.21935965239776803}

:::MLPv0.5.0 ssd 1541712461.443895817 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.21935965239776803}

:::MLPv0.5.0 ssd 1541712461.444700480 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541712461.445489883 (train.py:338) eval_stop: 48

:::MLPv0.5.0 ssd 1541712463.049874544 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1541712463.050548553 (train.py:707) run_final
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2018-11-08 09:27:48 PM
RESULT,OBJECT_DETECTION,,1635,nvidia,2018-11-08 09:00:33 PM
