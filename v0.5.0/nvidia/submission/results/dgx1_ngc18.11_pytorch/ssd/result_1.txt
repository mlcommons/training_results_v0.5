Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3

:::MLPv0.5.0 ssd 1541710833.504344463 (<string>:1) run_clear_caches
Launching on node sc-sdgx-363
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-363
+ srun -N 1 -n 1 -w sc-sdgx-363 docker exec -e DGXSYSTEM=DGX1 -e MULTI_NODE= -e SLURM_JOB_ID=155384 -e SLURM_NTASKS_PER_NODE=8 cont_155384 ./run_and_time.sh
Run vars: id 155384 gpus 8 mparams 
STARTING TIMING RUN AT 2018-11-08 09:00:33 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 152 --warmup 300 --nhwc --pad-input
1 Using seed = 1005661255
2 Using seed = 1005661256
3 Using seed = 1005661257
6 Using seed = 1005661260
4 Using seed = 1005661258
5 Using seed = 1005661259
7 Using seed = 1005661261
0 Using seed = 1005661254

:::MLPv0.5.0 ssd 1541710844.671447992 (train.py:371) run_start

:::MLPv0.5.0 ssd 1541710844.672306538 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1541710844.673012257 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1541710844.673701525 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1541710844.674423695 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1541710844.721494436 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1541710844.723444939 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1541710844.725316048 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1541710844.726886988 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1541710844.728337049 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1541710844.729749680 (train.py:382) input_size: 300
loading annotations into memory...
Done (t=0.49s)
creating index...
index created!
Done (t=0.53s)
creating index...
Done (t=0.53s)
creating index...
Done (t=0.53s)
creating index...
Done (t=0.53s)
creating index...
Done (t=0.53s)
creating index...
Done (t=0.54s)
creating index...
Done (t=0.54s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
time_check a: 1541710845.747517347
time_check b: 1541710869.472679615

:::MLPv0.5.0 ssd 1541710871.293964386 (train.py:413) input_order

:::MLPv0.5.0 ssd 1541710871.298742533 (train.py:414) input_batch_size: 152

:::MLPv0.5.0 ssd 1541710875.314339876 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710875.316627741 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710875.436650515 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1541710875.862787724 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1541710875.864217281 (train.py:477) opt_learning_rate: 0.095

:::MLPv0.5.0 ssd 1541710875.865521669 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1541710875.866940737 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1541710875.868286610 (train.py:483) opt_learning_rate_warmup_steps: 300

:::MLPv0.5.0 ssd 1541710879.894530296 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710879.896744728 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710879.954959154 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss

:::MLPv0.5.0 ssd 1541710884.661185265 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1541710884.661897421 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1541710884.665469646 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 22.638, Average Loss: 0.023, avg. samples / sec: 10536.99

:::MLPv0.5.0 ssd 1541710887.841913700 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.0003166666666666734}

:::MLPv0.5.0 ssd 1541710888.645179033 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0006333333333333468}

:::MLPv0.5.0 ssd 1541710889.109717607 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0009500000000000064}

:::MLPv0.5.0 ssd 1541710889.645298004 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0012666666666666798}

:::MLPv0.5.0 ssd 1541710890.134753227 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0015833333333333394}

:::MLPv0.5.0 ssd 1541710890.635845900 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0019000000000000128}

:::MLPv0.5.0 ssd 1541710891.091252804 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.0022166666666666723}

:::MLPv0.5.0 ssd 1541710891.532548189 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0025333333333333458}

:::MLPv0.5.0 ssd 1541710891.975768328 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0028500000000000053}

:::MLPv0.5.0 ssd 1541710892.455565214 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.0031666666666666787}

:::MLPv0.5.0 ssd 1541710892.900374174 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0034833333333333383}

:::MLPv0.5.0 ssd 1541710893.361577272 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.0038000000000000117}

:::MLPv0.5.0 ssd 1541710893.802582026 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.004116666666666671}

:::MLPv0.5.0 ssd 1541710894.259155035 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.004433333333333345}

:::MLPv0.5.0 ssd 1541710894.691368818 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.004750000000000004}

:::MLPv0.5.0 ssd 1541710895.178367376 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.005066666666666678}

:::MLPv0.5.0 ssd 1541710895.643298864 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.005383333333333337}

:::MLPv0.5.0 ssd 1541710896.107426405 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.005700000000000011}

:::MLPv0.5.0 ssd 1541710896.561794281 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.00601666666666667}

:::MLPv0.5.0 ssd 1541710897.029747725 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.006333333333333344}
Iteration:     20, Loss function: 20.668, Average Loss: 0.440, avg. samples / sec: 1967.41

:::MLPv0.5.0 ssd 1541710897.439295530 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.006650000000000003}

:::MLPv0.5.0 ssd 1541710897.843862534 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.0069666666666666766}

:::MLPv0.5.0 ssd 1541710898.282876492 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.007283333333333336}

:::MLPv0.5.0 ssd 1541710898.717215300 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.0076000000000000095}

:::MLPv0.5.0 ssd 1541710899.178721905 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.007916666666666669}

:::MLPv0.5.0 ssd 1541710899.624058723 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.008233333333333342}

:::MLPv0.5.0 ssd 1541710900.068673849 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.008550000000000002}

:::MLPv0.5.0 ssd 1541710900.498538971 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.008866666666666675}

:::MLPv0.5.0 ssd 1541710900.919895649 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.009183333333333335}

:::MLPv0.5.0 ssd 1541710901.315598965 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.009500000000000008}

:::MLPv0.5.0 ssd 1541710901.757573128 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.009816666666666668}

:::MLPv0.5.0 ssd 1541710902.201744556 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.010133333333333341}

:::MLPv0.5.0 ssd 1541710902.614779234 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.010450000000000001}

:::MLPv0.5.0 ssd 1541710903.039067030 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.010766666666666674}

:::MLPv0.5.0 ssd 1541710903.504121780 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.011083333333333334}

:::MLPv0.5.0 ssd 1541710903.884018898 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.011400000000000007}

:::MLPv0.5.0 ssd 1541710904.318391085 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.011716666666666667}

:::MLPv0.5.0 ssd 1541710904.712317467 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.01203333333333334}

:::MLPv0.5.0 ssd 1541710905.186322689 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.01235}

:::MLPv0.5.0 ssd 1541710905.614600658 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.012666666666666673}
Iteration:     40, Loss function: 13.745, Average Loss: 0.800, avg. samples / sec: 2834.36

:::MLPv0.5.0 ssd 1541710906.043609858 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.012983333333333333}

:::MLPv0.5.0 ssd 1541710906.466619492 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.013300000000000006}

:::MLPv0.5.0 ssd 1541710906.898337364 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.013616666666666666}

:::MLPv0.5.0 ssd 1541710907.363070011 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.01393333333333334}

:::MLPv0.5.0 ssd 1541710907.813305616 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.014250000000000013}

:::MLPv0.5.0 ssd 1541710908.185329199 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.014566666666666672}

:::MLPv0.5.0 ssd 1541710908.585099697 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.014883333333333346}

:::MLPv0.5.0 ssd 1541710908.930978298 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.015200000000000005}

:::MLPv0.5.0 ssd 1541710909.352346182 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.015516666666666679}

:::MLPv0.5.0 ssd 1541710909.809306145 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.015833333333333338}

:::MLPv0.5.0 ssd 1541710910.211846113 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.01615000000000001}

:::MLPv0.5.0 ssd 1541710910.695533514 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.01646666666666667}

:::MLPv0.5.0 ssd 1541710911.076087236 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.016783333333333345}

:::MLPv0.5.0 ssd 1541710911.442719460 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.017100000000000004}

:::MLPv0.5.0 ssd 1541710911.875627756 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.017416666666666678}

:::MLPv0.5.0 ssd 1541710912.267197609 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.017733333333333337}

:::MLPv0.5.0 ssd 1541710912.711099625 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.01805000000000001}

:::MLPv0.5.0 ssd 1541710913.180639744 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 58, "value": 0.01836666666666667}

:::MLPv0.5.0 ssd 1541710913.559270144 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 59, "value": 0.018683333333333343}

:::MLPv0.5.0 ssd 1541710913.939848423 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 60, "value": 0.019000000000000003}
Iteration:     60, Loss function: 11.836, Average Loss: 1.088, avg. samples / sec: 2902.88

:::MLPv0.5.0 ssd 1541710914.316803694 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 61, "value": 0.019316666666666676}

:::MLPv0.5.0 ssd 1541710914.694649458 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 62, "value": 0.019633333333333336}

:::MLPv0.5.0 ssd 1541710915.093746662 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 63, "value": 0.01995000000000001}

:::MLPv0.5.0 ssd 1541710915.496247292 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 64, "value": 0.02026666666666667}

:::MLPv0.5.0 ssd 1541710915.874895573 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 65, "value": 0.020583333333333342}

:::MLPv0.5.0 ssd 1541710916.250238180 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 66, "value": 0.020900000000000002}

:::MLPv0.5.0 ssd 1541710916.642796040 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 67, "value": 0.021216666666666675}

:::MLPv0.5.0 ssd 1541710916.986719131 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 68, "value": 0.021533333333333335}

:::MLPv0.5.0 ssd 1541710917.356037855 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 69, "value": 0.02185000000000001}

:::MLPv0.5.0 ssd 1541710917.719939470 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 70, "value": 0.022166666666666668}

:::MLPv0.5.0 ssd 1541710918.093646526 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 71, "value": 0.02248333333333334}

:::MLPv0.5.0 ssd 1541710918.492919445 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 72, "value": 0.0228}

:::MLPv0.5.0 ssd 1541710918.827909470 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 73, "value": 0.023116666666666674}

:::MLPv0.5.0 ssd 1541710919.165326357 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 74, "value": 0.023433333333333334}

:::MLPv0.5.0 ssd 1541710919.550885439 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 75, "value": 0.023750000000000007}

:::MLPv0.5.0 ssd 1541710919.881844282 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 76, "value": 0.024066666666666667}

:::MLPv0.5.0 ssd 1541710920.266366005 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 77, "value": 0.02438333333333334}

:::MLPv0.5.0 ssd 1541710920.604666710 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 78, "value": 0.0247}

:::MLPv0.5.0 ssd 1541710920.969516277 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 79, "value": 0.025016666666666673}

:::MLPv0.5.0 ssd 1541710921.333268881 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 80, "value": 0.025333333333333333}
Iteration:     80, Loss function: 9.742, Average Loss: 1.284, avg. samples / sec: 3312.07

:::MLPv0.5.0 ssd 1541710921.719017267 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 81, "value": 0.025650000000000006}

:::MLPv0.5.0 ssd 1541710922.097264051 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 82, "value": 0.025966666666666666}

:::MLPv0.5.0 ssd 1541710922.460355043 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 83, "value": 0.02628333333333334}

:::MLPv0.5.0 ssd 1541710922.854508877 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 84, "value": 0.0266}

:::MLPv0.5.0 ssd 1541710923.223902464 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 85, "value": 0.026916666666666672}

:::MLPv0.5.0 ssd 1541710923.576319695 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 86, "value": 0.02723333333333333}

:::MLPv0.5.0 ssd 1541710923.909405708 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 87, "value": 0.027550000000000005}

:::MLPv0.5.0 ssd 1541710924.255645037 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 88, "value": 0.02786666666666668}

:::MLPv0.5.0 ssd 1541710924.604743958 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 89, "value": 0.028183333333333338}

:::MLPv0.5.0 ssd 1541710924.938591003 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 90, "value": 0.02850000000000001}

:::MLPv0.5.0 ssd 1541710925.303999186 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 91, "value": 0.02881666666666667}

:::MLPv0.5.0 ssd 1541710925.646634102 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 92, "value": 0.029133333333333344}

:::MLPv0.5.0 ssd 1541710926.009794474 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 93, "value": 0.029450000000000004}

:::MLPv0.5.0 ssd 1541710926.377033710 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 94, "value": 0.029766666666666677}

:::MLPv0.5.0 ssd 1541710926.711825848 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 95, "value": 0.030083333333333337}

:::MLPv0.5.0 ssd 1541710927.054388046 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 96, "value": 0.03040000000000001}

:::MLPv0.5.0 ssd 1541710927.394151926 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 97, "value": 0.03071666666666667}

:::MLPv0.5.0 ssd 1541710927.699399710 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1541710927.736812830 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 98, "value": 0.031033333333333343}

:::MLPv0.5.0 ssd 1541710928.090371609 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 99, "value": 0.03135}

:::MLPv0.5.0 ssd 1541710928.469724655 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 100, "value": 0.031666666666666676}
Iteration:    100, Loss function: 9.253, Average Loss: 1.452, avg. samples / sec: 3407.00

:::MLPv0.5.0 ssd 1541710928.780343294 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 101, "value": 0.031983333333333336}

:::MLPv0.5.0 ssd 1541710929.127731800 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 102, "value": 0.03230000000000001}

:::MLPv0.5.0 ssd 1541710929.493233681 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 103, "value": 0.03261666666666667}

:::MLPv0.5.0 ssd 1541710929.821201324 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 104, "value": 0.032933333333333335}

:::MLPv0.5.0 ssd 1541710930.190788507 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 105, "value": 0.03325}

:::MLPv0.5.0 ssd 1541710930.599155903 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 106, "value": 0.03356666666666667}

:::MLPv0.5.0 ssd 1541710930.952567816 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 107, "value": 0.033883333333333335}

:::MLPv0.5.0 ssd 1541710931.314427137 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 108, "value": 0.03420000000000001}

:::MLPv0.5.0 ssd 1541710931.682171583 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 109, "value": 0.034516666666666675}

:::MLPv0.5.0 ssd 1541710932.026418209 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 110, "value": 0.03483333333333334}

:::MLPv0.5.0 ssd 1541710932.401045084 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 111, "value": 0.03515000000000001}

:::MLPv0.5.0 ssd 1541710932.749896049 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 112, "value": 0.035466666666666674}

:::MLPv0.5.0 ssd 1541710933.092650414 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 113, "value": 0.03578333333333334}

:::MLPv0.5.0 ssd 1541710933.467279434 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 114, "value": 0.03610000000000001}

:::MLPv0.5.0 ssd 1541710933.827108383 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 115, "value": 0.036416666666666674}

:::MLPv0.5.0 ssd 1541710934.167061329 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 116, "value": 0.03673333333333334}

:::MLPv0.5.0 ssd 1541710934.510400534 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 117, "value": 0.03705000000000001}

:::MLPv0.5.0 ssd 1541710934.844757080 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 118, "value": 0.03736666666666667}

:::MLPv0.5.0 ssd 1541710935.185508013 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 119, "value": 0.03768333333333334}

:::MLPv0.5.0 ssd 1541710935.517077684 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 120, "value": 0.038000000000000006}
Iteration:    120, Loss function: 8.942, Average Loss: 1.604, avg. samples / sec: 3447.31

:::MLPv0.5.0 ssd 1541710935.875156164 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 121, "value": 0.03831666666666667}

:::MLPv0.5.0 ssd 1541710936.210762739 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 122, "value": 0.03863333333333334}

:::MLPv0.5.0 ssd 1541710936.566334009 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 123, "value": 0.038950000000000005}

:::MLPv0.5.0 ssd 1541710936.884911060 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 124, "value": 0.03926666666666667}

:::MLPv0.5.0 ssd 1541710937.247796297 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 125, "value": 0.03958333333333334}

:::MLPv0.5.0 ssd 1541710937.589902639 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 126, "value": 0.039900000000000005}

:::MLPv0.5.0 ssd 1541710937.935178041 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 127, "value": 0.04021666666666667}

:::MLPv0.5.0 ssd 1541710938.295363903 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 128, "value": 0.04053333333333334}

:::MLPv0.5.0 ssd 1541710938.649080753 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 129, "value": 0.040850000000000004}

:::MLPv0.5.0 ssd 1541710939.032593966 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 130, "value": 0.04116666666666667}

:::MLPv0.5.0 ssd 1541710939.376641512 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 131, "value": 0.04148333333333334}

:::MLPv0.5.0 ssd 1541710939.766161203 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 132, "value": 0.041800000000000004}

:::MLPv0.5.0 ssd 1541710940.114521742 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 133, "value": 0.04211666666666667}

:::MLPv0.5.0 ssd 1541710940.471970081 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 134, "value": 0.04243333333333334}

:::MLPv0.5.0 ssd 1541710940.825096607 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 135, "value": 0.04275}

:::MLPv0.5.0 ssd 1541710941.170689106 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 136, "value": 0.04306666666666667}

:::MLPv0.5.0 ssd 1541710941.513187885 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 137, "value": 0.043383333333333336}

:::MLPv0.5.0 ssd 1541710941.905699730 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 138, "value": 0.0437}

:::MLPv0.5.0 ssd 1541710942.265846968 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 139, "value": 0.04401666666666667}

:::MLPv0.5.0 ssd 1541710942.620402098 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 140, "value": 0.044333333333333336}
Iteration:    140, Loss function: 8.887, Average Loss: 1.749, avg. samples / sec: 3426.11

:::MLPv0.5.0 ssd 1541710942.953842402 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 141, "value": 0.04465}

:::MLPv0.5.0 ssd 1541710943.290349007 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 142, "value": 0.04496666666666667}

:::MLPv0.5.0 ssd 1541710943.629840136 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 143, "value": 0.045283333333333335}

:::MLPv0.5.0 ssd 1541710943.961281300 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 144, "value": 0.0456}

:::MLPv0.5.0 ssd 1541710944.316544771 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 145, "value": 0.04591666666666667}

:::MLPv0.5.0 ssd 1541710944.647952318 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 146, "value": 0.046233333333333335}

:::MLPv0.5.0 ssd 1541710944.970397472 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 147, "value": 0.04655}

:::MLPv0.5.0 ssd 1541710945.307042837 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 148, "value": 0.04686666666666667}

:::MLPv0.5.0 ssd 1541710945.631224632 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 149, "value": 0.047183333333333334}

:::MLPv0.5.0 ssd 1541710945.988552332 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 150, "value": 0.0475}

:::MLPv0.5.0 ssd 1541710946.347886562 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 151, "value": 0.047816666666666674}

:::MLPv0.5.0 ssd 1541710946.685307741 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 152, "value": 0.04813333333333334}

:::MLPv0.5.0 ssd 1541710947.014579058 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 153, "value": 0.04845000000000001}

:::MLPv0.5.0 ssd 1541710947.342501402 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 154, "value": 0.04876666666666667}

:::MLPv0.5.0 ssd 1541710947.662887335 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 155, "value": 0.04908333333333334}

:::MLPv0.5.0 ssd 1541710947.992416143 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 156, "value": 0.049400000000000006}

:::MLPv0.5.0 ssd 1541710948.321386814 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 157, "value": 0.04971666666666667}

:::MLPv0.5.0 ssd 1541710948.654792309 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 158, "value": 0.05003333333333334}

:::MLPv0.5.0 ssd 1541710949.003762484 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 159, "value": 0.050350000000000006}

:::MLPv0.5.0 ssd 1541710949.331528425 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 160, "value": 0.05066666666666667}
Iteration:    160, Loss function: 8.400, Average Loss: 1.888, avg. samples / sec: 3627.77

:::MLPv0.5.0 ssd 1541710949.637032032 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 161, "value": 0.05098333333333334}

:::MLPv0.5.0 ssd 1541710949.977060556 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 162, "value": 0.051300000000000005}

:::MLPv0.5.0 ssd 1541710950.295211792 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 163, "value": 0.05161666666666667}

:::MLPv0.5.0 ssd 1541710950.622693062 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 164, "value": 0.05193333333333334}

:::MLPv0.5.0 ssd 1541710950.954582214 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 165, "value": 0.052250000000000005}

:::MLPv0.5.0 ssd 1541710951.286936522 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 166, "value": 0.05256666666666667}

:::MLPv0.5.0 ssd 1541710951.644425869 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 167, "value": 0.05288333333333334}

:::MLPv0.5.0 ssd 1541710951.983925104 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 168, "value": 0.053200000000000004}

:::MLPv0.5.0 ssd 1541710952.300892353 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 169, "value": 0.05351666666666667}

:::MLPv0.5.0 ssd 1541710952.623570442 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 170, "value": 0.05383333333333334}

:::MLPv0.5.0 ssd 1541710952.953955650 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 171, "value": 0.054150000000000004}

:::MLPv0.5.0 ssd 1541710953.279320717 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 172, "value": 0.05446666666666667}

:::MLPv0.5.0 ssd 1541710953.615680456 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 173, "value": 0.05478333333333334}

:::MLPv0.5.0 ssd 1541710953.952684641 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 174, "value": 0.0551}

:::MLPv0.5.0 ssd 1541710954.265789747 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 175, "value": 0.05541666666666667}

:::MLPv0.5.0 ssd 1541710954.595928907 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 176, "value": 0.055733333333333336}

:::MLPv0.5.0 ssd 1541710954.918243408 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 177, "value": 0.05605}

:::MLPv0.5.0 ssd 1541710955.239882469 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 178, "value": 0.05636666666666667}

:::MLPv0.5.0 ssd 1541710955.566381931 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 179, "value": 0.056683333333333336}

:::MLPv0.5.0 ssd 1541710955.889169216 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 180, "value": 0.057}
Iteration:    180, Loss function: 8.509, Average Loss: 2.020, avg. samples / sec: 3708.37

:::MLPv0.5.0 ssd 1541710956.220340490 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 181, "value": 0.05731666666666667}

:::MLPv0.5.0 ssd 1541710956.549847603 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 182, "value": 0.057633333333333335}

:::MLPv0.5.0 ssd 1541710956.888128281 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 183, "value": 0.05795}

:::MLPv0.5.0 ssd 1541710957.220036268 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 184, "value": 0.05826666666666667}

:::MLPv0.5.0 ssd 1541710957.543308258 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 185, "value": 0.058583333333333334}

:::MLPv0.5.0 ssd 1541710957.871962786 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 186, "value": 0.0589}

:::MLPv0.5.0 ssd 1541710958.201403618 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 187, "value": 0.05921666666666667}

:::MLPv0.5.0 ssd 1541710958.528617382 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 188, "value": 0.059533333333333334}

:::MLPv0.5.0 ssd 1541710958.849755764 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 189, "value": 0.05985}

:::MLPv0.5.0 ssd 1541710959.161255836 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 190, "value": 0.06016666666666667}

:::MLPv0.5.0 ssd 1541710959.498344898 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 191, "value": 0.06048333333333333}

:::MLPv0.5.0 ssd 1541710959.815912247 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 192, "value": 0.0608}

:::MLPv0.5.0 ssd 1541710960.145559549 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 193, "value": 0.061116666666666666}

:::MLPv0.5.0 ssd 1541710960.480012655 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 194, "value": 0.06143333333333334}

:::MLPv0.5.0 ssd 1541710960.776340723 (train.py:553) train_epoch: 2

:::MLPv0.5.0 ssd 1541710960.805480480 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 195, "value": 0.061750000000000006}

:::MLPv0.5.0 ssd 1541710961.117015839 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 196, "value": 0.06206666666666667}

:::MLPv0.5.0 ssd 1541710961.448758125 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 197, "value": 0.06238333333333334}

:::MLPv0.5.0 ssd 1541710961.776789427 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 198, "value": 0.0627}

:::MLPv0.5.0 ssd 1541710962.093260050 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 199, "value": 0.06301666666666667}

:::MLPv0.5.0 ssd 1541710962.418751478 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 200, "value": 0.06333333333333334}
Iteration:    200, Loss function: 8.158, Average Loss: 2.143, avg. samples / sec: 3724.67

:::MLPv0.5.0 ssd 1541710962.739312410 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 201, "value": 0.06365000000000001}

:::MLPv0.5.0 ssd 1541710963.060680866 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 202, "value": 0.06396666666666667}

:::MLPv0.5.0 ssd 1541710963.391184330 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 203, "value": 0.06428333333333333}

:::MLPv0.5.0 ssd 1541710963.715909958 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 204, "value": 0.0646}

:::MLPv0.5.0 ssd 1541710964.034467697 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 205, "value": 0.06491666666666668}

:::MLPv0.5.0 ssd 1541710964.343724012 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 206, "value": 0.06523333333333334}

:::MLPv0.5.0 ssd 1541710964.664462566 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 207, "value": 0.06555}

:::MLPv0.5.0 ssd 1541710964.982702255 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 208, "value": 0.06586666666666667}

:::MLPv0.5.0 ssd 1541710965.301065207 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 209, "value": 0.06618333333333334}

:::MLPv0.5.0 ssd 1541710965.645231724 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 210, "value": 0.0665}

:::MLPv0.5.0 ssd 1541710965.958119154 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 211, "value": 0.06681666666666666}

:::MLPv0.5.0 ssd 1541710966.320895910 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 212, "value": 0.06713333333333334}

:::MLPv0.5.0 ssd 1541710966.647099972 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 213, "value": 0.06745000000000001}

:::MLPv0.5.0 ssd 1541710966.965034008 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 214, "value": 0.06776666666666667}

:::MLPv0.5.0 ssd 1541710967.281100512 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 215, "value": 0.06808333333333333}

:::MLPv0.5.0 ssd 1541710967.595043421 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 216, "value": 0.0684}

:::MLPv0.5.0 ssd 1541710967.931781769 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 217, "value": 0.06871666666666668}

:::MLPv0.5.0 ssd 1541710968.259176493 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 218, "value": 0.06903333333333334}

:::MLPv0.5.0 ssd 1541710968.596199274 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 219, "value": 0.06935}

:::MLPv0.5.0 ssd 1541710968.932984352 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 220, "value": 0.06966666666666667}
Iteration:    220, Loss function: 8.127, Average Loss: 2.259, avg. samples / sec: 3732.21

:::MLPv0.5.0 ssd 1541710969.252392530 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 221, "value": 0.06998333333333334}

:::MLPv0.5.0 ssd 1541710969.578300953 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 222, "value": 0.0703}

:::MLPv0.5.0 ssd 1541710969.905330896 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 223, "value": 0.07061666666666666}

:::MLPv0.5.0 ssd 1541710970.224804163 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 224, "value": 0.07093333333333333}

:::MLPv0.5.0 ssd 1541710970.536591291 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 225, "value": 0.07125000000000001}

:::MLPv0.5.0 ssd 1541710970.839546919 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 226, "value": 0.07156666666666667}

:::MLPv0.5.0 ssd 1541710971.157270670 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 227, "value": 0.07188333333333334}

:::MLPv0.5.0 ssd 1541710971.470412016 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 228, "value": 0.0722}

:::MLPv0.5.0 ssd 1541710971.791363955 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 229, "value": 0.07251666666666667}

:::MLPv0.5.0 ssd 1541710972.148350477 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 230, "value": 0.07283333333333333}

:::MLPv0.5.0 ssd 1541710972.492751122 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 231, "value": 0.07315}

:::MLPv0.5.0 ssd 1541710972.804054260 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 232, "value": 0.07346666666666667}

:::MLPv0.5.0 ssd 1541710973.129148483 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 233, "value": 0.07378333333333334}

:::MLPv0.5.0 ssd 1541710973.438635826 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 234, "value": 0.0741}

:::MLPv0.5.0 ssd 1541710973.751143932 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 235, "value": 0.07441666666666667}

:::MLPv0.5.0 ssd 1541710974.068650007 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 236, "value": 0.07473333333333333}

:::MLPv0.5.0 ssd 1541710974.389111996 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 237, "value": 0.07505}

:::MLPv0.5.0 ssd 1541710974.700473309 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 238, "value": 0.07536666666666667}

:::MLPv0.5.0 ssd 1541710975.030214071 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 239, "value": 0.07568333333333334}

:::MLPv0.5.0 ssd 1541710975.342324972 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 240, "value": 0.076}
Iteration:    240, Loss function: 7.986, Average Loss: 2.370, avg. samples / sec: 3795.42

:::MLPv0.5.0 ssd 1541710975.651800156 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 241, "value": 0.07631666666666667}

:::MLPv0.5.0 ssd 1541710976.015023708 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 242, "value": 0.07663333333333333}

:::MLPv0.5.0 ssd 1541710976.318543911 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 243, "value": 0.07695}

:::MLPv0.5.0 ssd 1541710976.640018702 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 244, "value": 0.07726666666666666}

:::MLPv0.5.0 ssd 1541710976.988811493 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 245, "value": 0.07758333333333334}

:::MLPv0.5.0 ssd 1541710977.314890146 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 246, "value": 0.0779}

:::MLPv0.5.0 ssd 1541710977.628319979 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 247, "value": 0.07821666666666667}

:::MLPv0.5.0 ssd 1541710977.952281713 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 248, "value": 0.07853333333333334}

:::MLPv0.5.0 ssd 1541710978.277007341 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 249, "value": 0.07885}

:::MLPv0.5.0 ssd 1541710978.604175806 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 250, "value": 0.07916666666666666}

:::MLPv0.5.0 ssd 1541710978.917071104 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 251, "value": 0.07948333333333334}

:::MLPv0.5.0 ssd 1541710979.227983475 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 252, "value": 0.07980000000000001}

:::MLPv0.5.0 ssd 1541710979.550406456 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 253, "value": 0.08011666666666667}

:::MLPv0.5.0 ssd 1541710979.865581989 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 254, "value": 0.08043333333333333}

:::MLPv0.5.0 ssd 1541710980.185348034 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 255, "value": 0.08075}

:::MLPv0.5.0 ssd 1541710980.497586966 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 256, "value": 0.08106666666666668}

:::MLPv0.5.0 ssd 1541710980.821184874 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 257, "value": 0.08138333333333334}

:::MLPv0.5.0 ssd 1541710981.147958517 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 258, "value": 0.0817}

:::MLPv0.5.0 ssd 1541710981.472358465 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 259, "value": 0.08201666666666667}

:::MLPv0.5.0 ssd 1541710981.797297716 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 260, "value": 0.08233333333333334}
Iteration:    260, Loss function: 7.884, Average Loss: 2.485, avg. samples / sec: 3768.39

:::MLPv0.5.0 ssd 1541710982.101764917 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 261, "value": 0.08265}

:::MLPv0.5.0 ssd 1541710982.424309731 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 262, "value": 0.08296666666666666}

:::MLPv0.5.0 ssd 1541710982.753549337 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 263, "value": 0.08328333333333333}

:::MLPv0.5.0 ssd 1541710983.073633432 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 264, "value": 0.08360000000000001}

:::MLPv0.5.0 ssd 1541710983.386139154 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 265, "value": 0.08391666666666667}

:::MLPv0.5.0 ssd 1541710983.702763319 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 266, "value": 0.08423333333333334}

:::MLPv0.5.0 ssd 1541710984.020049334 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 267, "value": 0.08455}

:::MLPv0.5.0 ssd 1541710984.349095821 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 268, "value": 0.08486666666666667}

:::MLPv0.5.0 ssd 1541710984.675824642 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 269, "value": 0.08518333333333333}

:::MLPv0.5.0 ssd 1541710984.995982885 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 270, "value": 0.0855}

:::MLPv0.5.0 ssd 1541710985.309711933 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 271, "value": 0.08581666666666667}

:::MLPv0.5.0 ssd 1541710985.638878822 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 272, "value": 0.08613333333333334}

:::MLPv0.5.0 ssd 1541710985.949843168 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 273, "value": 0.08645}

:::MLPv0.5.0 ssd 1541710986.261413813 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 274, "value": 0.08676666666666667}

:::MLPv0.5.0 ssd 1541710986.585324287 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 275, "value": 0.08708333333333333}

:::MLPv0.5.0 ssd 1541710986.896051407 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 276, "value": 0.0874}

:::MLPv0.5.0 ssd 1541710987.192094803 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 277, "value": 0.08771666666666667}

:::MLPv0.5.0 ssd 1541710987.507613420 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 278, "value": 0.08803333333333334}

:::MLPv0.5.0 ssd 1541710987.827723265 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 279, "value": 0.08835}

:::MLPv0.5.0 ssd 1541710988.149496317 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 280, "value": 0.08866666666666667}
Iteration:    280, Loss function: 7.201, Average Loss: 2.586, avg. samples / sec: 3821.95

:::MLPv0.5.0 ssd 1541710988.466481686 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 281, "value": 0.08898333333333333}

:::MLPv0.5.0 ssd 1541710988.789768934 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 282, "value": 0.0893}

:::MLPv0.5.0 ssd 1541710989.098514795 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 283, "value": 0.08961666666666666}

:::MLPv0.5.0 ssd 1541710989.419988632 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 284, "value": 0.08993333333333334}

:::MLPv0.5.0 ssd 1541710989.733283758 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 285, "value": 0.09025}

:::MLPv0.5.0 ssd 1541710990.066420555 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 286, "value": 0.09056666666666667}

:::MLPv0.5.0 ssd 1541710990.380568504 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 287, "value": 0.09088333333333333}

:::MLPv0.5.0 ssd 1541710990.703456402 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 288, "value": 0.0912}

:::MLPv0.5.0 ssd 1541710991.023552656 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 289, "value": 0.09151666666666666}

:::MLPv0.5.0 ssd 1541710991.350056887 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 290, "value": 0.09183333333333334}

:::MLPv0.5.0 ssd 1541710991.671963930 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 291, "value": 0.09215}

:::MLPv0.5.0 ssd 1541710991.954834700 (train.py:553) train_epoch: 3

:::MLPv0.5.0 ssd 1541710991.983621359 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 292, "value": 0.09246666666666667}

:::MLPv0.5.0 ssd 1541710992.326685429 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 293, "value": 0.09278333333333333}

:::MLPv0.5.0 ssd 1541710992.636663914 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 294, "value": 0.0931}

:::MLPv0.5.0 ssd 1541710992.959231138 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 295, "value": 0.09341666666666666}

:::MLPv0.5.0 ssd 1541710993.281423569 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 296, "value": 0.09373333333333334}

:::MLPv0.5.0 ssd 1541710993.589645624 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 297, "value": 0.09405}

:::MLPv0.5.0 ssd 1541710993.916574240 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 298, "value": 0.09436666666666667}

:::MLPv0.5.0 ssd 1541710994.230762005 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 299, "value": 0.09468333333333333}
Iteration:    300, Loss function: 7.768, Average Loss: 2.681, avg. samples / sec: 3820.86
Iteration:    320, Loss function: 7.648, Average Loss: 2.777, avg. samples / sec: 3882.69
Iteration:    340, Loss function: 6.913, Average Loss: 2.864, avg. samples / sec: 3923.83
Iteration:    360, Loss function: 6.858, Average Loss: 2.944, avg. samples / sec: 3930.33
Iteration:    380, Loss function: 6.678, Average Loss: 3.021, avg. samples / sec: 3934.12

:::MLPv0.5.0 ssd 1541711022.462855101 (train.py:553) train_epoch: 4
Iteration:    400, Loss function: 6.498, Average Loss: 3.092, avg. samples / sec: 3917.89
Iteration:    420, Loss function: 6.581, Average Loss: 3.162, avg. samples / sec: 3956.80
Iteration:    440, Loss function: 6.282, Average Loss: 3.227, avg. samples / sec: 3941.15
Iteration:    460, Loss function: 6.185, Average Loss: 3.288, avg. samples / sec: 4012.53
Iteration:    480, Loss function: 6.102, Average Loss: 3.349, avg. samples / sec: 3978.43

:::MLPv0.5.0 ssd 1541711052.194463730 (train.py:553) train_epoch: 5
Iteration:    500, Loss function: 6.009, Average Loss: 3.406, avg. samples / sec: 3954.84
Iteration:    520, Loss function: 5.937, Average Loss: 3.458, avg. samples / sec: 3987.74
Iteration:    540, Loss function: 6.179, Average Loss: 3.513, avg. samples / sec: 4015.45
Iteration:    560, Loss function: 5.925, Average Loss: 3.560, avg. samples / sec: 4002.91
Iteration:    580, Loss function: 5.700, Average Loss: 3.605, avg. samples / sec: 3994.41

:::MLPv0.5.0 ssd 1541711081.742090464 (train.py:553) train_epoch: 6
Iteration:    600, Loss function: 5.739, Average Loss: 3.650, avg. samples / sec: 3959.41
Iteration:    620, Loss function: 5.547, Average Loss: 3.690, avg. samples / sec: 4005.63
Iteration:    640, Loss function: 5.589, Average Loss: 3.727, avg. samples / sec: 3985.99
Iteration:    660, Loss function: 5.410, Average Loss: 3.761, avg. samples / sec: 4009.33
Iteration:    680, Loss function: 5.688, Average Loss: 3.798, avg. samples / sec: 4012.64

:::MLPv0.5.0 ssd 1541711111.246579647 (train.py:553) train_epoch: 7
Iteration:    700, Loss function: 5.300, Average Loss: 3.830, avg. samples / sec: 4036.06
Iteration:    720, Loss function: 5.306, Average Loss: 3.859, avg. samples / sec: 3980.07
Iteration:    740, Loss function: 5.208, Average Loss: 3.886, avg. samples / sec: 4025.96
Iteration:    760, Loss function: 5.371, Average Loss: 3.917, avg. samples / sec: 4002.49

:::MLPv0.5.0 ssd 1541711140.977473974 (train.py:553) train_epoch: 8
Iteration:    780, Loss function: 5.301, Average Loss: 3.944, avg. samples / sec: 4008.36
Iteration:    800, Loss function: 5.097, Average Loss: 3.968, avg. samples / sec: 4044.98
Iteration:    820, Loss function: 4.984, Average Loss: 3.988, avg. samples / sec: 4034.46
Iteration:    840, Loss function: 5.043, Average Loss: 4.010, avg. samples / sec: 3999.63
Iteration:    860, Loss function: 5.069, Average Loss: 4.030, avg. samples / sec: 4054.28

:::MLPv0.5.0 ssd 1541711170.192195177 (train.py:553) train_epoch: 9
Iteration:    880, Loss function: 5.039, Average Loss: 4.052, avg. samples / sec: 4024.23
Iteration:    900, Loss function: 4.778, Average Loss: 4.071, avg. samples / sec: 4052.49
Iteration:    920, Loss function: 5.121, Average Loss: 4.090, avg. samples / sec: 4022.20
Iteration:    940, Loss function: 4.972, Average Loss: 4.107, avg. samples / sec: 4028.69
Iteration:    960, Loss function: 5.141, Average Loss: 4.124, avg. samples / sec: 3985.39

:::MLPv0.5.0 ssd 1541711199.585473061 (train.py:553) train_epoch: 10
Iteration:    980, Loss function: 4.775, Average Loss: 4.140, avg. samples / sec: 4007.92
Iteration:   1000, Loss function: 4.768, Average Loss: 4.153, avg. samples / sec: 4012.61
Iteration:   1020, Loss function: 4.804, Average Loss: 4.166, avg. samples / sec: 4036.22
Iteration:   1040, Loss function: 4.813, Average Loss: 4.180, avg. samples / sec: 4025.06
Iteration:   1060, Loss function: 5.160, Average Loss: 4.193, avg. samples / sec: 4007.66

:::MLPv0.5.0 ssd 1541711229.197142124 (train.py:553) train_epoch: 11
Iteration:   1080, Loss function: 4.805, Average Loss: 4.205, avg. samples / sec: 4041.29
Iteration:   1100, Loss function: 4.929, Average Loss: 4.216, avg. samples / sec: 3979.64
Iteration:   1120, Loss function: 4.745, Average Loss: 4.225, avg. samples / sec: 4067.07
Iteration:   1140, Loss function: 4.978, Average Loss: 4.235, avg. samples / sec: 4034.70
Iteration:   1160, Loss function: 4.751, Average Loss: 4.243, avg. samples / sec: 4013.34

:::MLPv0.5.0 ssd 1541711258.529809952 (train.py:553) train_epoch: 12
Iteration:   1180, Loss function: 4.730, Average Loss: 4.252, avg. samples / sec: 4001.50
Iteration:   1200, Loss function: 4.481, Average Loss: 4.260, avg. samples / sec: 4039.79
Iteration:   1220, Loss function: 4.679, Average Loss: 4.268, avg. samples / sec: 4025.97
Iteration:   1240, Loss function: 4.559, Average Loss: 4.274, avg. samples / sec: 4005.37
Iteration:   1260, Loss function: 4.441, Average Loss: 4.280, avg. samples / sec: 4033.12

:::MLPv0.5.0 ssd 1541711287.830795050 (train.py:553) train_epoch: 13
Iteration:   1280, Loss function: 4.348, Average Loss: 4.286, avg. samples / sec: 4065.95
Iteration:   1300, Loss function: 4.698, Average Loss: 4.294, avg. samples / sec: 4068.05
Iteration:   1320, Loss function: 4.303, Average Loss: 4.300, avg. samples / sec: 4046.62
Iteration:   1340, Loss function: 4.519, Average Loss: 4.304, avg. samples / sec: 4009.92
Iteration:   1360, Loss function: 4.454, Average Loss: 4.309, avg. samples / sec: 4042.21

:::MLPv0.5.0 ssd 1541711316.980761290 (train.py:553) train_epoch: 14
Iteration:   1380, Loss function: 4.390, Average Loss: 4.313, avg. samples / sec: 4056.60
Iteration:   1400, Loss function: 4.424, Average Loss: 4.318, avg. samples / sec: 4048.03
Iteration:   1420, Loss function: 4.159, Average Loss: 4.322, avg. samples / sec: 4010.69
Iteration:   1440, Loss function: 4.277, Average Loss: 4.326, avg. samples / sec: 4020.10

:::MLPv0.5.0 ssd 1541711346.475231171 (train.py:553) train_epoch: 15
Iteration:   1460, Loss function: 4.355, Average Loss: 4.327, avg. samples / sec: 4066.39
Iteration:   1480, Loss function: 4.395, Average Loss: 4.329, avg. samples / sec: 4055.28
Iteration:   1500, Loss function: 4.542, Average Loss: 4.333, avg. samples / sec: 4065.35
Iteration:   1520, Loss function: 4.089, Average Loss: 4.333, avg. samples / sec: 4031.09
Iteration:   1540, Loss function: 4.613, Average Loss: 4.333, avg. samples / sec: 4057.92

:::MLPv0.5.0 ssd 1541711375.597236633 (train.py:553) train_epoch: 16
Iteration:   1560, Loss function: 4.367, Average Loss: 4.335, avg. samples / sec: 4034.37
Iteration:   1580, Loss function: 4.577, Average Loss: 4.337, avg. samples / sec: 4042.72
Iteration:   1600, Loss function: 4.381, Average Loss: 4.339, avg. samples / sec: 4042.29
Iteration:   1620, Loss function: 4.366, Average Loss: 4.337, avg. samples / sec: 4067.04
Iteration:   1640, Loss function: 4.230, Average Loss: 4.338, avg. samples / sec: 4034.22

:::MLPv0.5.0 ssd 1541711404.780352831 (train.py:553) train_epoch: 17
Iteration:   1660, Loss function: 4.411, Average Loss: 4.339, avg. samples / sec: 4042.64
Iteration:   1680, Loss function: 4.732, Average Loss: 4.340, avg. samples / sec: 4079.21
Iteration:   1700, Loss function: 4.366, Average Loss: 4.341, avg. samples / sec: 4088.13
Iteration:   1720, Loss function: 4.254, Average Loss: 4.339, avg. samples / sec: 4038.89
Iteration:   1740, Loss function: 4.585, Average Loss: 4.341, avg. samples / sec: 4054.35

:::MLPv0.5.0 ssd 1541711433.824204683 (train.py:553) train_epoch: 18
Iteration:   1760, Loss function: 4.432, Average Loss: 4.343, avg. samples / sec: 4039.48
Iteration:   1780, Loss function: 4.194, Average Loss: 4.342, avg. samples / sec: 4069.13
Iteration:   1800, Loss function: 4.167, Average Loss: 4.342, avg. samples / sec: 4032.28
Iteration:   1820, Loss function: 4.117, Average Loss: 4.341, avg. samples / sec: 4055.60
Iteration:   1840, Loss function: 4.323, Average Loss: 4.340, avg. samples / sec: 4053.85

:::MLPv0.5.0 ssd 1541711463.223657846 (train.py:553) train_epoch: 19
Iteration:   1860, Loss function: 4.292, Average Loss: 4.337, avg. samples / sec: 4037.30
Iteration:   1880, Loss function: 4.440, Average Loss: 4.337, avg. samples / sec: 4062.56
Iteration:   1900, Loss function: 4.171, Average Loss: 4.336, avg. samples / sec: 4062.13
Iteration:   1920, Loss function: 4.203, Average Loss: 4.333, avg. samples / sec: 4052.70
Iteration:   1940, Loss function: 4.116, Average Loss: 4.331, avg. samples / sec: 4066.75

:::MLPv0.5.0 ssd 1541711492.283370972 (train.py:553) train_epoch: 20
Iteration:   1960, Loss function: 4.380, Average Loss: 4.329, avg. samples / sec: 4071.00
Iteration:   1980, Loss function: 4.078, Average Loss: 4.329, avg. samples / sec: 4060.32
Iteration:   2000, Loss function: 4.411, Average Loss: 4.327, avg. samples / sec: 4054.39
Iteration:   2020, Loss function: 4.062, Average Loss: 4.323, avg. samples / sec: 4040.34
Iteration:   2040, Loss function: 4.481, Average Loss: 4.322, avg. samples / sec: 4053.86

:::MLPv0.5.0 ssd 1541711521.373349190 (train.py:553) train_epoch: 21
Iteration:   2060, Loss function: 4.241, Average Loss: 4.321, avg. samples / sec: 4086.21
Iteration:   2080, Loss function: 4.151, Average Loss: 4.319, avg. samples / sec: 4038.52
Iteration:   2100, Loss function: 4.332, Average Loss: 4.315, avg. samples / sec: 4047.31
Iteration:   2120, Loss function: 3.967, Average Loss: 4.311, avg. samples / sec: 4072.81
Iteration:   2140, Loss function: 4.022, Average Loss: 4.309, avg. samples / sec: 4074.37

:::MLPv0.5.0 ssd 1541711550.723465204 (train.py:553) train_epoch: 22
Iteration:   2160, Loss function: 4.263, Average Loss: 4.304, avg. samples / sec: 4074.93
Iteration:   2180, Loss function: 4.261, Average Loss: 4.301, avg. samples / sec: 4051.48
Iteration:   2200, Loss function: 4.096, Average Loss: 4.299, avg. samples / sec: 4067.28
Iteration:   2220, Loss function: 4.324, Average Loss: 4.296, avg. samples / sec: 4039.52

:::MLPv0.5.0 ssd 1541711579.750091553 (train.py:553) train_epoch: 23
Iteration:   2240, Loss function: 4.064, Average Loss: 4.293, avg. samples / sec: 4077.15
Iteration:   2260, Loss function: 3.940, Average Loss: 4.290, avg. samples / sec: 4066.15
Iteration:   2280, Loss function: 3.947, Average Loss: 4.286, avg. samples / sec: 4053.44
Iteration:   2300, Loss function: 4.103, Average Loss: 4.282, avg. samples / sec: 4063.96
Iteration:   2320, Loss function: 4.139, Average Loss: 4.278, avg. samples / sec: 4061.34

:::MLPv0.5.0 ssd 1541711608.800166130 (train.py:553) train_epoch: 24
Iteration:   2340, Loss function: 4.071, Average Loss: 4.275, avg. samples / sec: 4047.70
Iteration:   2360, Loss function: 4.296, Average Loss: 4.274, avg. samples / sec: 4090.24
Iteration:   2380, Loss function: 3.876, Average Loss: 4.271, avg. samples / sec: 4067.54
Iteration:   2400, Loss function: 3.956, Average Loss: 4.266, avg. samples / sec: 4071.37
Iteration:   2420, Loss function: 4.123, Average Loss: 4.264, avg. samples / sec: 4080.50

:::MLPv0.5.0 ssd 1541711637.781227350 (train.py:553) train_epoch: 25
Iteration:   2440, Loss function: 4.323, Average Loss: 4.262, avg. samples / sec: 4059.86
Iteration:   2460, Loss function: 3.855, Average Loss: 4.257, avg. samples / sec: 4073.12
Iteration:   2480, Loss function: 4.062, Average Loss: 4.252, avg. samples / sec: 4049.39
Iteration:   2500, Loss function: 4.494, Average Loss: 4.249, avg. samples / sec: 4081.57
Iteration:   2520, Loss function: 4.016, Average Loss: 4.247, avg. samples / sec: 4085.81

:::MLPv0.5.0 ssd 1541711667.037127018 (train.py:553) train_epoch: 26
Iteration:   2540, Loss function: 3.990, Average Loss: 4.243, avg. samples / sec: 4071.09
Iteration:   2560, Loss function: 3.865, Average Loss: 4.240, avg. samples / sec: 4064.61
Iteration:   2580, Loss function: 3.959, Average Loss: 4.237, avg. samples / sec: 4098.59
Iteration:   2600, Loss function: 3.756, Average Loss: 4.233, avg. samples / sec: 4058.27
Iteration:   2620, Loss function: 3.869, Average Loss: 4.230, avg. samples / sec: 4089.45

:::MLPv0.5.0 ssd 1541711695.949904203 (train.py:553) train_epoch: 27
Iteration:   2640, Loss function: 4.345, Average Loss: 4.225, avg. samples / sec: 4088.98
Iteration:   2660, Loss function: 3.968, Average Loss: 4.224, avg. samples / sec: 4078.96
Iteration:   2680, Loss function: 4.315, Average Loss: 4.221, avg. samples / sec: 4054.25
Iteration:   2700, Loss function: 3.788, Average Loss: 4.216, avg. samples / sec: 4080.03
Iteration:   2720, Loss function: 4.034, Average Loss: 4.213, avg. samples / sec: 4054.76

:::MLPv0.5.0 ssd 1541711724.928044796 (train.py:553) train_epoch: 28
Iteration:   2740, Loss function: 4.043, Average Loss: 4.209, avg. samples / sec: 4096.61
Iteration:   2760, Loss function: 3.998, Average Loss: 4.206, avg. samples / sec: 4066.87
Iteration:   2780, Loss function: 4.217, Average Loss: 4.203, avg. samples / sec: 4063.21
Iteration:   2800, Loss function: 4.080, Average Loss: 4.199, avg. samples / sec: 4085.79
Iteration:   2820, Loss function: 3.820, Average Loss: 4.195, avg. samples / sec: 4093.84

:::MLPv0.5.0 ssd 1541711753.834467411 (train.py:553) train_epoch: 29
Iteration:   2840, Loss function: 3.927, Average Loss: 4.191, avg. samples / sec: 4071.21
Iteration:   2860, Loss function: 3.917, Average Loss: 4.188, avg. samples / sec: 4071.70
Iteration:   2880, Loss function: 3.905, Average Loss: 4.186, avg. samples / sec: 4055.56
Iteration:   2900, Loss function: 4.017, Average Loss: 4.181, avg. samples / sec: 4053.12

:::MLPv0.5.0 ssd 1541711783.156671286 (train.py:553) train_epoch: 30
Iteration:   2920, Loss function: 3.869, Average Loss: 4.177, avg. samples / sec: 4068.94
Iteration:   2940, Loss function: 3.995, Average Loss: 4.173, avg. samples / sec: 4098.80
Iteration:   2960, Loss function: 3.873, Average Loss: 4.168, avg. samples / sec: 4089.00
Iteration:   2980, Loss function: 4.251, Average Loss: 4.164, avg. samples / sec: 4064.97
Iteration:   3000, Loss function: 4.013, Average Loss: 4.161, avg. samples / sec: 4075.06

:::MLPv0.5.0 ssd 1541711812.104405403 (train.py:553) train_epoch: 31
Iteration:   3020, Loss function: 3.908, Average Loss: 4.157, avg. samples / sec: 4042.68
Iteration:   3040, Loss function: 4.150, Average Loss: 4.153, avg. samples / sec: 4090.68
Iteration:   3060, Loss function: 3.744, Average Loss: 4.150, avg. samples / sec: 4094.28
Iteration:   3080, Loss function: 3.759, Average Loss: 4.146, avg. samples / sec: 4077.89
Iteration:   3100, Loss function: 4.001, Average Loss: 4.142, avg. samples / sec: 4073.19

:::MLPv0.5.0 ssd 1541711840.980986118 (train.py:553) train_epoch: 32
Iteration:   3120, Loss function: 3.955, Average Loss: 4.138, avg. samples / sec: 4079.74
Iteration:   3140, Loss function: 3.963, Average Loss: 4.135, avg. samples / sec: 4058.37









:::MLPv0.5.0 ssd 1541711854.476840734 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711854.477705002 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711854.478472233 (train.py:220) eval_start: 32
Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3No object detected in idx: 6
Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Predicting Ended, total time: 40.85 s
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(373350, 7)
(373350, 7)
0/373350
0/373350
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(373350, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/373350
Loading and preparing results...
(373350, 7)
Loading and preparing results...
0/373350
Converting ndarray to lists...
(373350, 7)
(373350, 7)
Converting ndarray to lists...
0/373350
0/373350
(373350, 7)
0/373350
Converting ndarray to lists...
(373350, 7)
0/373350
DONE (t=2.53s)
creating index...
DONE (t=2.54s)
creating index...
DONE (t=2.57s)
creating index...
DONE (t=2.58s)
creating index...
DONE (t=2.60s)
creating index...
DONE (t=2.64s)
creating index...
DONE (t=2.64s)
creating index...
DONE (t=2.64s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=3.96s).
Accumulating evaluation results...
DONE (t=1.34s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.142
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.276
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.135
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.157
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.221
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.164
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.257
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.063
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.285
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393
Current AP: 0.14151 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711903.553828239 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711903.554748535 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.1415088816394731}

:::MLPv0.5.0 ssd 1541711903.555476904 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.1415088816394731}

:::MLPv0.5.0 ssd 1541711903.556238890 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711903.556988478 (train.py:338) eval_stop: 32
Iteration:   3160, Loss function: 4.094, Average Loss: 4.132, avg. samples / sec: 435.34
Iteration:   3180, Loss function: 3.957, Average Loss: 4.128, avg. samples / sec: 4097.39
Iteration:   3200, Loss function: 3.883, Average Loss: 4.123, avg. samples / sec: 4097.33

:::MLPv0.5.0 ssd 1541711920.083360672 (train.py:553) train_epoch: 33
Iteration:   3220, Loss function: 4.155, Average Loss: 4.120, avg. samples / sec: 4101.31
Iteration:   3240, Loss function: 3.948, Average Loss: 4.116, avg. samples / sec: 4108.26
Iteration:   3260, Loss function: 3.833, Average Loss: 4.113, avg. samples / sec: 4091.39
Iteration:   3280, Loss function: 3.725, Average Loss: 4.109, avg. samples / sec: 4100.32
Iteration:   3300, Loss function: 3.806, Average Loss: 4.106, avg. samples / sec: 4061.54

:::MLPv0.5.0 ssd 1541711948.933767796 (train.py:553) train_epoch: 34
Iteration:   3320, Loss function: 3.842, Average Loss: 4.102, avg. samples / sec: 4054.35
Iteration:   3340, Loss function: 3.738, Average Loss: 4.097, avg. samples / sec: 4048.98
Iteration:   3360, Loss function: 3.995, Average Loss: 4.095, avg. samples / sec: 4080.16
Iteration:   3380, Loss function: 4.104, Average Loss: 4.091, avg. samples / sec: 4082.62
Iteration:   3400, Loss function: 3.758, Average Loss: 4.086, avg. samples / sec: 4042.34

:::MLPv0.5.0 ssd 1541711977.951253653 (train.py:553) train_epoch: 35
Iteration:   3420, Loss function: 3.981, Average Loss: 4.084, avg. samples / sec: 4091.99
Iteration:   3440, Loss function: 3.908, Average Loss: 4.080, avg. samples / sec: 4058.54
Iteration:   3460, Loss function: 3.858, Average Loss: 4.078, avg. samples / sec: 4087.86
Iteration:   3480, Loss function: 3.827, Average Loss: 4.075, avg. samples / sec: 4043.24
Iteration:   3500, Loss function: 4.083, Average Loss: 4.072, avg. samples / sec: 4053.61

:::MLPv0.5.0 ssd 1541712006.963593721 (train.py:553) train_epoch: 36
Iteration:   3520, Loss function: 3.952, Average Loss: 4.067, avg. samples / sec: 4073.01
Iteration:   3540, Loss function: 3.928, Average Loss: 4.064, avg. samples / sec: 4079.70
Iteration:   3560, Loss function: 4.040, Average Loss: 4.061, avg. samples / sec: 4096.05
Iteration:   3580, Loss function: 3.856, Average Loss: 4.057, avg. samples / sec: 4063.11

:::MLPv0.5.0 ssd 1541712036.193692684 (train.py:553) train_epoch: 37
Iteration:   3600, Loss function: 3.760, Average Loss: 4.054, avg. samples / sec: 4072.64
Iteration:   3620, Loss function: 3.640, Average Loss: 4.051, avg. samples / sec: 4066.46
Iteration:   3640, Loss function: 4.066, Average Loss: 4.047, avg. samples / sec: 4066.25
Iteration:   3660, Loss function: 4.016, Average Loss: 4.043, avg. samples / sec: 4060.47
Iteration:   3680, Loss function: 3.864, Average Loss: 4.039, avg. samples / sec: 4070.24

:::MLPv0.5.0 ssd 1541712065.208119869 (train.py:553) train_epoch: 38
Iteration:   3700, Loss function: 3.877, Average Loss: 4.036, avg. samples / sec: 4071.98
Iteration:   3720, Loss function: 3.847, Average Loss: 4.031, avg. samples / sec: 4073.36
Iteration:   3740, Loss function: 3.847, Average Loss: 4.028, avg. samples / sec: 4076.59
Iteration:   3760, Loss function: 3.957, Average Loss: 4.025, avg. samples / sec: 4079.54
Iteration:   3780, Loss function: 3.779, Average Loss: 4.023, avg. samples / sec: 4057.83

:::MLPv0.5.0 ssd 1541712094.169407129 (train.py:553) train_epoch: 39
Iteration:   3800, Loss function: 4.081, Average Loss: 4.020, avg. samples / sec: 4074.41
Iteration:   3820, Loss function: 3.811, Average Loss: 4.018, avg. samples / sec: 4082.66
Iteration:   3840, Loss function: 3.926, Average Loss: 4.015, avg. samples / sec: 4092.38
Iteration:   3860, Loss function: 3.784, Average Loss: 4.010, avg. samples / sec: 4063.42
Iteration:   3880, Loss function: 4.004, Average Loss: 4.005, avg. samples / sec: 4081.03

:::MLPv0.5.0 ssd 1541712123.434318542 (train.py:553) train_epoch: 40
Iteration:   3900, Loss function: 3.672, Average Loss: 4.001, avg. samples / sec: 4038.10
Iteration:   3920, Loss function: 3.677, Average Loss: 3.997, avg. samples / sec: 4078.93
Iteration:   3940, Loss function: 3.952, Average Loss: 3.994, avg. samples / sec: 4082.55
Iteration:   3960, Loss function: 4.060, Average Loss: 3.990, avg. samples / sec: 4070.86
Iteration:   3980, Loss function: 3.957, Average Loss: 3.988, avg. samples / sec: 4075.63

:::MLPv0.5.0 ssd 1541712152.385664463 (train.py:553) train_epoch: 41
Iteration:   4000, Loss function: 3.966, Average Loss: 3.985, avg. samples / sec: 4062.50
Iteration:   4020, Loss function: 3.596, Average Loss: 3.982, avg. samples / sec: 4050.11
Iteration:   4040, Loss function: 3.968, Average Loss: 3.978, avg. samples / sec: 4086.89
Iteration:   4060, Loss function: 4.219, Average Loss: 3.975, avg. samples / sec: 4101.79
Iteration:   4080, Loss function: 3.827, Average Loss: 3.972, avg. samples / sec: 4083.29

:::MLPv0.5.0 ssd 1541712181.302212238 (train.py:553) train_epoch: 42
Iteration:   4100, Loss function: 3.848, Average Loss: 3.969, avg. samples / sec: 4083.38
Iteration:   4120, Loss function: 3.904, Average Loss: 3.966, avg. samples / sec: 4094.10
Iteration:   4140, Loss function: 3.844, Average Loss: 3.964, avg. samples / sec: 4076.45
Iteration:   4160, Loss function: 3.955, Average Loss: 3.962, avg. samples / sec: 4079.21
Iteration:   4180, Loss function: 3.725, Average Loss: 3.958, avg. samples / sec: 4080.41

:::MLPv0.5.0 ssd 1541712210.203723192 (train.py:553) train_epoch: 43
Iteration:   4200, Loss function: 3.868, Average Loss: 3.955, avg. samples / sec: 4078.03
lr decay step #1

:::MLPv0.5.0 ssd 1541712218.257531643 (train.py:578) opt_learning_rate: 0.009500000000000001









:::MLPv0.5.0 ssd 1541712218.554020643 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541712218.555052519 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541712218.555850983 (train.py:220) eval_start: 43
Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Predicting Ended, total time: 34.02 s
Loading and preparing results...
Converting ndarray to lists...
(321395, 7)
0/321395
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(321395, 7)
Converting ndarray to lists...
(321395, 7)
Loading and preparing results...
0/321395
0/321395
(321395, 7)
(321395, 7)
Converting ndarray to lists...
Loading and preparing results...
0/321395
0/321395
(321395, 7)
Converting ndarray to lists...
0/321395
(321395, 7)
0/321395
Converting ndarray to lists...
(321395, 7)
0/321395
DONE (t=1.85s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.07s)
creating index...
DONE (t=2.08s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.20s)
creating index...
index created!
index created!
DONE (t=2.24s)
creating index...
index created!
index created!
index created!
index created!
index created!
DONE (t=3.60s).
Accumulating evaluation results...
DONE (t=1.12s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.153
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.292
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.149
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.042
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.164
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.241
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.171
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.261
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.073
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.268
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408
Current AP: 0.15331 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541712259.889086246 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541712259.890014410 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.1533131451000706}

:::MLPv0.5.0 ssd 1541712259.890777826 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.1533131451000706}

:::MLPv0.5.0 ssd 1541712259.891478777 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541712259.892210484 (train.py:338) eval_stop: 43
Iteration:   4220, Loss function: 3.522, Average Loss: 3.951, avg. samples / sec: 509.75
Iteration:   4240, Loss function: 2.945, Average Loss: 3.942, avg. samples / sec: 4090.11
Iteration:   4260, Loss function: 3.296, Average Loss: 3.931, avg. samples / sec: 4114.45
Iteration:   4280, Loss function: 3.233, Average Loss: 3.918, avg. samples / sec: 4102.20

:::MLPv0.5.0 ssd 1541712281.054229259 (train.py:553) train_epoch: 44
Iteration:   4300, Loss function: 3.455, Average Loss: 3.906, avg. samples / sec: 4097.48
Iteration:   4320, Loss function: 3.098, Average Loss: 3.894, avg. samples / sec: 4086.91
Iteration:   4340, Loss function: 3.263, Average Loss: 3.880, avg. samples / sec: 4110.50
Iteration:   4360, Loss function: 3.151, Average Loss: 3.867, avg. samples / sec: 4087.65

:::MLPv0.5.0 ssd 1541712309.838126659 (train.py:553) train_epoch: 45
Iteration:   4380, Loss function: 3.342, Average Loss: 3.853, avg. samples / sec: 4106.51
Iteration:   4400, Loss function: 3.147, Average Loss: 3.840, avg. samples / sec: 4081.27
Iteration:   4420, Loss function: 3.275, Average Loss: 3.828, avg. samples / sec: 4074.97
Iteration:   4440, Loss function: 3.237, Average Loss: 3.815, avg. samples / sec: 4049.07
Iteration:   4460, Loss function: 3.023, Average Loss: 3.803, avg. samples / sec: 4080.56

:::MLPv0.5.0 ssd 1541712338.793021441 (train.py:553) train_epoch: 46
Iteration:   4480, Loss function: 3.103, Average Loss: 3.790, avg. samples / sec: 4077.60
Iteration:   4500, Loss function: 3.297, Average Loss: 3.777, avg. samples / sec: 4087.24
Iteration:   4520, Loss function: 3.408, Average Loss: 3.765, avg. samples / sec: 4077.72
Iteration:   4540, Loss function: 3.198, Average Loss: 3.753, avg. samples / sec: 4071.51
Iteration:   4560, Loss function: 3.020, Average Loss: 3.740, avg. samples / sec: 4085.72

:::MLPv0.5.0 ssd 1541712367.733623028 (train.py:553) train_epoch: 47
Iteration:   4580, Loss function: 2.779, Average Loss: 3.727, avg. samples / sec: 4073.41
Iteration:   4600, Loss function: 3.018, Average Loss: 3.714, avg. samples / sec: 4101.49
Iteration:   4620, Loss function: 3.041, Average Loss: 3.703, avg. samples / sec: 4103.46
Iteration:   4640, Loss function: 3.193, Average Loss: 3.690, avg. samples / sec: 4091.37
Iteration:   4660, Loss function: 3.108, Average Loss: 3.678, avg. samples / sec: 4056.78

:::MLPv0.5.0 ssd 1541712396.870628357 (train.py:553) train_epoch: 48
Iteration:   4680, Loss function: 2.906, Average Loss: 3.665, avg. samples / sec: 4089.61
Iteration:   4700, Loss function: 3.000, Average Loss: 3.653, avg. samples / sec: 4101.29
Iteration:   4720, Loss function: 2.973, Average Loss: 3.642, avg. samples / sec: 4088.11









:::MLPv0.5.0 ssd 1541712416.816698551 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541712416.817575216 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541712416.818292379 (train.py:220) eval_start: 48
Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Predicting Ended, total time: 41.12 s
Loading and preparing results...
Converting ndarray to lists...
(311919, 7)
0/311919
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(311919, 7)
(311919, 7)
(311919, 7)
(311919, 7)
0/311919
0/311919
0/311919
0/311919
Converting ndarray to lists...
(311919, 7)
0/311919
(311919, 7)
0/311919
Loading and preparing results...
Converting ndarray to lists...
(311919, 7)
0/311919
DONE (t=1.82s)
creating index...
DONE (t=1.94s)
creating index...
index created!
DONE (t=2.01s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.06s)
creating index...
index created!
DONE (t=2.09s)
creating index...
DONE (t=2.09s)
creating index...
DONE (t=2.12s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
DONE (t=3.65s).
Accumulating evaluation results...
DONE (t=1.14s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.373
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.225
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.227
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.213
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.309
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.093
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.347
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503
Current AP: 0.21726 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541712465.011938810 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541712465.012865305 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.21725867605852686}

:::MLPv0.5.0 ssd 1541712465.013608932 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.21725867605852686}

:::MLPv0.5.0 ssd 1541712465.014341354 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541712465.015064716 (train.py:338) eval_stop: 48

:::MLPv0.5.0 ssd 1541712466.621717215 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1541712466.622416019 (train.py:707) run_final
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2018-11-08 09:27:51 PM
RESULT,OBJECT_DETECTION,,1638,nvidia,2018-11-08 09:00:33 PM
