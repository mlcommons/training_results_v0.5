Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3

:::MLPv0.5.0 ssd 1541710836.505435467 (<string>:1) run_clear_caches
Launching on node sc-sdgx-371
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w sc-sdgx-371
+ srun -N 1 -n 1 -w sc-sdgx-371 docker exec -e DGXSYSTEM=DGX1 -e MULTI_NODE= -e SLURM_JOB_ID=155387 -e SLURM_NTASKS_PER_NODE=8 cont_155387 ./run_and_time.sh
Run vars: id 155387 gpus 8 mparams 
STARTING TIMING RUN AT 2018-11-08 09:00:36 PM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python bind_launch.py --nsockets_per_node 2 --ncores_per_socket 20 --nproc_per_node 8 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 152 --warmup 300 --nhwc --pad-input
1 Using seed = 101307281
2 Using seed = 101307282
3 Using seed = 101307283
0 Using seed = 101307280
6 Using seed = 101307286
5 Using seed = 101307285
7 Using seed = 101307287
4 Using seed = 101307284

:::MLPv0.5.0 ssd 1541710848.162922859 (train.py:371) run_start

:::MLPv0.5.0 ssd 1541710848.163776875 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1541710848.164478302 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1541710848.165189981 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1541710848.165929556 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1541710848.202463150 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1541710848.204639673 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1541710848.206465960 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1541710848.207868099 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1541710848.209134817 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1541710848.210365772 (train.py:382) input_size: 300
loading annotations into memory...
Done (t=0.47s)
creating index...
Done (t=0.48s)
creating index...
Done (t=0.49s)
creating index...
index created!
index created!
Done (t=0.51s)
creating index...
index created!
Done (t=0.53s)
creating index...
Done (t=0.53s)
creating index...
Done (t=0.53s)
creating index...
Done (t=0.53s)
creating index...
index created!
index created!
index created!
index created!
index created!
time_check a: 1541710849.214924335
time_check b: 1541710872.994302988

:::MLPv0.5.0 ssd 1541710874.759977102 (train.py:413) input_order

:::MLPv0.5.0 ssd 1541710874.764731646 (train.py:414) input_batch_size: 152

:::MLPv0.5.0 ssd 1541710878.713030815 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710878.714087009 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710878.770732880 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1541710879.185729504 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1541710879.187141657 (train.py:477) opt_learning_rate: 0.095

:::MLPv0.5.0 ssd 1541710879.188549280 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1541710879.190034628 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1541710879.191196918 (train.py:483) opt_learning_rate_warmup_steps: 300

:::MLPv0.5.0 ssd 1541710883.137469292 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541710883.138455153 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541710883.194951773 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss

:::MLPv0.5.0 ssd 1541710887.837427855 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1541710887.838182449 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1541710887.841856003 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 22.795, Average Loss: 0.023, avg. samples / sec: 10094.15

:::MLPv0.5.0 ssd 1541710891.069658995 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.0003166666666666734}

:::MLPv0.5.0 ssd 1541710891.869604349 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0006333333333333468}

:::MLPv0.5.0 ssd 1541710892.376772881 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0009500000000000064}

:::MLPv0.5.0 ssd 1541710892.884328127 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0012666666666666798}

:::MLPv0.5.0 ssd 1541710893.377998114 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0015833333333333394}

:::MLPv0.5.0 ssd 1541710893.879752398 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0019000000000000128}

:::MLPv0.5.0 ssd 1541710894.354737759 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.0022166666666666723}

:::MLPv0.5.0 ssd 1541710894.851599216 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0025333333333333458}

:::MLPv0.5.0 ssd 1541710895.347029209 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0028500000000000053}

:::MLPv0.5.0 ssd 1541710895.814162254 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.0031666666666666787}

:::MLPv0.5.0 ssd 1541710896.289845467 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0034833333333333383}

:::MLPv0.5.0 ssd 1541710896.762632370 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.0038000000000000117}

:::MLPv0.5.0 ssd 1541710897.225108624 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.004116666666666671}

:::MLPv0.5.0 ssd 1541710897.691998959 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.004433333333333345}

:::MLPv0.5.0 ssd 1541710898.119722843 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.004750000000000004}

:::MLPv0.5.0 ssd 1541710898.552586079 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.005066666666666678}

:::MLPv0.5.0 ssd 1541710899.029845715 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.005383333333333337}

:::MLPv0.5.0 ssd 1541710899.459744453 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.005700000000000011}

:::MLPv0.5.0 ssd 1541710899.885218859 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.00601666666666667}

:::MLPv0.5.0 ssd 1541710900.329320192 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.006333333333333344}
Iteration:     20, Loss function: 20.296, Average Loss: 0.440, avg. samples / sec: 1949.06

:::MLPv0.5.0 ssd 1541710900.760411978 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.006650000000000003}

:::MLPv0.5.0 ssd 1541710901.170996666 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.0069666666666666766}

:::MLPv0.5.0 ssd 1541710901.614110231 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.007283333333333336}

:::MLPv0.5.0 ssd 1541710902.056076527 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.0076000000000000095}

:::MLPv0.5.0 ssd 1541710902.456937313 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.007916666666666669}

:::MLPv0.5.0 ssd 1541710902.904220581 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.008233333333333342}

:::MLPv0.5.0 ssd 1541710903.340078115 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.008550000000000002}

:::MLPv0.5.0 ssd 1541710903.770727873 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.008866666666666675}

:::MLPv0.5.0 ssd 1541710904.181851387 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.009183333333333335}

:::MLPv0.5.0 ssd 1541710904.605312347 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.009500000000000008}

:::MLPv0.5.0 ssd 1541710905.012791872 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.009816666666666668}

:::MLPv0.5.0 ssd 1541710905.426432610 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.010133333333333341}

:::MLPv0.5.0 ssd 1541710905.818480730 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.010450000000000001}

:::MLPv0.5.0 ssd 1541710906.269651413 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.010766666666666674}

:::MLPv0.5.0 ssd 1541710906.666607857 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.011083333333333334}

:::MLPv0.5.0 ssd 1541710907.103463411 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.011400000000000007}

:::MLPv0.5.0 ssd 1541710907.520535946 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.011716666666666667}

:::MLPv0.5.0 ssd 1541710907.955526114 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.01203333333333334}

:::MLPv0.5.0 ssd 1541710908.331367731 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.01235}

:::MLPv0.5.0 ssd 1541710908.751630068 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.012666666666666673}
Iteration:     40, Loss function: 14.056, Average Loss: 0.800, avg. samples / sec: 2887.76

:::MLPv0.5.0 ssd 1541710909.123888969 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.012983333333333333}

:::MLPv0.5.0 ssd 1541710909.520810604 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.013300000000000006}

:::MLPv0.5.0 ssd 1541710909.954333067 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.013616666666666666}

:::MLPv0.5.0 ssd 1541710910.342051983 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.01393333333333334}

:::MLPv0.5.0 ssd 1541710910.739917517 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.014250000000000013}

:::MLPv0.5.0 ssd 1541710911.172495365 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.014566666666666672}

:::MLPv0.5.0 ssd 1541710911.546097517 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.014883333333333346}

:::MLPv0.5.0 ssd 1541710911.921427727 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.015200000000000005}

:::MLPv0.5.0 ssd 1541710912.346471071 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.015516666666666679}

:::MLPv0.5.0 ssd 1541710912.779466391 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.015833333333333338}

:::MLPv0.5.0 ssd 1541710913.183799982 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.01615000000000001}

:::MLPv0.5.0 ssd 1541710913.633309603 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.01646666666666667}

:::MLPv0.5.0 ssd 1541710914.039457083 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.016783333333333345}

:::MLPv0.5.0 ssd 1541710914.417285919 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.017100000000000004}

:::MLPv0.5.0 ssd 1541710914.786096811 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.017416666666666678}

:::MLPv0.5.0 ssd 1541710915.258580446 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.017733333333333337}

:::MLPv0.5.0 ssd 1541710915.618426561 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.01805000000000001}

:::MLPv0.5.0 ssd 1541710916.033816814 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 58, "value": 0.01836666666666667}

:::MLPv0.5.0 ssd 1541710916.424416780 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 59, "value": 0.018683333333333343}

:::MLPv0.5.0 ssd 1541710916.789962292 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 60, "value": 0.019000000000000003}
Iteration:     60, Loss function: 11.255, Average Loss: 1.041, avg. samples / sec: 3025.44

:::MLPv0.5.0 ssd 1541710917.220969200 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 61, "value": 0.019316666666666676}

:::MLPv0.5.0 ssd 1541710917.661228180 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 62, "value": 0.019633333333333336}

:::MLPv0.5.0 ssd 1541710918.027893782 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 63, "value": 0.01995000000000001}

:::MLPv0.5.0 ssd 1541710918.403669596 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 64, "value": 0.02026666666666667}

:::MLPv0.5.0 ssd 1541710918.787483692 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 65, "value": 0.020583333333333342}

:::MLPv0.5.0 ssd 1541710919.223545074 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 66, "value": 0.020900000000000002}

:::MLPv0.5.0 ssd 1541710919.553344727 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 67, "value": 0.021216666666666675}

:::MLPv0.5.0 ssd 1541710919.897133827 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 68, "value": 0.021533333333333335}

:::MLPv0.5.0 ssd 1541710920.260440350 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 69, "value": 0.02185000000000001}

:::MLPv0.5.0 ssd 1541710920.646555901 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 70, "value": 0.022166666666666668}

:::MLPv0.5.0 ssd 1541710921.025916815 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 71, "value": 0.02248333333333334}

:::MLPv0.5.0 ssd 1541710921.422662973 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 72, "value": 0.0228}

:::MLPv0.5.0 ssd 1541710921.848469973 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 73, "value": 0.023116666666666674}

:::MLPv0.5.0 ssd 1541710922.274384737 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 74, "value": 0.023433333333333334}

:::MLPv0.5.0 ssd 1541710922.600652695 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 75, "value": 0.023750000000000007}

:::MLPv0.5.0 ssd 1541710923.041209698 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 76, "value": 0.024066666666666667}

:::MLPv0.5.0 ssd 1541710923.477132082 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 77, "value": 0.02438333333333334}

:::MLPv0.5.0 ssd 1541710923.802589417 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 78, "value": 0.0247}

:::MLPv0.5.0 ssd 1541710924.160298586 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 79, "value": 0.025016666666666673}

:::MLPv0.5.0 ssd 1541710924.558189392 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 80, "value": 0.025333333333333333}
Iteration:     80, Loss function: 9.761, Average Loss: 1.228, avg. samples / sec: 3130.80

:::MLPv0.5.0 ssd 1541710924.948746204 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 81, "value": 0.025650000000000006}

:::MLPv0.5.0 ssd 1541710925.351586103 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 82, "value": 0.025966666666666666}

:::MLPv0.5.0 ssd 1541710925.739650726 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 83, "value": 0.02628333333333334}

:::MLPv0.5.0 ssd 1541710926.143130779 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 84, "value": 0.0266}

:::MLPv0.5.0 ssd 1541710926.555449247 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 85, "value": 0.026916666666666672}

:::MLPv0.5.0 ssd 1541710926.883482218 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 86, "value": 0.02723333333333333}

:::MLPv0.5.0 ssd 1541710927.244742155 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 87, "value": 0.027550000000000005}

:::MLPv0.5.0 ssd 1541710927.618550301 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 88, "value": 0.02786666666666668}

:::MLPv0.5.0 ssd 1541710928.004156351 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 89, "value": 0.028183333333333338}

:::MLPv0.5.0 ssd 1541710928.375134230 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 90, "value": 0.02850000000000001}

:::MLPv0.5.0 ssd 1541710928.740716457 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 91, "value": 0.02881666666666667}

:::MLPv0.5.0 ssd 1541710929.058552742 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 92, "value": 0.029133333333333344}

:::MLPv0.5.0 ssd 1541710929.419819593 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 93, "value": 0.029450000000000004}

:::MLPv0.5.0 ssd 1541710929.740339756 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 94, "value": 0.029766666666666677}

:::MLPv0.5.0 ssd 1541710930.088629723 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 95, "value": 0.030083333333333337}

:::MLPv0.5.0 ssd 1541710930.430218220 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 96, "value": 0.03040000000000001}

:::MLPv0.5.0 ssd 1541710930.810038090 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 97, "value": 0.03071666666666667}

:::MLPv0.5.0 ssd 1541710931.114411592 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1541710931.163476467 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 98, "value": 0.031033333333333343}

:::MLPv0.5.0 ssd 1541710931.500614882 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 99, "value": 0.03135}

:::MLPv0.5.0 ssd 1541710931.858006477 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 100, "value": 0.031666666666666676}
Iteration:    100, Loss function: 9.316, Average Loss: 1.393, avg. samples / sec: 3331.40

:::MLPv0.5.0 ssd 1541710932.196983099 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 101, "value": 0.031983333333333336}

:::MLPv0.5.0 ssd 1541710932.557721853 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 102, "value": 0.03230000000000001}

:::MLPv0.5.0 ssd 1541710932.916863203 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 103, "value": 0.03261666666666667}

:::MLPv0.5.0 ssd 1541710933.264930964 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 104, "value": 0.032933333333333335}

:::MLPv0.5.0 ssd 1541710933.589513063 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 105, "value": 0.03325}

:::MLPv0.5.0 ssd 1541710933.929396152 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 106, "value": 0.03356666666666667}

:::MLPv0.5.0 ssd 1541710934.269351482 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 107, "value": 0.033883333333333335}

:::MLPv0.5.0 ssd 1541710934.588845253 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 108, "value": 0.03420000000000001}

:::MLPv0.5.0 ssd 1541710934.929296494 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 109, "value": 0.034516666666666675}

:::MLPv0.5.0 ssd 1541710935.257301569 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 110, "value": 0.03483333333333334}

:::MLPv0.5.0 ssd 1541710935.646839619 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 111, "value": 0.03515000000000001}

:::MLPv0.5.0 ssd 1541710936.002856493 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 112, "value": 0.035466666666666674}

:::MLPv0.5.0 ssd 1541710936.358518600 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 113, "value": 0.03578333333333334}

:::MLPv0.5.0 ssd 1541710936.682935476 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 114, "value": 0.03610000000000001}

:::MLPv0.5.0 ssd 1541710937.021244526 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 115, "value": 0.036416666666666674}

:::MLPv0.5.0 ssd 1541710937.362695694 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 116, "value": 0.03673333333333334}

:::MLPv0.5.0 ssd 1541710937.684889078 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 117, "value": 0.03705000000000001}

:::MLPv0.5.0 ssd 1541710938.009009361 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 118, "value": 0.03736666666666667}

:::MLPv0.5.0 ssd 1541710938.343033552 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 119, "value": 0.03768333333333334}

:::MLPv0.5.0 ssd 1541710938.675143957 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 120, "value": 0.038000000000000006}
Iteration:    120, Loss function: 9.067, Average Loss: 1.547, avg. samples / sec: 3568.08

:::MLPv0.5.0 ssd 1541710939.006649256 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 121, "value": 0.03831666666666667}

:::MLPv0.5.0 ssd 1541710939.340507507 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 122, "value": 0.03863333333333334}

:::MLPv0.5.0 ssd 1541710939.667162895 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 123, "value": 0.038950000000000005}

:::MLPv0.5.0 ssd 1541710940.001445055 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 124, "value": 0.03926666666666667}

:::MLPv0.5.0 ssd 1541710940.361220598 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 125, "value": 0.03958333333333334}

:::MLPv0.5.0 ssd 1541710940.682926178 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 126, "value": 0.039900000000000005}

:::MLPv0.5.0 ssd 1541710941.018932343 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 127, "value": 0.04021666666666667}

:::MLPv0.5.0 ssd 1541710941.350459099 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 128, "value": 0.04053333333333334}

:::MLPv0.5.0 ssd 1541710941.672837973 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 129, "value": 0.040850000000000004}

:::MLPv0.5.0 ssd 1541710942.012598753 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 130, "value": 0.04116666666666667}

:::MLPv0.5.0 ssd 1541710942.334118605 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 131, "value": 0.04148333333333334}

:::MLPv0.5.0 ssd 1541710942.666836977 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 132, "value": 0.041800000000000004}

:::MLPv0.5.0 ssd 1541710943.003598690 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 133, "value": 0.04211666666666667}

:::MLPv0.5.0 ssd 1541710943.337153196 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 134, "value": 0.04243333333333334}

:::MLPv0.5.0 ssd 1541710943.661880732 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 135, "value": 0.04275}

:::MLPv0.5.0 ssd 1541710943.981533527 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 136, "value": 0.04306666666666667}

:::MLPv0.5.0 ssd 1541710944.302254677 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 137, "value": 0.043383333333333336}

:::MLPv0.5.0 ssd 1541710944.630474091 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 138, "value": 0.0437}

:::MLPv0.5.0 ssd 1541710944.965928793 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 139, "value": 0.04401666666666667}

:::MLPv0.5.0 ssd 1541710945.291169882 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 140, "value": 0.044333333333333336}
Iteration:    140, Loss function: 9.003, Average Loss: 1.694, avg. samples / sec: 3675.79

:::MLPv0.5.0 ssd 1541710945.628619671 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 141, "value": 0.04465}

:::MLPv0.5.0 ssd 1541710945.968817711 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 142, "value": 0.04496666666666667}

:::MLPv0.5.0 ssd 1541710946.288354874 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 143, "value": 0.045283333333333335}

:::MLPv0.5.0 ssd 1541710946.620750904 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 144, "value": 0.0456}

:::MLPv0.5.0 ssd 1541710946.955983162 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 145, "value": 0.04591666666666667}

:::MLPv0.5.0 ssd 1541710947.259298563 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 146, "value": 0.046233333333333335}

:::MLPv0.5.0 ssd 1541710947.577788591 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 147, "value": 0.04655}

:::MLPv0.5.0 ssd 1541710947.891863108 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 148, "value": 0.04686666666666667}

:::MLPv0.5.0 ssd 1541710948.197264910 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 149, "value": 0.047183333333333334}

:::MLPv0.5.0 ssd 1541710948.536721230 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 150, "value": 0.0475}

:::MLPv0.5.0 ssd 1541710948.847750187 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 151, "value": 0.047816666666666674}

:::MLPv0.5.0 ssd 1541710949.169831991 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 152, "value": 0.04813333333333334}

:::MLPv0.5.0 ssd 1541710949.505722284 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 153, "value": 0.04845000000000001}

:::MLPv0.5.0 ssd 1541710949.821883678 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 154, "value": 0.04876666666666667}

:::MLPv0.5.0 ssd 1541710950.139868975 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 155, "value": 0.04908333333333334}

:::MLPv0.5.0 ssd 1541710950.527869463 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 156, "value": 0.049400000000000006}

:::MLPv0.5.0 ssd 1541710950.853409767 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 157, "value": 0.04971666666666667}

:::MLPv0.5.0 ssd 1541710951.187828302 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 158, "value": 0.05003333333333334}

:::MLPv0.5.0 ssd 1541710951.514207840 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 159, "value": 0.050350000000000006}

:::MLPv0.5.0 ssd 1541710951.837413549 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 160, "value": 0.05066666666666667}
Iteration:    160, Loss function: 8.065, Average Loss: 1.830, avg. samples / sec: 3714.58

:::MLPv0.5.0 ssd 1541710952.158372164 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 161, "value": 0.05098333333333334}

:::MLPv0.5.0 ssd 1541710952.482801437 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 162, "value": 0.051300000000000005}

:::MLPv0.5.0 ssd 1541710952.822510481 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 163, "value": 0.05161666666666667}

:::MLPv0.5.0 ssd 1541710953.157924175 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 164, "value": 0.05193333333333334}

:::MLPv0.5.0 ssd 1541710953.483556747 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 165, "value": 0.052250000000000005}

:::MLPv0.5.0 ssd 1541710953.823763132 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 166, "value": 0.05256666666666667}

:::MLPv0.5.0 ssd 1541710954.123118401 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 167, "value": 0.05288333333333334}

:::MLPv0.5.0 ssd 1541710954.440030098 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 168, "value": 0.053200000000000004}

:::MLPv0.5.0 ssd 1541710954.763049126 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 169, "value": 0.05351666666666667}

:::MLPv0.5.0 ssd 1541710955.090460300 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 170, "value": 0.05383333333333334}

:::MLPv0.5.0 ssd 1541710955.412739992 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 171, "value": 0.054150000000000004}

:::MLPv0.5.0 ssd 1541710955.743683577 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 172, "value": 0.05446666666666667}

:::MLPv0.5.0 ssd 1541710956.069143057 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 173, "value": 0.05478333333333334}

:::MLPv0.5.0 ssd 1541710956.388333082 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 174, "value": 0.0551}

:::MLPv0.5.0 ssd 1541710956.721357346 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 175, "value": 0.05541666666666667}

:::MLPv0.5.0 ssd 1541710957.044925690 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 176, "value": 0.055733333333333336}

:::MLPv0.5.0 ssd 1541710957.368919373 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 177, "value": 0.05605}

:::MLPv0.5.0 ssd 1541710957.692951202 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 178, "value": 0.05636666666666667}

:::MLPv0.5.0 ssd 1541710958.030121803 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 179, "value": 0.056683333333333336}

:::MLPv0.5.0 ssd 1541710958.371143341 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 180, "value": 0.057}
Iteration:    180, Loss function: 8.309, Average Loss: 1.961, avg. samples / sec: 3722.72

:::MLPv0.5.0 ssd 1541710958.719221354 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 181, "value": 0.05731666666666667}

:::MLPv0.5.0 ssd 1541710959.051872730 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 182, "value": 0.057633333333333335}

:::MLPv0.5.0 ssd 1541710959.383150339 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 183, "value": 0.05795}

:::MLPv0.5.0 ssd 1541710959.714633703 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 184, "value": 0.05826666666666667}

:::MLPv0.5.0 ssd 1541710960.060596466 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 185, "value": 0.058583333333333334}

:::MLPv0.5.0 ssd 1541710960.369380236 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 186, "value": 0.0589}

:::MLPv0.5.0 ssd 1541710960.695859909 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 187, "value": 0.05921666666666667}

:::MLPv0.5.0 ssd 1541710961.009913206 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 188, "value": 0.059533333333333334}

:::MLPv0.5.0 ssd 1541710961.335755110 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 189, "value": 0.05985}

:::MLPv0.5.0 ssd 1541710961.664824724 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 190, "value": 0.06016666666666667}

:::MLPv0.5.0 ssd 1541710961.991996527 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 191, "value": 0.06048333333333333}

:::MLPv0.5.0 ssd 1541710962.327258110 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 192, "value": 0.0608}

:::MLPv0.5.0 ssd 1541710962.656731844 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 193, "value": 0.061116666666666666}

:::MLPv0.5.0 ssd 1541710962.961555481 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 194, "value": 0.06143333333333334}

:::MLPv0.5.0 ssd 1541710963.250589132 (train.py:553) train_epoch: 2

:::MLPv0.5.0 ssd 1541710963.287819862 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 195, "value": 0.061750000000000006}

:::MLPv0.5.0 ssd 1541710963.618775129 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 196, "value": 0.06206666666666667}

:::MLPv0.5.0 ssd 1541710963.936336279 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 197, "value": 0.06238333333333334}

:::MLPv0.5.0 ssd 1541710964.249262810 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 198, "value": 0.0627}

:::MLPv0.5.0 ssd 1541710964.571500301 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 199, "value": 0.06301666666666667}

:::MLPv0.5.0 ssd 1541710964.896695137 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 200, "value": 0.06333333333333334}
Iteration:    200, Loss function: 8.288, Average Loss: 2.087, avg. samples / sec: 3719.37

:::MLPv0.5.0 ssd 1541710965.229311705 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 201, "value": 0.06365000000000001}

:::MLPv0.5.0 ssd 1541710965.550700188 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 202, "value": 0.06396666666666667}

:::MLPv0.5.0 ssd 1541710965.912121534 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 203, "value": 0.06428333333333333}

:::MLPv0.5.0 ssd 1541710966.221319914 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 204, "value": 0.0646}

:::MLPv0.5.0 ssd 1541710966.543563843 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 205, "value": 0.06491666666666668}

:::MLPv0.5.0 ssd 1541710966.858797312 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 206, "value": 0.06523333333333334}

:::MLPv0.5.0 ssd 1541710967.164578199 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 207, "value": 0.06555}

:::MLPv0.5.0 ssd 1541710967.487687826 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 208, "value": 0.06586666666666667}

:::MLPv0.5.0 ssd 1541710967.814448595 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 209, "value": 0.06618333333333334}

:::MLPv0.5.0 ssd 1541710968.136655807 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 210, "value": 0.0665}

:::MLPv0.5.0 ssd 1541710968.453064203 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 211, "value": 0.06681666666666666}

:::MLPv0.5.0 ssd 1541710968.771521568 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 212, "value": 0.06713333333333334}

:::MLPv0.5.0 ssd 1541710969.083054066 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 213, "value": 0.06745000000000001}

:::MLPv0.5.0 ssd 1541710969.409224033 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 214, "value": 0.06776666666666667}

:::MLPv0.5.0 ssd 1541710969.735654593 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 215, "value": 0.06808333333333333}

:::MLPv0.5.0 ssd 1541710970.064186811 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 216, "value": 0.0684}

:::MLPv0.5.0 ssd 1541710970.382165194 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 217, "value": 0.06871666666666668}

:::MLPv0.5.0 ssd 1541710970.694129467 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 218, "value": 0.06903333333333334}

:::MLPv0.5.0 ssd 1541710971.018468618 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 219, "value": 0.06935}

:::MLPv0.5.0 ssd 1541710971.344461679 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 220, "value": 0.06966666666666667}
Iteration:    220, Loss function: 7.678, Average Loss: 2.204, avg. samples / sec: 3779.43

:::MLPv0.5.0 ssd 1541710971.667690516 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 221, "value": 0.06998333333333334}

:::MLPv0.5.0 ssd 1541710971.993338823 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 222, "value": 0.0703}

:::MLPv0.5.0 ssd 1541710972.305907249 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 223, "value": 0.07061666666666666}

:::MLPv0.5.0 ssd 1541710972.630995274 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 224, "value": 0.07093333333333333}

:::MLPv0.5.0 ssd 1541710972.928152084 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 225, "value": 0.07125000000000001}

:::MLPv0.5.0 ssd 1541710973.261564970 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 226, "value": 0.07156666666666667}

:::MLPv0.5.0 ssd 1541710973.585983992 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 227, "value": 0.07188333333333334}

:::MLPv0.5.0 ssd 1541710973.909742832 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 228, "value": 0.0722}

:::MLPv0.5.0 ssd 1541710974.227868080 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 229, "value": 0.07251666666666667}

:::MLPv0.5.0 ssd 1541710974.549192429 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 230, "value": 0.07283333333333333}

:::MLPv0.5.0 ssd 1541710974.866915941 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 231, "value": 0.07315}

:::MLPv0.5.0 ssd 1541710975.198307037 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 232, "value": 0.07346666666666667}

:::MLPv0.5.0 ssd 1541710975.521271467 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 233, "value": 0.07378333333333334}

:::MLPv0.5.0 ssd 1541710975.835679770 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 234, "value": 0.0741}

:::MLPv0.5.0 ssd 1541710976.157926321 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 235, "value": 0.07441666666666667}

:::MLPv0.5.0 ssd 1541710976.484317303 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 236, "value": 0.07473333333333333}

:::MLPv0.5.0 ssd 1541710976.800381184 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 237, "value": 0.07505}

:::MLPv0.5.0 ssd 1541710977.114971638 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 238, "value": 0.07536666666666667}

:::MLPv0.5.0 ssd 1541710977.425882101 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 239, "value": 0.07568333333333334}

:::MLPv0.5.0 ssd 1541710977.745922327 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 240, "value": 0.076}
Iteration:    240, Loss function: 7.555, Average Loss: 2.313, avg. samples / sec: 3799.62

:::MLPv0.5.0 ssd 1541710978.062443018 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 241, "value": 0.07631666666666667}

:::MLPv0.5.0 ssd 1541710978.371375322 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 242, "value": 0.07663333333333333}

:::MLPv0.5.0 ssd 1541710978.697858334 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 243, "value": 0.07695}

:::MLPv0.5.0 ssd 1541710979.013597012 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 244, "value": 0.07726666666666666}

:::MLPv0.5.0 ssd 1541710979.315500975 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 245, "value": 0.07758333333333334}

:::MLPv0.5.0 ssd 1541710979.629473209 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 246, "value": 0.0779}

:::MLPv0.5.0 ssd 1541710979.941401243 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 247, "value": 0.07821666666666667}

:::MLPv0.5.0 ssd 1541710980.272095203 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 248, "value": 0.07853333333333334}

:::MLPv0.5.0 ssd 1541710980.586333990 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 249, "value": 0.07885}

:::MLPv0.5.0 ssd 1541710980.898182154 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 250, "value": 0.07916666666666666}

:::MLPv0.5.0 ssd 1541710981.213146687 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 251, "value": 0.07948333333333334}

:::MLPv0.5.0 ssd 1541710981.532233953 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 252, "value": 0.07980000000000001}

:::MLPv0.5.0 ssd 1541710981.846421719 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 253, "value": 0.08011666666666667}

:::MLPv0.5.0 ssd 1541710982.167292833 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 254, "value": 0.08043333333333333}

:::MLPv0.5.0 ssd 1541710982.462920666 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 255, "value": 0.08075}

:::MLPv0.5.0 ssd 1541710982.773246050 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 256, "value": 0.08106666666666668}

:::MLPv0.5.0 ssd 1541710983.108448267 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 257, "value": 0.08138333333333334}

:::MLPv0.5.0 ssd 1541710983.407409191 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 258, "value": 0.0817}

:::MLPv0.5.0 ssd 1541710983.726295471 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 259, "value": 0.08201666666666667}

:::MLPv0.5.0 ssd 1541710984.062409163 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 260, "value": 0.08233333333333334}
Iteration:    260, Loss function: 7.631, Average Loss: 2.423, avg. samples / sec: 3850.37

:::MLPv0.5.0 ssd 1541710984.358551502 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 261, "value": 0.08265}

:::MLPv0.5.0 ssd 1541710984.682869911 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 262, "value": 0.08296666666666666}

:::MLPv0.5.0 ssd 1541710985.002358675 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 263, "value": 0.08328333333333333}

:::MLPv0.5.0 ssd 1541710985.313844919 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 264, "value": 0.08360000000000001}

:::MLPv0.5.0 ssd 1541710985.630586386 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 265, "value": 0.08391666666666667}

:::MLPv0.5.0 ssd 1541710985.959872723 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 266, "value": 0.08423333333333334}

:::MLPv0.5.0 ssd 1541710986.275632143 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 267, "value": 0.08455}

:::MLPv0.5.0 ssd 1541710986.598600149 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 268, "value": 0.08486666666666667}

:::MLPv0.5.0 ssd 1541710986.910970211 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 269, "value": 0.08518333333333333}

:::MLPv0.5.0 ssd 1541710987.237467766 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 270, "value": 0.0855}

:::MLPv0.5.0 ssd 1541710987.549242020 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 271, "value": 0.08581666666666667}

:::MLPv0.5.0 ssd 1541710987.867707968 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 272, "value": 0.08613333333333334}

:::MLPv0.5.0 ssd 1541710988.189601421 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 273, "value": 0.08645}

:::MLPv0.5.0 ssd 1541710988.507993937 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 274, "value": 0.08676666666666667}

:::MLPv0.5.0 ssd 1541710988.819984913 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 275, "value": 0.08708333333333333}

:::MLPv0.5.0 ssd 1541710989.136339188 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 276, "value": 0.0874}

:::MLPv0.5.0 ssd 1541710989.458114624 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 277, "value": 0.08771666666666667}

:::MLPv0.5.0 ssd 1541710989.769099951 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 278, "value": 0.08803333333333334}

:::MLPv0.5.0 ssd 1541710990.072685480 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 279, "value": 0.08835}

:::MLPv0.5.0 ssd 1541710990.378749371 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 280, "value": 0.08866666666666667}
Iteration:    280, Loss function: 7.161, Average Loss: 2.523, avg. samples / sec: 3850.21

:::MLPv0.5.0 ssd 1541710990.696225643 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 281, "value": 0.08898333333333333}

:::MLPv0.5.0 ssd 1541710991.010772705 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 282, "value": 0.0893}

:::MLPv0.5.0 ssd 1541710991.321543694 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 283, "value": 0.08961666666666666}

:::MLPv0.5.0 ssd 1541710991.633898973 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 284, "value": 0.08993333333333334}

:::MLPv0.5.0 ssd 1541710991.945055962 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 285, "value": 0.09025}

:::MLPv0.5.0 ssd 1541710992.256838322 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 286, "value": 0.09056666666666667}

:::MLPv0.5.0 ssd 1541710992.568639278 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 287, "value": 0.09088333333333333}

:::MLPv0.5.0 ssd 1541710992.878116846 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 288, "value": 0.0912}

:::MLPv0.5.0 ssd 1541710993.192309856 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 289, "value": 0.09151666666666666}

:::MLPv0.5.0 ssd 1541710993.503506422 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 290, "value": 0.09183333333333334}

:::MLPv0.5.0 ssd 1541710993.815614939 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 291, "value": 0.09215}

:::MLPv0.5.0 ssd 1541710994.098140717 (train.py:553) train_epoch: 3

:::MLPv0.5.0 ssd 1541710994.135285139 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 292, "value": 0.09246666666666667}

:::MLPv0.5.0 ssd 1541710994.458608627 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 293, "value": 0.09278333333333333}

:::MLPv0.5.0 ssd 1541710994.776010752 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 294, "value": 0.0931}

:::MLPv0.5.0 ssd 1541710995.091367006 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 295, "value": 0.09341666666666666}

:::MLPv0.5.0 ssd 1541710995.412734985 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 296, "value": 0.09373333333333334}

:::MLPv0.5.0 ssd 1541710995.728065252 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 297, "value": 0.09405}

:::MLPv0.5.0 ssd 1541710996.041401148 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 298, "value": 0.09436666666666667}

:::MLPv0.5.0 ssd 1541710996.371771574 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 299, "value": 0.09468333333333333}
Iteration:    300, Loss function: 7.748, Average Loss: 2.622, avg. samples / sec: 3864.50
Iteration:    320, Loss function: 6.971, Average Loss: 2.713, avg. samples / sec: 3856.63
Iteration:    340, Loss function: 7.214, Average Loss: 2.795, avg. samples / sec: 3870.91
Iteration:    360, Loss function: 6.601, Average Loss: 2.877, avg. samples / sec: 3945.21
Iteration:    380, Loss function: 6.421, Average Loss: 2.952, avg. samples / sec: 3908.21

:::MLPv0.5.0 ssd 1541711024.711270332 (train.py:553) train_epoch: 4
Iteration:    400, Loss function: 6.759, Average Loss: 3.024, avg. samples / sec: 3921.32
Iteration:    420, Loss function: 6.567, Average Loss: 3.095, avg. samples / sec: 3940.68
Iteration:    440, Loss function: 6.351, Average Loss: 3.159, avg. samples / sec: 3957.07
Iteration:    460, Loss function: 6.151, Average Loss: 3.217, avg. samples / sec: 3938.19
Iteration:    480, Loss function: 5.972, Average Loss: 3.277, avg. samples / sec: 3974.15

:::MLPv0.5.0 ssd 1541711054.599151134 (train.py:553) train_epoch: 5
Iteration:    500, Loss function: 6.036, Average Loss: 3.332, avg. samples / sec: 3978.98
Iteration:    520, Loss function: 6.171, Average Loss: 3.384, avg. samples / sec: 3975.33
Iteration:    540, Loss function: 6.026, Average Loss: 3.435, avg. samples / sec: 3961.75
Iteration:    560, Loss function: 5.820, Average Loss: 3.482, avg. samples / sec: 4007.23
Iteration:    580, Loss function: 5.516, Average Loss: 3.524, avg. samples / sec: 3970.74

:::MLPv0.5.0 ssd 1541711084.215534449 (train.py:553) train_epoch: 6
Iteration:    600, Loss function: 5.785, Average Loss: 3.567, avg. samples / sec: 4028.01
Iteration:    620, Loss function: 5.384, Average Loss: 3.606, avg. samples / sec: 3967.18
Iteration:    640, Loss function: 5.622, Average Loss: 3.645, avg. samples / sec: 3981.75
Iteration:    660, Loss function: 5.282, Average Loss: 3.680, avg. samples / sec: 3963.81
Iteration:    680, Loss function: 5.322, Average Loss: 3.713, avg. samples / sec: 3995.89

:::MLPv0.5.0 ssd 1541711113.818113565 (train.py:553) train_epoch: 7
Iteration:    700, Loss function: 5.505, Average Loss: 3.744, avg. samples / sec: 3969.33
Iteration:    720, Loss function: 5.046, Average Loss: 3.773, avg. samples / sec: 4006.24
Iteration:    740, Loss function: 5.281, Average Loss: 3.800, avg. samples / sec: 3980.68
Iteration:    760, Loss function: 5.284, Average Loss: 3.827, avg. samples / sec: 3972.09

:::MLPv0.5.0 ssd 1541711143.756815434 (train.py:553) train_epoch: 8
Iteration:    780, Loss function: 4.803, Average Loss: 3.853, avg. samples / sec: 3962.07
Iteration:    800, Loss function: 5.029, Average Loss: 3.877, avg. samples / sec: 4035.45
Iteration:    820, Loss function: 5.090, Average Loss: 3.900, avg. samples / sec: 4030.92
Iteration:    840, Loss function: 5.085, Average Loss: 3.924, avg. samples / sec: 4025.70
Iteration:    860, Loss function: 5.281, Average Loss: 3.945, avg. samples / sec: 3985.34

:::MLPv0.5.0 ssd 1541711173.150807858 (train.py:553) train_epoch: 9
Iteration:    880, Loss function: 4.990, Average Loss: 3.966, avg. samples / sec: 4007.26
Iteration:    900, Loss function: 4.716, Average Loss: 3.984, avg. samples / sec: 4001.57
Iteration:    920, Loss function: 5.153, Average Loss: 4.003, avg. samples / sec: 4020.39
Iteration:    940, Loss function: 4.745, Average Loss: 4.021, avg. samples / sec: 4010.51
Iteration:    960, Loss function: 4.951, Average Loss: 4.038, avg. samples / sec: 4000.53

:::MLPv0.5.0 ssd 1541711202.528459549 (train.py:553) train_epoch: 10
Iteration:    980, Loss function: 4.686, Average Loss: 4.053, avg. samples / sec: 4030.20
Iteration:   1000, Loss function: 4.642, Average Loss: 4.070, avg. samples / sec: 4017.68
Iteration:   1020, Loss function: 4.641, Average Loss: 4.083, avg. samples / sec: 3988.56
Iteration:   1040, Loss function: 5.086, Average Loss: 4.096, avg. samples / sec: 4012.96
Iteration:   1060, Loss function: 4.840, Average Loss: 4.111, avg. samples / sec: 4012.48

:::MLPv0.5.0 ssd 1541711232.246310234 (train.py:553) train_epoch: 11
Iteration:   1080, Loss function: 4.669, Average Loss: 4.124, avg. samples / sec: 4032.54
Iteration:   1100, Loss function: 4.726, Average Loss: 4.135, avg. samples / sec: 4039.04
Iteration:   1120, Loss function: 4.778, Average Loss: 4.145, avg. samples / sec: 4020.51
Iteration:   1140, Loss function: 4.638, Average Loss: 4.155, avg. samples / sec: 4044.90
Iteration:   1160, Loss function: 4.338, Average Loss: 4.165, avg. samples / sec: 4003.07

:::MLPv0.5.0 ssd 1541711261.531163931 (train.py:553) train_epoch: 12
Iteration:   1180, Loss function: 4.610, Average Loss: 4.174, avg. samples / sec: 4045.18
Iteration:   1200, Loss function: 4.838, Average Loss: 4.183, avg. samples / sec: 4061.41
Iteration:   1220, Loss function: 4.674, Average Loss: 4.192, avg. samples / sec: 4028.24
Iteration:   1240, Loss function: 4.408, Average Loss: 4.197, avg. samples / sec: 4039.88
Iteration:   1260, Loss function: 4.763, Average Loss: 4.204, avg. samples / sec: 4008.30

:::MLPv0.5.0 ssd 1541711290.760541916 (train.py:553) train_epoch: 13
Iteration:   1280, Loss function: 4.498, Average Loss: 4.212, avg. samples / sec: 4015.71
Iteration:   1300, Loss function: 4.663, Average Loss: 4.219, avg. samples / sec: 4039.63
Iteration:   1320, Loss function: 4.493, Average Loss: 4.226, avg. samples / sec: 4077.81
Iteration:   1340, Loss function: 4.630, Average Loss: 4.230, avg. samples / sec: 4019.22
Iteration:   1360, Loss function: 4.637, Average Loss: 4.237, avg. samples / sec: 4017.62

:::MLPv0.5.0 ssd 1541711319.985774279 (train.py:553) train_epoch: 14
Iteration:   1380, Loss function: 4.386, Average Loss: 4.242, avg. samples / sec: 4066.96
Iteration:   1400, Loss function: 4.261, Average Loss: 4.245, avg. samples / sec: 4019.94
Iteration:   1420, Loss function: 4.824, Average Loss: 4.249, avg. samples / sec: 4011.49
Iteration:   1440, Loss function: 4.363, Average Loss: 4.255, avg. samples / sec: 4054.56

:::MLPv0.5.0 ssd 1541711349.500983000 (train.py:553) train_epoch: 15
Iteration:   1460, Loss function: 4.514, Average Loss: 4.259, avg. samples / sec: 4038.15
Iteration:   1480, Loss function: 4.461, Average Loss: 4.261, avg. samples / sec: 4026.35
Iteration:   1500, Loss function: 4.308, Average Loss: 4.263, avg. samples / sec: 4053.22
Iteration:   1520, Loss function: 4.715, Average Loss: 4.265, avg. samples / sec: 4046.38
Iteration:   1540, Loss function: 4.362, Average Loss: 4.267, avg. samples / sec: 4064.76

:::MLPv0.5.0 ssd 1541711378.712201357 (train.py:553) train_epoch: 16
Iteration:   1560, Loss function: 4.577, Average Loss: 4.270, avg. samples / sec: 3994.12
Iteration:   1580, Loss function: 4.263, Average Loss: 4.272, avg. samples / sec: 4040.04
Iteration:   1600, Loss function: 4.341, Average Loss: 4.274, avg. samples / sec: 4040.38
Iteration:   1620, Loss function: 4.140, Average Loss: 4.273, avg. samples / sec: 4029.54
Iteration:   1640, Loss function: 4.465, Average Loss: 4.274, avg. samples / sec: 4050.88

:::MLPv0.5.0 ssd 1541711407.934840679 (train.py:553) train_epoch: 17
Iteration:   1660, Loss function: 4.580, Average Loss: 4.278, avg. samples / sec: 4033.98
Iteration:   1680, Loss function: 4.416, Average Loss: 4.279, avg. samples / sec: 4051.75
Iteration:   1700, Loss function: 4.360, Average Loss: 4.277, avg. samples / sec: 4038.39
Iteration:   1720, Loss function: 4.415, Average Loss: 4.278, avg. samples / sec: 4011.71
Iteration:   1740, Loss function: 4.435, Average Loss: 4.279, avg. samples / sec: 4012.43

:::MLPv0.5.0 ssd 1541711437.168069601 (train.py:553) train_epoch: 18
Iteration:   1760, Loss function: 4.035, Average Loss: 4.279, avg. samples / sec: 4072.03
Iteration:   1780, Loss function: 4.129, Average Loss: 4.279, avg. samples / sec: 4016.09
Iteration:   1800, Loss function: 4.261, Average Loss: 4.279, avg. samples / sec: 4047.50
Iteration:   1820, Loss function: 4.395, Average Loss: 4.278, avg. samples / sec: 4028.56
Iteration:   1840, Loss function: 4.317, Average Loss: 4.277, avg. samples / sec: 4031.19

:::MLPv0.5.0 ssd 1541711466.721169472 (train.py:553) train_epoch: 19
Iteration:   1860, Loss function: 4.357, Average Loss: 4.276, avg. samples / sec: 4035.13
Iteration:   1880, Loss function: 4.412, Average Loss: 4.276, avg. samples / sec: 4064.55
Iteration:   1900, Loss function: 4.445, Average Loss: 4.274, avg. samples / sec: 4038.85
Iteration:   1920, Loss function: 4.395, Average Loss: 4.274, avg. samples / sec: 4038.73
Iteration:   1940, Loss function: 4.270, Average Loss: 4.273, avg. samples / sec: 4020.03

:::MLPv0.5.0 ssd 1541711495.880897045 (train.py:553) train_epoch: 20
Iteration:   1960, Loss function: 4.241, Average Loss: 4.272, avg. samples / sec: 4079.25
Iteration:   1980, Loss function: 4.290, Average Loss: 4.272, avg. samples / sec: 4061.77
Iteration:   2000, Loss function: 4.243, Average Loss: 4.270, avg. samples / sec: 4038.36
Iteration:   2020, Loss function: 3.934, Average Loss: 4.266, avg. samples / sec: 4054.91
Iteration:   2040, Loss function: 4.256, Average Loss: 4.266, avg. samples / sec: 4040.80

:::MLPv0.5.0 ssd 1541711524.972442389 (train.py:553) train_epoch: 21
Iteration:   2060, Loss function: 4.121, Average Loss: 4.264, avg. samples / sec: 4073.94
Iteration:   2080, Loss function: 4.327, Average Loss: 4.263, avg. samples / sec: 4057.19
Iteration:   2100, Loss function: 4.080, Average Loss: 4.260, avg. samples / sec: 4053.42
Iteration:   2120, Loss function: 4.518, Average Loss: 4.257, avg. samples / sec: 4017.32
Iteration:   2140, Loss function: 4.364, Average Loss: 4.256, avg. samples / sec: 4026.43

:::MLPv0.5.0 ssd 1541711554.437961578 (train.py:553) train_epoch: 22
Iteration:   2160, Loss function: 4.124, Average Loss: 4.255, avg. samples / sec: 4047.45
Iteration:   2180, Loss function: 4.073, Average Loss: 4.252, avg. samples / sec: 4061.92
Iteration:   2200, Loss function: 4.164, Average Loss: 4.248, avg. samples / sec: 4009.66
Iteration:   2220, Loss function: 4.321, Average Loss: 4.245, avg. samples / sec: 4039.40

:::MLPv0.5.0 ssd 1541711583.601941109 (train.py:553) train_epoch: 23
Iteration:   2240, Loss function: 4.176, Average Loss: 4.243, avg. samples / sec: 4063.25
Iteration:   2260, Loss function: 3.968, Average Loss: 4.240, avg. samples / sec: 4067.97
Iteration:   2280, Loss function: 3.979, Average Loss: 4.237, avg. samples / sec: 4059.58
Iteration:   2300, Loss function: 3.924, Average Loss: 4.232, avg. samples / sec: 4053.65
Iteration:   2320, Loss function: 4.067, Average Loss: 4.230, avg. samples / sec: 4035.98

:::MLPv0.5.0 ssd 1541711612.719451427 (train.py:553) train_epoch: 24
Iteration:   2340, Loss function: 3.817, Average Loss: 4.228, avg. samples / sec: 4048.69
Iteration:   2360, Loss function: 4.150, Average Loss: 4.225, avg. samples / sec: 4045.77
Iteration:   2380, Loss function: 4.047, Average Loss: 4.222, avg. samples / sec: 4028.70
Iteration:   2400, Loss function: 3.817, Average Loss: 4.219, avg. samples / sec: 4048.16
Iteration:   2420, Loss function: 4.058, Average Loss: 4.215, avg. samples / sec: 4048.26

:::MLPv0.5.0 ssd 1541711641.847451925 (train.py:553) train_epoch: 25
Iteration:   2440, Loss function: 4.129, Average Loss: 4.213, avg. samples / sec: 4079.98
Iteration:   2460, Loss function: 4.035, Average Loss: 4.210, avg. samples / sec: 4052.73
Iteration:   2480, Loss function: 4.113, Average Loss: 4.206, avg. samples / sec: 4050.37
Iteration:   2500, Loss function: 4.059, Average Loss: 4.202, avg. samples / sec: 4061.90
Iteration:   2520, Loss function: 4.335, Average Loss: 4.200, avg. samples / sec: 4049.13

:::MLPv0.5.0 ssd 1541711671.222034693 (train.py:553) train_epoch: 26
Iteration:   2540, Loss function: 3.989, Average Loss: 4.198, avg. samples / sec: 4073.70
Iteration:   2560, Loss function: 3.941, Average Loss: 4.195, avg. samples / sec: 4046.38
Iteration:   2580, Loss function: 4.052, Average Loss: 4.190, avg. samples / sec: 4082.36
Iteration:   2600, Loss function: 3.919, Average Loss: 4.190, avg. samples / sec: 4049.09
Iteration:   2620, Loss function: 4.058, Average Loss: 4.187, avg. samples / sec: 4035.59

:::MLPv0.5.0 ssd 1541711700.299942970 (train.py:553) train_epoch: 27
Iteration:   2640, Loss function: 4.271, Average Loss: 4.184, avg. samples / sec: 4060.39
Iteration:   2660, Loss function: 4.257, Average Loss: 4.181, avg. samples / sec: 4061.00
Iteration:   2680, Loss function: 3.929, Average Loss: 4.177, avg. samples / sec: 4076.24
Iteration:   2700, Loss function: 4.060, Average Loss: 4.173, avg. samples / sec: 4048.30
Iteration:   2720, Loss function: 4.007, Average Loss: 4.170, avg. samples / sec: 4056.64

:::MLPv0.5.0 ssd 1541711729.353182316 (train.py:553) train_epoch: 28
Iteration:   2740, Loss function: 3.898, Average Loss: 4.165, avg. samples / sec: 4059.71
Iteration:   2760, Loss function: 3.952, Average Loss: 4.163, avg. samples / sec: 4055.39
Iteration:   2780, Loss function: 4.162, Average Loss: 4.159, avg. samples / sec: 4067.29
Iteration:   2800, Loss function: 4.013, Average Loss: 4.155, avg. samples / sec: 4078.97
Iteration:   2820, Loss function: 4.036, Average Loss: 4.151, avg. samples / sec: 4052.00

:::MLPv0.5.0 ssd 1541711758.375581741 (train.py:553) train_epoch: 29
Iteration:   2840, Loss function: 4.149, Average Loss: 4.148, avg. samples / sec: 4045.81
Iteration:   2860, Loss function: 4.020, Average Loss: 4.145, avg. samples / sec: 4061.53
Iteration:   2880, Loss function: 4.091, Average Loss: 4.141, avg. samples / sec: 4087.67
Iteration:   2900, Loss function: 3.785, Average Loss: 4.139, avg. samples / sec: 4062.35

:::MLPv0.5.0 ssd 1541711787.705766201 (train.py:553) train_epoch: 30
Iteration:   2920, Loss function: 3.917, Average Loss: 4.136, avg. samples / sec: 4063.17
Iteration:   2940, Loss function: 4.000, Average Loss: 4.132, avg. samples / sec: 4057.60
Iteration:   2960, Loss function: 4.130, Average Loss: 4.130, avg. samples / sec: 4056.86
Iteration:   2980, Loss function: 4.262, Average Loss: 4.127, avg. samples / sec: 4059.62
Iteration:   3000, Loss function: 3.980, Average Loss: 4.123, avg. samples / sec: 4058.82

:::MLPv0.5.0 ssd 1541711816.760409117 (train.py:553) train_epoch: 31
Iteration:   3020, Loss function: 3.936, Average Loss: 4.119, avg. samples / sec: 4069.15
Iteration:   3040, Loss function: 4.047, Average Loss: 4.115, avg. samples / sec: 4072.75
Iteration:   3060, Loss function: 3.983, Average Loss: 4.113, avg. samples / sec: 4058.25
Iteration:   3080, Loss function: 3.705, Average Loss: 4.109, avg. samples / sec: 4064.62
Iteration:   3100, Loss function: 4.220, Average Loss: 4.105, avg. samples / sec: 4063.78

:::MLPv0.5.0 ssd 1541711845.795419693 (train.py:553) train_epoch: 32
Iteration:   3120, Loss function: 3.758, Average Loss: 4.101, avg. samples / sec: 4053.64
Iteration:   3140, Loss function: 3.924, Average Loss: 4.099, avg. samples / sec: 4083.32









:::MLPv0.5.0 ssd 1541711859.222041368 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541711859.222912788 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541711859.223622561 (train.py:220) eval_start: 32
Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3No object detected in idx: 153
Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Predicting Ended, total time: 39.74 s
Loading and preparing results...
Converting ndarray to lists...
(355911, 7)
0/355911
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(355911, 7)
(355911, 7)
(355911, 7)
(355911, 7)
0/355911
0/355911
0/355911
0/355911
Converting ndarray to lists...
Converting ndarray to lists...
(355911, 7)
(355911, 7)
Loading and preparing results...
Converting ndarray to lists...
0/355911
(355911, 7)
0/355911
0/355911
DONE (t=2.56s)
creating index...
DONE (t=2.57s)
creating index...
DONE (t=2.58s)
creating index...
DONE (t=2.58s)
creating index...
DONE (t=2.58s)
creating index...
DONE (t=2.59s)
creating index...
DONE (t=2.60s)
creating index...
DONE (t=2.61s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
DONE (t=3.71s).
Accumulating evaluation results...
DONE (t=1.26s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.153
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.291
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.151
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.161
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.243
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.167
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.245
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.259
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.070
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.272
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.388
Current AP: 0.15343 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541711906.780843258 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541711906.781724691 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.15342934441605477}

:::MLPv0.5.0 ssd 1541711906.782469988 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.15342934441605477}

:::MLPv0.5.0 ssd 1541711906.783156395 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541711906.783861399 (train.py:338) eval_stop: 32
Iteration:   3160, Loss function: 3.948, Average Loss: 4.094, avg. samples / sec: 447.71
Iteration:   3180, Loss function: 3.903, Average Loss: 4.089, avg. samples / sec: 4049.04
Iteration:   3200, Loss function: 3.945, Average Loss: 4.086, avg. samples / sec: 4085.94

:::MLPv0.5.0 ssd 1541711923.385780334 (train.py:553) train_epoch: 33
Iteration:   3220, Loss function: 3.935, Average Loss: 4.084, avg. samples / sec: 4082.51
Iteration:   3240, Loss function: 4.019, Average Loss: 4.081, avg. samples / sec: 4059.16
Iteration:   3260, Loss function: 3.827, Average Loss: 4.078, avg. samples / sec: 4088.71
Iteration:   3280, Loss function: 3.952, Average Loss: 4.073, avg. samples / sec: 4069.21
Iteration:   3300, Loss function: 3.913, Average Loss: 4.070, avg. samples / sec: 4074.25

:::MLPv0.5.0 ssd 1541711952.355739594 (train.py:553) train_epoch: 34
Iteration:   3320, Loss function: 3.884, Average Loss: 4.067, avg. samples / sec: 4063.44
Iteration:   3340, Loss function: 3.998, Average Loss: 4.065, avg. samples / sec: 4075.35
Iteration:   3360, Loss function: 3.805, Average Loss: 4.061, avg. samples / sec: 4069.37
Iteration:   3380, Loss function: 3.939, Average Loss: 4.057, avg. samples / sec: 4065.01
Iteration:   3400, Loss function: 3.800, Average Loss: 4.055, avg. samples / sec: 4064.04

:::MLPv0.5.0 ssd 1541711981.361924171 (train.py:553) train_epoch: 35
Iteration:   3420, Loss function: 3.648, Average Loss: 4.051, avg. samples / sec: 4053.26
Iteration:   3440, Loss function: 3.898, Average Loss: 4.049, avg. samples / sec: 4063.17
Iteration:   3460, Loss function: 3.706, Average Loss: 4.045, avg. samples / sec: 4050.90
Iteration:   3480, Loss function: 3.846, Average Loss: 4.040, avg. samples / sec: 4039.81
Iteration:   3500, Loss function: 3.999, Average Loss: 4.039, avg. samples / sec: 4046.65

:::MLPv0.5.0 ssd 1541712010.486481667 (train.py:553) train_epoch: 36
Iteration:   3520, Loss function: 4.006, Average Loss: 4.037, avg. samples / sec: 4042.00
Iteration:   3540, Loss function: 4.272, Average Loss: 4.034, avg. samples / sec: 4060.34
Iteration:   3560, Loss function: 3.884, Average Loss: 4.030, avg. samples / sec: 4063.89
Iteration:   3580, Loss function: 3.897, Average Loss: 4.025, avg. samples / sec: 4073.90

:::MLPv0.5.0 ssd 1541712039.852329731 (train.py:553) train_epoch: 37
Iteration:   3600, Loss function: 3.627, Average Loss: 4.021, avg. samples / sec: 4043.60
Iteration:   3620, Loss function: 4.220, Average Loss: 4.019, avg. samples / sec: 4083.59
Iteration:   3640, Loss function: 3.560, Average Loss: 4.017, avg. samples / sec: 4044.04
Iteration:   3660, Loss function: 3.763, Average Loss: 4.013, avg. samples / sec: 4081.64
Iteration:   3680, Loss function: 3.603, Average Loss: 4.010, avg. samples / sec: 4075.75

:::MLPv0.5.0 ssd 1541712068.861794710 (train.py:553) train_epoch: 38
Iteration:   3700, Loss function: 3.739, Average Loss: 4.007, avg. samples / sec: 4050.74
Iteration:   3720, Loss function: 4.083, Average Loss: 4.004, avg. samples / sec: 4073.78
Iteration:   3740, Loss function: 3.775, Average Loss: 4.000, avg. samples / sec: 4064.84
Iteration:   3760, Loss function: 3.756, Average Loss: 3.997, avg. samples / sec: 4061.71
Iteration:   3780, Loss function: 3.660, Average Loss: 3.994, avg. samples / sec: 4042.35

:::MLPv0.5.0 ssd 1541712097.928987980 (train.py:553) train_epoch: 39
Iteration:   3800, Loss function: 3.816, Average Loss: 3.991, avg. samples / sec: 4037.05
Iteration:   3820, Loss function: 3.798, Average Loss: 3.988, avg. samples / sec: 4066.04
Iteration:   3840, Loss function: 3.818, Average Loss: 3.985, avg. samples / sec: 4055.41
Iteration:   3860, Loss function: 3.755, Average Loss: 3.980, avg. samples / sec: 4079.80
Iteration:   3880, Loss function: 3.957, Average Loss: 3.976, avg. samples / sec: 4073.03

:::MLPv0.5.0 ssd 1541712127.251207113 (train.py:553) train_epoch: 40
Iteration:   3900, Loss function: 3.737, Average Loss: 3.972, avg. samples / sec: 4053.69
Iteration:   3920, Loss function: 3.696, Average Loss: 3.970, avg. samples / sec: 4055.43
Iteration:   3940, Loss function: 3.668, Average Loss: 3.967, avg. samples / sec: 4061.98
Iteration:   3960, Loss function: 3.913, Average Loss: 3.964, avg. samples / sec: 4073.06
Iteration:   3980, Loss function: 3.759, Average Loss: 3.963, avg. samples / sec: 4059.30

:::MLPv0.5.0 ssd 1541712156.294331789 (train.py:553) train_epoch: 41
Iteration:   4000, Loss function: 3.788, Average Loss: 3.960, avg. samples / sec: 4063.18
Iteration:   4020, Loss function: 3.882, Average Loss: 3.956, avg. samples / sec: 4077.45
Iteration:   4040, Loss function: 3.915, Average Loss: 3.953, avg. samples / sec: 4061.40
Iteration:   4060, Loss function: 3.471, Average Loss: 3.950, avg. samples / sec: 4068.81
Iteration:   4080, Loss function: 4.058, Average Loss: 3.949, avg. samples / sec: 4080.92

:::MLPv0.5.0 ssd 1541712185.256616592 (train.py:553) train_epoch: 42
Iteration:   4100, Loss function: 3.604, Average Loss: 3.946, avg. samples / sec: 4073.93
Iteration:   4120, Loss function: 3.901, Average Loss: 3.943, avg. samples / sec: 4076.86
Iteration:   4140, Loss function: 3.809, Average Loss: 3.940, avg. samples / sec: 4055.20
Iteration:   4160, Loss function: 3.761, Average Loss: 3.938, avg. samples / sec: 4049.67
Iteration:   4180, Loss function: 3.804, Average Loss: 3.935, avg. samples / sec: 4080.29

:::MLPv0.5.0 ssd 1541712214.257190466 (train.py:553) train_epoch: 43
Iteration:   4200, Loss function: 3.744, Average Loss: 3.934, avg. samples / sec: 4064.49
lr decay step #1

:::MLPv0.5.0 ssd 1541712222.359228849 (train.py:578) opt_learning_rate: 0.009500000000000001









:::MLPv0.5.0 ssd 1541712222.642041922 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541712222.642994881 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541712222.643664122 (train.py:220) eval_start: 43
Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Predicting Ended, total time: 35.68 s
Loading and preparing results...
Converting ndarray to lists...
(335719, 7)
0/335719
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(335719, 7)
(335719, 7)
(335719, 7)
(335719, 7)
0/335719
Converting ndarray to lists...
0/335719
0/335719
0/335719
(335719, 7)
0/335719
Converting ndarray to lists...
(335719, 7)
Loading and preparing results...
0/335719
Converting ndarray to lists...
(335719, 7)
0/335719
DONE (t=1.93s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.95s)
creating index...
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.20s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.27s)
creating index...
index created!
index created!
index created!
index created!
DONE (t=3.90s).
Accumulating evaluation results...
DONE (t=1.22s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.160
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.299
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.155
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.172
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.173
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.252
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.266
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.074
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.289
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.391
Current AP: 0.16007 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541712266.054634333 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541712266.055559158 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.16007078856974027}

:::MLPv0.5.0 ssd 1541712266.056284666 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.16007078856974027}

:::MLPv0.5.0 ssd 1541712266.056999922 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541712266.057706833 (train.py:338) eval_stop: 43
Iteration:   4220, Loss function: 3.627, Average Loss: 3.930, avg. samples / sec: 488.24
Iteration:   4240, Loss function: 3.349, Average Loss: 3.921, avg. samples / sec: 4085.41
Iteration:   4260, Loss function: 3.094, Average Loss: 3.909, avg. samples / sec: 4110.91
Iteration:   4280, Loss function: 3.251, Average Loss: 3.896, avg. samples / sec: 4066.29

:::MLPv0.5.0 ssd 1541712287.324311018 (train.py:553) train_epoch: 44
Iteration:   4300, Loss function: 3.270, Average Loss: 3.883, avg. samples / sec: 4071.42
Iteration:   4320, Loss function: 3.232, Average Loss: 3.872, avg. samples / sec: 4067.88
Iteration:   4340, Loss function: 3.210, Average Loss: 3.859, avg. samples / sec: 4071.05
Iteration:   4360, Loss function: 3.187, Average Loss: 3.846, avg. samples / sec: 4070.47

:::MLPv0.5.0 ssd 1541712316.292751551 (train.py:553) train_epoch: 45
Iteration:   4380, Loss function: 3.160, Average Loss: 3.833, avg. samples / sec: 4069.65
Iteration:   4400, Loss function: 3.072, Average Loss: 3.820, avg. samples / sec: 4084.94
Iteration:   4420, Loss function: 3.134, Average Loss: 3.809, avg. samples / sec: 4051.16
Iteration:   4440, Loss function: 3.385, Average Loss: 3.796, avg. samples / sec: 4079.59
Iteration:   4460, Loss function: 3.298, Average Loss: 3.783, avg. samples / sec: 4065.61

:::MLPv0.5.0 ssd 1541712345.280560970 (train.py:553) train_epoch: 46
Iteration:   4480, Loss function: 3.085, Average Loss: 3.770, avg. samples / sec: 4067.96
Iteration:   4500, Loss function: 2.951, Average Loss: 3.757, avg. samples / sec: 4065.84
Iteration:   4520, Loss function: 2.983, Average Loss: 3.745, avg. samples / sec: 4063.28
Iteration:   4540, Loss function: 3.197, Average Loss: 3.733, avg. samples / sec: 4066.95
Iteration:   4560, Loss function: 2.995, Average Loss: 3.720, avg. samples / sec: 4056.22

:::MLPv0.5.0 ssd 1541712374.297319412 (train.py:553) train_epoch: 47
Iteration:   4580, Loss function: 3.041, Average Loss: 3.708, avg. samples / sec: 4069.12
Iteration:   4600, Loss function: 3.039, Average Loss: 3.695, avg. samples / sec: 4066.78
Iteration:   4620, Loss function: 2.914, Average Loss: 3.684, avg. samples / sec: 4064.72
Iteration:   4640, Loss function: 3.068, Average Loss: 3.671, avg. samples / sec: 4076.73
Iteration:   4660, Loss function: 3.048, Average Loss: 3.659, avg. samples / sec: 4075.34

:::MLPv0.5.0 ssd 1541712403.565814495 (train.py:553) train_epoch: 48
Iteration:   4680, Loss function: 3.040, Average Loss: 3.647, avg. samples / sec: 4073.64
Iteration:   4700, Loss function: 2.844, Average Loss: 3.635, avg. samples / sec: 4070.32
Iteration:   4720, Loss function: 3.119, Average Loss: 3.625, avg. samples / sec: 4055.09









:::MLPv0.5.0 ssd 1541712423.629355669 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541712423.630208254 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541712423.630950689 (train.py:220) eval_start: 48
Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 0/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 1/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Parsing batch: 2/3Predicting Ended, total time: 34.18 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(304046, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(304046, 7)
(304046, 7)
(304046, 7)
0/304046
(304046, 7)
(304046, 7)
Converting ndarray to lists...
0/304046
0/304046
0/304046
0/304046
0/304046
(304046, 7)
0/304046
Converting ndarray to lists...
(304046, 7)
0/304046
DONE (t=1.90s)
creating index...
DONE (t=1.91s)
creating index...
DONE (t=1.91s)
creating index...
DONE (t=1.92s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.95s)
creating index...
DONE (t=1.96s)
creating index...
index created!
index created!
index created!
index created!
DONE (t=2.08s)
creating index...
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.57s).
Accumulating evaluation results...
DONE (t=1.13s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.374
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.350
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.214
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.308
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.503
Current AP: 0.21732 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541712464.791342735 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541712464.792176723 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.21731524778932038}

:::MLPv0.5.0 ssd 1541712464.792937040 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.21731524778932038}

:::MLPv0.5.0 ssd 1541712464.793691397 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541712464.794440746 (train.py:338) eval_stop: 48

:::MLPv0.5.0 ssd 1541712466.391106129 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1541712466.391791582 (train.py:707) run_final
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2018-11-08 09:27:51 PM
RESULT,OBJECT_DETECTION,,1635,nvidia,2018-11-08 09:00:36 PM
