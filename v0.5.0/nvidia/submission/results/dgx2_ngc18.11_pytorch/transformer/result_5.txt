Beginning trial 1 of 1
Clearing caches
:::MLPv0.5.0 transformer 1541701098.477167130 (<string>:1) run_clear_caches
Launching on node xpl-dvt-70
+ pids+=($!)
+ set +x
++ eval echo
+++ echo
+ docker exec -e DGXSYSTEM=DGX2 -e MULTI_NODE= -e SEED=12929 -e SLURM_JOB_ID=1541701071 -e SLURM_NTASKS_PER_NODE= -e MODE=TRAIN cont_1541701071 ./run_and_time.sh
+ SEED=12929
+ MAX_TOKENS=10240
+ DATASET_DIR=/data
+ MODE=TRAIN
+ case "$MODE" in
+ source run_training.sh
Run vars: id 1541701071 gpus 16 mparams 
+++ date +%s
++ START=1541701099
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2018-11-08 06:18:19 PM
++ START_FMT='2018-11-08 06:18:19 PM'
++ echo 'STARTING TIMING RUN AT 2018-11-08 06:18:19 PM'
++ python -m torch.distributed.launch --nproc_per_node 16 train.py /data --seed 12929 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 1200 --lr 1.35e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 10240 --max-epoch 12 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --distributed-init-method env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 0
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 15
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 8
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 12
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 3
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 4
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 9
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 13
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 7
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 6
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 10
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 2
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 1
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 11
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 14
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 5
| distributed init done!
| initialized host xpl-dvt-70 as rank 0 and device id 0
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLPv0.5.0 transformer 1541701114.365891933 (/workspace/translation/train.py:34) run_clear_caches
:::MLPv0.5.0 transformer 1541701134.503054619 (/workspace/translation/train.py:40) run_start
Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', data='/data', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=0, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=0, distributed_world_size=16, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=True, fuse_dropout_add=False, fuse_relu_dropout=False, gen_subset='test', ignore_case=True, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source='True', left_pad_target='False', lenpen=1, local_rank=0, log_format=None, log_interval=1000, log_translations=False, lr=[0.00135], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=12, max_len_a=0, max_len_b=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=10240, max_update=0, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_progress_bar=False, no_save=True, no_token_positional_embeddings=False, num_shards=1, online_eval=False, optimizer='adam', pad_sequence=1, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=None, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe=None, replace_unk=None, restore_file='checkpoint_last.pt', sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='checkpoints', save_interval=1, save_interval_updates=0, score_reference=False, seed=12929, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_bleu=25.0, target_lang=None, task='translation', train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=1200, weight_decay=0.0)
:::MLPv0.5.0 transformer 1541701134.504344225 (/workspace/translation/train.py:44) opt_name: "adam"
:::MLPv0.5.0 transformer 1541701134.504730463 (/workspace/translation/train.py:45) opt_learning_rate: [0.00135]
:::MLPv0.5.0 transformer 1541701134.505114079 (/workspace/translation/train.py:46) opt_hp_Adam_beta1: 0.9
:::MLPv0.5.0 transformer 1541701134.505390644 (/workspace/translation/train.py:47) opt_hp_Adam_beta2: 0.997
:::MLPv0.5.0 transformer 1541701134.505649328 (/workspace/translation/train.py:48) opt_hp_Adam_epsilon: 1e-09
:::MLPv0.5.0 transformer 1541701134.550803661 (/workspace/translation/train.py:53) run_set_random_seed: 12929
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
:::MLPv0.5.0 transformer 1541701134.624023914 (/workspace/translation/train.py:61) model_hp_sequence_beam_search: {"alpha": 1, "beam_size": 4, "extra_decode_length": 200, "vocab_size": 33712}
| /data train 4575616 examples
| Sentences are being padded to multiples of: 1
| /data valid 3000 examples
| Sentences are being padded to multiples of: 1
:::MLPv0.5.0 transformer 1541701135.759724140 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701135.771896839 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701135.809693098 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701135.827029228 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701135.833774567 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701135.840066671 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701135.860996723 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701135.863003492 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701135.864518642 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701135.867419243 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701135.870459080 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701135.870906830 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701135.912796974 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701135.930852890 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701135.939953804 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701135.945369482 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701135.948267698 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701135.952187300 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701135.961177588 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701135.967633009 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701136.001085043 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701136.004514217 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701136.008878231 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701136.010616541 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701136.021181107 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701136.023794651 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701136.028823137 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701136.031660795 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701136.044259071 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701136.050683022 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701136.095030785 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541701136.100466013 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541701136.511570215 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.512281895 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.535852432 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.545127153 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.545837641 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.547306061 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.547994137 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.559802532 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.573372841 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.607616663 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.634328842 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.649733305 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.650433302 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.657965899 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.663730621 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.686118126 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.686849833 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.701998234 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.702764750 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.713696480 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.719610929 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.744967937 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.750064373 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.753297567 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.754012585 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.767282248 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.772021770 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.777331114 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.797906160 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.798627615 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.802963972 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.818285227 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.818983078 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.828379393 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701136.829010248 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701136.829563379 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.830105305 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701136.830751181 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.831336498 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.832816362 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.833506346 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.840952873 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701136.841582060 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701136.842138767 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.842684269 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701136.843320131 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.843903065 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.844594002 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.856798649 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.867773294 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.887368202 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.891629934 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.899403334 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.902687311 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.910178900 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.916699648 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.917406797 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.921751976 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.937536240 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.945890188 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.958352089 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.959125519 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.966378450 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.967071772 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.972771168 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.978834629 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.984364986 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.986006260 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701136.986706018 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701136.987237215 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701136.996878386 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.019304514 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.023051023 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.023997545 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.036042929 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.037651062 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.039486170 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.042302370 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.044723988 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.046172619 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.050321102 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.050857544 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.051528454 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.052063465 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.052685499 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.053344011 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.058246374 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.063363075 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.069795132 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.073409319 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.077030182 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.087156296 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.087460041 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.095391273 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.097582817 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.098388195 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.103672981 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.107684851 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.108231068 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.108714581 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.109248877 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.109957695 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.110522985 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.114051104 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.120619297 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.128088474 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.135640860 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.142598867 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.144088507 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.146330357 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.147430182 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.147935390 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.148383141 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.148808241 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.149312496 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.149770975 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.153204679 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.155745506 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.157984018 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.158938885 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.161527872 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.166902781 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.167467833 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.167930365 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.168390512 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.168921232 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.169401169 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.170567513 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.171094418 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.171537399 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.171970606 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.172498703 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.172964811 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.175274134 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.178668022 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.178984880 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.179517508 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.179947615 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.181692839 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.182058334 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.182257175 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.182691097 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.182713747 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.184706926 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.185212135 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.185647726 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.186074734 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.186444044 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.186583757 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.187320471 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.189373493 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.190029144 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.190700054 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.191350937 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.192095518 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.192815542 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.193269014 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.193648577 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.194182634 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.197846174 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.199900627 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.200299501 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.201718092 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.202476263 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.206697226 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.211141109 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.211817741 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.212388277 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.212939024 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.213586330 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.214176416 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.216106892 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.217060328 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.219366312 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.223524570 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.224162102 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.224728346 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.225281239 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.225817204 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.225923777 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.226516485 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.226526737 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.227220535 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.230612278 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.231167316 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.231619120 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.232063055 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.232863426 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.233078003 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.233407974 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.234494686 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.236122608 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.236928225 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.237427711 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.237578392 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.238138676 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.238693953 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.239346743 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.239453077 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.239948511 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.239967346 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.244021416 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.244476557 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.248543978 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.252976894 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.260971546 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.266357422 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.267707825 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.269328356 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.270151854 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.270915031 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.271643400 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.272081614 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.272603035 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.272751093 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.273164511 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.273331881 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.273416519 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.273885250 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.274536848 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.275133848 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.276280880 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.280369520 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.285247803 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.285635233 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.285653591 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.286428452 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.286683083 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.288078308 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.292634487 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.293688536 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.299129009 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.314306736 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.315189362 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.315876722 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.316444874 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.317231655 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.318083286 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.318062067 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.318519831 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.318540573 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.324198246 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.324848175 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.325401783 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.325948715 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.326585770 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.327170372 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.327798843 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.327882051 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.329625845 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.331052303 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.334033489 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.334925890 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.340090036 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.348058939 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.349176168 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.353548527 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.356863260 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.357499838 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.358055830 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.358597279 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.359244585 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.359833717 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.363181591 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.372071743 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.372753620 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.376022100 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.378188133 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.383382559 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.383724451 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.384545565 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.385649443 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.386306047 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.386869907 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.387423515 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.388083935 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.389445066 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.391186476 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.391834974 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.392399311 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.392947435 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.393586874 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.394172430 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.400645494 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.401128292 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.401533127 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.401930094 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.402285576 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.402401209 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.402827740 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.403767824 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.404021978 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.404778957 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.405639887 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.411561489 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.413923264 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.415072441 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.415409327 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.415510178 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.416142702 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.416419744 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.416705608 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.416940212 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.417251825 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.417381048 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.417813063 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.417893171 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.418332338 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.418482065 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.418793678 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.424898863 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.427480936 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.427568674 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.428054810 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.429957151 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.430839777 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.431339264 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.431735992 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.432123184 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.432648420 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.433106422 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.434310198 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.434320450 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.434798956 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.434815645 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.435197592 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.435214043 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.435585737 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.435605526 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.436048985 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.436067581 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.436507463 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.436526775 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.436885595 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.438592434 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.441221237 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.441781044 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.441890955 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.441994190 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.442244768 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.442359209 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.442638874 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.442754745 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.443025112 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.443146229 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.443236828 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.443480253 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.443610191 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.443897247 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.444032907 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.445513248 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.445564270 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.447907925 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.452715158 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.452899218 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.452863455 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.455123425 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.455600977 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.456011772 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.456436396 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.456942797 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.457411051 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.458294153 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.458325386 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.466204882 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.466961861 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.467445612 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.467859507 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.468260765 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.468807697 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.469283342 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.472040653 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.472627878 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.473316431 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.474967480 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.476770878 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.476785183 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.478095531 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.478157997 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.478575468 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.478976250 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.479365349 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.479830742 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.480252743 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.483565331 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.483908415 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.484784365 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.489116669 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.491326571 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.491570711 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.497282982 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.505609512 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.505927086 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.506713867 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.508991957 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.509444714 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.509509087 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.509912491 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.510241985 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.510253429 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.510325909 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.510800123 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.511227846 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.515254498 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.516059637 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.520084381 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.520145178 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.530056715 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.530458450 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.530848026 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.531324387 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.531717062 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.532102108 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.532613039 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.533083916 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.533147335 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.533363342 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.533629179 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.533892155 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.534301281 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.534686565 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.535152197 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.535568476 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.541947842 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.543107033 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.544385910 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.548671722 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.551120996 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.551480293 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.553164959 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.555228710 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.555695772 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.556090117 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.556514978 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.557015181 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.557470322 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.558557749 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.558899403 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.559049129 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.559401751 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.559464216 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.559824944 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.559870481 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.560239077 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.560364008 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.560786486 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.560807705 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.561257839 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.564524889 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.566347837 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.568069696 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.569208622 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.570083380 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.570341825 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.572396040 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.573213100 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.573270798 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.573856592 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.574313641 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.574706793 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.575090647 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.575368404 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.575547457 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.575963497 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.576385975 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.576870441 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.577276707 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.577684402 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.578192711 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.578622103 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.584584951 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.584928751 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.587234020 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.587636709 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.589855909 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.590347767 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.590742826 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.591135025 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.591596365 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.592010021 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.593361855 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.593883038 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.594280005 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.594687462 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.595169067 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.595588923 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.595683098 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.596169233 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.596610546 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.597029686 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.597541094 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.597613573 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.597850323 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.598082066 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.598388195 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.598482847 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.598430634 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.598874092 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.598894119 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.598933697 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.599292517 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.599336624 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.599681854 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.599759817 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.600140810 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.600588799 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.600982189 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.601342201 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.601690531 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.604577541 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.606616497 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.607060909 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.608409882 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.609226704 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.611499786 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.612710714 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.612818480 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.613294125 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.613707066 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.614097118 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.614564419 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.614995003 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.615083218 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.617928505 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.626152515 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.627037287 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.627520800 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.627935886 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.628365517 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.628829956 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.630350351 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.632118464 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.634668112 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.635715246 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.637085438 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.637625694 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.637835741 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.638025999 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.638416052 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.638907909 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.639192581 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.639363527 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.640070677 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.642636299 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.648248434 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.648284674 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.649372578 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.652153254 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.652655840 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.655961037 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.661594868 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.664247513 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.665682316 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.667781115 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.668244362 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.668691397 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.669118166 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.669222593 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.669630527 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.670042515 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.671356440 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.672616005 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.673505068 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.676802874 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.678930521 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.679548264 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.686781883 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.687574387 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.689299822 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.689479113 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.689994574 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.690386534 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.690767050 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.691225529 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.691642046 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.692594767 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.693233728 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.700498343 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.702023983 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.702483416 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.702876329 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.703260422 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.703715563 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.704129219 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.705849171 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.706916809 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.710220814 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.711936712 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.712461233 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.712735176 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.712800026 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.712913036 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.712953806 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.713231087 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.713351488 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.713641644 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.713841200 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.714045525 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.714263201 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.714516401 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.714942694 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.717849970 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.718331575 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.718732595 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.719123363 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.719612360 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.720048428 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.723077536 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.723304510 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.723420620 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.726708174 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.726906538 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.728881121 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.728869677 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.730030060 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.730955124 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.730976343 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.731421709 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.731650829 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.731823444 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.732138157 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.732210398 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.732660055 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.732729197 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.733099937 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.733191013 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.733534575 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.733995676 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.734416723 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.741978645 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.743204355 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.743365526 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.744023800 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.746867657 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.748662233 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.749196768 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.749684811 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.750140190 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.750659227 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.751123905 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.752114534 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.752668142 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.753105879 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.753541231 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.754024982 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.754107952 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.754189014 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.754476547 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.754587173 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.754584551 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.754976749 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.755112171 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.755357504 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.755576849 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.755815029 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.755975485 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.756039381 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.756228447 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.756392241 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.756536245 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.756898165 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.756969929 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.757365227 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.757395267 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.757877350 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.758287430 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.759950161 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.760281324 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.763336420 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.765075207 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.765189171 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.766190052 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.767036200 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.770484686 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.772031069 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.772543192 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.772988319 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.772997379 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.773418665 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.773913383 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.774340630 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.774338722 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.783143520 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.783549070 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.784875870 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.785963297 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.787287474 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.791080236 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.793436766 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.794456005 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.795618296 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.796138525 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.796140909 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.796578884 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.797010899 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.797372580 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.797519684 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.797971249 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.798119068 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.801034927 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.806194782 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.806763411 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.808847189 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.810326099 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.810832739 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.811259747 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.811659575 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.812028170 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701137.814274073 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.815259695 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.815790892 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.825026751 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.825268507 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.830302238 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.831459284 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.833294392 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.833306074 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.833934546 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.834484339 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.835024595 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.837938070 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.838864088 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.841803074 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.846695423 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.849375486 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.849638224 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.853229761 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.853375435 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.854200840 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.854716301 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.855614185 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.863753319 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.863774300 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.864020109 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.864379406 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.868503332 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.868535519 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.869015694 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.869420767 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.869817972 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.870288134 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.870715618 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.872230291 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.872751713 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.873201370 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.873640776 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.873980045 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.874045134 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701137.874652863 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.875443697 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.875925303 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.876074553 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.876579285 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.877342939 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.877960205 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.879376650 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.880019188 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.881603003 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.888716459 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.889384747 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.889955997 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.890522718 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.890965939 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.891209364 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.891523838 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.891820192 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.893004894 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.893584490 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.894239187 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.894474745 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.894805908 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.895363092 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.895830870 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.896457434 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.897022009 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.897510052 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.897988081 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.910090685 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.910863400 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.911546946 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.912215710 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.913021088 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.913757086 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.918069601 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.919177294 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.923518181 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.923699856 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.923706770 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.924700260 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.927614927 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.927776337 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.928314209 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.928411961 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.929736376 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.931811094 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.932467461 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.933023453 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.933571815 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.933731318 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.934334517 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.934527636 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.934978724 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.935077667 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.935541391 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.936089277 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.936738968 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.937335253 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.937407255 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.939997673 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.941480160 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.942550421 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.943337917 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.944036961 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.944721937 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.945533514 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.946264744 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.946676016 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.950453281 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.954130173 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.956668377 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.957342863 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.957900524 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.958448887 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.959098101 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.959687948 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.961205482 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.961212873 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.961848497 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.961863279 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701137.962064266 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.962161064 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.962408781 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.962426424 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.962958097 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.962978363 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701137.963122368 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.963628769 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.963737249 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.964234352 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.970905066 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.972705603 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.976729870 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.977257729 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.980936766 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.982079029 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.987013102 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.987321377 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701137.987368822 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701137.987927198 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.996607542 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701137.999470234 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.007240772 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.007458925 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.015511751 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.016062737 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.016662359 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.017222643 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.017307043 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.017718554 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.017790318 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.018289804 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.022944212 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.023760557 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.028153896 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.031560421 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.033714771 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.037152767 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.037746668 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.040065289 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.040749550 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.043419600 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.051250696 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.052131891 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.054079533 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.062929153 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.063313961 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.063429356 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.063858032 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.064278841 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.064669371 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701138.067811489 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.067824841 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.068483353 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.068642378 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.072064400 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.072144032 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.076129198 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.076198816 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.076712608 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.077156544 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.077477455 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.077591181 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.078088999 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.078547716 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.081449986 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.087619781 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.098397017 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.102550268 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.103056431 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.103488445 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.103912592 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.104436159 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.104466915 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.104898214 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.107883692 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.111608982 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.112236023 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.112798929 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.113341808 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.113822460 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701138.114289999 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.118812561 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.119469166 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.122059345 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.122901201 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.123517752 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.126658916 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.127518415 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.128136158 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.131351233 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.132972717 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.133299112 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.134255171 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.135732412 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.136394501 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.136625767 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.136768341 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.136962891 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.137514591 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.138004065 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701138.139429331 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.141198158 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.141759396 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.153084755 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.157009363 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.165809631 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.166643381 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.167296886 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.167868137 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.168440580 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.169103861 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.169724703 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.169734716 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.170688629 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.171325445 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.171879530 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.172128439 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.172762394 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.173131943 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.173322201 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.173605919 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.173720598 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701138.173873663 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.174469233 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.174521685 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.174520493 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.175084591 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.175117731 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.175197124 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.176838875 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.177481890 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.178028822 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.178567648 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.179041624 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701138.179475307 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.180109262 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.180739641 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.181296825 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.181787729 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701138.187226534 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.188168049 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.188776255 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.189877033 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.198284149 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.198483229 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.198965311 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.199045420 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.199533463 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.200015068 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.200455189 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701138.204338789 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.204975367 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.205536604 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.206089258 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.206579685 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701138.206802130 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.207076073 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.207451105 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.218025684 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.220798731 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.235511541 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.236326456 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.236877203 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.248903990 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.254446268 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.255062580 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.255278349 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.255575895 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.255984545 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.256080151 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.256533861 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701138.267441034 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.268302679 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.268928766 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.282168627 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.285677671 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.286353350 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.289766550 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.312117100 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.312853575 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.313971758 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.317005873 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.319461584 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.326075792 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.326170921 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.326772690 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.331796408 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.337727785 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.339123249 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.352714777 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.353288889 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.372362852 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.373290300 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.373337030 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.373980999 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.380998373 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.386656284 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.387299776 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.387413025 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.387866020 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.397227049 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.404551506 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.405205011 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.405772448 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.406323910 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.406770468 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701138.414410353 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.415313244 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.415942192 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.425603867 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.426136732 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701138.427008629 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.438296080 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.440099955 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.440895319 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.442307234 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.442882299 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.443440914 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.443783760 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.444098711 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.444709301 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.449152946 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.450122356 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.450786591 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.456301451 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.457008600 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.459546566 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.460439205 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.461035728 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.463325024 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.464910746 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.469728708 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.471161127 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.472651720 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.476164103 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.477299213 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.486009359 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.488358736 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.498798132 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.499928474 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.500970125 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.501065969 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.501863480 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.507349730 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.509793520 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.510684252 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.513751268 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.514290094 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.514754295 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.515193462 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.515715361 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.516187906 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.521324635 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.522299290 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.522935629 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.523486376 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.524029970 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.524521828 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701138.525957584 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.527235270 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.543793917 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.544386625 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.544568300 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.544890642 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.545210123 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.545374155 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.545771837 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.545945644 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.546321154 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.546448708 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.546956539 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.547541618 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.549434185 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.550109625 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.550689220 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.550924778 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.551172495 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.551584721 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701138.557818413 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.560413837 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.560733795 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.561634064 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.562257051 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.563581228 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.573678493 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.575652122 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.580447197 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.582819223 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.594004393 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.597325325 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.599186182 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.599414110 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.600064993 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.600695610 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.611466646 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.612115145 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.612682819 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.613232136 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.613880396 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.614470005 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.620576620 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.621223450 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.621609688 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.621789455 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.622351885 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.622382402 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.622927189 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.623006105 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.623606682 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.627276897 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.629280806 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.629955053 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.635748863 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.636446714 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.636897802 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.637538195 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.638097763 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.638645649 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.639291048 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.639883757 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.640585423 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.645146132 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.653103113 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.654334784 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.655257225 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.655882359 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.657634497 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.660818338 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.661506891 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.662078381 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.662635803 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.663295746 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.663897514 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.667113543 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.668085098 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.668660879 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.668927193 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.669541121 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.670151472 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.671527624 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.676898718 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.677077532 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.678088427 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.678726196 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.680172205 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.681898594 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.682744265 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.683362722 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.684331894 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.691693544 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.695096731 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.695739746 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.696500301 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.696909666 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.697027445 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.697147131 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.697803974 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.697839975 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.698270559 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.698393822 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.698401690 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.699081898 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.699637651 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.709404945 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.710006714 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.710004330 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.710516214 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.711020231 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.711029768 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.711351871 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.711394310 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.711607456 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.712146521 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.713044405 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.723079681 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.723836899 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.724023581 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.724659443 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.724989414 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.725335121 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.726117611 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.726656675 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.732195616 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.738287449 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.738334656 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.751481533 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.752631187 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.753422260 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.754152298 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.754837036 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.755563259 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701138.757609844 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.758394957 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.758934736 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.759593487 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.766207457 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.767015457 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.767575264 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.767552137 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.770655632 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.770739317 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.780679464 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.786558390 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.797582865 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.803946018 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.804614305 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.810970068 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.811678886 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.812306166 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.812920570 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.813638210 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.813668013 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.813871622 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.814290524 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.819294930 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.823305368 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.828442812 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.834487200 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.843967438 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.846795082 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.847411633 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.847811937 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.848301649 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.848834515 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.849082947 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.849868059 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.850764275 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.851594448 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.851937771 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.857794046 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.861902237 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.864529133 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.871402502 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.872904301 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.873397589 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.874011993 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.874453545 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.874749899 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.874867201 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.875365973 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.875534296 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.875816345 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.875853777 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.876175404 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.876642704 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.877254486 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.877733469 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.880110502 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.880985260 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.881072283 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.881488800 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.881744623 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.881913424 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.882314205 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.882791281 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.883225918 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.884827137 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.884994507 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.885507107 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.886062622 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.886607647 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.887253046 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.887849569 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.888044596 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.892162323 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.894845486 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.895646811 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.896231413 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.896444321 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.896509171 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.897015810 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.897158861 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.897451878 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.897879601 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.897969246 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.898385525 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.898713350 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.898849010 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.899563313 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.901536465 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.902203083 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.902678728 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.902670860 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.903317213 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.903879166 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.904442072 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.905089378 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.905704975 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.908136845 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.909755945 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.911932468 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.914635420 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.918710709 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.919466972 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.919645786 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.920337439 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.920395851 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.920897961 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.920959234 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.922170401 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.922850370 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.923485518 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.924426556 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.925052643 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.925474167 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.926797867 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.928205729 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.930963039 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.932261944 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.932523727 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.934162617 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.934172630 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.934627771 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.936427116 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.936928511 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.937361479 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.937785149 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.938286543 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.938742876 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.940345764 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.941039085 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.941518545 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.941831827 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.942698240 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.943306446 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.948301792 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.950900316 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.954948664 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.958051682 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.958570957 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.959014654 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.959449053 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.959954500 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.960433245 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.961369276 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.964082956 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.964685917 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.964744806 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.965317726 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.965483427 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.965457201 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.965877771 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.966031790 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.966108084 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.966530323 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.966669798 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.967131853 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.967219830 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.967867374 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.968468428 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.969854116 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.971585512 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.972272873 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.972254753 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.972848415 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.973357916 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.973408222 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701138.974075317 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.974138021 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.974674940 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.977584600 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.978312492 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.980053186 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.981262684 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.981459618 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.981971502 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701138.982451439 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.987698078 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.990043640 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.991670370 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701138.998607874 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.999248981 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701138.999809027 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701138.999827862 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.000367165 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.001010895 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.001615524 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.001578569 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.002261877 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.003352404 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.004029989 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.004479647 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.010932922 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.012609482 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.013212681 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.018139601 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.018714666 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.019371748 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.019663095 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.019826174 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.020141602 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.020336866 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.020788431 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.023900747 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.024659157 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.025146008 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.028614283 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.029662609 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.029811382 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.033937931 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.035962343 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.041898727 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.042554617 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.043001175 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.046716213 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.051679850 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.057936668 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.058485985 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.059899092 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.061310768 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.061812401 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.062231541 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.062202930 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.062636614 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.063108444 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.063565731 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.072276115 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.075549126 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.076626539 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.077538967 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.083369255 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.083603859 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.083858967 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.084080219 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.084271193 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.084558249 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.084768772 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.084994078 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.085302353 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.085439920 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.085510731 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.085606337 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.085788488 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.085948706 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.085959673 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.086351633 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.086746216 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.087213993 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.087638378 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.092792511 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.093325615 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.094613552 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.094721317 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.096482038 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.097625017 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.100370169 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.101017237 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.101535082 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.101943493 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.102334023 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.102553129 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.102795601 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.103015900 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.103217363 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.103411674 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.103744030 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.103804111 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.104268551 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.104312897 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.104505062 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.104763508 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.105014324 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.110640526 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.111118793 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.111525059 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.111925364 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.112013102 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.112394333 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.112824202 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.113603354 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.113842249 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.115515947 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.115779161 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.121181488 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.125695229 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.125931978 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.126101971 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.126343966 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.126576900 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.126792192 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.127012491 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.127886772 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.128603220 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.129094124 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.129464388 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.129928112 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.130318403 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.130700827 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.131149292 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.131563663 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.134253979 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.135504961 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.135825634 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.138689518 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.140357494 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.143078566 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.143709898 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.144135475 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.144587040 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.145272493 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.145748138 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.150866270 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.151016474 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.151070833 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.151520014 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.151963949 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.152685642 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.153045654 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.153191328 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.153635740 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.154020786 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.154477835 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.154492855 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.154891253 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.155355215 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.157982111 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.158446074 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.158844709 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.159233332 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.159419298 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.159695387 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.159897327 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.160118818 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.160299301 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.160307884 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.160757542 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.161265373 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.161732197 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.163650036 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.168992519 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.170556068 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.171458244 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.172084570 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.172556400 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.173221350 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.176258087 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.176363707 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.176786900 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.176904440 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.177223444 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.177344561 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.177657604 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.177772999 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.178137541 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.178250790 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.178373575 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.178571701 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.178693533 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.180470467 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.181003094 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.181411266 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.181445360 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.181879997 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.182373047 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.182661057 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.182799816 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.187364340 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.187564611 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.191530466 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.194705725 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.195353508 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.195785046 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.198284626 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.198751211 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.199151278 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.199158430 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.199540854 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.199765921 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.200008392 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.200136423 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.200463533 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.200776339 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.201199055 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.201636314 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.202296019 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.202743053 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.202884197 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.204562902 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.207773447 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.208249331 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.208714724 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.209147215 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.209290028 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.209650517 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.209993362 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.210064411 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.211526155 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.216728210 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.218310118 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.218357801 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.218840361 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.218865395 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.218999624 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.219437361 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.219475985 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.219932556 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.221320152 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.222603798 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.223295689 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.223774672 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.228362799 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.229004145 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.232572317 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.236467123 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.240523815 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.241196394 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.241683960 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.245222330 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.249897242 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.250463247 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.250571012 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.250999689 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.256713629 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.257508278 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.259766579 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.260552883 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.261716366 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.262227297 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.262650013 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.263072014 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.263544559 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.263973713 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.272814512 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.273887157 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.274467945 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.274868727 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.275583029 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.275964022 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.277171135 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.281891584 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.282360554 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.282764435 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.282704592 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.283155441 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.283249617 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.283624411 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.283730984 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.284049988 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.284169197 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.284687281 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.285151243 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.285731316 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.286215067 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.286619186 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.287011623 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.287477255 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.287897825 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.292855740 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.293223858 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.294173002 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.294399738 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.296757460 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.297325373 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.299529076 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.299991608 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.300418854 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.300846338 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.301133394 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.301354647 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.301640511 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.301805258 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.302034140 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.302210331 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.302419424 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.302687407 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.302876234 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.302933216 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.303093672 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.303288460 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.303491354 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.303956270 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.304394245 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.305157900 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.305908442 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.306365252 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.310601711 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.312024117 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.312791348 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.315226793 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.315242767 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.323709249 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.323907375 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.324606657 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.325093746 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.325371504 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.326081514 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.326524019 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.327820539 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.328065634 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.328518391 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.328573942 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.329006910 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.329010725 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.329432011 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.329911470 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.330326080 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.331369400 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.333234787 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.333851576 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.335449457 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.337816715 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.338759422 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.339155912 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.341716051 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.342377424 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.342520475 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.342813492 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.343128681 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.343181133 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.343626261 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.343780279 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.344210386 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.351399899 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.351755619 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.351972818 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.353059292 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.353210449 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.355491161 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.356439114 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.356940031 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.357370138 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.357780695 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.358234882 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.358374834 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.358652592 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.358855486 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.359258652 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.359652042 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.360116243 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.360587597 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.364862204 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.365474701 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.365908146 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.366297245 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.366771698 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.367190361 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.367424488 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.369521141 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.370404959 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.371042967 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.371472836 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.373846769 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.376008272 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.376535892 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.377042770 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.377478123 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.377890110 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.378299952 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.378367901 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.378780365 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.378804207 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.379191399 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.379595518 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.380077839 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.380240679 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.380592346 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.380730391 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.381186962 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.381271601 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.381639242 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.381716490 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.382116318 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.382600546 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.383032322 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.387706995 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.389544249 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.391917467 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.397760630 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.398469925 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.398957014 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.399118423 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.399440527 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.399554968 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.399848938 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.400247335 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.400623560 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.400660992 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.400772333 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.401237249 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.401344061 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.401821852 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.401939154 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.406715155 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.407190084 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.407202005 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.407579184 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.407853842 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.407961607 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.408296585 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.408276558 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.408451080 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.408904791 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.410192966 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.410568953 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.412638903 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.415723562 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.417125463 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.417209387 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.417714834 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.419207096 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.419891357 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.420354843 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.421134949 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.421885967 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.422351122 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.423388720 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.424073458 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.424562931 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.429287195 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.431258678 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.433509350 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.439670801 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.441615105 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.442231894 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.442651987 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.443793774 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.448698997 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.449368000 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.449813366 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.450883627 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.454715729 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.458336592 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.458616018 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.458835602 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.465049028 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.465584993 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.466010571 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.466422081 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.466910362 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.467340946 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.468539953 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.471337318 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.472800732 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.473853588 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.474261999 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.476260662 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.479817867 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.480281115 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.480728388 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.481128693 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.481606007 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.482025385 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.483796597 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.483974695 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.484338284 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.484498262 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.484793425 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.484920740 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.485229731 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.485327244 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.485743046 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.485828400 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.486176491 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.486260891 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.486280918 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.490632296 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.493584633 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.493816614 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.494063854 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.494469404 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.494874477 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.494944096 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.495063543 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.495348215 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.495782375 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.496149302 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.497853994 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.498053551 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.498317719 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.498714924 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.499103785 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.499281883 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.499563932 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.499745131 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.499983311 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.500138044 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.500554562 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.500728369 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.501019955 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.501444578 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.504129887 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.508018970 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.508701324 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.508853674 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.509354353 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.510071039 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.513904095 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.518324852 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.521182775 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.521294594 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.521839142 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.522279263 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.525488377 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.525746584 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.526150465 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.526209831 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.526433229 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.526593447 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.526609421 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.527002335 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.527084827 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.527459621 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.527518988 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.527877092 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.527978420 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.530628443 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.530755758 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.533945799 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.534612656 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.535066605 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.535103321 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.536409855 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.536452770 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.539090395 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.539734364 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.540163279 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.540464163 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.541106462 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.541538239 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.543418884 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.548709869 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.549042463 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.550020695 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.551779270 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.552994013 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.553455114 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.553844929 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.554232121 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.554536104 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.554690838 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.555105686 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.555746317 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.556082487 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.556230068 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.556642056 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.557038307 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.557503939 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.557929993 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.563590288 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.566515446 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.566684723 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.567312717 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.567741632 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.571668148 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.576220512 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.576445103 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.576745033 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.576916695 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.577156305 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.577429771 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.577568054 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.577861071 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.578036785 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.578089476 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.578299284 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.578462362 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.578687668 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701139.579704046 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.580203295 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.580701113 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.581137657 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.581298590 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.581541538 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701139.581831455 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.582241535 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.582642078 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.583125353 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.583563089 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.583630323 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.587251902 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.592519283 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.593201160 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.594284296 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.594913721 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.595343828 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.596849680 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.597371578 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.597409964 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.597811222 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.597854376 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.598065853 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.598249674 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.598509789 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.598755598 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.599183798 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.602989197 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.603181839 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.603656292 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.604049683 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.604054689 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.604321003 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.604459524 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.604930162 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.605355024 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.606837273 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.608169794 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.611739874 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.612908363 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.614022970 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.618099689 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.618754864 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.619187593 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.624304295 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.625039101 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.625521421 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.627817392 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.634439468 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.639585972 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.639658689 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.640348911 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.640838146 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.641928434 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.644814253 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.645479441 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.645918369 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.649767637 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.650298357 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.654548407 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.654984474 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.660266638 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.660886049 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.666912079 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.667186737 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.667669535 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.668072939 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.668516159 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.668858767 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.668861628 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.668927193 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701139.669958591 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.675381899 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.675851345 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.676249743 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.676676273 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.677155256 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.677598000 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.680480003 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.680961609 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.681364059 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.681759834 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.682343245 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.682779551 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.685317516 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.685797453 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.686085701 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.686127663 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.686207533 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.686627865 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.686637640 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.686997175 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701139.687056065 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.687461376 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.687858343 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701139.691350937 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.693963766 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.694437742 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.694832563 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.694997787 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.695219517 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.695460796 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.695774794 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.695855379 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.696205139 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.696239948 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.696702480 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.696856260 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.697120428 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.698397160 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.699425697 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.704778910 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.705680370 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.714261532 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.716364145 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.717018127 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.717096567 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.717457771 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.721710920 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.722375393 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.722820282 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.723464251 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.723464012 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.723922968 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.724333525 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.724748135 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.725102425 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701139.725646257 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.725963593 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.731338739 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.735115767 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.735768318 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.735972404 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.736195564 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.736649752 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.737087965 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.744513273 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.745578766 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.748522997 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.748980522 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.749372721 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.749759912 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.750212193 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.750634909 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.750827074 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.751304150 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.751705170 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.752098083 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.752464533 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701139.757625580 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.759252071 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.759607792 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.772260666 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.773878574 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.782891750 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.783392906 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.783791542 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.784182787 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.784594059 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
| model transformer_wmt_en_de_big_t2t, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 210808832
:::MLPv0.5.0 transformer 1541701139.787768364 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.787986755 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.788456440 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.788852453 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.789237738 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.789609909 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.789695263 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.790113688 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.790302038 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.790734529 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.793500900 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.797424316 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.797911644 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.798333168 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.798685074 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.798730612 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.799088240 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701139.799017906 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.799241304 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.799492836 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.799886227 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.800267458 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.800734997 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.801147699 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.805994987 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.809696198 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.810914278 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.829030991 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.829687357 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.830124855 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.838757277 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.840337753 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.841068745 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.841508150 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.844833612 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.850257635 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.850845098 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.862264395 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.863787413 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.869995117 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.870459557 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.870857716 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.871244192 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.871596813 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701139.872110605 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.875979424 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.876482964 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.876899958 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.877328634 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.877728701 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701139.888842344 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.889320135 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.889714718 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.890103102 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.890455246 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701139.899074793 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.899549484 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.899943113 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.900354862 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.900833130 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.901397228 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.908339262 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.910578251 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.913768530 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.919147730 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.944178581 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.944207668 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.944646120 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.944869280 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541701139.945043802 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.945306063 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.945431709 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.945781946 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701139.954236746 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.970330715 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.971115828 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701139.995434523 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.995889425 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.996278048 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.996697187 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.997055769 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701139.997545958 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.998001575 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701139.998394012 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701139.998777628 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701139.999178410 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541701140.019979000 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701140.079672337 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541701140.105613708 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541701140.106205463 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541701140.106618166 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541701140.107002974 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541701140.107353449 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
| training on 16 GPUs
| max tokens per GPU = 10240 and max sentences per GPU = None
:::MLPv0.5.0 transformer 1541701141.592582703 (/workspace/translation/train.py:88) input_batch_size: 10240
:::MLPv0.5.0 transformer 1541701141.592895031 (/workspace/translation/train.py:89) input_order
:::MLPv0.5.0 transformer 1541701148.808227539 (/workspace/translation/fairseq/optim/lr_scheduler/inverse_square_root_schedule.py:42) opt_learning_rate_warmup_steps: 1200
:::MLPv0.5.0 transformer 1541701148.808689356 (/workspace/translation/train.py:114) train_loop
:::MLPv0.5.0 transformer 1541701148.808963060 (/workspace/translation/train.py:116) train_epoch: 0
generated batches in  2.1393637657165527 s
got epoch iterator 2.141275644302368
| WARNING: overflow detected, setting loss scale to: 64.0
| WARNING: overflow detected, setting loss scale to: 32.0
| WARNING: overflow detected, setting loss scale to: 16.0
| WARNING: overflow detected, setting loss scale to: 8.0
| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 001 | loss 8.073 | nll_loss 6.944 | ppl 123.15 | wps 534964 | ups 3.5 | wpb 146367 | bsz 4701 | num_updates 967 | lr 0.00108788 | gnorm 37253.489 | clip 100% | oom 0 | loss_scale 4.000 | wall 275
epoch time  265.1358630657196
generated batches in  0.0006959438323974609 s
| epoch 001 | valid on 'valid' subset | valid_loss 5.17285 | valid_nll_loss 3.54959 | valid_ppl 11.71 | num_updates 967
:::MLPv0.5.0 transformer 1541701416.333910227 (/workspace/translation/train.py:149) eval_start: -1
| /data test 3003 examples
| Sentences are being padded to multiples of: 1
generated batches in  0.0005707740783691406 s
| Translated 192 sentences (4071 tokens) in 2.4s (80.49 sentences/s, 1706.62 tokens/s)
| Generate test with beam=4: BLEU4 = 18.03, 50.6/23.3/12.6/7.1 (BP=1.000, ratio=1.050, syslen=67701, reflen=64503)
| Eval completed in: 4.00s
:::MLPv0.5.0 transformer 1541701420.342757463 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": -1, "value": 18.03271525344806}
:::MLPv0.5.0 transformer 1541701420.343166351 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541701420.343405247 (/workspace/translation/train.py:154) eval_stop: -1
validation and scoring  4.257174968719482
:::MLPv0.5.0 transformer 1541701420.343708992 (/workspace/translation/train.py:116) train_epoch: 1
generated batches in  2.1620049476623535 s
got epoch iterator 2.277350664138794
| epoch 002 | loss 5.136 | nll_loss 3.585 | ppl 12.00 | wps 539305 | ups 3.6 | wpb 146353 | bsz 4707 | num_updates 1939 | lr 0.00106203 | gnorm 22721.208 | clip 100% | oom 0 | loss_scale 4.000 | wall 545
epoch time  263.9497904777527
generated batches in  0.000667572021484375 s
| epoch 002 | valid on 'valid' subset | valid_loss 4.46615 | valid_nll_loss 2.80559 | valid_ppl 6.99 | num_updates 1939
:::MLPv0.5.0 transformer 1541701686.854402781 (/workspace/translation/train.py:149) eval_start: 0
generated batches in  0.0006470680236816406 s
| Translated 192 sentences (3955 tokens) in 1.3s (146.95 sentences/s, 3027.05 tokens/s)
| Generate test with beam=4: BLEU4 = 22.87, 55.5/28.6/16.7/10.3 (BP=1.000, ratio=1.015, syslen=65482, reflen=64503)
| Eval completed in: 3.86s
:::MLPv0.5.0 transformer 1541701690.716345549 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": 0, "value": 22.867936991019235}
:::MLPv0.5.0 transformer 1541701690.716849327 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541701690.717102289 (/workspace/translation/train.py:154) eval_stop: 0
validation and scoring  4.1463072299957275
:::MLPv0.5.0 transformer 1541701690.717476130 (/workspace/translation/train.py:116) train_epoch: 2
generated batches in  2.156015157699585 s
got epoch iterator 2.263490915298462
| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 003 | loss 4.710 | nll_loss 3.123 | ppl 8.71 | wps 538898 | ups 3.6 | wpb 146370 | bsz 4705 | num_updates 2910 | lr 0.000866918 | gnorm 17861.863 | clip 100% | oom 0 | loss_scale 4.000 | wall 815
epoch time  263.9812660217285
generated batches in  0.0006773471832275391 s
| epoch 003 | valid on 'valid' subset | valid_loss 4.25129 | valid_nll_loss 2.55862 | valid_ppl 5.89 | num_updates 2910
:::MLPv0.5.0 transformer 1541701957.227700233 (/workspace/translation/train.py:149) eval_start: 1
generated batches in  0.0006244182586669922 s
| Translated 192 sentences (4127 tokens) in 1.5s (132.40 sentences/s, 2845.89 tokens/s)
| Generate test with beam=4: BLEU4 = 23.77, 55.9/29.5/17.6/11.0 (BP=1.000, ratio=1.049, syslen=67693, reflen=64503)
| Eval completed in: 4.06s
:::MLPv0.5.0 transformer 1541701961.288724899 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": 1, "value": 23.768935132802387}
:::MLPv0.5.0 transformer 1541701961.289219618 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541701961.289472818 (/workspace/translation/train.py:154) eval_stop: 1
validation and scoring  4.32726526260376
:::MLPv0.5.0 transformer 1541701961.289803743 (/workspace/translation/train.py:116) train_epoch: 3
generated batches in  2.1789629459381104 s
got epoch iterator 2.2578861713409424
| epoch 004 | loss 4.523 | nll_loss 2.918 | ppl 7.56 | wps 539061 | ups 3.6 | wpb 146353 | bsz 4707 | num_updates 3882 | lr 0.000750579 | gnorm 14593.033 | clip 100% | oom 0 | loss_scale 4.000 | wall 1086
epoch time  264.2818284034729
generated batches in  0.0006148815155029297 s
| epoch 004 | valid on 'valid' subset | valid_loss 4.1417 | valid_nll_loss 2.43329 | valid_ppl 5.40 | num_updates 3882
:::MLPv0.5.0 transformer 1541702228.163402081 (/workspace/translation/train.py:149) eval_start: 2
generated batches in  0.0006048679351806641 s
| Translated 192 sentences (4110 tokens) in 1.4s (140.25 sentences/s, 3002.27 tokens/s)
| Generate test with beam=4: BLEU4 = 24.65, 56.3/30.2/18.4/11.8 (BP=1.000, ratio=1.058, syslen=68271, reflen=64503)
| Eval completed in: 4.29s
:::MLPv0.5.0 transformer 1541702232.460990667 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": 2, "value": 24.65431727270596}
:::MLPv0.5.0 transformer 1541702232.461384535 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541702232.461638212 (/workspace/translation/train.py:154) eval_stop: 2
validation and scoring  4.63212513923645
:::MLPv0.5.0 transformer 1541702232.461934566 (/workspace/translation/train.py:116) train_epoch: 4
generated batches in  2.0047988891601562 s
got epoch iterator 2.114269495010376
| epoch 005 | loss 4.406 | nll_loss 2.789 | ppl 6.91 | wps 539645 | ups 3.6 | wpb 146353 | bsz 4707 | num_updates 4854 | lr 0.000671235 | gnorm 12620.377 | clip 100% | oom 0 | loss_scale 8.000 | wall 1357
epoch time  263.8055844306946
generated batches in  0.0006320476531982422 s
| epoch 005 | valid on 'valid' subset | valid_loss 4.05611 | valid_nll_loss 2.33297 | valid_ppl 5.04 | num_updates 4854
:::MLPv0.5.0 transformer 1541702498.677943707 (/workspace/translation/train.py:149) eval_start: 3
generated batches in  0.0005857944488525391 s
| Translated 192 sentences (4090 tokens) in 1.3s (146.46 sentences/s, 3119.87 tokens/s)
| Generate test with beam=4: BLEU4 = 25.95, 57.8/31.6/19.6/12.7 (BP=1.000, ratio=1.038, syslen=66956, reflen=64503)
| Eval completed in: 4.00s
:::MLPv0.5.0 transformer 1541702502.682340860 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": 3, "value": 25.945390958423193}
:::MLPv0.5.0 transformer 1541702502.682704210 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541702502.682933569 (/workspace/translation/train.py:154) eval_stop: 3
validation and scoring  4.3011474609375
:::MLPv0.5.0 transformer 1541702502.683222532 (/workspace/translation/train.py:167) run_stop
:::MLPv0.5.0 transformer 1541702502.683440924 (/workspace/translation/train.py:168) run_final
| done training in 1353.9 seconds
+++ date +%s
++ END=1541702511
+++ date '+%Y-%m-%d %r'
ENDING TIMING RUN AT 2018-11-08 06:41:51 PM
RESULT,transformer,12929,1412,,2018-11-08 06:18:19 PM
++ END_FMT='2018-11-08 06:41:51 PM'
++ echo 'ENDING TIMING RUN AT 2018-11-08 06:41:51 PM'
++ RESULT=1412
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,12929,1412,,2018-11-08 06:18:19 PM'
+ set +x
