Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3

:::MLPv0.5.0 ssd 1541756708.815406561 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541756710.792233706 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541756710.433260441 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541756708.175913572 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541756709.688848019 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541756705.251522779 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541756711.559623480 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541756705.461814880 (<string>:1) run_clear_caches
Launching on node circe-n027
+ pids+=($!)
+ set +x
Launching on node circe-n028
+ pids+=($!)
+ set +x
Launching on node circe-n029
+ pids+=($!)
+ set +x
Launching on node circe-n030
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n027
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n028
+ pids+=($!)
+ set +x
Launching on node circe-n031
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n029
+ srun -N 1 -n 1 -w circe-n027 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=0 --master_addr=10.0.1.27 --master_port=4242' -e SLURM_JOB_ID=35119 -e SLURM_NTASKS_PER_NODE=8 cont_35119 ./run_and_time.sh
+ srun -N 1 -n 1 -w circe-n028 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=1 --master_addr=10.0.1.27 --master_port=4242' -e SLURM_JOB_ID=35119 -e SLURM_NTASKS_PER_NODE=8 cont_35119 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n032
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n030
+ srun -N 1 -n 1 -w circe-n029 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=2 --master_addr=10.0.1.27 --master_port=4242' -e SLURM_JOB_ID=35119 -e SLURM_NTASKS_PER_NODE=8 cont_35119 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n033
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n031
+ pids+=($!)
+ set +x
+ srun -N 1 -n 1 -w circe-n030 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=3 --master_addr=10.0.1.27 --master_port=4242' -e SLURM_JOB_ID=35119 -e SLURM_NTASKS_PER_NODE=8 cont_35119 ./run_and_time.sh
Launching on node circe-n034
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n032
+ srun -N 1 -n 1 -w circe-n031 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=4 --master_addr=10.0.1.27 --master_port=4242' -e SLURM_JOB_ID=35119 -e SLURM_NTASKS_PER_NODE=8 cont_35119 ./run_and_time.sh
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n033
+ srun -N 1 -n 1 -w circe-n032 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=5 --master_addr=10.0.1.27 --master_port=4242' -e SLURM_JOB_ID=35119 -e SLURM_NTASKS_PER_NODE=8 cont_35119 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n034
+ srun -N 1 -n 1 -w circe-n033 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=6 --master_addr=10.0.1.27 --master_port=4242' -e SLURM_JOB_ID=35119 -e SLURM_NTASKS_PER_NODE=8 cont_35119 ./run_and_time.sh
+ srun -N 1 -n 1 -w circe-n034 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=7 --master_addr=10.0.1.27 --master_port=4242' -e SLURM_JOB_ID=35119 -e SLURM_NTASKS_PER_NODE=8 cont_35119 ./run_and_time.sh
Run vars: id 35119 gpus 8 mparams  --nnodes=8 --node_rank=0 --master_addr=10.0.1.27 --master_port=4242
Run vars: id 35119 gpus 8 mparams  --nnodes=8 --node_rank=3 --master_addr=10.0.1.27 --master_port=4242
Run vars: id 35119 gpus 8 mparams  --nnodes=8 --node_rank=6 --master_addr=10.0.1.27 --master_port=4242
Run vars: id 35119 gpus 8 mparams  --nnodes=8 --node_rank=1 --master_addr=10.0.1.27 --master_port=4242
Run vars: id 35119 gpus 8 mparams  --nnodes=8 --node_rank=2 --master_addr=10.0.1.27 --master_port=4242
Run vars: id 35119 gpus 8 mparams  --nnodes=8 --node_rank=5 --master_addr=10.0.1.27 --master_port=4242
Run vars: id 35119 gpus 8 mparams  --nnodes=8 --node_rank=4 --master_addr=10.0.1.27 --master_port=4242
Run vars: id 35119 gpus 8 mparams  --nnodes=8 --node_rank=7 --master_addr=10.0.1.27 --master_port=4242
STARTING TIMING RUN AT 2018-11-09 09:45:09 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=0 --master_addr=10.0.1.27 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:45:11 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=3 --master_addr=10.0.1.27 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:45:09 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=6 --master_addr=10.0.1.27 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:45:05 AM
running benchmark
STARTING TIMING RUN AT 2018-11-09 09:45:11 AM
+ echo 'running benchmark'
running benchmark
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=2 --master_addr=10.0.1.27 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=1 --master_addr=10.0.1.27 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:45:10 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=5 --master_addr=10.0.1.27 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:45:05 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=7 --master_addr=10.0.1.27 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:45:08 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=4 --master_addr=10.0.1.27 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
0 Using seed = 3632130797
1 Using seed = 3632130798
2 Using seed = 3632130799
4 Using seed = 3632130801
3 Using seed = 3632130800
15 Using seed = 3632130812
12 Using seed = 3632130809
14 Using seed = 3632130811
13 Using seed = 3632130810
10 Using seed = 3632130807
11 Using seed = 3632130808
9 Using seed = 3632130806
8 Using seed = 3632130805
22 Using seed = 3632130819
21 Using seed = 3632130818
23 Using seed = 3632130820
16 Using seed = 3632130813
19 Using seed = 3632130816
18 Using seed = 3632130815
17 Using seed = 3632130814
20 Using seed = 3632130817
30 Using seed = 3632130827
31 Using seed = 3632130828
29 Using seed = 3632130826
28 Using seed = 3632130825
24 Using seed = 3632130821
25 Using seed = 3632130822
26 Using seed = 3632130823
27 Using seed = 3632130824
37 Using seed = 3632130834
39 Using seed = 3632130836
35 Using seed = 3632130832
38 Using seed = 3632130835
33 Using seed = 3632130830
32 Using seed = 3632130829
34 Using seed = 3632130831
36 Using seed = 3632130833
45 Using seed = 3632130842
42 Using seed = 3632130839
43 Using seed = 3632130840
41 Using seed = 3632130838
40 Using seed = 3632130837
44 Using seed = 3632130841
47 Using seed = 3632130844
46 Using seed = 3632130843
54 Using seed = 3632130851
50 Using seed = 3632130847
55 Using seed = 3632130852
48 Using seed = 3632130845
49 Using seed = 3632130846
53 Using seed = 3632130850
52 Using seed = 3632130849
51 Using seed = 3632130848
58 Using seed = 3632130855
56 Using seed = 3632130853
57 Using seed = 3632130854
59 Using seed = 3632130856
63 Using seed = 3632130860
61 Using seed = 3632130858
62 Using seed = 3632130859
60 Using seed = 3632130857
6 Using seed = 3632130803
7 Using seed = 3632130804
5 Using seed = 3632130802

:::MLPv0.5.0 ssd 1541756723.734439850 (train.py:371) run_start

:::MLPv0.5.0 ssd 1541756723.736657619 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1541756723.737132072 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1541756723.750337362 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1541756723.750784874 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1541756723.798339367 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1541756723.817857981 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1541756723.832237005 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1541756723.852071762 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1541756723.872081757 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1541756723.879270077 (train.py:382) input_size: 300
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.42s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.42s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
Done (t=0.43s)
creating index...
index created!
Done (t=0.43s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
Done (t=0.44s)
creating index...
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
index created!
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
index created!
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
index created!
Done (t=0.44s)
creating index...
index created!
index created!
index created!
index created!
index created!
Done (t=0.45s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.45s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Done (t=0.46s)
creating index...
index created!
index created!
Done (t=0.46s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
time_check a: 1541756720.527644873
time_check a: 1541756724.785195351
time_check a: 1541756725.897662401
time_check a: 1541756723.932530642
time_check a: 1541756720.354539156
time_check a: 1541756726.651681185
time_check a: 1541756725.554984808
time_check a: 1541756723.300211668
time_check b: 1541756745.928314686
time_check b: 1541756748.677887678
time_check b: 1541756747.932731152
time_check b: 1541756742.578782797
time_check b: 1541756742.392818689
time_check b: 1541756745.377353191
time_check b: 1541756747.710972309
time_check b: 1541756747.456341267

:::MLPv0.5.0 ssd 1541756747.973914146 (train.py:413) input_order

:::MLPv0.5.0 ssd 1541756747.981257439 (train.py:414) input_batch_size: 32

:::MLPv0.5.0 ssd 1541756749.192812443 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541756749.193461418 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541756749.225200891 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1541756751.085418701 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1541756751.085986376 (train.py:477) opt_learning_rate: 0.16

:::MLPv0.5.0 ssd 1541756751.086421013 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1541756751.086837769 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1541756751.087250710 (train.py:483) opt_learning_rate_warmup_steps: 900

:::MLPv0.5.0 ssd 1541756752.455909014 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541756752.456517696 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541756752.478730202 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss

:::MLPv0.5.0 ssd 1541756755.728321552 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1541756755.728844404 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1541756755.731762409 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 22.961, Average Loss: 0.023, avg. samples / sec: 45689.93
Iteration:      0, Loss function: 23.220, Average Loss: 0.023, avg. samples / sec: 33532.95
Iteration:      0, Loss function: 22.850, Average Loss: 0.023, avg. samples / sec: 35384.91
Iteration:      0, Loss function: 22.110, Average Loss: 0.022, avg. samples / sec: 42370.47
Iteration:      0, Loss function: 22.902, Average Loss: 0.023, avg. samples / sec: 19381.62
Iteration:      0, Loss function: 22.944, Average Loss: 0.023, avg. samples / sec: 51477.11
Iteration:      0, Loss function: 23.081, Average Loss: 0.023, avg. samples / sec: 19190.67
Iteration:      0, Loss function: 22.212, Average Loss: 0.022, avg. samples / sec: 33221.44

:::MLPv0.5.0 ssd 1541756758.235565424 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.0001777777777777767}

:::MLPv0.5.0 ssd 1541756758.607941151 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0003555555555555534}

:::MLPv0.5.0 ssd 1541756758.707078457 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0005333333333333301}

:::MLPv0.5.0 ssd 1541756758.806184530 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0007111111111111068}

:::MLPv0.5.0 ssd 1541756758.900788069 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0008888888888888835}

:::MLPv0.5.0 ssd 1541756758.996617079 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0010666666666666602}

:::MLPv0.5.0 ssd 1541756759.090178013 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.001244444444444437}

:::MLPv0.5.0 ssd 1541756759.184030533 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0014222222222222136}

:::MLPv0.5.0 ssd 1541756759.277260780 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0015999999999999903}

:::MLPv0.5.0 ssd 1541756759.372302294 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.001777777777777767}

:::MLPv0.5.0 ssd 1541756759.462798119 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0019555555555555437}

:::MLPv0.5.0 ssd 1541756759.564496994 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.0021333333333333204}

:::MLPv0.5.0 ssd 1541756759.660987139 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.002311111111111097}

:::MLPv0.5.0 ssd 1541756759.755164862 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.002488888888888874}

:::MLPv0.5.0 ssd 1541756759.846582890 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.0026666666666666505}

:::MLPv0.5.0 ssd 1541756759.938208342 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.0028444444444444272}

:::MLPv0.5.0 ssd 1541756760.027066946 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.0030222222222222317}

:::MLPv0.5.0 ssd 1541756760.116065979 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.0032000000000000084}

:::MLPv0.5.0 ssd 1541756760.205286741 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.003377777777777785}

:::MLPv0.5.0 ssd 1541756760.301628351 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.003555555555555562}
Iteration:     20, Loss function: 20.796, Average Loss: 0.441, avg. samples / sec: 8977.29
Iteration:     20, Loss function: 20.635, Average Loss: 0.443, avg. samples / sec: 8977.17
Iteration:     20, Loss function: 20.665, Average Loss: 0.443, avg. samples / sec: 8976.68
Iteration:     20, Loss function: 19.374, Average Loss: 0.440, avg. samples / sec: 8974.56
Iteration:     20, Loss function: 21.137, Average Loss: 0.445, avg. samples / sec: 8973.35
Iteration:     20, Loss function: 20.712, Average Loss: 0.442, avg. samples / sec: 8972.72
Iteration:     20, Loss function: 19.994, Average Loss: 0.444, avg. samples / sec: 8971.55
Iteration:     20, Loss function: 20.731, Average Loss: 0.443, avg. samples / sec: 8969.80

:::MLPv0.5.0 ssd 1541756760.391179562 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.0037333333333333385}

:::MLPv0.5.0 ssd 1541756760.482450962 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.003911111111111115}

:::MLPv0.5.0 ssd 1541756760.574812174 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.004088888888888892}

:::MLPv0.5.0 ssd 1541756760.667351246 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.004266666666666669}

:::MLPv0.5.0 ssd 1541756760.756643295 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.004444444444444445}

:::MLPv0.5.0 ssd 1541756760.844534159 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.004622222222222222}

:::MLPv0.5.0 ssd 1541756760.932865858 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.004799999999999999}

:::MLPv0.5.0 ssd 1541756761.023803234 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.004977777777777775}

:::MLPv0.5.0 ssd 1541756761.108938217 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.005155555555555552}

:::MLPv0.5.0 ssd 1541756761.196926832 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.005333333333333329}

:::MLPv0.5.0 ssd 1541756761.285852909 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.0055111111111111055}

:::MLPv0.5.0 ssd 1541756761.374721050 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.005688888888888882}

:::MLPv0.5.0 ssd 1541756761.464402199 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.005866666666666659}

:::MLPv0.5.0 ssd 1541756761.553519249 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.006044444444444436}

:::MLPv0.5.0 ssd 1541756761.641397238 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.006222222222222212}

:::MLPv0.5.0 ssd 1541756761.728912592 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.006399999999999989}

:::MLPv0.5.0 ssd 1541756761.815882921 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.006577777777777766}

:::MLPv0.5.0 ssd 1541756761.902734756 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.0067555555555555424}

:::MLPv0.5.0 ssd 1541756761.987499952 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.006933333333333319}

:::MLPv0.5.0 ssd 1541756762.073509693 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.007111111111111096}
Iteration:     40, Loss function: 16.148, Average Loss: 0.814, avg. samples / sec: 23127.02
Iteration:     40, Loss function: 16.679, Average Loss: 0.821, avg. samples / sec: 23154.41
Iteration:     40, Loss function: 16.789, Average Loss: 0.816, avg. samples / sec: 23162.72
Iteration:     40, Loss function: 16.272, Average Loss: 0.815, avg. samples / sec: 23138.34
Iteration:     40, Loss function: 16.803, Average Loss: 0.817, avg. samples / sec: 23171.40
Iteration:     40, Loss function: 16.180, Average Loss: 0.817, avg. samples / sec: 23120.71
Iteration:     40, Loss function: 15.726, Average Loss: 0.811, avg. samples / sec: 23122.92
Iteration:     40, Loss function: 16.330, Average Loss: 0.815, avg. samples / sec: 23107.70

:::MLPv0.5.0 ssd 1541756762.160712481 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.0072888888888888725}

:::MLPv0.5.0 ssd 1541756762.250627279 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.007466666666666649}

:::MLPv0.5.0 ssd 1541756762.335367680 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.007644444444444454}

:::MLPv0.5.0 ssd 1541756762.423324108 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.00782222222222223}

:::MLPv0.5.0 ssd 1541756762.508297205 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.008000000000000007}

:::MLPv0.5.0 ssd 1541756762.597280025 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.008177777777777784}

:::MLPv0.5.0 ssd 1541756762.685454607 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.00835555555555556}

:::MLPv0.5.0 ssd 1541756762.772810936 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.008533333333333337}

:::MLPv0.5.0 ssd 1541756762.863264799 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.008711111111111114}

:::MLPv0.5.0 ssd 1541756762.948846102 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.00888888888888889}

:::MLPv0.5.0 ssd 1541756763.034319878 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.009066666666666667}

:::MLPv0.5.0 ssd 1541756763.121854544 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.009244444444444444}

:::MLPv0.5.0 ssd 1541756763.209091663 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.00942222222222222}

:::MLPv0.5.0 ssd 1541756763.297839403 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.009599999999999997}

:::MLPv0.5.0 ssd 1541756763.385162354 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.009777777777777774}

:::MLPv0.5.0 ssd 1541756763.470689297 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.00995555555555555}

:::MLPv0.5.0 ssd 1541756763.557612181 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.010133333333333328}

:::MLPv0.5.0 ssd 1541756763.642615795 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1541756763.646867514 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 58, "value": 0.010311111111111104}

:::MLPv0.5.0 ssd 1541756763.735236883 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 59, "value": 0.010488888888888881}

:::MLPv0.5.0 ssd 1541756763.822759628 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 60, "value": 0.010666666666666658}
Iteration:     60, Loss function: 10.316, Average Loss: 1.055, avg. samples / sec: 23423.23
Iteration:     60, Loss function: 10.626, Average Loss: 1.060, avg. samples / sec: 23402.44
Iteration:     60, Loss function: 11.437, Average Loss: 1.049, avg. samples / sec: 23424.30
Iteration:     60, Loss function: 10.741, Average Loss: 1.054, avg. samples / sec: 23442.62
Iteration:     60, Loss function: 10.732, Average Loss: 1.057, avg. samples / sec: 23389.77
Iteration:     60, Loss function: 10.674, Average Loss: 1.055, avg. samples / sec: 23390.60
Iteration:     60, Loss function: 10.103, Average Loss: 1.053, avg. samples / sec: 23368.39
Iteration:     60, Loss function: 10.900, Average Loss: 1.056, avg. samples / sec: 23376.19

:::MLPv0.5.0 ssd 1541756763.907472134 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 61, "value": 0.010844444444444434}

:::MLPv0.5.0 ssd 1541756763.994606733 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 62, "value": 0.011022222222222211}

:::MLPv0.5.0 ssd 1541756764.084806681 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 63, "value": 0.011199999999999988}

:::MLPv0.5.0 ssd 1541756764.170838594 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 64, "value": 0.011377777777777764}

:::MLPv0.5.0 ssd 1541756764.255849123 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 65, "value": 0.011555555555555541}

:::MLPv0.5.0 ssd 1541756764.358006477 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 66, "value": 0.011733333333333318}

:::MLPv0.5.0 ssd 1541756764.448812246 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 67, "value": 0.011911111111111095}

:::MLPv0.5.0 ssd 1541756764.537743330 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 68, "value": 0.012088888888888899}

:::MLPv0.5.0 ssd 1541756764.623636723 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 69, "value": 0.012266666666666676}

:::MLPv0.5.0 ssd 1541756764.709007263 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 70, "value": 0.012444444444444452}

:::MLPv0.5.0 ssd 1541756764.797101736 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 71, "value": 0.012622222222222229}

:::MLPv0.5.0 ssd 1541756764.883190632 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 72, "value": 0.012800000000000006}

:::MLPv0.5.0 ssd 1541756764.969730377 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 73, "value": 0.012977777777777783}

:::MLPv0.5.0 ssd 1541756765.055872917 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 74, "value": 0.01315555555555556}

:::MLPv0.5.0 ssd 1541756765.142656088 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 75, "value": 0.013333333333333336}

:::MLPv0.5.0 ssd 1541756765.227854967 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 76, "value": 0.013511111111111113}

:::MLPv0.5.0 ssd 1541756765.314674854 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 77, "value": 0.01368888888888889}

:::MLPv0.5.0 ssd 1541756765.400552511 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 78, "value": 0.013866666666666666}

:::MLPv0.5.0 ssd 1541756765.486062527 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 79, "value": 0.014044444444444443}

:::MLPv0.5.0 ssd 1541756765.573579550 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 80, "value": 0.01422222222222222}
Iteration:     80, Loss function: 10.372, Average Loss: 1.248, avg. samples / sec: 23414.34
Iteration:     80, Loss function: 9.748, Average Loss: 1.252, avg. samples / sec: 23442.04
Iteration:     80, Loss function: 9.386, Average Loss: 1.256, avg. samples / sec: 23407.07
Iteration:     80, Loss function: 10.363, Average Loss: 1.255, avg. samples / sec: 23413.83
Iteration:     80, Loss function: 9.927, Average Loss: 1.254, avg. samples / sec: 23382.47
Iteration:     80, Loss function: 9.847, Average Loss: 1.256, avg. samples / sec: 23419.57
Iteration:     80, Loss function: 10.338, Average Loss: 1.258, avg. samples / sec: 23392.13
Iteration:     80, Loss function: 10.123, Average Loss: 1.260, avg. samples / sec: 23386.04

:::MLPv0.5.0 ssd 1541756765.658720970 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 81, "value": 0.014399999999999996}

:::MLPv0.5.0 ssd 1541756765.744849920 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 82, "value": 0.014577777777777773}

:::MLPv0.5.0 ssd 1541756765.829784155 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 83, "value": 0.01475555555555555}

:::MLPv0.5.0 ssd 1541756765.915080547 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 84, "value": 0.014933333333333326}

:::MLPv0.5.0 ssd 1541756766.002320290 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 85, "value": 0.015111111111111103}

:::MLPv0.5.0 ssd 1541756766.087019205 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 86, "value": 0.01528888888888888}

:::MLPv0.5.0 ssd 1541756766.171611309 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 87, "value": 0.015466666666666656}

:::MLPv0.5.0 ssd 1541756766.258530855 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 88, "value": 0.015644444444444433}

:::MLPv0.5.0 ssd 1541756766.343320608 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 89, "value": 0.01582222222222221}

:::MLPv0.5.0 ssd 1541756766.428085804 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 90, "value": 0.015999999999999986}

:::MLPv0.5.0 ssd 1541756766.516010284 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 91, "value": 0.016177777777777763}

:::MLPv0.5.0 ssd 1541756766.602966547 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 92, "value": 0.01635555555555554}

:::MLPv0.5.0 ssd 1541756766.688535690 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 93, "value": 0.016533333333333317}

:::MLPv0.5.0 ssd 1541756766.775151968 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 94, "value": 0.01671111111111112}

:::MLPv0.5.0 ssd 1541756766.860054493 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 95, "value": 0.016888888888888898}

:::MLPv0.5.0 ssd 1541756766.944416046 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 96, "value": 0.017066666666666674}

:::MLPv0.5.0 ssd 1541756767.029559612 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 97, "value": 0.01724444444444445}

:::MLPv0.5.0 ssd 1541756767.113979578 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 98, "value": 0.017422222222222228}

:::MLPv0.5.0 ssd 1541756767.198102713 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 99, "value": 0.017600000000000005}

:::MLPv0.5.0 ssd 1541756767.285375834 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 100, "value": 0.01777777777777778}
Iteration:    100, Loss function: 9.168, Average Loss: 1.419, avg. samples / sec: 23937.11
Iteration:    100, Loss function: 9.139, Average Loss: 1.415, avg. samples / sec: 23928.99
Iteration:    100, Loss function: 9.346, Average Loss: 1.418, avg. samples / sec: 23952.43
Iteration:    100, Loss function: 9.547, Average Loss: 1.411, avg. samples / sec: 23910.63
Iteration:    100, Loss function: 8.717, Average Loss: 1.419, avg. samples / sec: 23919.20
Iteration:    100, Loss function: 8.940, Average Loss: 1.415, avg. samples / sec: 23909.19
Iteration:    100, Loss function: 8.994, Average Loss: 1.423, avg. samples / sec: 23964.38
Iteration:    100, Loss function: 8.647, Average Loss: 1.417, avg. samples / sec: 23886.60

:::MLPv0.5.0 ssd 1541756767.373300791 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 101, "value": 0.017955555555555558}

:::MLPv0.5.0 ssd 1541756767.458939552 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 102, "value": 0.018133333333333335}

:::MLPv0.5.0 ssd 1541756767.545077801 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 103, "value": 0.01831111111111111}

:::MLPv0.5.0 ssd 1541756767.629760265 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 104, "value": 0.018488888888888888}

:::MLPv0.5.0 ssd 1541756767.715445280 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 105, "value": 0.018666666666666665}

:::MLPv0.5.0 ssd 1541756767.803248882 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 106, "value": 0.01884444444444444}

:::MLPv0.5.0 ssd 1541756767.890516281 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 107, "value": 0.019022222222222218}

:::MLPv0.5.0 ssd 1541756767.976032972 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 108, "value": 0.019199999999999995}

:::MLPv0.5.0 ssd 1541756768.061129808 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 109, "value": 0.01937777777777777}

:::MLPv0.5.0 ssd 1541756768.145892382 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 110, "value": 0.019555555555555548}

:::MLPv0.5.0 ssd 1541756768.231995583 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 111, "value": 0.019733333333333325}

:::MLPv0.5.0 ssd 1541756768.317595243 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 112, "value": 0.0199111111111111}

:::MLPv0.5.0 ssd 1541756768.406858921 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 113, "value": 0.02008888888888888}

:::MLPv0.5.0 ssd 1541756768.494667530 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 114, "value": 0.020266666666666655}

:::MLPv0.5.0 ssd 1541756768.578960896 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 115, "value": 0.020444444444444432}

:::MLPv0.5.0 ssd 1541756768.662267923 (train.py:553) train_epoch: 2

:::MLPv0.5.0 ssd 1541756768.666526556 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 116, "value": 0.02062222222222221}

:::MLPv0.5.0 ssd 1541756768.751065493 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 117, "value": 0.020799999999999985}

:::MLPv0.5.0 ssd 1541756768.837368250 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 118, "value": 0.020977777777777762}

:::MLPv0.5.0 ssd 1541756768.922021866 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 119, "value": 0.02115555555555554}

:::MLPv0.5.0 ssd 1541756769.008036375 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 120, "value": 0.021333333333333343}
Iteration:    120, Loss function: 9.195, Average Loss: 1.571, avg. samples / sec: 23776.64
Iteration:    120, Loss function: 9.392, Average Loss: 1.567, avg. samples / sec: 23776.86
Iteration:    120, Loss function: 9.204, Average Loss: 1.570, avg. samples / sec: 23815.43
Iteration:    120, Loss function: 9.244, Average Loss: 1.575, avg. samples / sec: 23785.97
Iteration:    120, Loss function: 9.114, Average Loss: 1.571, avg. samples / sec: 23764.84
Iteration:    120, Loss function: 9.218, Average Loss: 1.568, avg. samples / sec: 23752.66
Iteration:    120, Loss function: 8.979, Average Loss: 1.568, avg. samples / sec: 23753.68
Iteration:    120, Loss function: 9.131, Average Loss: 1.561, avg. samples / sec: 23737.31

:::MLPv0.5.0 ssd 1541756769.096251488 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 121, "value": 0.02151111111111112}

:::MLPv0.5.0 ssd 1541756769.181209803 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 122, "value": 0.021688888888888896}

:::MLPv0.5.0 ssd 1541756769.264725447 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 123, "value": 0.021866666666666673}

:::MLPv0.5.0 ssd 1541756769.350156307 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 124, "value": 0.02204444444444445}

:::MLPv0.5.0 ssd 1541756769.435717344 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 125, "value": 0.022222222222222227}

:::MLPv0.5.0 ssd 1541756769.520974398 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 126, "value": 0.022400000000000003}

:::MLPv0.5.0 ssd 1541756769.604886532 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 127, "value": 0.02257777777777778}

:::MLPv0.5.0 ssd 1541756769.689615250 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 128, "value": 0.022755555555555557}

:::MLPv0.5.0 ssd 1541756769.775287628 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 129, "value": 0.022933333333333333}

:::MLPv0.5.0 ssd 1541756769.861820221 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 130, "value": 0.02311111111111111}

:::MLPv0.5.0 ssd 1541756769.948658943 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 131, "value": 0.023288888888888887}

:::MLPv0.5.0 ssd 1541756770.035157204 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 132, "value": 0.023466666666666663}

:::MLPv0.5.0 ssd 1541756770.119452477 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 133, "value": 0.02364444444444444}

:::MLPv0.5.0 ssd 1541756770.203851223 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 134, "value": 0.023822222222222217}

:::MLPv0.5.0 ssd 1541756770.289149284 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 135, "value": 0.023999999999999994}

:::MLPv0.5.0 ssd 1541756770.375021458 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 136, "value": 0.02417777777777777}

:::MLPv0.5.0 ssd 1541756770.461686850 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 137, "value": 0.024355555555555547}

:::MLPv0.5.0 ssd 1541756770.547632694 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 138, "value": 0.024533333333333324}

:::MLPv0.5.0 ssd 1541756770.632550955 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 139, "value": 0.0247111111111111}

:::MLPv0.5.0 ssd 1541756770.716979980 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 140, "value": 0.024888888888888877}
Iteration:    140, Loss function: 8.581, Average Loss: 1.714, avg. samples / sec: 23970.69
Iteration:    140, Loss function: 8.845, Average Loss: 1.717, avg. samples / sec: 23993.76
Iteration:    140, Loss function: 8.477, Average Loss: 1.706, avg. samples / sec: 24009.51
Iteration:    140, Loss function: 8.615, Average Loss: 1.718, avg. samples / sec: 23967.58
Iteration:    140, Loss function: 9.209, Average Loss: 1.716, avg. samples / sec: 23943.19
Iteration:    140, Loss function: 8.543, Average Loss: 1.709, avg. samples / sec: 23984.54
Iteration:    140, Loss function: 8.584, Average Loss: 1.710, avg. samples / sec: 23930.07
Iteration:    140, Loss function: 8.748, Average Loss: 1.711, avg. samples / sec: 23951.51

:::MLPv0.5.0 ssd 1541756770.801136971 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 141, "value": 0.025066666666666654}

:::MLPv0.5.0 ssd 1541756770.886516809 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 142, "value": 0.02524444444444443}

:::MLPv0.5.0 ssd 1541756770.973352432 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 143, "value": 0.025422222222222207}

:::MLPv0.5.0 ssd 1541756771.059616089 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 144, "value": 0.025599999999999984}

:::MLPv0.5.0 ssd 1541756771.143565893 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 145, "value": 0.02577777777777779}

:::MLPv0.5.0 ssd 1541756771.227047205 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 146, "value": 0.025955555555555565}

:::MLPv0.5.0 ssd 1541756771.311160564 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 147, "value": 0.026133333333333342}

:::MLPv0.5.0 ssd 1541756771.395791054 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 148, "value": 0.02631111111111112}

:::MLPv0.5.0 ssd 1541756771.482249737 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 149, "value": 0.026488888888888895}

:::MLPv0.5.0 ssd 1541756771.567309380 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 150, "value": 0.026666666666666672}

:::MLPv0.5.0 ssd 1541756771.652007103 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 151, "value": 0.02684444444444445}

:::MLPv0.5.0 ssd 1541756771.736716032 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 152, "value": 0.027022222222222225}

:::MLPv0.5.0 ssd 1541756771.822222233 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 153, "value": 0.027200000000000002}

:::MLPv0.5.0 ssd 1541756771.907607079 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 154, "value": 0.02737777777777778}

:::MLPv0.5.0 ssd 1541756771.994060516 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 155, "value": 0.027555555555555555}

:::MLPv0.5.0 ssd 1541756772.078440905 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 156, "value": 0.027733333333333332}

:::MLPv0.5.0 ssd 1541756772.163955927 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 157, "value": 0.02791111111111111}

:::MLPv0.5.0 ssd 1541756772.249354124 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 158, "value": 0.028088888888888885}

:::MLPv0.5.0 ssd 1541756772.333590031 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 159, "value": 0.028266666666666662}

:::MLPv0.5.0 ssd 1541756772.420343399 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 160, "value": 0.02844444444444444}
Iteration:    160, Loss function: 8.302, Average Loss: 1.847, avg. samples / sec: 24034.12
Iteration:    160, Loss function: 8.263, Average Loss: 1.842, avg. samples / sec: 24049.14
Iteration:    160, Loss function: 8.184, Average Loss: 1.844, avg. samples / sec: 24054.96
Iteration:    160, Loss function: 7.742, Average Loss: 1.837, avg. samples / sec: 24031.67
Iteration:    160, Loss function: 8.354, Average Loss: 1.850, avg. samples / sec: 24035.87
Iteration:    160, Loss function: 8.319, Average Loss: 1.850, avg. samples / sec: 24009.46
Iteration:    160, Loss function: 7.963, Average Loss: 1.851, avg. samples / sec: 24006.85
Iteration:    160, Loss function: 8.416, Average Loss: 1.844, avg. samples / sec: 24043.67

:::MLPv0.5.0 ssd 1541756772.504977703 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 161, "value": 0.028622222222222216}

:::MLPv0.5.0 ssd 1541756772.589513540 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 162, "value": 0.028799999999999992}

:::MLPv0.5.0 ssd 1541756772.675274134 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 163, "value": 0.02897777777777777}

:::MLPv0.5.0 ssd 1541756772.760298729 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 164, "value": 0.029155555555555546}

:::MLPv0.5.0 ssd 1541756772.845237017 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 165, "value": 0.029333333333333322}

:::MLPv0.5.0 ssd 1541756772.929151297 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 166, "value": 0.0295111111111111}

:::MLPv0.5.0 ssd 1541756773.013362169 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 167, "value": 0.029688888888888876}

:::MLPv0.5.0 ssd 1541756773.099184036 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 168, "value": 0.029866666666666652}

:::MLPv0.5.0 ssd 1541756773.185547829 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 169, "value": 0.03004444444444443}

:::MLPv0.5.0 ssd 1541756773.271390438 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 170, "value": 0.030222222222222206}

:::MLPv0.5.0 ssd 1541756773.355019331 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 171, "value": 0.03040000000000001}

:::MLPv0.5.0 ssd 1541756773.445107698 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 172, "value": 0.030577777777777787}

:::MLPv0.5.0 ssd 1541756773.529813766 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 173, "value": 0.030755555555555564}

:::MLPv0.5.0 ssd 1541756773.612280369 (train.py:553) train_epoch: 3

:::MLPv0.5.0 ssd 1541756773.616559982 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 174, "value": 0.03093333333333334}

:::MLPv0.5.0 ssd 1541756773.702504635 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 175, "value": 0.031111111111111117}

:::MLPv0.5.0 ssd 1541756773.789623260 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 176, "value": 0.031288888888888894}

:::MLPv0.5.0 ssd 1541756773.873864651 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 177, "value": 0.03146666666666667}

:::MLPv0.5.0 ssd 1541756773.958375692 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 178, "value": 0.03164444444444445}

:::MLPv0.5.0 ssd 1541756774.042134047 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 179, "value": 0.031822222222222224}

:::MLPv0.5.0 ssd 1541756774.127304554 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 180, "value": 0.032}
Iteration:    180, Loss function: 8.250, Average Loss: 1.982, avg. samples / sec: 24044.62
Iteration:    180, Loss function: 7.949, Average Loss: 1.979, avg. samples / sec: 24004.03
Iteration:    180, Loss function: 8.372, Average Loss: 1.980, avg. samples / sec: 24038.05
Iteration:    180, Loss function: 8.241, Average Loss: 1.975, avg. samples / sec: 24048.57
Iteration:    180, Loss function: 7.909, Average Loss: 1.973, avg. samples / sec: 24011.28
Iteration:    180, Loss function: 8.434, Average Loss: 1.969, avg. samples / sec: 23993.68
Iteration:    180, Loss function: 8.511, Average Loss: 1.983, avg. samples / sec: 24013.17
Iteration:    180, Loss function: 8.548, Average Loss: 1.975, avg. samples / sec: 23977.20

:::MLPv0.5.0 ssd 1541756774.212605953 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 181, "value": 0.03217777777777778}

:::MLPv0.5.0 ssd 1541756774.296729803 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 182, "value": 0.032355555555555554}

:::MLPv0.5.0 ssd 1541756774.380617380 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 183, "value": 0.03253333333333333}

:::MLPv0.5.0 ssd 1541756774.466249943 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 184, "value": 0.03271111111111111}

:::MLPv0.5.0 ssd 1541756774.555010080 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 185, "value": 0.032888888888888884}

:::MLPv0.5.0 ssd 1541756774.638864279 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 186, "value": 0.03306666666666666}

:::MLPv0.5.0 ssd 1541756774.724931240 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 187, "value": 0.03324444444444444}

:::MLPv0.5.0 ssd 1541756774.808837414 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 188, "value": 0.033422222222222214}

:::MLPv0.5.0 ssd 1541756774.894316912 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 189, "value": 0.03359999999999999}

:::MLPv0.5.0 ssd 1541756774.978189468 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 190, "value": 0.03377777777777777}

:::MLPv0.5.0 ssd 1541756775.069807529 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 191, "value": 0.033955555555555544}

:::MLPv0.5.0 ssd 1541756775.154291630 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 192, "value": 0.03413333333333332}

:::MLPv0.5.0 ssd 1541756775.238665581 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 193, "value": 0.0343111111111111}

:::MLPv0.5.0 ssd 1541756775.324224472 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 194, "value": 0.034488888888888874}

:::MLPv0.5.0 ssd 1541756775.408363581 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 195, "value": 0.03466666666666665}

:::MLPv0.5.0 ssd 1541756775.492281199 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 196, "value": 0.03484444444444443}

:::MLPv0.5.0 ssd 1541756775.577226162 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 197, "value": 0.03502222222222222}

:::MLPv0.5.0 ssd 1541756775.661943913 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 198, "value": 0.035199999999999995}

:::MLPv0.5.0 ssd 1541756775.745932341 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 199, "value": 0.03537777777777777}

:::MLPv0.5.0 ssd 1541756775.837880135 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 200, "value": 0.03555555555555555}
Iteration:    200, Loss function: 7.759, Average Loss: 2.101, avg. samples / sec: 23945.06
Iteration:    200, Loss function: 8.266, Average Loss: 2.096, avg. samples / sec: 23956.66
Iteration:    200, Loss function: 7.974, Average Loss: 2.103, avg. samples / sec: 23931.29
Iteration:    200, Loss function: 8.545, Average Loss: 2.095, avg. samples / sec: 23939.22
Iteration:    200, Loss function: 8.119, Average Loss: 2.092, avg. samples / sec: 23963.80
Iteration:    200, Loss function: 7.719, Average Loss: 2.101, avg. samples / sec: 23919.81
Iteration:    200, Loss function: 8.191, Average Loss: 2.098, avg. samples / sec: 23953.56
Iteration:    200, Loss function: 7.867, Average Loss: 2.103, avg. samples / sec: 23939.57

:::MLPv0.5.0 ssd 1541756775.921946049 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 201, "value": 0.035733333333333325}

:::MLPv0.5.0 ssd 1541756776.006940365 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 202, "value": 0.0359111111111111}

:::MLPv0.5.0 ssd 1541756776.091096163 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 203, "value": 0.03608888888888889}

:::MLPv0.5.0 ssd 1541756776.176015854 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 204, "value": 0.03626666666666667}

:::MLPv0.5.0 ssd 1541756776.259819031 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 205, "value": 0.036444444444444446}

:::MLPv0.5.0 ssd 1541756776.344309807 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 206, "value": 0.03662222222222222}

:::MLPv0.5.0 ssd 1541756776.429601192 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 207, "value": 0.0368}

:::MLPv0.5.0 ssd 1541756776.513293028 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 208, "value": 0.036977777777777776}

:::MLPv0.5.0 ssd 1541756776.599241257 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 209, "value": 0.03715555555555555}

:::MLPv0.5.0 ssd 1541756776.683116913 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 210, "value": 0.03733333333333333}

:::MLPv0.5.0 ssd 1541756776.767811060 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 211, "value": 0.037511111111111106}

:::MLPv0.5.0 ssd 1541756776.852560043 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 212, "value": 0.03768888888888888}

:::MLPv0.5.0 ssd 1541756776.936610460 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 213, "value": 0.03786666666666666}

:::MLPv0.5.0 ssd 1541756777.020469904 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 214, "value": 0.038044444444444436}

:::MLPv0.5.0 ssd 1541756777.105756998 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 215, "value": 0.03822222222222221}

:::MLPv0.5.0 ssd 1541756777.195048332 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 216, "value": 0.038400000000000004}

:::MLPv0.5.0 ssd 1541756777.279880047 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 217, "value": 0.03857777777777778}

:::MLPv0.5.0 ssd 1541756777.366917610 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 218, "value": 0.03875555555555556}

:::MLPv0.5.0 ssd 1541756777.451775551 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 219, "value": 0.038933333333333334}

:::MLPv0.5.0 ssd 1541756777.537505627 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 220, "value": 0.03911111111111111}
Iteration:    220, Loss function: 7.839, Average Loss: 2.221, avg. samples / sec: 24146.14
Iteration:    220, Loss function: 8.390, Average Loss: 2.222, avg. samples / sec: 24125.66
Iteration:    220, Loss function: 7.806, Average Loss: 2.218, avg. samples / sec: 24102.55
Iteration:    220, Loss function: 7.784, Average Loss: 2.222, avg. samples / sec: 24099.36
Iteration:    220, Loss function: 8.288, Average Loss: 2.218, avg. samples / sec: 24104.86
Iteration:    220, Loss function: 8.033, Average Loss: 2.216, avg. samples / sec: 24082.31
Iteration:    220, Loss function: 8.007, Average Loss: 2.226, avg. samples / sec: 24111.05
Iteration:    220, Loss function: 8.711, Average Loss: 2.226, avg. samples / sec: 24056.31

:::MLPv0.5.0 ssd 1541756777.621422529 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 221, "value": 0.03928888888888889}

:::MLPv0.5.0 ssd 1541756777.705790043 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 222, "value": 0.039466666666666664}

:::MLPv0.5.0 ssd 1541756777.791972399 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 223, "value": 0.03964444444444444}

:::MLPv0.5.0 ssd 1541756777.875614643 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 224, "value": 0.03982222222222222}

:::MLPv0.5.0 ssd 1541756777.959254503 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 225, "value": 0.039999999999999994}

:::MLPv0.5.0 ssd 1541756778.042866468 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 226, "value": 0.04017777777777777}

:::MLPv0.5.0 ssd 1541756778.127250671 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 227, "value": 0.04035555555555555}

:::MLPv0.5.0 ssd 1541756778.211475611 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 228, "value": 0.04053333333333334}

:::MLPv0.5.0 ssd 1541756778.295349360 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 229, "value": 0.040711111111111115}

:::MLPv0.5.0 ssd 1541756778.380187273 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 230, "value": 0.04088888888888889}

:::MLPv0.5.0 ssd 1541756778.464947462 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 231, "value": 0.04106666666666667}

:::MLPv0.5.0 ssd 1541756778.546572685 (train.py:553) train_epoch: 4

:::MLPv0.5.0 ssd 1541756778.550698280 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 232, "value": 0.041244444444444445}

:::MLPv0.5.0 ssd 1541756778.638492107 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 233, "value": 0.04142222222222222}

:::MLPv0.5.0 ssd 1541756778.722928524 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 234, "value": 0.0416}

:::MLPv0.5.0 ssd 1541756778.806739569 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 235, "value": 0.041777777777777775}

:::MLPv0.5.0 ssd 1541756778.891606092 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 236, "value": 0.04195555555555555}

:::MLPv0.5.0 ssd 1541756778.976419926 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 237, "value": 0.04213333333333333}

:::MLPv0.5.0 ssd 1541756779.060780287 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 238, "value": 0.042311111111111105}

:::MLPv0.5.0 ssd 1541756779.145187855 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 239, "value": 0.04248888888888888}

:::MLPv0.5.0 ssd 1541756779.229357004 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 240, "value": 0.04266666666666666}
Iteration:    240, Loss function: 7.969, Average Loss: 2.334, avg. samples / sec: 24216.98
Iteration:    240, Loss function: 7.353, Average Loss: 2.327, avg. samples / sec: 24205.15
Iteration:    240, Loss function: 7.534, Average Loss: 2.333, avg. samples / sec: 24248.18
Iteration:    240, Loss function: 7.455, Average Loss: 2.331, avg. samples / sec: 24198.14
Iteration:    240, Loss function: 7.081, Average Loss: 2.325, avg. samples / sec: 24212.61
Iteration:    240, Loss function: 7.346, Average Loss: 2.330, avg. samples / sec: 24171.57
Iteration:    240, Loss function: 7.778, Average Loss: 2.337, avg. samples / sec: 24211.56
Iteration:    240, Loss function: 7.376, Average Loss: 2.328, avg. samples / sec: 24159.56

:::MLPv0.5.0 ssd 1541756779.313546896 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 241, "value": 0.04284444444444445}

:::MLPv0.5.0 ssd 1541756779.397887707 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 242, "value": 0.043022222222222226}

:::MLPv0.5.0 ssd 1541756779.482493877 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 243, "value": 0.0432}

:::MLPv0.5.0 ssd 1541756779.570677519 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 244, "value": 0.04337777777777778}

:::MLPv0.5.0 ssd 1541756779.654194832 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 245, "value": 0.043555555555555556}

:::MLPv0.5.0 ssd 1541756779.738554716 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 246, "value": 0.04373333333333333}

:::MLPv0.5.0 ssd 1541756779.822922945 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 247, "value": 0.04391111111111111}

:::MLPv0.5.0 ssd 1541756779.908363581 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 248, "value": 0.044088888888888886}

:::MLPv0.5.0 ssd 1541756779.992234468 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 249, "value": 0.04426666666666666}

:::MLPv0.5.0 ssd 1541756780.076467752 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 250, "value": 0.04444444444444444}

:::MLPv0.5.0 ssd 1541756780.160629272 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 251, "value": 0.044622222222222216}

:::MLPv0.5.0 ssd 1541756780.245126486 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 252, "value": 0.04479999999999999}

:::MLPv0.5.0 ssd 1541756780.329100847 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 253, "value": 0.04497777777777777}

:::MLPv0.5.0 ssd 1541756780.413273335 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 254, "value": 0.04515555555555556}

:::MLPv0.5.0 ssd 1541756780.497146368 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 255, "value": 0.04533333333333334}

:::MLPv0.5.0 ssd 1541756780.580940008 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 256, "value": 0.04551111111111111}

:::MLPv0.5.0 ssd 1541756780.665097713 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 257, "value": 0.04568888888888889}

:::MLPv0.5.0 ssd 1541756780.750071287 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 258, "value": 0.04586666666666667}

:::MLPv0.5.0 ssd 1541756780.834173679 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 259, "value": 0.04604444444444444}

:::MLPv0.5.0 ssd 1541756780.918162107 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 260, "value": 0.04622222222222222}
Iteration:    260, Loss function: 8.416, Average Loss: 2.432, avg. samples / sec: 24256.31
Iteration:    260, Loss function: 8.135, Average Loss: 2.435, avg. samples / sec: 24244.22
Iteration:    260, Loss function: 8.065, Average Loss: 2.437, avg. samples / sec: 24258.77
Iteration:    260, Loss function: 8.395, Average Loss: 2.435, avg. samples / sec: 24279.35
Iteration:    260, Loss function: 7.904, Average Loss: 2.440, avg. samples / sec: 24256.78
Iteration:    260, Loss function: 8.274, Average Loss: 2.431, avg. samples / sec: 24279.17
Iteration:    260, Loss function: 7.855, Average Loss: 2.430, avg. samples / sec: 24249.40
Iteration:    260, Loss function: 8.070, Average Loss: 2.438, avg. samples / sec: 24210.37

:::MLPv0.5.0 ssd 1541756781.001958370 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 261, "value": 0.0464}

:::MLPv0.5.0 ssd 1541756781.086133242 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 262, "value": 0.046577777777777774}

:::MLPv0.5.0 ssd 1541756781.169980764 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 263, "value": 0.04675555555555555}

:::MLPv0.5.0 ssd 1541756781.253741264 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 264, "value": 0.04693333333333333}

:::MLPv0.5.0 ssd 1541756781.338076591 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 265, "value": 0.047111111111111104}

:::MLPv0.5.0 ssd 1541756781.423096895 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 266, "value": 0.04728888888888888}

:::MLPv0.5.0 ssd 1541756781.511894226 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 267, "value": 0.04746666666666667}

:::MLPv0.5.0 ssd 1541756781.595728874 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 268, "value": 0.04764444444444445}

:::MLPv0.5.0 ssd 1541756781.679336786 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 269, "value": 0.047822222222222224}

:::MLPv0.5.0 ssd 1541756781.763364315 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 270, "value": 0.048}

:::MLPv0.5.0 ssd 1541756781.847859621 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 271, "value": 0.04817777777777778}

:::MLPv0.5.0 ssd 1541756781.932583094 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 272, "value": 0.048355555555555554}

:::MLPv0.5.0 ssd 1541756782.017150640 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 273, "value": 0.04853333333333333}

:::MLPv0.5.0 ssd 1541756782.101122379 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 274, "value": 0.04871111111111111}

:::MLPv0.5.0 ssd 1541756782.185271263 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 275, "value": 0.048888888888888885}

:::MLPv0.5.0 ssd 1541756782.270443201 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 276, "value": 0.04906666666666666}

:::MLPv0.5.0 ssd 1541756782.355163097 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 277, "value": 0.04924444444444444}

:::MLPv0.5.0 ssd 1541756782.439713478 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 278, "value": 0.049422222222222215}

:::MLPv0.5.0 ssd 1541756782.525776386 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 279, "value": 0.04959999999999999}

:::MLPv0.5.0 ssd 1541756782.609830618 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 280, "value": 0.04977777777777778}
Iteration:    280, Loss function: 7.178, Average Loss: 2.553, avg. samples / sec: 24275.06
Iteration:    280, Loss function: 7.476, Average Loss: 2.550, avg. samples / sec: 24221.83
Iteration:    280, Loss function: 7.783, Average Loss: 2.546, avg. samples / sec: 24243.70
Iteration:    280, Loss function: 7.693, Average Loss: 2.547, avg. samples / sec: 24214.08
Iteration:    280, Loss function: 7.895, Average Loss: 2.549, avg. samples / sec: 24212.98
Iteration:    280, Loss function: 7.717, Average Loss: 2.548, avg. samples / sec: 24230.30
Iteration:    280, Loss function: 7.751, Average Loss: 2.554, avg. samples / sec: 24201.70
Iteration:    280, Loss function: 7.749, Average Loss: 2.555, avg. samples / sec: 24211.30

:::MLPv0.5.0 ssd 1541756782.693768024 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 281, "value": 0.04995555555555556}

:::MLPv0.5.0 ssd 1541756782.778048754 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 282, "value": 0.050133333333333335}

:::MLPv0.5.0 ssd 1541756782.862965345 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 283, "value": 0.05031111111111111}

:::MLPv0.5.0 ssd 1541756782.947006702 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 284, "value": 0.05048888888888889}

:::MLPv0.5.0 ssd 1541756783.031351566 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 285, "value": 0.050666666666666665}

:::MLPv0.5.0 ssd 1541756783.115491867 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 286, "value": 0.05084444444444444}

:::MLPv0.5.0 ssd 1541756783.199572086 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 287, "value": 0.05102222222222222}

:::MLPv0.5.0 ssd 1541756783.284347534 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 288, "value": 0.051199999999999996}

:::MLPv0.5.0 ssd 1541756783.367289305 (train.py:553) train_epoch: 5

:::MLPv0.5.0 ssd 1541756783.371434689 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 289, "value": 0.05137777777777777}

:::MLPv0.5.0 ssd 1541756783.455667496 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 290, "value": 0.05155555555555555}

:::MLPv0.5.0 ssd 1541756783.540192604 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 291, "value": 0.051733333333333326}

:::MLPv0.5.0 ssd 1541756783.624403715 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 292, "value": 0.0519111111111111}

:::MLPv0.5.0 ssd 1541756783.708507538 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 293, "value": 0.05208888888888889}

:::MLPv0.5.0 ssd 1541756783.792940617 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 294, "value": 0.05226666666666667}

:::MLPv0.5.0 ssd 1541756783.877295256 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 295, "value": 0.052444444444444446}

:::MLPv0.5.0 ssd 1541756783.961256742 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 296, "value": 0.05262222222222222}

:::MLPv0.5.0 ssd 1541756784.045597553 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 297, "value": 0.0528}

:::MLPv0.5.0 ssd 1541756784.130026817 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 298, "value": 0.052977777777777776}

:::MLPv0.5.0 ssd 1541756784.214428425 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 299, "value": 0.05315555555555555}

:::MLPv0.5.0 ssd 1541756784.298665762 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 300, "value": 0.05333333333333333}
Iteration:    300, Loss function: 6.852, Average Loss: 2.647, avg. samples / sec: 24256.07
Iteration:    300, Loss function: 7.125, Average Loss: 2.645, avg. samples / sec: 24251.92
Iteration:    300, Loss function: 7.093, Average Loss: 2.641, avg. samples / sec: 24234.32
Iteration:    300, Loss function: 6.704, Average Loss: 2.649, avg. samples / sec: 24246.93
Iteration:    300, Loss function: 6.724, Average Loss: 2.647, avg. samples / sec: 24259.97
Iteration:    300, Loss function: 7.275, Average Loss: 2.643, avg. samples / sec: 24212.03
Iteration:    300, Loss function: 7.369, Average Loss: 2.646, avg. samples / sec: 24221.16
Iteration:    300, Loss function: 7.449, Average Loss: 2.642, avg. samples / sec: 24224.42

:::MLPv0.5.0 ssd 1541756784.382655144 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 301, "value": 0.053511111111111107}

:::MLPv0.5.0 ssd 1541756784.466470957 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 302, "value": 0.05368888888888888}

:::MLPv0.5.0 ssd 1541756784.550935268 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 303, "value": 0.05386666666666666}

:::MLPv0.5.0 ssd 1541756784.634727001 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 304, "value": 0.05404444444444444}

:::MLPv0.5.0 ssd 1541756784.718758106 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 305, "value": 0.05422222222222223}

:::MLPv0.5.0 ssd 1541756784.802326441 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 306, "value": 0.054400000000000004}

:::MLPv0.5.0 ssd 1541756784.887784004 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 307, "value": 0.05457777777777778}

:::MLPv0.5.0 ssd 1541756784.971798182 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 308, "value": 0.05475555555555556}

:::MLPv0.5.0 ssd 1541756785.055729866 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 309, "value": 0.054933333333333334}

:::MLPv0.5.0 ssd 1541756785.139914751 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 310, "value": 0.05511111111111111}

:::MLPv0.5.0 ssd 1541756785.226667881 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 311, "value": 0.05528888888888889}

:::MLPv0.5.0 ssd 1541756785.311342001 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 312, "value": 0.055466666666666664}

:::MLPv0.5.0 ssd 1541756785.394984961 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 313, "value": 0.05564444444444444}

:::MLPv0.5.0 ssd 1541756785.478433371 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 314, "value": 0.05582222222222222}

:::MLPv0.5.0 ssd 1541756785.562762022 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 315, "value": 0.055999999999999994}

:::MLPv0.5.0 ssd 1541756785.646818876 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 316, "value": 0.05617777777777777}

:::MLPv0.5.0 ssd 1541756785.731220245 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 317, "value": 0.05635555555555555}

:::MLPv0.5.0 ssd 1541756785.815035105 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 318, "value": 0.05653333333333334}

:::MLPv0.5.0 ssd 1541756785.898332357 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 319, "value": 0.056711111111111115}

:::MLPv0.5.0 ssd 1541756785.983445168 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 320, "value": 0.05688888888888889}
Iteration:    320, Loss function: 6.975, Average Loss: 2.735, avg. samples / sec: 24317.15
Iteration:    320, Loss function: 6.843, Average Loss: 2.731, avg. samples / sec: 24320.39
Iteration:    320, Loss function: 6.626, Average Loss: 2.729, avg. samples / sec: 24341.27
Iteration:    320, Loss function: 7.509, Average Loss: 2.731, avg. samples / sec: 24365.48
Iteration:    320, Loss function: 7.164, Average Loss: 2.736, avg. samples / sec: 24333.70
Iteration:    320, Loss function: 6.966, Average Loss: 2.732, avg. samples / sec: 24337.22
Iteration:    320, Loss function: 6.746, Average Loss: 2.732, avg. samples / sec: 24332.60
Iteration:    320, Loss function: 7.365, Average Loss: 2.732, avg. samples / sec: 24297.10

:::MLPv0.5.0 ssd 1541756786.066725492 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 321, "value": 0.05706666666666667}

:::MLPv0.5.0 ssd 1541756786.151134253 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 322, "value": 0.057244444444444445}

:::MLPv0.5.0 ssd 1541756786.235386133 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 323, "value": 0.05742222222222222}

:::MLPv0.5.0 ssd 1541756786.320172310 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 324, "value": 0.0576}

:::MLPv0.5.0 ssd 1541756786.405081272 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 325, "value": 0.057777777777777775}

:::MLPv0.5.0 ssd 1541756786.489110231 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 326, "value": 0.05795555555555555}

:::MLPv0.5.0 ssd 1541756786.573787928 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 327, "value": 0.05813333333333333}

:::MLPv0.5.0 ssd 1541756786.657821894 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 328, "value": 0.058311111111111105}

:::MLPv0.5.0 ssd 1541756786.741753101 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 329, "value": 0.05848888888888888}

:::MLPv0.5.0 ssd 1541756786.825718641 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 330, "value": 0.05866666666666666}

:::MLPv0.5.0 ssd 1541756786.909593344 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 331, "value": 0.05884444444444445}

:::MLPv0.5.0 ssd 1541756786.993370056 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 332, "value": 0.059022222222222226}

:::MLPv0.5.0 ssd 1541756787.077147245 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 333, "value": 0.0592}

:::MLPv0.5.0 ssd 1541756787.161046743 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 334, "value": 0.05937777777777778}

:::MLPv0.5.0 ssd 1541756787.244940519 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 335, "value": 0.059555555555555556}

:::MLPv0.5.0 ssd 1541756787.328611135 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 336, "value": 0.05973333333333333}

:::MLPv0.5.0 ssd 1541756787.412321091 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 337, "value": 0.05991111111111111}

:::MLPv0.5.0 ssd 1541756787.496756077 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 338, "value": 0.060088888888888886}

:::MLPv0.5.0 ssd 1541756787.580304384 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 339, "value": 0.06026666666666666}

:::MLPv0.5.0 ssd 1541756787.664421082 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 340, "value": 0.06044444444444444}
Iteration:    340, Loss function: 7.219, Average Loss: 2.815, avg. samples / sec: 24364.06
Iteration:    340, Loss function: 7.094, Average Loss: 2.820, avg. samples / sec: 24350.72
Iteration:    340, Loss function: 7.258, Average Loss: 2.816, avg. samples / sec: 24350.26
Iteration:    340, Loss function: 6.712, Average Loss: 2.815, avg. samples / sec: 24337.20
Iteration:    340, Loss function: 7.471, Average Loss: 2.821, avg. samples / sec: 24347.31
Iteration:    340, Loss function: 6.866, Average Loss: 2.817, avg. samples / sec: 24350.50
Iteration:    340, Loss function: 6.759, Average Loss: 2.820, avg. samples / sec: 24334.03
Iteration:    340, Loss function: 7.045, Average Loss: 2.817, avg. samples / sec: 24356.30

:::MLPv0.5.0 ssd 1541756787.749855280 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 341, "value": 0.060622222222222216}

:::MLPv0.5.0 ssd 1541756787.833747149 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 342, "value": 0.06079999999999999}

:::MLPv0.5.0 ssd 1541756787.917880535 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 343, "value": 0.06097777777777777}

:::MLPv0.5.0 ssd 1541756788.001961231 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 344, "value": 0.06115555555555556}

:::MLPv0.5.0 ssd 1541756788.087819338 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 345, "value": 0.06133333333333334}

:::MLPv0.5.0 ssd 1541756788.172014475 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 346, "value": 0.061511111111111114}

:::MLPv0.5.0 ssd 1541756788.253801584 (train.py:553) train_epoch: 6

:::MLPv0.5.0 ssd 1541756788.257971764 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 347, "value": 0.06168888888888889}

:::MLPv0.5.0 ssd 1541756788.342140198 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 348, "value": 0.06186666666666667}

:::MLPv0.5.0 ssd 1541756788.427615404 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 349, "value": 0.062044444444444444}

:::MLPv0.5.0 ssd 1541756788.511632442 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 350, "value": 0.06222222222222222}

:::MLPv0.5.0 ssd 1541756788.595138550 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 351, "value": 0.0624}

:::MLPv0.5.0 ssd 1541756788.678990126 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 352, "value": 0.06257777777777777}

:::MLPv0.5.0 ssd 1541756788.762810469 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 353, "value": 0.06275555555555555}

:::MLPv0.5.0 ssd 1541756788.847091913 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 354, "value": 0.06293333333333333}

:::MLPv0.5.0 ssd 1541756788.930489540 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 355, "value": 0.0631111111111111}

:::MLPv0.5.0 ssd 1541756789.014753103 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 356, "value": 0.0632888888888889}

:::MLPv0.5.0 ssd 1541756789.098447323 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 357, "value": 0.06346666666666667}

:::MLPv0.5.0 ssd 1541756789.182705402 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 358, "value": 0.06364444444444445}

:::MLPv0.5.0 ssd 1541756789.266706228 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 359, "value": 0.06382222222222222}

:::MLPv0.5.0 ssd 1541756789.350315571 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 360, "value": 0.064}
Iteration:    360, Loss function: 6.469, Average Loss: 2.902, avg. samples / sec: 24308.03
Iteration:    360, Loss function: 7.228, Average Loss: 2.897, avg. samples / sec: 24293.98
Iteration:    360, Loss function: 6.213, Average Loss: 2.898, avg. samples / sec: 24306.09
Iteration:    360, Loss function: 6.475, Average Loss: 2.902, avg. samples / sec: 24333.00
Iteration:    360, Loss function: 6.424, Average Loss: 2.897, avg. samples / sec: 24312.10
Iteration:    360, Loss function: 7.029, Average Loss: 2.898, avg. samples / sec: 24326.17
Iteration:    360, Loss function: 7.016, Average Loss: 2.897, avg. samples / sec: 24321.17
Iteration:    360, Loss function: 7.031, Average Loss: 2.904, avg. samples / sec: 24300.78

:::MLPv0.5.0 ssd 1541756789.433982134 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 361, "value": 0.06417777777777778}

:::MLPv0.5.0 ssd 1541756789.517827034 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 362, "value": 0.06435555555555555}

:::MLPv0.5.0 ssd 1541756789.601371765 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 363, "value": 0.06453333333333333}

:::MLPv0.5.0 ssd 1541756789.686662197 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 364, "value": 0.06471111111111111}

:::MLPv0.5.0 ssd 1541756789.770705223 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 365, "value": 0.06488888888888888}

:::MLPv0.5.0 ssd 1541756789.854707003 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 366, "value": 0.06506666666666666}

:::MLPv0.5.0 ssd 1541756789.938827515 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 367, "value": 0.06524444444444444}

:::MLPv0.5.0 ssd 1541756790.023014307 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 368, "value": 0.06542222222222221}

:::MLPv0.5.0 ssd 1541756790.107277870 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 369, "value": 0.0656}

:::MLPv0.5.0 ssd 1541756790.190799475 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 370, "value": 0.06577777777777778}

:::MLPv0.5.0 ssd 1541756790.274148703 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 371, "value": 0.06595555555555556}

:::MLPv0.5.0 ssd 1541756790.358549356 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 372, "value": 0.06613333333333334}

:::MLPv0.5.0 ssd 1541756790.442746639 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 373, "value": 0.06631111111111111}

:::MLPv0.5.0 ssd 1541756790.526742935 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 374, "value": 0.06648888888888889}

:::MLPv0.5.0 ssd 1541756790.610633612 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 375, "value": 0.06666666666666667}

:::MLPv0.5.0 ssd 1541756790.694973707 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 376, "value": 0.06684444444444444}

:::MLPv0.5.0 ssd 1541756790.779752731 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 377, "value": 0.06702222222222222}

:::MLPv0.5.0 ssd 1541756790.864963770 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 378, "value": 0.0672}

:::MLPv0.5.0 ssd 1541756790.949283600 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 379, "value": 0.06737777777777777}

:::MLPv0.5.0 ssd 1541756791.040243864 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 380, "value": 0.06755555555555555}
Iteration:    380, Loss function: 6.905, Average Loss: 2.980, avg. samples / sec: 24242.48
Iteration:    380, Loss function: 6.828, Average Loss: 2.975, avg. samples / sec: 24247.24
Iteration:    380, Loss function: 7.033, Average Loss: 2.980, avg. samples / sec: 24229.51
Iteration:    380, Loss function: 6.904, Average Loss: 2.977, avg. samples / sec: 24262.40
Iteration:    380, Loss function: 6.559, Average Loss: 2.975, avg. samples / sec: 24232.55
Iteration:    380, Loss function: 6.787, Average Loss: 2.974, avg. samples / sec: 24227.59
Iteration:    380, Loss function: 6.820, Average Loss: 2.983, avg. samples / sec: 24247.71
Iteration:    380, Loss function: 7.020, Average Loss: 2.979, avg. samples / sec: 24209.68

:::MLPv0.5.0 ssd 1541756791.124180317 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 381, "value": 0.06773333333333333}

:::MLPv0.5.0 ssd 1541756791.207980871 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 382, "value": 0.06791111111111112}

:::MLPv0.5.0 ssd 1541756791.292186975 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 383, "value": 0.0680888888888889}

:::MLPv0.5.0 ssd 1541756791.376118422 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 384, "value": 0.06826666666666667}

:::MLPv0.5.0 ssd 1541756791.460303545 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 385, "value": 0.06844444444444445}

:::MLPv0.5.0 ssd 1541756791.543920994 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 386, "value": 0.06862222222222222}

:::MLPv0.5.0 ssd 1541756791.627920628 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 387, "value": 0.0688}

:::MLPv0.5.0 ssd 1541756791.712380648 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 388, "value": 0.06897777777777778}

:::MLPv0.5.0 ssd 1541756791.796803236 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 389, "value": 0.06915555555555555}

:::MLPv0.5.0 ssd 1541756791.880863428 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 390, "value": 0.06933333333333333}

:::MLPv0.5.0 ssd 1541756791.964814663 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 391, "value": 0.0695111111111111}

:::MLPv0.5.0 ssd 1541756792.050982237 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 392, "value": 0.06968888888888888}

:::MLPv0.5.0 ssd 1541756792.135571718 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 393, "value": 0.06986666666666666}

:::MLPv0.5.0 ssd 1541756792.220268965 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 394, "value": 0.07004444444444444}

:::MLPv0.5.0 ssd 1541756792.304288626 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 395, "value": 0.07022222222222223}

:::MLPv0.5.0 ssd 1541756792.388206482 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 396, "value": 0.0704}

:::MLPv0.5.0 ssd 1541756792.472369432 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 397, "value": 0.07057777777777778}

:::MLPv0.5.0 ssd 1541756792.556480408 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 398, "value": 0.07075555555555556}

:::MLPv0.5.0 ssd 1541756792.640870810 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 399, "value": 0.07093333333333333}

:::MLPv0.5.0 ssd 1541756792.725028276 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 400, "value": 0.07111111111111111}
Iteration:    400, Loss function: 6.500, Average Loss: 3.048, avg. samples / sec: 24327.10
Iteration:    400, Loss function: 5.953, Average Loss: 3.048, avg. samples / sec: 24336.40
Iteration:    400, Loss function: 6.726, Average Loss: 3.052, avg. samples / sec: 24319.86
Iteration:    400, Loss function: 6.600, Average Loss: 3.051, avg. samples / sec: 24354.09
Iteration:    400, Loss function: 6.232, Average Loss: 3.049, avg. samples / sec: 24301.96
Iteration:    400, Loss function: 6.411, Average Loss: 3.049, avg. samples / sec: 24291.40
Iteration:    400, Loss function: 6.893, Average Loss: 3.055, avg. samples / sec: 24270.29
Iteration:    400, Loss function: 6.037, Average Loss: 3.053, avg. samples / sec: 24280.86

:::MLPv0.5.0 ssd 1541756792.808970928 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 401, "value": 0.07128888888888889}

:::MLPv0.5.0 ssd 1541756792.893273830 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 402, "value": 0.07146666666666666}

:::MLPv0.5.0 ssd 1541756792.977279663 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 403, "value": 0.07164444444444444}

:::MLPv0.5.0 ssd 1541756793.061869860 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 404, "value": 0.07182222222222222}

:::MLPv0.5.0 ssd 1541756793.142802715 (train.py:553) train_epoch: 7

:::MLPv0.5.0 ssd 1541756793.146943808 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 405, "value": 0.072}

:::MLPv0.5.0 ssd 1541756793.230789661 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 406, "value": 0.07217777777777777}

:::MLPv0.5.0 ssd 1541756793.314582825 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 407, "value": 0.07235555555555555}

:::MLPv0.5.0 ssd 1541756793.398623705 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 408, "value": 0.07253333333333334}

:::MLPv0.5.0 ssd 1541756793.482453823 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 409, "value": 0.07271111111111112}

:::MLPv0.5.0 ssd 1541756793.566289902 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 410, "value": 0.07288888888888889}

:::MLPv0.5.0 ssd 1541756793.650146008 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 411, "value": 0.07306666666666667}

:::MLPv0.5.0 ssd 1541756793.734315634 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 412, "value": 0.07324444444444445}

:::MLPv0.5.0 ssd 1541756793.818437576 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 413, "value": 0.07342222222222222}

:::MLPv0.5.0 ssd 1541756793.902117491 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 414, "value": 0.0736}

:::MLPv0.5.0 ssd 1541756793.985911369 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 415, "value": 0.07377777777777778}

:::MLPv0.5.0 ssd 1541756794.069776058 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 416, "value": 0.07395555555555555}

:::MLPv0.5.0 ssd 1541756794.153643370 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 417, "value": 0.07413333333333333}

:::MLPv0.5.0 ssd 1541756794.237677097 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 418, "value": 0.0743111111111111}

:::MLPv0.5.0 ssd 1541756794.321158409 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 419, "value": 0.07448888888888888}

:::MLPv0.5.0 ssd 1541756794.405025959 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 420, "value": 0.07466666666666666}
Iteration:    420, Loss function: 6.247, Average Loss: 3.122, avg. samples / sec: 24413.52
Iteration:    420, Loss function: 6.471, Average Loss: 3.117, avg. samples / sec: 24378.49
Iteration:    420, Loss function: 6.441, Average Loss: 3.117, avg. samples / sec: 24352.92
Iteration:    420, Loss function: 6.105, Average Loss: 3.121, avg. samples / sec: 24349.71
Iteration:    420, Loss function: 6.729, Average Loss: 3.118, avg. samples / sec: 24377.32
Iteration:    420, Loss function: 6.190, Average Loss: 3.122, avg. samples / sec: 24345.84
Iteration:    420, Loss function: 6.275, Average Loss: 3.116, avg. samples / sec: 24337.74
Iteration:    420, Loss function: 6.380, Average Loss: 3.120, avg. samples / sec: 24403.97

:::MLPv0.5.0 ssd 1541756794.488991499 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 421, "value": 0.07484444444444445}

:::MLPv0.5.0 ssd 1541756794.572884321 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 422, "value": 0.07502222222222223}

:::MLPv0.5.0 ssd 1541756794.657094479 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 423, "value": 0.0752}

:::MLPv0.5.0 ssd 1541756794.741159439 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 424, "value": 0.07537777777777778}

:::MLPv0.5.0 ssd 1541756794.825419426 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 425, "value": 0.07555555555555556}

:::MLPv0.5.0 ssd 1541756794.909310102 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 426, "value": 0.07573333333333333}

:::MLPv0.5.0 ssd 1541756794.995296717 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 427, "value": 0.07591111111111111}

:::MLPv0.5.0 ssd 1541756795.079915762 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 428, "value": 0.07608888888888889}

:::MLPv0.5.0 ssd 1541756795.164443254 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 429, "value": 0.07626666666666666}

:::MLPv0.5.0 ssd 1541756795.248912096 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 430, "value": 0.07644444444444444}

:::MLPv0.5.0 ssd 1541756795.332961082 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 431, "value": 0.07662222222222222}

:::MLPv0.5.0 ssd 1541756795.416705847 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 432, "value": 0.0768}

:::MLPv0.5.0 ssd 1541756795.501170397 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 433, "value": 0.07697777777777778}

:::MLPv0.5.0 ssd 1541756795.585381746 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 434, "value": 0.07715555555555556}

:::MLPv0.5.0 ssd 1541756795.669618130 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 435, "value": 0.07733333333333334}

:::MLPv0.5.0 ssd 1541756795.753153324 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 436, "value": 0.07751111111111111}

:::MLPv0.5.0 ssd 1541756795.836747169 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 437, "value": 0.07768888888888889}

:::MLPv0.5.0 ssd 1541756795.921171427 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 438, "value": 0.07786666666666667}

:::MLPv0.5.0 ssd 1541756796.005392790 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 439, "value": 0.07804444444444444}

:::MLPv0.5.0 ssd 1541756796.089734077 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 440, "value": 0.07822222222222222}
Iteration:    440, Loss function: 6.032, Average Loss: 3.190, avg. samples / sec: 24353.31
Iteration:    440, Loss function: 6.127, Average Loss: 3.188, avg. samples / sec: 24329.07
Iteration:    440, Loss function: 6.208, Average Loss: 3.189, avg. samples / sec: 24318.33
Iteration:    440, Loss function: 6.647, Average Loss: 3.188, avg. samples / sec: 24330.71
Iteration:    440, Loss function: 6.124, Average Loss: 3.184, avg. samples / sec: 24309.93
Iteration:    440, Loss function: 6.784, Average Loss: 3.192, avg. samples / sec: 24298.14
Iteration:    440, Loss function: 6.236, Average Loss: 3.187, avg. samples / sec: 24292.00
Iteration:    440, Loss function: 6.328, Average Loss: 3.184, avg. samples / sec: 24294.21

:::MLPv0.5.0 ssd 1541756796.173538923 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 441, "value": 0.0784}

:::MLPv0.5.0 ssd 1541756796.257536173 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 442, "value": 0.07857777777777777}

:::MLPv0.5.0 ssd 1541756796.341617584 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 443, "value": 0.07875555555555555}

:::MLPv0.5.0 ssd 1541756796.426016331 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 444, "value": 0.07893333333333333}

:::MLPv0.5.0 ssd 1541756796.509877682 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 445, "value": 0.0791111111111111}

:::MLPv0.5.0 ssd 1541756796.593916655 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 446, "value": 0.0792888888888889}

:::MLPv0.5.0 ssd 1541756796.677770138 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 447, "value": 0.07946666666666667}

:::MLPv0.5.0 ssd 1541756796.761749744 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 448, "value": 0.07964444444444445}

:::MLPv0.5.0 ssd 1541756796.845911264 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 449, "value": 0.07982222222222222}

:::MLPv0.5.0 ssd 1541756796.930108309 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 450, "value": 0.08}

:::MLPv0.5.0 ssd 1541756797.015136719 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 451, "value": 0.08017777777777778}

:::MLPv0.5.0 ssd 1541756797.099118233 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 452, "value": 0.08035555555555556}

:::MLPv0.5.0 ssd 1541756797.183541298 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 453, "value": 0.08053333333333333}

:::MLPv0.5.0 ssd 1541756797.267621517 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 454, "value": 0.08071111111111111}

:::MLPv0.5.0 ssd 1541756797.351804733 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 455, "value": 0.08088888888888889}

:::MLPv0.5.0 ssd 1541756797.435824156 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 456, "value": 0.08106666666666666}

:::MLPv0.5.0 ssd 1541756797.519967079 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 457, "value": 0.08124444444444444}

:::MLPv0.5.0 ssd 1541756797.603774071 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 458, "value": 0.08142222222222222}

:::MLPv0.5.0 ssd 1541756797.688134193 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 459, "value": 0.0816}

:::MLPv0.5.0 ssd 1541756797.772321701 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 460, "value": 0.08177777777777778}
Iteration:    460, Loss function: 6.153, Average Loss: 3.255, avg. samples / sec: 24338.04
Iteration:    460, Loss function: 6.167, Average Loss: 3.252, avg. samples / sec: 24328.91
Iteration:    460, Loss function: 6.561, Average Loss: 3.247, avg. samples / sec: 24373.72
Iteration:    460, Loss function: 6.523, Average Loss: 3.247, avg. samples / sec: 24339.88
Iteration:    460, Loss function: 6.022, Average Loss: 3.250, avg. samples / sec: 24326.00
Iteration:    460, Loss function: 6.009, Average Loss: 3.252, avg. samples / sec: 24344.85
Iteration:    460, Loss function: 5.696, Average Loss: 3.256, avg. samples / sec: 24335.61
Iteration:    460, Loss function: 6.539, Average Loss: 3.251, avg. samples / sec: 24292.44

:::MLPv0.5.0 ssd 1541756797.857277393 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 461, "value": 0.08195555555555556}

:::MLPv0.5.0 ssd 1541756797.941653967 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 462, "value": 0.08213333333333334}

:::MLPv0.5.0 ssd 1541756798.023189545 (train.py:553) train_epoch: 8

:::MLPv0.5.0 ssd 1541756798.027529240 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 463, "value": 0.08231111111111111}

:::MLPv0.5.0 ssd 1541756798.112993717 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 464, "value": 0.08248888888888889}

:::MLPv0.5.0 ssd 1541756798.196934462 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 465, "value": 0.08266666666666667}

:::MLPv0.5.0 ssd 1541756798.281121254 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 466, "value": 0.08284444444444444}

:::MLPv0.5.0 ssd 1541756798.365114450 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 467, "value": 0.08302222222222222}

:::MLPv0.5.0 ssd 1541756798.449850321 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 468, "value": 0.0832}

:::MLPv0.5.0 ssd 1541756798.533466339 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 469, "value": 0.08337777777777777}

:::MLPv0.5.0 ssd 1541756798.617275953 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 470, "value": 0.08355555555555555}

:::MLPv0.5.0 ssd 1541756798.702226162 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 471, "value": 0.08373333333333333}

:::MLPv0.5.0 ssd 1541756798.786569118 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 472, "value": 0.08391111111111112}

:::MLPv0.5.0 ssd 1541756798.870286226 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 473, "value": 0.0840888888888889}

:::MLPv0.5.0 ssd 1541756798.954651117 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 474, "value": 0.08426666666666667}

:::MLPv0.5.0 ssd 1541756799.038799047 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 475, "value": 0.08444444444444445}

:::MLPv0.5.0 ssd 1541756799.122786999 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 476, "value": 0.08462222222222222}

:::MLPv0.5.0 ssd 1541756799.207255602 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 477, "value": 0.0848}

:::MLPv0.5.0 ssd 1541756799.291255236 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 478, "value": 0.08497777777777778}

:::MLPv0.5.0 ssd 1541756799.375009775 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 479, "value": 0.08515555555555555}

:::MLPv0.5.0 ssd 1541756799.459109306 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 480, "value": 0.08533333333333333}
Iteration:    480, Loss function: 6.424, Average Loss: 3.317, avg. samples / sec: 24302.27
Iteration:    480, Loss function: 6.581, Average Loss: 3.322, avg. samples / sec: 24344.14
Iteration:    480, Loss function: 6.031, Average Loss: 3.315, avg. samples / sec: 24301.11
Iteration:    480, Loss function: 6.100, Average Loss: 3.313, avg. samples / sec: 24275.83
Iteration:    480, Loss function: 6.361, Average Loss: 3.323, avg. samples / sec: 24248.02
Iteration:    480, Loss function: 6.358, Average Loss: 3.322, avg. samples / sec: 24293.60
Iteration:    480, Loss function: 6.259, Average Loss: 3.319, avg. samples / sec: 24260.47
Iteration:    480, Loss function: 6.212, Average Loss: 3.321, avg. samples / sec: 24277.06

:::MLPv0.5.0 ssd 1541756799.543549299 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 481, "value": 0.08551111111111111}

:::MLPv0.5.0 ssd 1541756799.627271175 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 482, "value": 0.08568888888888888}

:::MLPv0.5.0 ssd 1541756799.712433100 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 483, "value": 0.08586666666666666}

:::MLPv0.5.0 ssd 1541756799.796890974 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 484, "value": 0.08604444444444445}

:::MLPv0.5.0 ssd 1541756799.880614519 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 485, "value": 0.08622222222222223}

:::MLPv0.5.0 ssd 1541756799.964820147 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 486, "value": 0.0864}

:::MLPv0.5.0 ssd 1541756800.048726082 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 487, "value": 0.08657777777777778}

:::MLPv0.5.0 ssd 1541756800.132812262 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 488, "value": 0.08675555555555556}

:::MLPv0.5.0 ssd 1541756800.216859818 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 489, "value": 0.08693333333333333}

:::MLPv0.5.0 ssd 1541756800.300765514 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 490, "value": 0.08711111111111111}

:::MLPv0.5.0 ssd 1541756800.384541512 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 491, "value": 0.08728888888888889}

:::MLPv0.5.0 ssd 1541756800.468646288 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 492, "value": 0.08746666666666666}

:::MLPv0.5.0 ssd 1541756800.552074671 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 493, "value": 0.08764444444444444}

:::MLPv0.5.0 ssd 1541756800.636850357 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 494, "value": 0.08782222222222222}

:::MLPv0.5.0 ssd 1541756800.720908642 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 495, "value": 0.088}

:::MLPv0.5.0 ssd 1541756800.804632425 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 496, "value": 0.08817777777777777}

:::MLPv0.5.0 ssd 1541756800.888503551 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 497, "value": 0.08835555555555556}

:::MLPv0.5.0 ssd 1541756800.972778082 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 498, "value": 0.08853333333333334}

:::MLPv0.5.0 ssd 1541756801.056768179 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 499, "value": 0.08871111111111112}

:::MLPv0.5.0 ssd 1541756801.140531778 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 500, "value": 0.08888888888888889}
Iteration:    500, Loss function: 6.209, Average Loss: 3.378, avg. samples / sec: 24370.90
Iteration:    500, Loss function: 5.749, Average Loss: 3.374, avg. samples / sec: 24419.39
Iteration:    500, Loss function: 6.474, Average Loss: 3.374, avg. samples / sec: 24361.98
Iteration:    500, Loss function: 5.944, Average Loss: 3.373, avg. samples / sec: 24357.05
Iteration:    500, Loss function: 6.121, Average Loss: 3.379, avg. samples / sec: 24374.77
Iteration:    500, Loss function: 5.980, Average Loss: 3.374, avg. samples / sec: 24378.85
Iteration:    500, Loss function: 6.164, Average Loss: 3.377, avg. samples / sec: 24369.16
Iteration:    500, Loss function: 5.682, Average Loss: 3.368, avg. samples / sec: 24335.98

:::MLPv0.5.0 ssd 1541756801.225352287 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 501, "value": 0.08906666666666667}

:::MLPv0.5.0 ssd 1541756801.309274673 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 502, "value": 0.08924444444444445}

:::MLPv0.5.0 ssd 1541756801.393534660 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 503, "value": 0.08942222222222222}

:::MLPv0.5.0 ssd 1541756801.477264881 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 504, "value": 0.0896}

:::MLPv0.5.0 ssd 1541756801.561586857 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 505, "value": 0.08977777777777778}

:::MLPv0.5.0 ssd 1541756801.645372629 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 506, "value": 0.08995555555555555}

:::MLPv0.5.0 ssd 1541756801.729369402 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 507, "value": 0.09013333333333333}

:::MLPv0.5.0 ssd 1541756801.813432693 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 508, "value": 0.0903111111111111}

:::MLPv0.5.0 ssd 1541756801.897233725 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 509, "value": 0.09048888888888888}

:::MLPv0.5.0 ssd 1541756801.980870962 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 510, "value": 0.09066666666666667}

:::MLPv0.5.0 ssd 1541756802.065131664 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 511, "value": 0.09084444444444445}

:::MLPv0.5.0 ssd 1541756802.149422646 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 512, "value": 0.09102222222222223}

:::MLPv0.5.0 ssd 1541756802.233487368 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 513, "value": 0.0912}

:::MLPv0.5.0 ssd 1541756802.317504168 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 514, "value": 0.09137777777777778}

:::MLPv0.5.0 ssd 1541756802.401503325 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 515, "value": 0.09155555555555556}

:::MLPv0.5.0 ssd 1541756802.485008478 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 516, "value": 0.09173333333333333}

:::MLPv0.5.0 ssd 1541756802.569250345 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 517, "value": 0.09191111111111111}

:::MLPv0.5.0 ssd 1541756802.653937101 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 518, "value": 0.09208888888888889}

:::MLPv0.5.0 ssd 1541756802.737688780 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 519, "value": 0.09226666666666666}

:::MLPv0.5.0 ssd 1541756802.818239689 (train.py:553) train_epoch: 9

:::MLPv0.5.0 ssd 1541756802.822434902 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 520, "value": 0.09244444444444444}
Iteration:    520, Loss function: 6.241, Average Loss: 3.425, avg. samples / sec: 24362.36
Iteration:    520, Loss function: 6.566, Average Loss: 3.431, avg. samples / sec: 24355.56
Iteration:    520, Loss function: 6.549, Average Loss: 3.430, avg. samples / sec: 24378.63
Iteration:    520, Loss function: 6.697, Average Loss: 3.425, avg. samples / sec: 24377.86
Iteration:    520, Loss function: 6.517, Average Loss: 3.426, avg. samples / sec: 24338.17
Iteration:    520, Loss function: 5.458, Average Loss: 3.425, avg. samples / sec: 24335.38
Iteration:    520, Loss function: 6.970, Average Loss: 3.424, avg. samples / sec: 24370.50
Iteration:    520, Loss function: 6.569, Average Loss: 3.428, avg. samples / sec: 24337.08

:::MLPv0.5.0 ssd 1541756802.905798197 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 521, "value": 0.09262222222222222}

:::MLPv0.5.0 ssd 1541756802.989822388 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 522, "value": 0.0928}

:::MLPv0.5.0 ssd 1541756803.073784590 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 523, "value": 0.09297777777777778}

:::MLPv0.5.0 ssd 1541756803.158129692 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 524, "value": 0.09315555555555556}

:::MLPv0.5.0 ssd 1541756803.242017269 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 525, "value": 0.09333333333333334}

:::MLPv0.5.0 ssd 1541756803.325346470 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 526, "value": 0.09351111111111111}

:::MLPv0.5.0 ssd 1541756803.408959150 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 527, "value": 0.09368888888888889}

:::MLPv0.5.0 ssd 1541756803.492578983 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 528, "value": 0.09386666666666667}

:::MLPv0.5.0 ssd 1541756803.577010155 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 529, "value": 0.09404444444444444}

:::MLPv0.5.0 ssd 1541756803.661174059 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 530, "value": 0.09422222222222222}

:::MLPv0.5.0 ssd 1541756803.745371580 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 531, "value": 0.0944}

:::MLPv0.5.0 ssd 1541756803.828807831 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 532, "value": 0.09457777777777777}

:::MLPv0.5.0 ssd 1541756803.912707329 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 533, "value": 0.09475555555555555}

:::MLPv0.5.0 ssd 1541756803.996914387 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 534, "value": 0.09493333333333333}

:::MLPv0.5.0 ssd 1541756804.080804586 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 535, "value": 0.0951111111111111}

:::MLPv0.5.0 ssd 1541756804.165337086 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 536, "value": 0.0952888888888889}

:::MLPv0.5.0 ssd 1541756804.249377728 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 537, "value": 0.09546666666666667}

:::MLPv0.5.0 ssd 1541756804.333902836 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 538, "value": 0.09564444444444445}

:::MLPv0.5.0 ssd 1541756804.417560577 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 539, "value": 0.09582222222222223}

:::MLPv0.5.0 ssd 1541756804.503029108 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 540, "value": 0.096}
Iteration:    540, Loss function: 5.978, Average Loss: 3.487, avg. samples / sec: 24430.28
Iteration:    540, Loss function: 6.339, Average Loss: 3.484, avg. samples / sec: 24370.07
Iteration:    540, Loss function: 5.997, Average Loss: 3.483, avg. samples / sec: 24381.69
Iteration:    540, Loss function: 6.421, Average Loss: 3.489, avg. samples / sec: 24373.98
Iteration:    540, Loss function: 5.695, Average Loss: 3.488, avg. samples / sec: 24366.41
Iteration:    540, Loss function: 6.506, Average Loss: 3.482, avg. samples / sec: 24409.15
Iteration:    540, Loss function: 5.645, Average Loss: 3.481, avg. samples / sec: 24392.42
Iteration:    540, Loss function: 6.681, Average Loss: 3.485, avg. samples / sec: 24373.54

:::MLPv0.5.0 ssd 1541756804.587024927 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 541, "value": 0.09617777777777778}

:::MLPv0.5.0 ssd 1541756804.671735525 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 542, "value": 0.09635555555555556}

:::MLPv0.5.0 ssd 1541756804.756628990 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 543, "value": 0.09653333333333333}

:::MLPv0.5.0 ssd 1541756804.840515614 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 544, "value": 0.09671111111111111}

:::MLPv0.5.0 ssd 1541756804.925113678 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 545, "value": 0.09688888888888889}

:::MLPv0.5.0 ssd 1541756805.008959532 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 546, "value": 0.09706666666666666}

:::MLPv0.5.0 ssd 1541756805.092718363 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 547, "value": 0.09724444444444444}

:::MLPv0.5.0 ssd 1541756805.176888227 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 548, "value": 0.09742222222222222}

:::MLPv0.5.0 ssd 1541756805.260704994 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 549, "value": 0.09759999999999999}

:::MLPv0.5.0 ssd 1541756805.344415903 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 550, "value": 0.09777777777777777}

:::MLPv0.5.0 ssd 1541756805.428552628 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 551, "value": 0.09795555555555555}

:::MLPv0.5.0 ssd 1541756805.513243675 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 552, "value": 0.09813333333333334}

:::MLPv0.5.0 ssd 1541756805.597905874 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 553, "value": 0.09831111111111111}

:::MLPv0.5.0 ssd 1541756805.681986809 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 554, "value": 0.09848888888888889}

:::MLPv0.5.0 ssd 1541756805.766233206 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 555, "value": 0.09866666666666667}

:::MLPv0.5.0 ssd 1541756805.850494385 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 556, "value": 0.09884444444444444}

:::MLPv0.5.0 ssd 1541756805.934545994 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 557, "value": 0.09902222222222222}

:::MLPv0.5.0 ssd 1541756806.018857956 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 558, "value": 0.09920000000000001}

:::MLPv0.5.0 ssd 1541756806.103134871 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 559, "value": 0.09937777777777779}

:::MLPv0.5.0 ssd 1541756806.187278986 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 560, "value": 0.09955555555555556}
Iteration:    560, Loss function: 5.976, Average Loss: 3.531, avg. samples / sec: 24324.32
Iteration:    560, Loss function: 5.674, Average Loss: 3.535, avg. samples / sec: 24329.18
Iteration:    560, Loss function: 5.820, Average Loss: 3.528, avg. samples / sec: 24323.64
Iteration:    560, Loss function: 4.770, Average Loss: 3.526, avg. samples / sec: 24317.73
Iteration:    560, Loss function: 6.082, Average Loss: 3.528, avg. samples / sec: 24305.15
Iteration:    560, Loss function: 5.608, Average Loss: 3.529, avg. samples / sec: 24319.70
Iteration:    560, Loss function: 5.423, Average Loss: 3.533, avg. samples / sec: 24287.50
Iteration:    560, Loss function: 5.204, Average Loss: 3.536, avg. samples / sec: 24289.01

:::MLPv0.5.0 ssd 1541756806.271319389 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 561, "value": 0.09973333333333334}

:::MLPv0.5.0 ssd 1541756806.355108261 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 562, "value": 0.09991111111111112}

:::MLPv0.5.0 ssd 1541756806.438929558 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 563, "value": 0.1000888888888889}

:::MLPv0.5.0 ssd 1541756806.522956133 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 564, "value": 0.10026666666666667}

:::MLPv0.5.0 ssd 1541756806.607425451 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 565, "value": 0.10044444444444445}

:::MLPv0.5.0 ssd 1541756806.691537142 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 566, "value": 0.10062222222222222}

:::MLPv0.5.0 ssd 1541756806.775944471 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 567, "value": 0.1008}

:::MLPv0.5.0 ssd 1541756806.860130072 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 568, "value": 0.10097777777777778}

:::MLPv0.5.0 ssd 1541756806.944163799 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 569, "value": 0.10115555555555555}

:::MLPv0.5.0 ssd 1541756807.028446198 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 570, "value": 0.10133333333333333}

:::MLPv0.5.0 ssd 1541756807.112218142 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 571, "value": 0.10151111111111111}

:::MLPv0.5.0 ssd 1541756807.196176052 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 572, "value": 0.10168888888888888}

:::MLPv0.5.0 ssd 1541756807.280010223 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 573, "value": 0.10186666666666666}

:::MLPv0.5.0 ssd 1541756807.364456654 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 574, "value": 0.10204444444444444}

:::MLPv0.5.0 ssd 1541756807.448323011 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 575, "value": 0.10222222222222221}

:::MLPv0.5.0 ssd 1541756807.532465935 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 576, "value": 0.10239999999999999}

:::MLPv0.5.0 ssd 1541756807.616513491 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 577, "value": 0.10257777777777778}

:::MLPv0.5.0 ssd 1541756807.697584391 (train.py:553) train_epoch: 10

:::MLPv0.5.0 ssd 1541756807.701799154 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 578, "value": 0.10275555555555556}

:::MLPv0.5.0 ssd 1541756807.785318375 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 579, "value": 0.10293333333333334}

:::MLPv0.5.0 ssd 1541756807.869156837 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 580, "value": 0.10311111111111111}
Iteration:    580, Loss function: 5.212, Average Loss: 3.567, avg. samples / sec: 24355.19
Iteration:    580, Loss function: 5.247, Average Loss: 3.575, avg. samples / sec: 24351.28
Iteration:    580, Loss function: 5.895, Average Loss: 3.577, avg. samples / sec: 24384.54
Iteration:    580, Loss function: 5.319, Average Loss: 3.576, avg. samples / sec: 24344.70
Iteration:    580, Loss function: 5.694, Average Loss: 3.570, avg. samples / sec: 24356.76
Iteration:    580, Loss function: 5.138, Average Loss: 3.572, avg. samples / sec: 24353.72
Iteration:    580, Loss function: 5.670, Average Loss: 3.575, avg. samples / sec: 24338.94
Iteration:    580, Loss function: 5.822, Average Loss: 3.572, avg. samples / sec: 24322.62

:::MLPv0.5.0 ssd 1541756807.952709675 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 581, "value": 0.10328888888888889}

:::MLPv0.5.0 ssd 1541756808.036782503 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 582, "value": 0.10346666666666667}

:::MLPv0.5.0 ssd 1541756808.120882034 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 583, "value": 0.10364444444444444}

:::MLPv0.5.0 ssd 1541756808.204190969 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 584, "value": 0.10382222222222223}

:::MLPv0.5.0 ssd 1541756808.288475037 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 585, "value": 0.10400000000000001}

:::MLPv0.5.0 ssd 1541756808.372193813 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 586, "value": 0.10417777777777779}

:::MLPv0.5.0 ssd 1541756808.457470894 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 587, "value": 0.10435555555555556}

:::MLPv0.5.0 ssd 1541756808.541007042 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 588, "value": 0.10453333333333334}

:::MLPv0.5.0 ssd 1541756808.625931978 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 589, "value": 0.10471111111111112}

:::MLPv0.5.0 ssd 1541756808.709773779 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 590, "value": 0.10488888888888889}

:::MLPv0.5.0 ssd 1541756808.793711662 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 591, "value": 0.10506666666666667}

:::MLPv0.5.0 ssd 1541756808.877939224 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 592, "value": 0.10524444444444445}

:::MLPv0.5.0 ssd 1541756808.963060617 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 593, "value": 0.10542222222222222}

:::MLPv0.5.0 ssd 1541756809.047185659 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 594, "value": 0.1056}

:::MLPv0.5.0 ssd 1541756809.131525278 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 595, "value": 0.10577777777777778}

:::MLPv0.5.0 ssd 1541756809.215802193 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 596, "value": 0.10595555555555555}

:::MLPv0.5.0 ssd 1541756809.299969435 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 597, "value": 0.10613333333333333}

:::MLPv0.5.0 ssd 1541756809.384119749 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 598, "value": 0.1063111111111111}

:::MLPv0.5.0 ssd 1541756809.468477964 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 599, "value": 0.10648888888888888}

:::MLPv0.5.0 ssd 1541756809.553475857 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 600, "value": 0.10666666666666666}
Iteration:    600, Loss function: 5.844, Average Loss: 3.622, avg. samples / sec: 24324.16
Iteration:    600, Loss function: 5.784, Average Loss: 3.613, avg. samples / sec: 24322.41
Iteration:    600, Loss function: 5.971, Average Loss: 3.622, avg. samples / sec: 24324.19
Iteration:    600, Loss function: 6.319, Average Loss: 3.626, avg. samples / sec: 24315.87
Iteration:    600, Loss function: 5.761, Average Loss: 3.621, avg. samples / sec: 24333.98
Iteration:    600, Loss function: 5.658, Average Loss: 3.619, avg. samples / sec: 24357.28
Iteration:    600, Loss function: 6.132, Average Loss: 3.619, avg. samples / sec: 24305.96
Iteration:    600, Loss function: 5.844, Average Loss: 3.623, avg. samples / sec: 24308.17

:::MLPv0.5.0 ssd 1541756809.637319088 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 601, "value": 0.10684444444444444}

:::MLPv0.5.0 ssd 1541756809.721258640 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 602, "value": 0.10702222222222221}

:::MLPv0.5.0 ssd 1541756809.805397272 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 603, "value": 0.1072}

:::MLPv0.5.0 ssd 1541756809.890184164 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 604, "value": 0.10737777777777778}

:::MLPv0.5.0 ssd 1541756809.974087715 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 605, "value": 0.10755555555555556}

:::MLPv0.5.0 ssd 1541756810.057355881 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 606, "value": 0.10773333333333333}

:::MLPv0.5.0 ssd 1541756810.143052578 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 607, "value": 0.10791111111111111}

:::MLPv0.5.0 ssd 1541756810.227932930 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 608, "value": 0.10808888888888889}

:::MLPv0.5.0 ssd 1541756810.311827898 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 609, "value": 0.10826666666666668}

:::MLPv0.5.0 ssd 1541756810.396427870 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 610, "value": 0.10844444444444445}

:::MLPv0.5.0 ssd 1541756810.480467558 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 611, "value": 0.10862222222222223}

:::MLPv0.5.0 ssd 1541756810.564366341 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 612, "value": 0.10880000000000001}

:::MLPv0.5.0 ssd 1541756810.648848057 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 613, "value": 0.10897777777777778}

:::MLPv0.5.0 ssd 1541756810.732720852 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 614, "value": 0.10915555555555556}

:::MLPv0.5.0 ssd 1541756810.816879988 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 615, "value": 0.10933333333333334}

:::MLPv0.5.0 ssd 1541756810.900733232 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 616, "value": 0.10951111111111111}

:::MLPv0.5.0 ssd 1541756810.984420776 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 617, "value": 0.10968888888888889}

:::MLPv0.5.0 ssd 1541756811.068624973 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 618, "value": 0.10986666666666667}

:::MLPv0.5.0 ssd 1541756811.152611256 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 619, "value": 0.11004444444444444}

:::MLPv0.5.0 ssd 1541756811.236482620 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 620, "value": 0.11022222222222222}
Iteration:    620, Loss function: 5.857, Average Loss: 3.654, avg. samples / sec: 24339.19
Iteration:    620, Loss function: 5.774, Average Loss: 3.662, avg. samples / sec: 24358.52
Iteration:    620, Loss function: 6.159, Average Loss: 3.662, avg. samples / sec: 24331.51
Iteration:    620, Loss function: 5.548, Average Loss: 3.661, avg. samples / sec: 24338.48
Iteration:    620, Loss function: 5.094, Average Loss: 3.665, avg. samples / sec: 24317.75
Iteration:    620, Loss function: 5.666, Average Loss: 3.661, avg. samples / sec: 24331.24
Iteration:    620, Loss function: 5.417, Average Loss: 3.665, avg. samples / sec: 24353.01
Iteration:    620, Loss function: 4.999, Average Loss: 3.662, avg. samples / sec: 24293.51

:::MLPv0.5.0 ssd 1541756811.320352793 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 621, "value": 0.1104}

:::MLPv0.5.0 ssd 1541756811.404498339 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 622, "value": 0.11057777777777777}

:::MLPv0.5.0 ssd 1541756811.488804579 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 623, "value": 0.11075555555555555}

:::MLPv0.5.0 ssd 1541756811.572679520 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 624, "value": 0.11093333333333333}

:::MLPv0.5.0 ssd 1541756811.656769514 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 625, "value": 0.1111111111111111}

:::MLPv0.5.0 ssd 1541756811.741426229 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 626, "value": 0.11128888888888888}

:::MLPv0.5.0 ssd 1541756811.827332973 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 627, "value": 0.11146666666666666}

:::MLPv0.5.0 ssd 1541756811.911479950 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 628, "value": 0.11164444444444445}

:::MLPv0.5.0 ssd 1541756811.995335817 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 629, "value": 0.11182222222222223}

:::MLPv0.5.0 ssd 1541756812.080110550 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 630, "value": 0.112}

:::MLPv0.5.0 ssd 1541756812.164417028 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 631, "value": 0.11217777777777778}

:::MLPv0.5.0 ssd 1541756812.248435020 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 632, "value": 0.11235555555555556}

:::MLPv0.5.0 ssd 1541756812.332847118 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 633, "value": 0.11253333333333333}

:::MLPv0.5.0 ssd 1541756812.417206049 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 634, "value": 0.11271111111111111}

:::MLPv0.5.0 ssd 1541756812.501071453 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 635, "value": 0.1128888888888889}

:::MLPv0.5.0 ssd 1541756812.582097292 (train.py:553) train_epoch: 11

:::MLPv0.5.0 ssd 1541756812.586282730 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 636, "value": 0.11306666666666668}

:::MLPv0.5.0 ssd 1541756812.670092821 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 637, "value": 0.11324444444444445}

:::MLPv0.5.0 ssd 1541756812.754142284 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 638, "value": 0.11342222222222223}

:::MLPv0.5.0 ssd 1541756812.838589668 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 639, "value": 0.1136}

:::MLPv0.5.0 ssd 1541756812.923036098 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 640, "value": 0.11377777777777778}
Iteration:    640, Loss function: 7.579, Average Loss: 3.720, avg. samples / sec: 24287.44
Iteration:    640, Loss function: 8.000, Average Loss: 3.710, avg. samples / sec: 24286.66
Iteration:    640, Loss function: 7.712, Average Loss: 3.716, avg. samples / sec: 24290.60
Iteration:    640, Loss function: 7.605, Average Loss: 3.719, avg. samples / sec: 24323.85
Iteration:    640, Loss function: 7.846, Average Loss: 3.719, avg. samples / sec: 24294.93
Iteration:    640, Loss function: 7.840, Average Loss: 3.723, avg. samples / sec: 24292.51
Iteration:    640, Loss function: 7.847, Average Loss: 3.719, avg. samples / sec: 24307.93
Iteration:    640, Loss function: 7.318, Average Loss: 3.719, avg. samples / sec: 24236.55

:::MLPv0.5.0 ssd 1541756813.007560492 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 641, "value": 0.11395555555555556}

:::MLPv0.5.0 ssd 1541756813.091588259 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 642, "value": 0.11413333333333334}

:::MLPv0.5.0 ssd 1541756813.175304413 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 643, "value": 0.11431111111111111}

:::MLPv0.5.0 ssd 1541756813.259850502 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 644, "value": 0.11448888888888889}

:::MLPv0.5.0 ssd 1541756813.343919516 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 645, "value": 0.11466666666666667}

:::MLPv0.5.0 ssd 1541756813.428125381 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 646, "value": 0.11484444444444444}

:::MLPv0.5.0 ssd 1541756813.512357473 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 647, "value": 0.11502222222222222}

:::MLPv0.5.0 ssd 1541756813.596331596 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 648, "value": 0.1152}

:::MLPv0.5.0 ssd 1541756813.680496216 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 649, "value": 0.11537777777777777}

:::MLPv0.5.0 ssd 1541756813.764610291 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 650, "value": 0.11555555555555555}

:::MLPv0.5.0 ssd 1541756813.848472118 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 651, "value": 0.11573333333333333}

:::MLPv0.5.0 ssd 1541756813.932008743 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 652, "value": 0.1159111111111111}

:::MLPv0.5.0 ssd 1541756814.016763926 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 653, "value": 0.11608888888888888}

:::MLPv0.5.0 ssd 1541756814.100923538 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 654, "value": 0.11626666666666667}

:::MLPv0.5.0 ssd 1541756814.184827089 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 655, "value": 0.11644444444444445}

:::MLPv0.5.0 ssd 1541756814.269622564 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 656, "value": 0.11662222222222222}

:::MLPv0.5.0 ssd 1541756814.353391647 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 657, "value": 0.1168}

:::MLPv0.5.0 ssd 1541756814.438859463 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 658, "value": 0.11697777777777778}

:::MLPv0.5.0 ssd 1541756814.522553682 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 659, "value": 0.11715555555555555}

:::MLPv0.5.0 ssd 1541756814.606878042 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 660, "value": 0.11733333333333333}
Iteration:    660, Loss function: 9.404, Average Loss: 3.823, avg. samples / sec: 24329.16
Iteration:    660, Loss function: 9.042, Average Loss: 3.837, avg. samples / sec: 24326.70
Iteration:    660, Loss function: 9.033, Average Loss: 3.833, avg. samples / sec: 24361.90
Iteration:    660, Loss function: 8.927, Average Loss: 3.836, avg. samples / sec: 24347.15
Iteration:    660, Loss function: 8.600, Average Loss: 3.832, avg. samples / sec: 24341.85
Iteration:    660, Loss function: 9.004, Average Loss: 3.830, avg. samples / sec: 24306.26
Iteration:    660, Loss function: 8.423, Average Loss: 3.833, avg. samples / sec: 24308.60
Iteration:    660, Loss function: 8.893, Average Loss: 3.827, avg. samples / sec: 24322.70

:::MLPv0.5.0 ssd 1541756814.690772295 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 661, "value": 0.11751111111111112}

:::MLPv0.5.0 ssd 1541756814.774327755 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 662, "value": 0.1176888888888889}

:::MLPv0.5.0 ssd 1541756814.858143330 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 663, "value": 0.11786666666666668}

:::MLPv0.5.0 ssd 1541756814.942289829 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 664, "value": 0.11804444444444445}

:::MLPv0.5.0 ssd 1541756815.027903080 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 665, "value": 0.11822222222222223}

:::MLPv0.5.0 ssd 1541756815.111587763 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 666, "value": 0.1184}

:::MLPv0.5.0 ssd 1541756815.194956779 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 667, "value": 0.11857777777777778}

:::MLPv0.5.0 ssd 1541756815.278778553 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 668, "value": 0.11875555555555556}

:::MLPv0.5.0 ssd 1541756815.362300396 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 669, "value": 0.11893333333333334}

:::MLPv0.5.0 ssd 1541756815.446430683 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 670, "value": 0.11911111111111111}

:::MLPv0.5.0 ssd 1541756815.530463934 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 671, "value": 0.11928888888888889}

:::MLPv0.5.0 ssd 1541756815.614270687 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 672, "value": 0.11946666666666667}

:::MLPv0.5.0 ssd 1541756815.698056221 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 673, "value": 0.11964444444444444}

:::MLPv0.5.0 ssd 1541756815.783365726 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 674, "value": 0.11982222222222222}

:::MLPv0.5.0 ssd 1541756815.868036509 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 675, "value": 0.12}

:::MLPv0.5.0 ssd 1541756815.951842785 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 676, "value": 0.12017777777777777}

:::MLPv0.5.0 ssd 1541756816.036141157 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 677, "value": 0.12035555555555555}

:::MLPv0.5.0 ssd 1541756816.119696140 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 678, "value": 0.12053333333333333}

:::MLPv0.5.0 ssd 1541756816.203545809 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 679, "value": 0.1207111111111111}

:::MLPv0.5.0 ssd 1541756816.287682533 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 680, "value": 0.12088888888888889}
Iteration:    680, Loss function: 7.362, Average Loss: 3.905, avg. samples / sec: 24368.64
Iteration:    680, Loss function: 7.578, Average Loss: 3.913, avg. samples / sec: 24388.17
Iteration:    680, Loss function: 7.334, Average Loss: 3.915, avg. samples / sec: 24372.56
Iteration:    680, Loss function: 7.290, Average Loss: 3.920, avg. samples / sec: 24341.67
Iteration:    680, Loss function: 7.431, Average Loss: 3.918, avg. samples / sec: 24339.01
Iteration:    680, Loss function: 7.830, Average Loss: 3.912, avg. samples / sec: 24383.09
Iteration:    680, Loss function: 7.398, Average Loss: 3.915, avg. samples / sec: 24341.93
Iteration:    680, Loss function: 7.195, Average Loss: 3.920, avg. samples / sec: 24305.91

:::MLPv0.5.0 ssd 1541756816.371788740 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 681, "value": 0.12106666666666667}

:::MLPv0.5.0 ssd 1541756816.455751181 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 682, "value": 0.12124444444444445}

:::MLPv0.5.0 ssd 1541756816.539739370 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 683, "value": 0.12142222222222222}

:::MLPv0.5.0 ssd 1541756816.624150038 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 684, "value": 0.1216}

:::MLPv0.5.0 ssd 1541756816.707948446 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 685, "value": 0.12177777777777778}

:::MLPv0.5.0 ssd 1541756816.792163134 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 686, "value": 0.12195555555555557}

:::MLPv0.5.0 ssd 1541756816.876171827 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 687, "value": 0.12213333333333334}

:::MLPv0.5.0 ssd 1541756816.959815025 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 688, "value": 0.12231111111111112}

:::MLPv0.5.0 ssd 1541756817.043843508 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 689, "value": 0.1224888888888889}

:::MLPv0.5.0 ssd 1541756817.127655983 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 690, "value": 0.12266666666666667}

:::MLPv0.5.0 ssd 1541756817.211943865 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 691, "value": 0.12284444444444445}

:::MLPv0.5.0 ssd 1541756817.296600342 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 692, "value": 0.12302222222222223}

:::MLPv0.5.0 ssd 1541756817.380675316 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 693, "value": 0.1232}

:::MLPv0.5.0 ssd 1541756817.461968422 (train.py:553) train_epoch: 12

:::MLPv0.5.0 ssd 1541756817.466119051 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 694, "value": 0.12337777777777778}

:::MLPv0.5.0 ssd 1541756817.550491571 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 695, "value": 0.12355555555555556}

:::MLPv0.5.0 ssd 1541756817.634729147 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 696, "value": 0.12373333333333333}

:::MLPv0.5.0 ssd 1541756817.718596697 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 697, "value": 0.12391111111111111}

:::MLPv0.5.0 ssd 1541756817.802778006 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 698, "value": 0.12408888888888889}

:::MLPv0.5.0 ssd 1541756817.886896610 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 699, "value": 0.12426666666666666}

:::MLPv0.5.0 ssd 1541756817.970639706 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 700, "value": 0.12444444444444444}
Iteration:    700, Loss function: 6.306, Average Loss: 3.963, avg. samples / sec: 24338.26
Iteration:    700, Loss function: 6.841, Average Loss: 3.975, avg. samples / sec: 24339.32
Iteration:    700, Loss function: 6.496, Average Loss: 3.979, avg. samples / sec: 24378.92
Iteration:    700, Loss function: 7.108, Average Loss: 3.975, avg. samples / sec: 24319.36
Iteration:    700, Loss function: 6.414, Average Loss: 3.977, avg. samples / sec: 24344.37
Iteration:    700, Loss function: 6.524, Average Loss: 3.979, avg. samples / sec: 24333.66
Iteration:    700, Loss function: 6.512, Average Loss: 3.975, avg. samples / sec: 24334.21
Iteration:    700, Loss function: 6.664, Average Loss: 3.970, avg. samples / sec: 24317.47

:::MLPv0.5.0 ssd 1541756818.054756880 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 701, "value": 0.12462222222222222}

:::MLPv0.5.0 ssd 1541756818.139152527 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 702, "value": 0.1248}

:::MLPv0.5.0 ssd 1541756818.223247766 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 703, "value": 0.12497777777777777}

:::MLPv0.5.0 ssd 1541756818.307072878 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 704, "value": 0.12515555555555555}

:::MLPv0.5.0 ssd 1541756818.391470671 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 705, "value": 0.12533333333333335}

:::MLPv0.5.0 ssd 1541756818.475622654 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 706, "value": 0.12551111111111113}

:::MLPv0.5.0 ssd 1541756818.559157133 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 707, "value": 0.1256888888888889}

:::MLPv0.5.0 ssd 1541756818.644041538 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 708, "value": 0.12586666666666668}

:::MLPv0.5.0 ssd 1541756818.727643967 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 709, "value": 0.12604444444444446}

:::MLPv0.5.0 ssd 1541756818.811026096 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 710, "value": 0.12622222222222224}

:::MLPv0.5.0 ssd 1541756818.895987272 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 711, "value": 0.1264}

:::MLPv0.5.0 ssd 1541756818.979862690 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 712, "value": 0.1265777777777778}

:::MLPv0.5.0 ssd 1541756819.063825130 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 713, "value": 0.12675555555555557}

:::MLPv0.5.0 ssd 1541756819.147887945 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 714, "value": 0.12693333333333334}

:::MLPv0.5.0 ssd 1541756819.232003927 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 715, "value": 0.12711111111111112}

:::MLPv0.5.0 ssd 1541756819.316063166 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 716, "value": 0.1272888888888889}

:::MLPv0.5.0 ssd 1541756819.400013208 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 717, "value": 0.12746666666666667}

:::MLPv0.5.0 ssd 1541756819.484086037 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 718, "value": 0.12764444444444445}

:::MLPv0.5.0 ssd 1541756819.568096399 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 719, "value": 0.12782222222222223}

:::MLPv0.5.0 ssd 1541756819.652257681 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 720, "value": 0.128}
Iteration:    720, Loss function: 6.560, Average Loss: 4.016, avg. samples / sec: 24359.54
Iteration:    720, Loss function: 6.496, Average Loss: 4.027, avg. samples / sec: 24361.10
Iteration:    720, Loss function: 6.349, Average Loss: 4.032, avg. samples / sec: 24375.03
Iteration:    720, Loss function: 6.277, Average Loss: 4.027, avg. samples / sec: 24373.30
Iteration:    720, Loss function: 6.189, Average Loss: 4.029, avg. samples / sec: 24370.58
Iteration:    720, Loss function: 6.068, Average Loss: 4.031, avg. samples / sec: 24368.99
Iteration:    720, Loss function: 6.498, Average Loss: 4.022, avg. samples / sec: 24380.24
Iteration:    720, Loss function: 5.657, Average Loss: 4.031, avg. samples / sec: 24358.12

:::MLPv0.5.0 ssd 1541756819.736438274 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 721, "value": 0.12817777777777778}

:::MLPv0.5.0 ssd 1541756819.820571423 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 722, "value": 0.12835555555555556}

:::MLPv0.5.0 ssd 1541756819.904905319 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 723, "value": 0.12853333333333333}

:::MLPv0.5.0 ssd 1541756819.988233328 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 724, "value": 0.1287111111111111}

:::MLPv0.5.0 ssd 1541756820.072084904 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 725, "value": 0.1288888888888889}

:::MLPv0.5.0 ssd 1541756820.156027555 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 726, "value": 0.12906666666666666}

:::MLPv0.5.0 ssd 1541756820.240159512 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 727, "value": 0.12924444444444444}

:::MLPv0.5.0 ssd 1541756820.324191809 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 728, "value": 0.12942222222222222}

:::MLPv0.5.0 ssd 1541756820.407891750 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 729, "value": 0.1296}

:::MLPv0.5.0 ssd 1541756820.492150307 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 730, "value": 0.12977777777777777}

:::MLPv0.5.0 ssd 1541756820.575834513 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 731, "value": 0.12995555555555555}

:::MLPv0.5.0 ssd 1541756820.660044909 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 732, "value": 0.13013333333333332}

:::MLPv0.5.0 ssd 1541756820.744044781 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 733, "value": 0.1303111111111111}

:::MLPv0.5.0 ssd 1541756820.828290462 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 734, "value": 0.13048888888888888}

:::MLPv0.5.0 ssd 1541756820.911734104 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 735, "value": 0.13066666666666665}

:::MLPv0.5.0 ssd 1541756820.995595455 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 736, "value": 0.13084444444444446}

:::MLPv0.5.0 ssd 1541756821.079492331 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 737, "value": 0.13102222222222223}

:::MLPv0.5.0 ssd 1541756821.163488150 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 738, "value": 0.1312}

:::MLPv0.5.0 ssd 1541756821.247764111 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 739, "value": 0.1313777777777778}

:::MLPv0.5.0 ssd 1541756821.334291697 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 740, "value": 0.13155555555555556}
Iteration:    740, Loss function: 5.921, Average Loss: 4.059, avg. samples / sec: 24348.01
Iteration:    740, Loss function: 5.872, Average Loss: 4.067, avg. samples / sec: 24350.48
Iteration:    740, Loss function: 5.714, Average Loss: 4.069, avg. samples / sec: 24356.49
Iteration:    740, Loss function: 5.355, Average Loss: 4.069, avg. samples / sec: 24351.93
Iteration:    740, Loss function: 5.977, Average Loss: 4.073, avg. samples / sec: 24372.87
Iteration:    740, Loss function: 6.061, Average Loss: 4.071, avg. samples / sec: 24345.45
Iteration:    740, Loss function: 5.713, Average Loss: 4.074, avg. samples / sec: 24303.78
Iteration:    740, Loss function: 6.024, Average Loss: 4.064, avg. samples / sec: 24335.50

:::MLPv0.5.0 ssd 1541756821.417816877 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 741, "value": 0.13173333333333334}

:::MLPv0.5.0 ssd 1541756821.501692057 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 742, "value": 0.13191111111111112}

:::MLPv0.5.0 ssd 1541756821.585705519 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 743, "value": 0.1320888888888889}

:::MLPv0.5.0 ssd 1541756821.669886112 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 744, "value": 0.13226666666666667}

:::MLPv0.5.0 ssd 1541756821.753955364 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 745, "value": 0.13244444444444445}

:::MLPv0.5.0 ssd 1541756821.837798834 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 746, "value": 0.13262222222222222}

:::MLPv0.5.0 ssd 1541756821.921984911 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 747, "value": 0.1328}

:::MLPv0.5.0 ssd 1541756822.005649805 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 748, "value": 0.13297777777777778}

:::MLPv0.5.0 ssd 1541756822.089374304 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 749, "value": 0.13315555555555555}

:::MLPv0.5.0 ssd 1541756822.172865391 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 750, "value": 0.13333333333333333}

:::MLPv0.5.0 ssd 1541756822.254463196 (train.py:553) train_epoch: 13

:::MLPv0.5.0 ssd 1541756822.258640051 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 751, "value": 0.1335111111111111}

:::MLPv0.5.0 ssd 1541756822.342847824 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 752, "value": 0.13368888888888888}

:::MLPv0.5.0 ssd 1541756822.426573992 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 753, "value": 0.13386666666666666}

:::MLPv0.5.0 ssd 1541756822.510866404 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 754, "value": 0.13404444444444444}

:::MLPv0.5.0 ssd 1541756822.594917774 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 755, "value": 0.13422222222222221}

:::MLPv0.5.0 ssd 1541756822.678858519 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 756, "value": 0.1344}

:::MLPv0.5.0 ssd 1541756822.762705326 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 757, "value": 0.13457777777777777}

:::MLPv0.5.0 ssd 1541756822.846776485 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 758, "value": 0.13475555555555557}

:::MLPv0.5.0 ssd 1541756822.930958509 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 759, "value": 0.13493333333333335}

:::MLPv0.5.0 ssd 1541756823.014689684 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 760, "value": 0.13511111111111113}
Iteration:    760, Loss function: 5.795, Average Loss: 4.097, avg. samples / sec: 24381.17
Iteration:    760, Loss function: 5.676, Average Loss: 4.103, avg. samples / sec: 24371.93
Iteration:    760, Loss function: 5.593, Average Loss: 4.109, avg. samples / sec: 24406.72
Iteration:    760, Loss function: 5.674, Average Loss: 4.106, avg. samples / sec: 24375.38
Iteration:    760, Loss function: 5.475, Average Loss: 4.110, avg. samples / sec: 24397.90
Iteration:    760, Loss function: 5.195, Average Loss: 4.103, avg. samples / sec: 24356.54
Iteration:    760, Loss function: 5.612, Average Loss: 4.111, avg. samples / sec: 24358.40
Iteration:    760, Loss function: 5.874, Average Loss: 4.101, avg. samples / sec: 24382.23

:::MLPv0.5.0 ssd 1541756823.098829985 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 761, "value": 0.1352888888888889}

:::MLPv0.5.0 ssd 1541756823.182718277 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 762, "value": 0.13546666666666668}

:::MLPv0.5.0 ssd 1541756823.266695023 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 763, "value": 0.13564444444444446}

:::MLPv0.5.0 ssd 1541756823.350526094 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 764, "value": 0.13582222222222223}

:::MLPv0.5.0 ssd 1541756823.434501410 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 765, "value": 0.136}

:::MLPv0.5.0 ssd 1541756823.518267393 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 766, "value": 0.1361777777777778}

:::MLPv0.5.0 ssd 1541756823.602925777 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 767, "value": 0.13635555555555556}

:::MLPv0.5.0 ssd 1541756823.687286615 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 768, "value": 0.13653333333333334}

:::MLPv0.5.0 ssd 1541756823.771618843 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 769, "value": 0.13671111111111112}

:::MLPv0.5.0 ssd 1541756823.855938196 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 770, "value": 0.1368888888888889}

:::MLPv0.5.0 ssd 1541756823.940248251 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 771, "value": 0.13706666666666667}

:::MLPv0.5.0 ssd 1541756824.024201632 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 772, "value": 0.13724444444444445}

:::MLPv0.5.0 ssd 1541756824.108410597 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 773, "value": 0.13742222222222222}

:::MLPv0.5.0 ssd 1541756824.192617416 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 774, "value": 0.1376}

:::MLPv0.5.0 ssd 1541756824.276313305 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 775, "value": 0.13777777777777778}

:::MLPv0.5.0 ssd 1541756824.360469818 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 776, "value": 0.13795555555555555}

:::MLPv0.5.0 ssd 1541756824.444367886 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 777, "value": 0.13813333333333333}

:::MLPv0.5.0 ssd 1541756824.528144836 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 778, "value": 0.1383111111111111}

:::MLPv0.5.0 ssd 1541756824.612333298 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 779, "value": 0.13848888888888888}

:::MLPv0.5.0 ssd 1541756824.697148561 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 780, "value": 0.13866666666666666}
Iteration:    780, Loss function: 5.551, Average Loss: 4.131, avg. samples / sec: 24345.07
Iteration:    780, Loss function: 5.917, Average Loss: 4.141, avg. samples / sec: 24395.74
Iteration:    780, Loss function: 5.860, Average Loss: 4.139, avg. samples / sec: 24347.40
Iteration:    780, Loss function: 5.801, Average Loss: 4.142, avg. samples / sec: 24346.64
Iteration:    780, Loss function: 5.952, Average Loss: 4.132, avg. samples / sec: 24388.97
Iteration:    780, Loss function: 5.982, Average Loss: 4.142, avg. samples / sec: 24366.32
Iteration:    780, Loss function: 5.858, Average Loss: 4.139, avg. samples / sec: 24338.01
Iteration:    780, Loss function: 5.574, Average Loss: 4.136, avg. samples / sec: 24349.32

:::MLPv0.5.0 ssd 1541756824.781513929 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 781, "value": 0.13884444444444444}

:::MLPv0.5.0 ssd 1541756824.865469456 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 782, "value": 0.1390222222222222}

:::MLPv0.5.0 ssd 1541756824.949529648 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 783, "value": 0.1392}

:::MLPv0.5.0 ssd 1541756825.033449411 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 784, "value": 0.13937777777777777}

:::MLPv0.5.0 ssd 1541756825.117623568 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 785, "value": 0.13955555555555554}

:::MLPv0.5.0 ssd 1541756825.201457262 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 786, "value": 0.13973333333333332}

:::MLPv0.5.0 ssd 1541756825.286260128 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 787, "value": 0.13991111111111112}

:::MLPv0.5.0 ssd 1541756825.369623184 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 788, "value": 0.1400888888888889}

:::MLPv0.5.0 ssd 1541756825.453949451 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 789, "value": 0.14026666666666668}

:::MLPv0.5.0 ssd 1541756825.538069248 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 790, "value": 0.14044444444444446}

:::MLPv0.5.0 ssd 1541756825.622807741 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 791, "value": 0.14062222222222223}

:::MLPv0.5.0 ssd 1541756825.706645489 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 792, "value": 0.1408}

:::MLPv0.5.0 ssd 1541756825.791030884 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 793, "value": 0.14097777777777779}

:::MLPv0.5.0 ssd 1541756825.874843836 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 794, "value": 0.14115555555555556}

:::MLPv0.5.0 ssd 1541756825.958964586 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 795, "value": 0.14133333333333334}

:::MLPv0.5.0 ssd 1541756826.043009758 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 796, "value": 0.14151111111111112}

:::MLPv0.5.0 ssd 1541756826.127161026 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 797, "value": 0.1416888888888889}

:::MLPv0.5.0 ssd 1541756826.210988045 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 798, "value": 0.14186666666666667}

:::MLPv0.5.0 ssd 1541756826.295539379 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 799, "value": 0.14204444444444445}

:::MLPv0.5.0 ssd 1541756826.379669189 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 800, "value": 0.14222222222222222}
Iteration:    800, Loss function: 5.348, Average Loss: 4.177, avg. samples / sec: 24354.61
Iteration:    800, Loss function: 5.716, Average Loss: 4.174, avg. samples / sec: 24347.15
Iteration:    800, Loss function: 5.649, Average Loss: 4.174, avg. samples / sec: 24350.71
Iteration:    800, Loss function: 5.959, Average Loss: 4.175, avg. samples / sec: 24341.14
Iteration:    800, Loss function: 5.541, Average Loss: 4.172, avg. samples / sec: 24335.13
Iteration:    800, Loss function: 5.844, Average Loss: 4.167, avg. samples / sec: 24349.14
Iteration:    800, Loss function: 4.994, Average Loss: 4.166, avg. samples / sec: 24313.14
Iteration:    800, Loss function: 5.990, Average Loss: 4.165, avg. samples / sec: 24290.52

:::MLPv0.5.0 ssd 1541756826.465110779 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 801, "value": 0.1424}

:::MLPv0.5.0 ssd 1541756826.548626184 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 802, "value": 0.14257777777777778}

:::MLPv0.5.0 ssd 1541756826.632645130 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 803, "value": 0.14275555555555555}

:::MLPv0.5.0 ssd 1541756826.716767311 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 804, "value": 0.14293333333333333}

:::MLPv0.5.0 ssd 1541756826.800758362 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 805, "value": 0.1431111111111111}

:::MLPv0.5.0 ssd 1541756826.884840488 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 806, "value": 0.14328888888888888}

:::MLPv0.5.0 ssd 1541756826.969039202 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 807, "value": 0.14346666666666666}

:::MLPv0.5.0 ssd 1541756827.052788734 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 808, "value": 0.14364444444444444}

:::MLPv0.5.0 ssd 1541756827.134669065 (train.py:553) train_epoch: 14

:::MLPv0.5.0 ssd 1541756827.138823748 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 809, "value": 0.14382222222222224}

:::MLPv0.5.0 ssd 1541756827.222995281 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 810, "value": 0.14400000000000002}

:::MLPv0.5.0 ssd 1541756827.307447910 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 811, "value": 0.1441777777777778}

:::MLPv0.5.0 ssd 1541756827.391645193 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 812, "value": 0.14435555555555557}

:::MLPv0.5.0 ssd 1541756827.475455523 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 813, "value": 0.14453333333333335}

:::MLPv0.5.0 ssd 1541756827.559227705 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 814, "value": 0.14471111111111112}

:::MLPv0.5.0 ssd 1541756827.643225431 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 815, "value": 0.1448888888888889}

:::MLPv0.5.0 ssd 1541756827.727354527 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 816, "value": 0.14506666666666668}

:::MLPv0.5.0 ssd 1541756827.811162949 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 817, "value": 0.14524444444444445}

:::MLPv0.5.0 ssd 1541756827.895379305 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 818, "value": 0.14542222222222223}

:::MLPv0.5.0 ssd 1541756827.979139805 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 819, "value": 0.1456}

:::MLPv0.5.0 ssd 1541756828.063240051 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 820, "value": 0.14577777777777778}
Iteration:    820, Loss function: 5.043, Average Loss: 4.204, avg. samples / sec: 24329.84
Iteration:    820, Loss function: 5.853, Average Loss: 4.201, avg. samples / sec: 24330.01
Iteration:    820, Loss function: 5.483, Average Loss: 4.189, avg. samples / sec: 24359.49
Iteration:    820, Loss function: 5.972, Average Loss: 4.201, avg. samples / sec: 24316.25
Iteration:    820, Loss function: 5.266, Average Loss: 4.193, avg. samples / sec: 24352.08
Iteration:    820, Loss function: 5.612, Average Loss: 4.194, avg. samples / sec: 24327.89
Iteration:    820, Loss function: 5.535, Average Loss: 4.199, avg. samples / sec: 24319.73
Iteration:    820, Loss function: 5.196, Average Loss: 4.201, avg. samples / sec: 24287.92

:::MLPv0.5.0 ssd 1541756828.147184134 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 821, "value": 0.14595555555555556}

:::MLPv0.5.0 ssd 1541756828.231442690 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 822, "value": 0.14613333333333334}

:::MLPv0.5.0 ssd 1541756828.315880299 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 823, "value": 0.14631111111111111}

:::MLPv0.5.0 ssd 1541756828.399997950 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 824, "value": 0.1464888888888889}

:::MLPv0.5.0 ssd 1541756828.483979464 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 825, "value": 0.14666666666666667}

:::MLPv0.5.0 ssd 1541756828.568216801 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 826, "value": 0.14684444444444444}

:::MLPv0.5.0 ssd 1541756828.652548552 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 827, "value": 0.14702222222222222}

:::MLPv0.5.0 ssd 1541756828.736559391 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 828, "value": 0.1472}

:::MLPv0.5.0 ssd 1541756828.820753813 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 829, "value": 0.14737777777777777}

:::MLPv0.5.0 ssd 1541756828.905065060 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 830, "value": 0.14755555555555555}

:::MLPv0.5.0 ssd 1541756828.989217758 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 831, "value": 0.14773333333333333}

:::MLPv0.5.0 ssd 1541756829.073330641 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 832, "value": 0.1479111111111111}

:::MLPv0.5.0 ssd 1541756829.157867193 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 833, "value": 0.14808888888888888}

:::MLPv0.5.0 ssd 1541756829.241868258 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 834, "value": 0.14826666666666666}

:::MLPv0.5.0 ssd 1541756829.325939894 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 835, "value": 0.14844444444444443}

:::MLPv0.5.0 ssd 1541756829.410595179 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 836, "value": 0.1486222222222222}

:::MLPv0.5.0 ssd 1541756829.494386673 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 837, "value": 0.14880000000000002}

:::MLPv0.5.0 ssd 1541756829.578366756 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 838, "value": 0.1489777777777778}

:::MLPv0.5.0 ssd 1541756829.662349463 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 839, "value": 0.14915555555555557}

:::MLPv0.5.0 ssd 1541756829.746431589 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 840, "value": 0.14933333333333335}
Iteration:    840, Loss function: 5.840, Average Loss: 4.234, avg. samples / sec: 24335.91
Iteration:    840, Loss function: 6.003, Average Loss: 4.232, avg. samples / sec: 24335.56
Iteration:    840, Loss function: 6.506, Average Loss: 4.231, avg. samples / sec: 24343.22
Iteration:    840, Loss function: 5.490, Average Loss: 4.223, avg. samples / sec: 24353.08
Iteration:    840, Loss function: 5.818, Average Loss: 4.230, avg. samples / sec: 24344.61
Iteration:    840, Loss function: 5.962, Average Loss: 4.234, avg. samples / sec: 24338.10
Iteration:    840, Loss function: 5.111, Average Loss: 4.221, avg. samples / sec: 24297.07
Iteration:    840, Loss function: 6.288, Average Loss: 4.225, avg. samples / sec: 24312.47

:::MLPv0.5.0 ssd 1541756829.830697298 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 841, "value": 0.14951111111111112}

:::MLPv0.5.0 ssd 1541756829.914528370 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 842, "value": 0.1496888888888889}

:::MLPv0.5.0 ssd 1541756829.998425245 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 843, "value": 0.14986666666666668}

:::MLPv0.5.0 ssd 1541756830.082824945 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 844, "value": 0.15004444444444445}

:::MLPv0.5.0 ssd 1541756830.166890383 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 845, "value": 0.15022222222222223}

:::MLPv0.5.0 ssd 1541756830.251328230 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 846, "value": 0.1504}

:::MLPv0.5.0 ssd 1541756830.335530043 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 847, "value": 0.15057777777777778}

:::MLPv0.5.0 ssd 1541756830.419452429 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 848, "value": 0.15075555555555556}

:::MLPv0.5.0 ssd 1541756830.504772663 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 849, "value": 0.15093333333333334}

:::MLPv0.5.0 ssd 1541756830.588922977 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 850, "value": 0.1511111111111111}

:::MLPv0.5.0 ssd 1541756830.673364162 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 851, "value": 0.1512888888888889}

:::MLPv0.5.0 ssd 1541756830.757415056 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 852, "value": 0.15146666666666667}

:::MLPv0.5.0 ssd 1541756830.841575861 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 853, "value": 0.15164444444444444}

:::MLPv0.5.0 ssd 1541756830.925844908 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 854, "value": 0.15182222222222222}

:::MLPv0.5.0 ssd 1541756831.010519743 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 855, "value": 0.152}

:::MLPv0.5.0 ssd 1541756831.094021082 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 856, "value": 0.15217777777777777}

:::MLPv0.5.0 ssd 1541756831.177800417 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 857, "value": 0.15235555555555555}

:::MLPv0.5.0 ssd 1541756831.262455702 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 858, "value": 0.15253333333333333}

:::MLPv0.5.0 ssd 1541756831.346797943 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 859, "value": 0.1527111111111111}

:::MLPv0.5.0 ssd 1541756831.431029797 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 860, "value": 0.15288888888888888}
Iteration:    860, Loss function: 5.256, Average Loss: 4.260, avg. samples / sec: 24315.71
Iteration:    860, Loss function: 5.355, Average Loss: 4.251, avg. samples / sec: 24300.86
Iteration:    860, Loss function: 5.357, Average Loss: 4.255, avg. samples / sec: 24314.76
Iteration:    860, Loss function: 5.354, Average Loss: 4.260, avg. samples / sec: 24284.84
Iteration:    860, Loss function: 5.285, Average Loss: 4.260, avg. samples / sec: 24330.13
Iteration:    860, Loss function: 5.658, Average Loss: 4.250, avg. samples / sec: 24334.12
Iteration:    860, Loss function: 5.418, Average Loss: 4.249, avg. samples / sec: 24320.77
Iteration:    860, Loss function: 5.470, Average Loss: 4.256, avg. samples / sec: 24272.60

:::MLPv0.5.0 ssd 1541756831.514771461 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 861, "value": 0.15306666666666666}

:::MLPv0.5.0 ssd 1541756831.599237680 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 862, "value": 0.15324444444444446}

:::MLPv0.5.0 ssd 1541756831.683327913 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 863, "value": 0.15342222222222224}

:::MLPv0.5.0 ssd 1541756831.767284393 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 864, "value": 0.15360000000000001}

:::MLPv0.5.0 ssd 1541756831.851619005 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 865, "value": 0.1537777777777778}

:::MLPv0.5.0 ssd 1541756831.936023474 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 866, "value": 0.15395555555555557}

:::MLPv0.5.0 ssd 1541756832.017813683 (train.py:553) train_epoch: 15

:::MLPv0.5.0 ssd 1541756832.022000790 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 867, "value": 0.15413333333333334}

:::MLPv0.5.0 ssd 1541756832.105770826 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 868, "value": 0.15431111111111112}

:::MLPv0.5.0 ssd 1541756832.189446449 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 869, "value": 0.1544888888888889}

:::MLPv0.5.0 ssd 1541756832.273980141 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 870, "value": 0.15466666666666667}

:::MLPv0.5.0 ssd 1541756832.358213425 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 871, "value": 0.15484444444444445}

:::MLPv0.5.0 ssd 1541756832.442053080 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 872, "value": 0.15502222222222223}

:::MLPv0.5.0 ssd 1541756832.526373148 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 873, "value": 0.1552}

:::MLPv0.5.0 ssd 1541756832.610419273 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 874, "value": 0.15537777777777778}

:::MLPv0.5.0 ssd 1541756832.694658518 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 875, "value": 0.15555555555555556}

:::MLPv0.5.0 ssd 1541756832.779263496 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 876, "value": 0.15573333333333333}

:::MLPv0.5.0 ssd 1541756832.863263607 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 877, "value": 0.1559111111111111}

:::MLPv0.5.0 ssd 1541756832.947139740 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 878, "value": 0.1560888888888889}

:::MLPv0.5.0 ssd 1541756833.031214952 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 879, "value": 0.15626666666666666}

:::MLPv0.5.0 ssd 1541756833.115329504 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 880, "value": 0.15644444444444444}
Iteration:    880, Loss function: 6.121, Average Loss: 4.284, avg. samples / sec: 24318.61
Iteration:    880, Loss function: 6.352, Average Loss: 4.282, avg. samples / sec: 24348.23
Iteration:    880, Loss function: 5.701, Average Loss: 4.276, avg. samples / sec: 24328.56
Iteration:    880, Loss function: 5.023, Average Loss: 4.272, avg. samples / sec: 24320.03
Iteration:    880, Loss function: 5.397, Average Loss: 4.277, avg. samples / sec: 24327.29
Iteration:    880, Loss function: 5.616, Average Loss: 4.283, avg. samples / sec: 24306.00
Iteration:    880, Loss function: 5.942, Average Loss: 4.276, avg. samples / sec: 24318.08
Iteration:    880, Loss function: 5.888, Average Loss: 4.283, avg. samples / sec: 24296.31

:::MLPv0.5.0 ssd 1541756833.199422121 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 881, "value": 0.15662222222222222}

:::MLPv0.5.0 ssd 1541756833.283614159 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 882, "value": 0.1568}

:::MLPv0.5.0 ssd 1541756833.367508173 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 883, "value": 0.15697777777777777}

:::MLPv0.5.0 ssd 1541756833.451425552 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 884, "value": 0.15715555555555555}

:::MLPv0.5.0 ssd 1541756833.535839796 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 885, "value": 0.15733333333333333}

:::MLPv0.5.0 ssd 1541756833.619845867 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 886, "value": 0.1575111111111111}

:::MLPv0.5.0 ssd 1541756833.703890800 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 887, "value": 0.15768888888888888}

:::MLPv0.5.0 ssd 1541756833.787876844 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 888, "value": 0.15786666666666668}

:::MLPv0.5.0 ssd 1541756833.872204065 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 889, "value": 0.15804444444444446}

:::MLPv0.5.0 ssd 1541756833.956448793 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 890, "value": 0.15822222222222224}

:::MLPv0.5.0 ssd 1541756834.040890932 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 891, "value": 0.1584}

:::MLPv0.5.0 ssd 1541756834.124734879 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 892, "value": 0.1585777777777778}

:::MLPv0.5.0 ssd 1541756834.208204746 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 893, "value": 0.15875555555555557}

:::MLPv0.5.0 ssd 1541756834.291990280 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 894, "value": 0.15893333333333334}

:::MLPv0.5.0 ssd 1541756834.376163721 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 895, "value": 0.15911111111111112}

:::MLPv0.5.0 ssd 1541756834.461417437 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 896, "value": 0.1592888888888889}

:::MLPv0.5.0 ssd 1541756834.544901609 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 897, "value": 0.15946666666666667}

:::MLPv0.5.0 ssd 1541756834.628628016 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 898, "value": 0.15964444444444445}

:::MLPv0.5.0 ssd 1541756834.712856054 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 899, "value": 0.15982222222222223}
Iteration:    900, Loss function: 5.156, Average Loss: 4.307, avg. samples / sec: 24378.75
Iteration:    900, Loss function: 5.898, Average Loss: 4.307, avg. samples / sec: 24423.66
Iteration:    900, Loss function: 5.593, Average Loss: 4.308, avg. samples / sec: 24432.49
Iteration:    900, Loss function: 4.782, Average Loss: 4.299, avg. samples / sec: 24396.00
Iteration:    900, Loss function: 4.927, Average Loss: 4.296, avg. samples / sec: 24384.81
Iteration:    900, Loss function: 6.041, Average Loss: 4.309, avg. samples / sec: 24341.83
Iteration:    900, Loss function: 5.346, Average Loss: 4.301, avg. samples / sec: 24381.88
Iteration:    900, Loss function: 5.233, Average Loss: 4.299, avg. samples / sec: 24364.17
Iteration:    920, Loss function: 5.238, Average Loss: 4.327, avg. samples / sec: 24582.92
Iteration:    920, Loss function: 5.380, Average Loss: 4.319, avg. samples / sec: 24579.42
Iteration:    920, Loss function: 4.710, Average Loss: 4.316, avg. samples / sec: 24595.18
Iteration:    920, Loss function: 4.749, Average Loss: 4.329, avg. samples / sec: 24587.58
Iteration:    920, Loss function: 5.361, Average Loss: 4.326, avg. samples / sec: 24538.50
Iteration:    920, Loss function: 4.938, Average Loss: 4.320, avg. samples / sec: 24582.70
Iteration:    920, Loss function: 5.100, Average Loss: 4.328, avg. samples / sec: 24534.09
Iteration:    920, Loss function: 5.213, Average Loss: 4.320, avg. samples / sec: 24586.17

:::MLPv0.5.0 ssd 1541756836.876514196 (train.py:553) train_epoch: 16
Iteration:    940, Loss function: 5.955, Average Loss: 4.346, avg. samples / sec: 24553.37
Iteration:    940, Loss function: 5.691, Average Loss: 4.333, avg. samples / sec: 24554.54
Iteration:    940, Loss function: 5.155, Average Loss: 4.338, avg. samples / sec: 24573.83
Iteration:    940, Loss function: 5.537, Average Loss: 4.346, avg. samples / sec: 24557.80
Iteration:    940, Loss function: 5.338, Average Loss: 4.343, avg. samples / sec: 24560.84
Iteration:    940, Loss function: 5.701, Average Loss: 4.337, avg. samples / sec: 24568.82
Iteration:    940, Loss function: 5.532, Average Loss: 4.344, avg. samples / sec: 24553.44
Iteration:    940, Loss function: 5.473, Average Loss: 4.334, avg. samples / sec: 24504.50
Iteration:    960, Loss function: 5.188, Average Loss: 4.364, avg. samples / sec: 24578.58
Iteration:    960, Loss function: 4.984, Average Loss: 4.366, avg. samples / sec: 24536.44
Iteration:    960, Loss function: 5.014, Average Loss: 4.353, avg. samples / sec: 24545.39
Iteration:    960, Loss function: 4.964, Average Loss: 4.363, avg. samples / sec: 24549.06
Iteration:    960, Loss function: 5.103, Average Loss: 4.354, avg. samples / sec: 24574.26
Iteration:    960, Loss function: 5.308, Average Loss: 4.355, avg. samples / sec: 24552.40
Iteration:    960, Loss function: 4.916, Average Loss: 4.362, avg. samples / sec: 24558.04
Iteration:    960, Loss function: 5.215, Average Loss: 4.359, avg. samples / sec: 24522.66
Iteration:    980, Loss function: 5.467, Average Loss: 4.379, avg. samples / sec: 24576.77
Iteration:    980, Loss function: 5.182, Average Loss: 4.377, avg. samples / sec: 24572.06
Iteration:    980, Loss function: 4.557, Average Loss: 4.366, avg. samples / sec: 24573.50
Iteration:    980, Loss function: 5.180, Average Loss: 4.376, avg. samples / sec: 24579.75
Iteration:    980, Loss function: 5.356, Average Loss: 4.378, avg. samples / sec: 24577.83
Iteration:    980, Loss function: 5.191, Average Loss: 4.370, avg. samples / sec: 24569.52
Iteration:    980, Loss function: 5.048, Average Loss: 4.372, avg. samples / sec: 24551.07
Iteration:    980, Loss function: 4.846, Average Loss: 4.373, avg. samples / sec: 24556.43

:::MLPv0.5.0 ssd 1541756841.630216599 (train.py:553) train_epoch: 17
Iteration:   1000, Loss function: 5.139, Average Loss: 4.392, avg. samples / sec: 24519.89
Iteration:   1000, Loss function: 4.607, Average Loss: 4.379, avg. samples / sec: 24514.88
Iteration:   1000, Loss function: 5.307, Average Loss: 4.391, avg. samples / sec: 24527.30
Iteration:   1000, Loss function: 4.818, Average Loss: 4.387, avg. samples / sec: 24498.31
Iteration:   1000, Loss function: 4.965, Average Loss: 4.383, avg. samples / sec: 24515.57
Iteration:   1000, Loss function: 4.779, Average Loss: 4.385, avg. samples / sec: 24529.30
Iteration:   1000, Loss function: 5.113, Average Loss: 4.387, avg. samples / sec: 24475.25
Iteration:   1000, Loss function: 5.003, Average Loss: 4.385, avg. samples / sec: 24521.69
Iteration:   1020, Loss function: 3.982, Average Loss: 4.395, avg. samples / sec: 24598.50
Iteration:   1020, Loss function: 5.142, Average Loss: 4.408, avg. samples / sec: 24554.32
Iteration:   1020, Loss function: 4.676, Average Loss: 4.393, avg. samples / sec: 24549.94
Iteration:   1020, Loss function: 5.379, Average Loss: 4.400, avg. samples / sec: 24579.70
Iteration:   1020, Loss function: 5.416, Average Loss: 4.403, avg. samples / sec: 24545.24
Iteration:   1020, Loss function: 5.261, Average Loss: 4.400, avg. samples / sec: 24556.85
Iteration:   1020, Loss function: 5.325, Average Loss: 4.401, avg. samples / sec: 24565.41
Iteration:   1020, Loss function: 5.316, Average Loss: 4.399, avg. samples / sec: 24548.40

:::MLPv0.5.0 ssd 1541756846.471084833 (train.py:553) train_epoch: 18
Iteration:   1040, Loss function: 5.046, Average Loss: 4.421, avg. samples / sec: 24537.46
Iteration:   1040, Loss function: 5.070, Average Loss: 4.410, avg. samples / sec: 24529.09
Iteration:   1040, Loss function: 4.808, Average Loss: 4.411, avg. samples / sec: 24598.12
Iteration:   1040, Loss function: 4.564, Average Loss: 4.405, avg. samples / sec: 24531.53
Iteration:   1040, Loss function: 5.135, Average Loss: 4.412, avg. samples / sec: 24544.05
Iteration:   1040, Loss function: 5.005, Average Loss: 4.414, avg. samples / sec: 24526.87
Iteration:   1040, Loss function: 5.450, Average Loss: 4.416, avg. samples / sec: 24498.95
Iteration:   1040, Loss function: 5.348, Average Loss: 4.415, avg. samples / sec: 24485.54
Iteration:   1060, Loss function: 4.556, Average Loss: 4.414, avg. samples / sec: 24526.96
Iteration:   1060, Loss function: 4.927, Average Loss: 4.421, avg. samples / sec: 24515.05
Iteration:   1060, Loss function: 5.513, Average Loss: 4.430, avg. samples / sec: 24486.51
Iteration:   1060, Loss function: 4.662, Average Loss: 4.423, avg. samples / sec: 24526.61
Iteration:   1060, Loss function: 4.600, Average Loss: 4.423, avg. samples / sec: 24553.25
Iteration:   1060, Loss function: 4.607, Average Loss: 4.422, avg. samples / sec: 24504.88
Iteration:   1060, Loss function: 4.737, Average Loss: 4.422, avg. samples / sec: 24543.23
Iteration:   1060, Loss function: 5.006, Average Loss: 4.417, avg. samples / sec: 24457.83
Iteration:   1080, Loss function: 5.070, Average Loss: 4.441, avg. samples / sec: 24553.34
Iteration:   1080, Loss function: 4.922, Average Loss: 4.423, avg. samples / sec: 24524.67
Iteration:   1080, Loss function: 5.501, Average Loss: 4.432, avg. samples / sec: 24556.12
Iteration:   1080, Loss function: 5.145, Average Loss: 4.432, avg. samples / sec: 24525.34
Iteration:   1080, Loss function: 4.915, Average Loss: 4.432, avg. samples / sec: 24483.79
Iteration:   1080, Loss function: 4.503, Average Loss: 4.430, avg. samples / sec: 24510.15
Iteration:   1080, Loss function: 4.824, Average Loss: 4.427, avg. samples / sec: 24531.17
Iteration:   1080, Loss function: 4.911, Average Loss: 4.432, avg. samples / sec: 24502.45

:::MLPv0.5.0 ssd 1541756851.312771082 (train.py:553) train_epoch: 19
Iteration:   1100, Loss function: 4.926, Average Loss: 4.433, avg. samples / sec: 24540.34
Iteration:   1100, Loss function: 5.063, Average Loss: 4.449, avg. samples / sec: 24525.29
Iteration:   1100, Loss function: 4.826, Average Loss: 4.441, avg. samples / sec: 24572.71
Iteration:   1100, Loss function: 4.465, Average Loss: 4.442, avg. samples / sec: 24546.03
Iteration:   1100, Loss function: 4.883, Average Loss: 4.440, avg. samples / sec: 24564.84
Iteration:   1100, Loss function: 4.868, Average Loss: 4.436, avg. samples / sec: 24546.65
Iteration:   1100, Loss function: 5.194, Average Loss: 4.442, avg. samples / sec: 24503.13
Iteration:   1100, Loss function: 5.623, Average Loss: 4.443, avg. samples / sec: 24528.38
Iteration:   1120, Loss function: 4.538, Average Loss: 4.447, avg. samples / sec: 24590.64
Iteration:   1120, Loss function: 4.039, Average Loss: 4.440, avg. samples / sec: 24558.55
Iteration:   1120, Loss function: 4.843, Average Loss: 4.448, avg. samples / sec: 24572.21
Iteration:   1120, Loss function: 5.251, Average Loss: 4.442, avg. samples / sec: 24602.60
Iteration:   1120, Loss function: 5.154, Average Loss: 4.454, avg. samples / sec: 24591.69
Iteration:   1120, Loss function: 4.879, Average Loss: 4.459, avg. samples / sec: 24549.25
Iteration:   1120, Loss function: 4.870, Average Loss: 4.450, avg. samples / sec: 24560.45
Iteration:   1120, Loss function: 4.187, Average Loss: 4.449, avg. samples / sec: 24560.04
Iteration:   1140, Loss function: 4.991, Average Loss: 4.450, avg. samples / sec: 24587.67
Iteration:   1140, Loss function: 5.496, Average Loss: 4.460, avg. samples / sec: 24581.88
Iteration:   1140, Loss function: 5.432, Average Loss: 4.465, avg. samples / sec: 24586.93
Iteration:   1140, Loss function: 5.265, Average Loss: 4.472, avg. samples / sec: 24583.26
Iteration:   1140, Loss function: 5.318, Average Loss: 4.462, avg. samples / sec: 24580.98
Iteration:   1140, Loss function: 5.017, Average Loss: 4.458, avg. samples / sec: 24554.33
Iteration:   1140, Loss function: 5.051, Average Loss: 4.456, avg. samples / sec: 24555.53
Iteration:   1140, Loss function: 4.856, Average Loss: 4.459, avg. samples / sec: 24569.80

:::MLPv0.5.0 ssd 1541756856.149725437 (train.py:553) train_epoch: 20
Iteration:   1160, Loss function: 4.893, Average Loss: 4.468, avg. samples / sec: 24575.95
Iteration:   1160, Loss function: 4.869, Average Loss: 4.459, avg. samples / sec: 24544.84
Iteration:   1160, Loss function: 5.185, Average Loss: 4.469, avg. samples / sec: 24547.76
Iteration:   1160, Loss function: 4.773, Average Loss: 4.462, avg. samples / sec: 24571.25
Iteration:   1160, Loss function: 4.709, Average Loss: 4.479, avg. samples / sec: 24550.46
Iteration:   1160, Loss function: 5.201, Average Loss: 4.470, avg. samples / sec: 24537.89
Iteration:   1160, Loss function: 4.643, Average Loss: 4.466, avg. samples / sec: 24539.96
Iteration:   1160, Loss function: 5.104, Average Loss: 4.468, avg. samples / sec: 24555.41
Iteration:   1180, Loss function: 4.466, Average Loss: 4.466, avg. samples / sec: 24609.23
Iteration:   1180, Loss function: 4.797, Average Loss: 4.478, avg. samples / sec: 24605.09
Iteration:   1180, Loss function: 4.990, Average Loss: 4.475, avg. samples / sec: 24636.01
Iteration:   1180, Loss function: 4.960, Average Loss: 4.477, avg. samples / sec: 24602.50
Iteration:   1180, Loss function: 4.707, Average Loss: 4.471, avg. samples / sec: 24628.14
Iteration:   1180, Loss function: 5.402, Average Loss: 4.487, avg. samples / sec: 24593.30
Iteration:   1180, Loss function: 4.094, Average Loss: 4.474, avg. samples / sec: 24569.52
Iteration:   1180, Loss function: 4.979, Average Loss: 4.468, avg. samples / sec: 24568.18
Iteration:   1200, Loss function: 4.099, Average Loss: 4.468, avg. samples / sec: 24629.90
Iteration:   1200, Loss function: 5.017, Average Loss: 4.489, avg. samples / sec: 24661.47
Iteration:   1200, Loss function: 5.030, Average Loss: 4.478, avg. samples / sec: 24630.74
Iteration:   1200, Loss function: 5.024, Average Loss: 4.479, avg. samples / sec: 24635.66
Iteration:   1200, Loss function: 4.703, Average Loss: 4.481, avg. samples / sec: 24631.36
Iteration:   1200, Loss function: 5.054, Average Loss: 4.482, avg. samples / sec: 24588.96
Iteration:   1200, Loss function: 4.294, Average Loss: 4.474, avg. samples / sec: 24600.58
Iteration:   1200, Loss function: 4.801, Average Loss: 4.474, avg. samples / sec: 24612.16

:::MLPv0.5.0 ssd 1541756860.895494461 (train.py:553) train_epoch: 21
Iteration:   1220, Loss function: 4.670, Average Loss: 4.496, avg. samples / sec: 24570.99
Iteration:   1220, Loss function: 4.683, Average Loss: 4.486, avg. samples / sec: 24570.58
Iteration:   1220, Loss function: 4.373, Average Loss: 4.475, avg. samples / sec: 24560.87
Iteration:   1220, Loss function: 4.383, Average Loss: 4.478, avg. samples / sec: 24605.45
Iteration:   1220, Loss function: 4.688, Average Loss: 4.485, avg. samples / sec: 24564.82
Iteration:   1220, Loss function: 4.976, Average Loss: 4.489, avg. samples / sec: 24561.79
Iteration:   1220, Loss function: 5.103, Average Loss: 4.483, avg. samples / sec: 24588.80
Iteration:   1220, Loss function: 4.685, Average Loss: 4.488, avg. samples / sec: 24557.24
Iteration:   1240, Loss function: 5.007, Average Loss: 4.500, avg. samples / sec: 24567.09
Iteration:   1240, Loss function: 4.358, Average Loss: 4.492, avg. samples / sec: 24624.09
Iteration:   1240, Loss function: 4.840, Average Loss: 4.480, avg. samples / sec: 24571.14
Iteration:   1240, Loss function: 4.236, Average Loss: 4.486, avg. samples / sec: 24597.05
Iteration:   1240, Loss function: 4.746, Average Loss: 4.492, avg. samples / sec: 24558.66
Iteration:   1240, Loss function: 4.886, Average Loss: 4.491, avg. samples / sec: 24576.39
Iteration:   1240, Loss function: 4.679, Average Loss: 4.495, avg. samples / sec: 24567.17
Iteration:   1240, Loss function: 4.651, Average Loss: 4.484, avg. samples / sec: 24553.72
Iteration:   1260, Loss function: 4.228, Average Loss: 4.503, avg. samples / sec: 24553.92
Iteration:   1260, Loss function: 4.533, Average Loss: 4.498, avg. samples / sec: 24584.66
Iteration:   1260, Loss function: 4.481, Average Loss: 4.482, avg. samples / sec: 24554.46
Iteration:   1260, Loss function: 4.620, Average Loss: 4.496, avg. samples / sec: 24545.57
Iteration:   1260, Loss function: 4.564, Average Loss: 4.495, avg. samples / sec: 24541.30
Iteration:   1260, Loss function: 3.976, Average Loss: 4.498, avg. samples / sec: 24528.63
Iteration:   1260, Loss function: 4.216, Average Loss: 4.486, avg. samples / sec: 24517.34
Iteration:   1260, Loss function: 4.497, Average Loss: 4.491, avg. samples / sec: 24540.96

:::MLPv0.5.0 ssd 1541756865.732936859 (train.py:553) train_epoch: 22
Iteration:   1280, Loss function: 4.700, Average Loss: 4.483, avg. samples / sec: 24555.58
Iteration:   1280, Loss function: 4.600, Average Loss: 4.505, avg. samples / sec: 24549.64
Iteration:   1280, Loss function: 4.604, Average Loss: 4.501, avg. samples / sec: 24588.52
Iteration:   1280, Loss function: 4.735, Average Loss: 4.501, avg. samples / sec: 24529.05
Iteration:   1280, Loss function: 4.858, Average Loss: 4.498, avg. samples / sec: 24557.29
Iteration:   1280, Loss function: 4.772, Average Loss: 4.499, avg. samples / sec: 24532.97
Iteration:   1280, Loss function: 4.515, Average Loss: 4.493, avg. samples / sec: 24567.36
Iteration:   1280, Loss function: 4.583, Average Loss: 4.486, avg. samples / sec: 24562.69
Iteration:   1300, Loss function: 4.879, Average Loss: 4.490, avg. samples / sec: 24561.51
Iteration:   1300, Loss function: 5.014, Average Loss: 4.501, avg. samples / sec: 24554.47
Iteration:   1300, Loss function: 4.904, Average Loss: 4.490, avg. samples / sec: 24519.57
Iteration:   1300, Loss function: 4.722, Average Loss: 4.506, avg. samples / sec: 24520.56
Iteration:   1300, Loss function: 4.660, Average Loss: 4.504, avg. samples / sec: 24500.49
Iteration:   1300, Loss function: 5.204, Average Loss: 4.506, avg. samples / sec: 24516.33
Iteration:   1300, Loss function: 5.125, Average Loss: 4.503, avg. samples / sec: 24512.78
Iteration:   1300, Loss function: 4.828, Average Loss: 4.499, avg. samples / sec: 24496.16
Iteration:   1320, Loss function: 5.236, Average Loss: 4.496, avg. samples / sec: 24571.14
Iteration:   1320, Loss function: 4.503, Average Loss: 4.511, avg. samples / sec: 24569.11
Iteration:   1320, Loss function: 4.715, Average Loss: 4.494, avg. samples / sec: 24549.13
Iteration:   1320, Loss function: 4.494, Average Loss: 4.512, avg. samples / sec: 24566.41
Iteration:   1320, Loss function: 4.605, Average Loss: 4.508, avg. samples / sec: 24552.82
Iteration:   1320, Loss function: 4.554, Average Loss: 4.506, avg. samples / sec: 24520.71
Iteration:   1320, Loss function: 4.636, Average Loss: 4.509, avg. samples / sec: 24550.70
Iteration:   1320, Loss function: 4.755, Average Loss: 4.506, avg. samples / sec: 24566.47

:::MLPv0.5.0 ssd 1541756870.572160006 (train.py:553) train_epoch: 23
Iteration:   1340, Loss function: 4.737, Average Loss: 4.499, avg. samples / sec: 24530.18
Iteration:   1340, Loss function: 4.445, Average Loss: 4.509, avg. samples / sec: 24530.43
Iteration:   1340, Loss function: 4.524, Average Loss: 4.496, avg. samples / sec: 24542.12
Iteration:   1340, Loss function: 3.821, Average Loss: 4.510, avg. samples / sec: 24561.12
Iteration:   1340, Loss function: 4.465, Average Loss: 4.507, avg. samples / sec: 24577.00
Iteration:   1340, Loss function: 4.909, Average Loss: 4.514, avg. samples / sec: 24533.36
Iteration:   1340, Loss function: 4.832, Average Loss: 4.508, avg. samples / sec: 24539.20
Iteration:   1340, Loss function: 4.295, Average Loss: 4.509, avg. samples / sec: 24524.19
Iteration:   1360, Loss function: 5.070, Average Loss: 4.504, avg. samples / sec: 24542.81
Iteration:   1360, Loss function: 3.987, Average Loss: 4.500, avg. samples / sec: 24544.97
Iteration:   1360, Loss function: 4.778, Average Loss: 4.513, avg. samples / sec: 24533.33
Iteration:   1360, Loss function: 4.552, Average Loss: 4.517, avg. samples / sec: 24545.23
Iteration:   1360, Loss function: 4.508, Average Loss: 4.511, avg. samples / sec: 24541.05
Iteration:   1360, Loss function: 4.662, Average Loss: 4.512, avg. samples / sec: 24555.11
Iteration:   1360, Loss function: 4.642, Average Loss: 4.512, avg. samples / sec: 24490.29
Iteration:   1360, Loss function: 4.577, Average Loss: 4.513, avg. samples / sec: 24482.74
Iteration:   1380, Loss function: 5.068, Average Loss: 4.502, avg. samples / sec: 24565.29
Iteration:   1380, Loss function: 4.266, Average Loss: 4.515, avg. samples / sec: 24567.56
Iteration:   1380, Loss function: 4.734, Average Loss: 4.513, avg. samples / sec: 24587.40
Iteration:   1380, Loss function: 4.990, Average Loss: 4.519, avg. samples / sec: 24561.20
Iteration:   1380, Loss function: 4.017, Average Loss: 4.513, avg. samples / sec: 24596.68
Iteration:   1380, Loss function: 4.707, Average Loss: 4.516, avg. samples / sec: 24596.99
Iteration:   1380, Loss function: 4.393, Average Loss: 4.507, avg. samples / sec: 24525.35
Iteration:   1380, Loss function: 5.115, Average Loss: 4.514, avg. samples / sec: 24553.51

:::MLPv0.5.0 ssd 1541756875.414034843 (train.py:553) train_epoch: 24
Iteration:   1400, Loss function: 4.675, Average Loss: 4.503, avg. samples / sec: 24531.46
Iteration:   1400, Loss function: 4.307, Average Loss: 4.521, avg. samples / sec: 24561.58
Iteration:   1400, Loss function: 4.275, Average Loss: 4.515, avg. samples / sec: 24569.54
Iteration:   1400, Loss function: 4.513, Average Loss: 4.513, avg. samples / sec: 24510.68
Iteration:   1400, Loss function: 4.695, Average Loss: 4.511, avg. samples / sec: 24539.36
Iteration:   1400, Loss function: 4.797, Average Loss: 4.515, avg. samples / sec: 24536.36
Iteration:   1400, Loss function: 4.613, Average Loss: 4.513, avg. samples / sec: 24496.99
Iteration:   1400, Loss function: 3.911, Average Loss: 4.517, avg. samples / sec: 24503.52
Iteration:   1420, Loss function: 4.293, Average Loss: 4.505, avg. samples / sec: 24588.59
Iteration:   1420, Loss function: 4.562, Average Loss: 4.515, avg. samples / sec: 24607.91
Iteration:   1420, Loss function: 4.262, Average Loss: 4.514, avg. samples / sec: 24604.96
Iteration:   1420, Loss function: 4.194, Average Loss: 4.521, avg. samples / sec: 24562.78
Iteration:   1420, Loss function: 4.529, Average Loss: 4.514, avg. samples / sec: 24567.12
Iteration:   1420, Loss function: 4.101, Average Loss: 4.515, avg. samples / sec: 24599.11
Iteration:   1420, Loss function: 5.201, Average Loss: 4.515, avg. samples / sec: 24566.39
Iteration:   1420, Loss function: 4.649, Average Loss: 4.514, avg. samples / sec: 24586.23
Iteration:   1440, Loss function: 4.475, Average Loss: 4.505, avg. samples / sec: 24588.06
Iteration:   1440, Loss function: 5.121, Average Loss: 4.514, avg. samples / sec: 24639.18
Iteration:   1440, Loss function: 4.741, Average Loss: 4.517, avg. samples / sec: 24636.77
Iteration:   1440, Loss function: 4.519, Average Loss: 4.512, avg. samples / sec: 24590.37
Iteration:   1440, Loss function: 4.673, Average Loss: 4.517, avg. samples / sec: 24587.39
Iteration:   1440, Loss function: 4.380, Average Loss: 4.516, avg. samples / sec: 24604.38
Iteration:   1440, Loss function: 4.018, Average Loss: 4.522, avg. samples / sec: 24556.39
Iteration:   1440, Loss function: 4.450, Average Loss: 4.514, avg. samples / sec: 24584.21

:::MLPv0.5.0 ssd 1541756880.164416552 (train.py:553) train_epoch: 25
Iteration:   1460, Loss function: 4.607, Average Loss: 4.507, avg. samples / sec: 24555.76
Iteration:   1460, Loss function: 4.461, Average Loss: 4.514, avg. samples / sec: 24562.82
Iteration:   1460, Loss function: 4.695, Average Loss: 4.514, avg. samples / sec: 24538.07
Iteration:   1460, Loss function: 4.677, Average Loss: 4.524, avg. samples / sec: 24576.04
Iteration:   1460, Loss function: 4.546, Average Loss: 4.517, avg. samples / sec: 24584.58
Iteration:   1460, Loss function: 4.386, Average Loss: 4.517, avg. samples / sec: 24505.02
Iteration:   1460, Loss function: 4.957, Average Loss: 4.520, avg. samples / sec: 24502.06
Iteration:   1460, Loss function: 5.006, Average Loss: 4.518, avg. samples / sec: 24508.22
Iteration:   1480, Loss function: 4.060, Average Loss: 4.514, avg. samples / sec: 24525.34
Iteration:   1480, Loss function: 4.698, Average Loss: 4.506, avg. samples / sec: 24513.08
Iteration:   1480, Loss function: 4.586, Average Loss: 4.520, avg. samples / sec: 24512.31
Iteration:   1480, Loss function: 4.778, Average Loss: 4.515, avg. samples / sec: 24504.78
Iteration:   1480, Loss function: 4.688, Average Loss: 4.519, avg. samples / sec: 24524.70
Iteration:   1480, Loss function: 4.736, Average Loss: 4.519, avg. samples / sec: 24515.10
Iteration:   1480, Loss function: 4.931, Average Loss: 4.519, avg. samples / sec: 24515.56
Iteration:   1480, Loss function: 4.615, Average Loss: 4.524, avg. samples / sec: 24491.64
Iteration:   1500, Loss function: 4.229, Average Loss: 4.520, avg. samples / sec: 24636.50
Iteration:   1500, Loss function: 5.223, Average Loss: 4.526, avg. samples / sec: 24643.24
Iteration:   1500, Loss function: 4.524, Average Loss: 4.509, avg. samples / sec: 24577.96
Iteration:   1500, Loss function: 4.724, Average Loss: 4.516, avg. samples / sec: 24532.89
Iteration:   1500, Loss function: 4.505, Average Loss: 4.521, avg. samples / sec: 24595.29
Iteration:   1500, Loss function: 4.637, Average Loss: 4.521, avg. samples / sec: 24574.45
Iteration:   1500, Loss function: 4.494, Average Loss: 4.517, avg. samples / sec: 24573.94
Iteration:   1500, Loss function: 4.463, Average Loss: 4.523, avg. samples / sec: 24576.67

:::MLPv0.5.0 ssd 1541756885.000746965 (train.py:553) train_epoch: 26
Iteration:   1520, Loss function: 3.997, Average Loss: 4.508, avg. samples / sec: 24609.98
Iteration:   1520, Loss function: 4.146, Average Loss: 4.524, avg. samples / sec: 24601.09
Iteration:   1520, Loss function: 4.348, Average Loss: 4.519, avg. samples / sec: 24598.24
Iteration:   1520, Loss function: 3.983, Average Loss: 4.511, avg. samples / sec: 24625.49
Iteration:   1520, Loss function: 4.624, Average Loss: 4.522, avg. samples / sec: 24619.67
Iteration:   1520, Loss function: 4.279, Average Loss: 4.517, avg. samples / sec: 24592.78
Iteration:   1520, Loss function: 4.648, Average Loss: 4.518, avg. samples / sec: 24586.66
Iteration:   1520, Loss function: 4.392, Average Loss: 4.522, avg. samples / sec: 24591.16
Iteration:   1540, Loss function: 4.677, Average Loss: 4.516, avg. samples / sec: 24662.38
Iteration:   1540, Loss function: 4.741, Average Loss: 4.509, avg. samples / sec: 24599.21
Iteration:   1540, Loss function: 4.551, Average Loss: 4.517, avg. samples / sec: 24599.82
Iteration:   1540, Loss function: 4.531, Average Loss: 4.523, avg. samples / sec: 24582.96
Iteration:   1540, Loss function: 4.299, Average Loss: 4.516, avg. samples / sec: 24637.21
Iteration:   1540, Loss function: 4.850, Average Loss: 4.509, avg. samples / sec: 24590.06
Iteration:   1540, Loss function: 4.435, Average Loss: 4.521, avg. samples / sec: 24608.14
Iteration:   1540, Loss function: 4.785, Average Loss: 4.521, avg. samples / sec: 24582.23

:::MLPv0.5.0 ssd 1541756889.832882643 (train.py:553) train_epoch: 27
Iteration:   1560, Loss function: 3.834, Average Loss: 4.505, avg. samples / sec: 24544.38
Iteration:   1560, Loss function: 3.928, Average Loss: 4.515, avg. samples / sec: 24545.33
Iteration:   1560, Loss function: 4.375, Average Loss: 4.506, avg. samples / sec: 24569.16
Iteration:   1560, Loss function: 3.897, Average Loss: 4.511, avg. samples / sec: 24547.51
Iteration:   1560, Loss function: 4.604, Average Loss: 4.520, avg. samples / sec: 24559.03
Iteration:   1560, Loss function: 4.215, Average Loss: 4.515, avg. samples / sec: 24495.10
Iteration:   1560, Loss function: 4.462, Average Loss: 4.521, avg. samples / sec: 24511.40
Iteration:   1560, Loss function: 4.446, Average Loss: 4.520, avg. samples / sec: 24536.35
Iteration:   1580, Loss function: 5.067, Average Loss: 4.514, avg. samples / sec: 24612.61
Iteration:   1580, Loss function: 4.456, Average Loss: 4.504, avg. samples / sec: 24601.40
Iteration:   1580, Loss function: 4.684, Average Loss: 4.514, avg. samples / sec: 24641.30
Iteration:   1580, Loss function: 4.678, Average Loss: 4.520, avg. samples / sec: 24626.26
Iteration:   1580, Loss function: 4.458, Average Loss: 4.511, avg. samples / sec: 24599.71
Iteration:   1580, Loss function: 4.764, Average Loss: 4.519, avg. samples / sec: 24623.03
Iteration:   1580, Loss function: 4.287, Average Loss: 4.505, avg. samples / sec: 24557.59
Iteration:   1580, Loss function: 4.580, Average Loss: 4.521, avg. samples / sec: 24609.01
Iteration:   1600, Loss function: 4.387, Average Loss: 4.521, avg. samples / sec: 24550.83
Iteration:   1600, Loss function: 4.658, Average Loss: 4.503, avg. samples / sec: 24533.42
Iteration:   1600, Loss function: 4.301, Average Loss: 4.512, avg. samples / sec: 24558.83
Iteration:   1600, Loss function: 4.137, Average Loss: 4.505, avg. samples / sec: 24577.75
Iteration:   1600, Loss function: 4.296, Average Loss: 4.516, avg. samples / sec: 24518.34
Iteration:   1600, Loss function: 4.802, Average Loss: 4.520, avg. samples / sec: 24580.61
Iteration:   1600, Loss function: 4.265, Average Loss: 4.513, avg. samples / sec: 24502.37
Iteration:   1600, Loss function: 4.468, Average Loss: 4.518, avg. samples / sec: 24515.78

:::MLPv0.5.0 ssd 1541756894.668979645 (train.py:553) train_epoch: 28
Iteration:   1620, Loss function: 4.290, Average Loss: 4.517, avg. samples / sec: 24555.53
Iteration:   1620, Loss function: 4.693, Average Loss: 4.499, avg. samples / sec: 24554.40
Iteration:   1620, Loss function: 4.400, Average Loss: 4.514, avg. samples / sec: 24566.85
Iteration:   1620, Loss function: 4.667, Average Loss: 4.510, avg. samples / sec: 24585.16
Iteration:   1620, Loss function: 4.849, Average Loss: 4.514, avg. samples / sec: 24576.90
Iteration:   1620, Loss function: 4.442, Average Loss: 4.509, avg. samples / sec: 24504.02
Iteration:   1620, Loss function: 4.179, Average Loss: 4.514, avg. samples / sec: 24513.82
Iteration:   1620, Loss function: 4.499, Average Loss: 4.502, avg. samples / sec: 24508.04
Iteration:   1640, Loss function: 4.956, Average Loss: 4.517, avg. samples / sec: 24492.86
Iteration:   1640, Loss function: 4.321, Average Loss: 4.498, avg. samples / sec: 24490.52
Iteration:   1640, Loss function: 4.727, Average Loss: 4.500, avg. samples / sec: 24528.02
Iteration:   1640, Loss function: 4.634, Average Loss: 4.508, avg. samples / sec: 24480.30
Iteration:   1640, Loss function: 4.239, Average Loss: 4.511, avg. samples / sec: 24494.25
Iteration:   1640, Loss function: 5.080, Average Loss: 4.512, avg. samples / sec: 24502.46
Iteration:   1640, Loss function: 4.729, Average Loss: 4.513, avg. samples / sec: 24433.93
Iteration:   1640, Loss function: 4.179, Average Loss: 4.505, avg. samples / sec: 24480.26
Iteration:   1660, Loss function: 4.189, Average Loss: 4.501, avg. samples / sec: 24581.38
Iteration:   1660, Loss function: 3.890, Average Loss: 4.515, avg. samples / sec: 24551.55
Iteration:   1660, Loss function: 4.519, Average Loss: 4.497, avg. samples / sec: 24537.30
Iteration:   1660, Loss function: 4.269, Average Loss: 4.507, avg. samples / sec: 24558.18
Iteration:   1660, Loss function: 3.750, Average Loss: 4.511, avg. samples / sec: 24551.18
Iteration:   1660, Loss function: 4.076, Average Loss: 4.513, avg. samples / sec: 24579.93
Iteration:   1660, Loss function: 4.610, Average Loss: 4.501, avg. samples / sec: 24582.45
Iteration:   1660, Loss function: 4.493, Average Loss: 4.511, avg. samples / sec: 24554.41

:::MLPv0.5.0 ssd 1541756899.427658081 (train.py:553) train_epoch: 29
Iteration:   1680, Loss function: 4.672, Average Loss: 4.510, avg. samples / sec: 24577.52
Iteration:   1680, Loss function: 4.113, Average Loss: 4.496, avg. samples / sec: 24534.83
Iteration:   1680, Loss function: 4.139, Average Loss: 4.496, avg. samples / sec: 24562.52
Iteration:   1680, Loss function: 4.603, Average Loss: 4.505, avg. samples / sec: 24554.21
Iteration:   1680, Loss function: 4.194, Average Loss: 4.507, avg. samples / sec: 24551.04
Iteration:   1680, Loss function: 3.914, Average Loss: 4.504, avg. samples / sec: 24523.87
Iteration:   1680, Loss function: 4.410, Average Loss: 4.492, avg. samples / sec: 24501.00
Iteration:   1680, Loss function: 4.762, Average Loss: 4.512, avg. samples / sec: 24480.70
Iteration:   1700, Loss function: 4.732, Average Loss: 4.506, avg. samples / sec: 24544.02
Iteration:   1700, Loss function: 4.514, Average Loss: 4.496, avg. samples / sec: 24498.60
Iteration:   1700, Loss function: 3.989, Average Loss: 4.507, avg. samples / sec: 24475.78
Iteration:   1700, Loss function: 4.392, Average Loss: 4.502, avg. samples / sec: 24494.57
Iteration:   1700, Loss function: 4.569, Average Loss: 4.510, avg. samples / sec: 24516.00
Iteration:   1700, Loss function: 4.367, Average Loss: 4.493, avg. samples / sec: 24461.74
Iteration:   1700, Loss function: 4.585, Average Loss: 4.504, avg. samples / sec: 24459.92
Iteration:   1700, Loss function: 4.354, Average Loss: 4.492, avg. samples / sec: 24501.48
Iteration:   1720, Loss function: 4.040, Average Loss: 4.505, avg. samples / sec: 24633.42
Iteration:   1720, Loss function: 4.133, Average Loss: 4.503, avg. samples / sec: 24568.69
Iteration:   1720, Loss function: 4.161, Average Loss: 4.493, avg. samples / sec: 24580.26
Iteration:   1720, Loss function: 4.426, Average Loss: 4.490, avg. samples / sec: 24635.19
Iteration:   1720, Loss function: 4.069, Average Loss: 4.502, avg. samples / sec: 24615.00
Iteration:   1720, Loss function: 4.158, Average Loss: 4.502, avg. samples / sec: 24583.89
Iteration:   1720, Loss function: 4.294, Average Loss: 4.507, avg. samples / sec: 24557.98
Iteration:   1720, Loss function: 4.752, Average Loss: 4.490, avg. samples / sec: 24583.23

:::MLPv0.5.0 ssd 1541756904.269366026 (train.py:553) train_epoch: 30
Iteration:   1740, Loss function: 4.738, Average Loss: 4.486, avg. samples / sec: 24538.31
Iteration:   1740, Loss function: 4.598, Average Loss: 4.491, avg. samples / sec: 24530.55
Iteration:   1740, Loss function: 4.739, Average Loss: 4.502, avg. samples / sec: 24514.90
Iteration:   1740, Loss function: 3.839, Average Loss: 4.502, avg. samples / sec: 24550.64
Iteration:   1740, Loss function: 4.176, Average Loss: 4.488, avg. samples / sec: 24554.62
Iteration:   1740, Loss function: 4.325, Average Loss: 4.500, avg. samples / sec: 24538.84
Iteration:   1740, Loss function: 4.405, Average Loss: 4.500, avg. samples / sec: 24517.34
Iteration:   1740, Loss function: 4.170, Average Loss: 4.500, avg. samples / sec: 24486.50
Iteration:   1760, Loss function: 5.043, Average Loss: 4.482, avg. samples / sec: 24560.92
Iteration:   1760, Loss function: 4.226, Average Loss: 4.496, avg. samples / sec: 24588.84
Iteration:   1760, Loss function: 4.233, Average Loss: 4.489, avg. samples / sec: 24563.03
Iteration:   1760, Loss function: 4.186, Average Loss: 4.497, avg. samples / sec: 24600.62
Iteration:   1760, Loss function: 3.865, Average Loss: 4.483, avg. samples / sec: 24580.38
Iteration:   1760, Loss function: 4.401, Average Loss: 4.495, avg. samples / sec: 24587.09
Iteration:   1760, Loss function: 3.868, Average Loss: 4.495, avg. samples / sec: 24556.53
Iteration:   1760, Loss function: 4.870, Average Loss: 4.495, avg. samples / sec: 24514.77
Iteration:   1780, Loss function: 4.215, Average Loss: 4.489, avg. samples / sec: 24625.50
Iteration:   1780, Loss function: 4.905, Average Loss: 4.487, avg. samples / sec: 24555.64
Iteration:   1780, Loss function: 4.814, Average Loss: 4.480, avg. samples / sec: 24547.24
Iteration:   1780, Loss function: 4.667, Average Loss: 4.490, avg. samples / sec: 24547.93
Iteration:   1780, Loss function: 5.220, Average Loss: 4.492, avg. samples / sec: 24543.46
Iteration:   1780, Loss function: 4.685, Average Loss: 4.491, avg. samples / sec: 24509.97
Iteration:   1780, Loss function: 4.208, Average Loss: 4.489, avg. samples / sec: 24541.64
Iteration:   1780, Loss function: 4.805, Average Loss: 4.477, avg. samples / sec: 24503.02

:::MLPv0.5.0 ssd 1541756909.106725454 (train.py:553) train_epoch: 31
Iteration:   1800, Loss function: 5.314, Average Loss: 4.493, avg. samples / sec: 24612.19
Iteration:   1800, Loss function: 4.724, Average Loss: 4.495, avg. samples / sec: 24557.33
Iteration:   1800, Loss function: 4.907, Average Loss: 4.505, avg. samples / sec: 24530.37
Iteration:   1800, Loss function: 4.692, Average Loss: 4.502, avg. samples / sec: 24542.33
Iteration:   1800, Loss function: 4.830, Average Loss: 4.504, avg. samples / sec: 24531.01
Iteration:   1800, Loss function: 4.357, Average Loss: 4.506, avg. samples / sec: 24555.41
Iteration:   1800, Loss function: 4.904, Average Loss: 4.509, avg. samples / sec: 24517.76
Iteration:   1800, Loss function: 4.318, Average Loss: 4.503, avg. samples / sec: 24549.29
Iteration:   1820, Loss function: 3.985, Average Loss: 4.495, avg. samples / sec: 24528.49
Iteration:   1820, Loss function: 4.581, Average Loss: 4.508, avg. samples / sec: 24562.26
Iteration:   1820, Loss function: 4.414, Average Loss: 4.499, avg. samples / sec: 24515.08
Iteration:   1820, Loss function: 4.481, Average Loss: 4.505, avg. samples / sec: 24498.30
Iteration:   1820, Loss function: 4.281, Average Loss: 4.504, avg. samples / sec: 24520.15
Iteration:   1820, Loss function: 4.312, Average Loss: 4.508, avg. samples / sec: 24532.26
Iteration:   1820, Loss function: 4.594, Average Loss: 4.507, avg. samples / sec: 24465.91
Iteration:   1820, Loss function: 4.241, Average Loss: 4.505, avg. samples / sec: 24509.55
Iteration:   1840, Loss function: 4.095, Average Loss: 4.492, avg. samples / sec: 24552.01
Iteration:   1840, Loss function: 4.260, Average Loss: 4.501, avg. samples / sec: 24619.06
Iteration:   1840, Loss function: 3.927, Average Loss: 4.499, avg. samples / sec: 24578.58
Iteration:   1840, Loss function: 3.992, Average Loss: 4.501, avg. samples / sec: 24559.59
Iteration:   1840, Loss function: 4.221, Average Loss: 4.501, avg. samples / sec: 24573.68
Iteration:   1840, Loss function: 4.585, Average Loss: 4.505, avg. samples / sec: 24505.82
Iteration:   1840, Loss function: 4.018, Average Loss: 4.494, avg. samples / sec: 24510.96
Iteration:   1840, Loss function: 4.720, Average Loss: 4.504, avg. samples / sec: 24542.69

:::MLPv0.5.0 ssd 1541756913.948375225 (train.py:553) train_epoch: 32
Iteration:   1860, Loss function: 4.459, Average Loss: 4.493, avg. samples / sec: 24513.22
Iteration:   1860, Loss function: 4.678, Average Loss: 4.494, avg. samples / sec: 24545.61
Iteration:   1860, Loss function: 4.531, Average Loss: 4.488, avg. samples / sec: 24493.55
Iteration:   1860, Loss function: 4.196, Average Loss: 4.500, avg. samples / sec: 24550.27
Iteration:   1860, Loss function: 4.292, Average Loss: 4.496, avg. samples / sec: 24495.35
Iteration:   1860, Loss function: 4.687, Average Loss: 4.490, avg. samples / sec: 24537.23
Iteration:   1860, Loss function: 4.157, Average Loss: 4.495, avg. samples / sec: 24516.80
Iteration:   1860, Loss function: 4.371, Average Loss: 4.499, avg. samples / sec: 24491.31

































































:::MLPv0.5.0 ssd 1541756916.208365917 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541756916.208877802 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541756916.209336042 (train.py:220) eval_start: 32
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 15
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 28
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 74
Predicting Ended, total time: 5.91 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.91 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.91 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.91 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.91 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.91 s
Predicting Ended, total time: 5.91 s
Predicting Ended, total time: 5.91 s
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(249996, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(249996, 7)
(249996, 7)
0/249996
0/249996
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(249996, 7)
Converting ndarray to lists...
(249996, 7)
Loading and preparing results...
(249996, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(249996, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(249996, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(249996, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/249996
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(249996, 7)
Loading and preparing results...
(249996, 7)
Loading and preparing results...
0/249996
Loading and preparing results...
(249996, 7)
(249996, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
0/249996
0/249996
Loading and preparing results...
0/249996
Converting ndarray to lists...
(249996, 7)
Converting ndarray to lists...
(249996, 7)
(249996, 7)
Loading and preparing results...
Converting ndarray to lists...
(249996, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/249996
0/249996
0/249996
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/249996
0/249996
(249996, 7)
0/249996
(249996, 7)
(249996, 7)
(249996, 7)
0/249996
Converting ndarray to lists...
Converting ndarray to lists...
(249996, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(249996, 7)
Converting ndarray to lists...
(249996, 7)
(249996, 7)
Loading and preparing results...
Converting ndarray to lists...
(249996, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(249996, 7)
Loading and preparing results...
0/249996
(249996, 7)
0/249996
Converting ndarray to lists...
(249996, 7)
(249996, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/249996
Converting ndarray to lists...
(249996, 7)
0/249996
0/249996
Converting ndarray to lists...
0/249996
(249996, 7)
0/249996
Converting ndarray to lists...
Loading and preparing results...
0/249996
Converting ndarray to lists...
Converting ndarray to lists...
(249996, 7)
0/249996
Loading and preparing results...
0/249996
Loading and preparing results...
0/249996
(249996, 7)
(249996, 7)
(249996, 7)
0/249996
0/249996
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/249996
(249996, 7)
0/249996
(249996, 7)
0/249996
0/249996
0/249996
Converting ndarray to lists...
Loading and preparing results...
0/249996
Converting ndarray to lists...
Converting ndarray to lists...
0/249996
(249996, 7)
(249996, 7)
(249996, 7)
(249996, 7)
Converting ndarray to lists...
0/249996
Converting ndarray to lists...
(249996, 7)
Converting ndarray to lists...
(249996, 7)
(249996, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/249996
Converting ndarray to lists...
0/249996
0/249996
Loading and preparing results...
(249996, 7)
(249996, 7)
0/249996
(249996, 7)
(249996, 7)
(249996, 7)
(249996, 7)
(249996, 7)
(249996, 7)
0/249996
Loading and preparing results...
(249996, 7)
Converting ndarray to lists...
0/249996
0/249996
0/249996
0/249996
0/249996
Converting ndarray to lists...
0/249996
(249996, 7)
0/249996
(249996, 7)
0/249996
(249996, 7)
0/249996
0/249996
0/249996
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/249996
(249996, 7)
0/249996
(249996, 7)
(249996, 7)
(249996, 7)
(249996, 7)
0/249996
0/249996
0/249996
0/249996
0/249996
0/249996
0/249996
(249996, 7)
(249996, 7)
0/249996
0/249996
0/249996
0/249996
DONE (t=1.66s)
creating index...
DONE (t=1.67s)
creating index...
DONE (t=1.67s)
creating index...
DONE (t=1.67s)
DONE (t=1.67s)
creating index...
creating index...
DONE (t=1.67s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.74s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.86s).
Accumulating evaluation results...
DONE (t=2.86s).
Accumulating evaluation results...
DONE (t=2.87s).
Accumulating evaluation results...
DONE (t=2.88s).
Accumulating evaluation results...
DONE (t=2.88s).
Accumulating evaluation results...
DONE (t=2.86s).
Accumulating evaluation results...
DONE (t=2.89s).
Accumulating evaluation results...
DONE (t=2.94s).
Accumulating evaluation results...
DONE (t=0.94s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.113
DONE (t=0.96s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.113
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.109
DONE (t=0.95s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.028
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.109
DONE (t=0.95s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.121
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.113
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.028
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.174
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.134
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.113
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.121
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.194
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.204
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.220
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.306
Current AP: 0.11264 AP goal: 0.21200
DONE (t=0.96s).
DONE (t=0.96s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.109
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.174
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.134
DONE (t=0.95s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.028
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.109
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.194
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.204
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.220
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.306
Current AP: 0.11264 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.113
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.113
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.121
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.028
DONE (t=0.97s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.113
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.174
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.121
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.134
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.109
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.113
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.174
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.109
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.194
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.204
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.220
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.306
Current AP: 0.11264 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.134
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.109
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.028
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.028
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.194
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.204
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.220
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.306
Current AP: 0.11264 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.028
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.121
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.121
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.109
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.121
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.174
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.174
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.028
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.134
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.174
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.134
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.194
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.134
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.204
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.220
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.306
Current AP: 0.11264 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.194
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.204
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.220
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.306
Current AP: 0.11264 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.121
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.194
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.204
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.220
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.306
Current AP: 0.11264 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.174
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.134
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.194
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.204
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.220
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.306
Current AP: 0.11264 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541756927.878977776 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541756927.879572153 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.11264357302583786}

:::MLPv0.5.0 ssd 1541756927.880025148 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.11264357302583786}

:::MLPv0.5.0 ssd 1541756927.880492449 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541756927.880934715 (train.py:338) eval_stop: 32
Iteration:   1880, Loss function: 4.194, Average Loss: 4.495, avg. samples / sec: 2929.30
Iteration:   1880, Loss function: 4.380, Average Loss: 4.499, avg. samples / sec: 2929.11
Iteration:   1880, Loss function: 3.770, Average Loss: 4.492, avg. samples / sec: 2929.23
Iteration:   1880, Loss function: 4.519, Average Loss: 4.496, avg. samples / sec: 2929.07
Iteration:   1880, Loss function: 4.803, Average Loss: 4.495, avg. samples / sec: 2928.87
Iteration:   1880, Loss function: 4.379, Average Loss: 4.488, avg. samples / sec: 2928.88
Iteration:   1880, Loss function: 4.076, Average Loss: 4.501, avg. samples / sec: 2929.47
Iteration:   1880, Loss function: 4.204, Average Loss: 4.497, avg. samples / sec: 2928.67
Iteration:   1900, Loss function: 3.821, Average Loss: 4.485, avg. samples / sec: 24556.93
Iteration:   1900, Loss function: 4.142, Average Loss: 4.491, avg. samples / sec: 24561.05
Iteration:   1900, Loss function: 4.603, Average Loss: 4.482, avg. samples / sec: 24565.47
Iteration:   1900, Loss function: 4.109, Average Loss: 4.495, avg. samples / sec: 24545.87
Iteration:   1900, Loss function: 4.661, Average Loss: 4.489, avg. samples / sec: 24533.12
Iteration:   1900, Loss function: 4.849, Average Loss: 4.489, avg. samples / sec: 24489.54
Iteration:   1900, Loss function: 4.098, Average Loss: 4.491, avg. samples / sec: 24546.41
Iteration:   1900, Loss function: 3.945, Average Loss: 4.494, avg. samples / sec: 24532.01

:::MLPv0.5.0 ssd 1541756931.020167112 (train.py:553) train_epoch: 33
Iteration:   1920, Loss function: 4.102, Average Loss: 4.484, avg. samples / sec: 24615.44
Iteration:   1920, Loss function: 4.345, Average Loss: 4.483, avg. samples / sec: 24600.98
Iteration:   1920, Loss function: 4.073, Average Loss: 4.485, avg. samples / sec: 24563.54
Iteration:   1920, Loss function: 4.361, Average Loss: 4.479, avg. samples / sec: 24560.32
Iteration:   1920, Loss function: 4.248, Average Loss: 4.486, avg. samples / sec: 24570.58
Iteration:   1920, Loss function: 4.319, Average Loss: 4.482, avg. samples / sec: 24589.55
Iteration:   1920, Loss function: 4.845, Average Loss: 4.489, avg. samples / sec: 24563.90
Iteration:   1920, Loss function: 4.322, Average Loss: 4.475, avg. samples / sec: 24517.60
Iteration:   1940, Loss function: 4.622, Average Loss: 4.482, avg. samples / sec: 24635.73
Iteration:   1940, Loss function: 4.265, Average Loss: 4.474, avg. samples / sec: 24631.89
Iteration:   1940, Loss function: 4.353, Average Loss: 4.485, avg. samples / sec: 24631.01
Iteration:   1940, Loss function: 4.810, Average Loss: 4.478, avg. samples / sec: 24649.21
Iteration:   1940, Loss function: 4.782, Average Loss: 4.480, avg. samples / sec: 24624.58
Iteration:   1940, Loss function: 4.780, Average Loss: 4.472, avg. samples / sec: 24666.62
Iteration:   1940, Loss function: 4.291, Average Loss: 4.486, avg. samples / sec: 24657.66
Iteration:   1940, Loss function: 4.622, Average Loss: 4.483, avg. samples / sec: 24576.61
Iteration:   1960, Loss function: 3.721, Average Loss: 4.475, avg. samples / sec: 24602.29
Iteration:   1960, Loss function: 4.159, Average Loss: 4.477, avg. samples / sec: 24660.64
Iteration:   1960, Loss function: 3.408, Average Loss: 4.484, avg. samples / sec: 24625.13
Iteration:   1960, Loss function: 4.398, Average Loss: 4.474, avg. samples / sec: 24604.09
Iteration:   1960, Loss function: 3.967, Average Loss: 4.472, avg. samples / sec: 24599.80
Iteration:   1960, Loss function: 3.710, Average Loss: 4.469, avg. samples / sec: 24594.65
Iteration:   1960, Loss function: 4.484, Average Loss: 4.481, avg. samples / sec: 24594.35
Iteration:   1960, Loss function: 4.519, Average Loss: 4.465, avg. samples / sec: 24589.25

:::MLPv0.5.0 ssd 1541756935.849450350 (train.py:553) train_epoch: 34
Iteration:   1980, Loss function: 4.116, Average Loss: 4.466, avg. samples / sec: 24594.59
Iteration:   1980, Loss function: 4.168, Average Loss: 4.480, avg. samples / sec: 24598.99
Iteration:   1980, Loss function: 4.504, Average Loss: 4.476, avg. samples / sec: 24601.57
Iteration:   1980, Loss function: 4.201, Average Loss: 4.471, avg. samples / sec: 24585.73
Iteration:   1980, Loss function: 4.091, Average Loss: 4.462, avg. samples / sec: 24597.21
Iteration:   1980, Loss function: 4.593, Average Loss: 4.470, avg. samples / sec: 24563.28
Iteration:   1980, Loss function: 4.828, Average Loss: 4.461, avg. samples / sec: 24584.49
Iteration:   1980, Loss function: 4.771, Average Loss: 4.466, avg. samples / sec: 24541.68
Iteration:   2000, Loss function: 3.989, Average Loss: 4.463, avg. samples / sec: 24603.04
Iteration:   2000, Loss function: 4.324, Average Loss: 4.456, avg. samples / sec: 24613.93
Iteration:   2000, Loss function: 4.661, Average Loss: 4.464, avg. samples / sec: 24642.46
Iteration:   2000, Loss function: 4.007, Average Loss: 4.472, avg. samples / sec: 24601.89
Iteration:   2000, Loss function: 4.206, Average Loss: 4.461, avg. samples / sec: 24638.39
Iteration:   2000, Loss function: 4.198, Average Loss: 4.457, avg. samples / sec: 24615.32
Iteration:   2000, Loss function: 4.728, Average Loss: 4.468, avg. samples / sec: 24563.39
Iteration:   2000, Loss function: 3.987, Average Loss: 4.476, avg. samples / sec: 24545.87
Iteration:   2020, Loss function: 3.829, Average Loss: 4.449, avg. samples / sec: 24565.58
Iteration:   2020, Loss function: 4.164, Average Loss: 4.458, avg. samples / sec: 24560.03
Iteration:   2020, Loss function: 4.227, Average Loss: 4.463, avg. samples / sec: 24611.88
Iteration:   2020, Loss function: 3.928, Average Loss: 4.451, avg. samples / sec: 24581.12
Iteration:   2020, Loss function: 3.923, Average Loss: 4.453, avg. samples / sec: 24565.16
Iteration:   2020, Loss function: 3.784, Average Loss: 4.471, avg. samples / sec: 24578.57
Iteration:   2020, Loss function: 4.281, Average Loss: 4.466, avg. samples / sec: 24519.94
Iteration:   2020, Loss function: 4.266, Average Loss: 4.461, avg. samples / sec: 24510.77

:::MLPv0.5.0 ssd 1541756940.680747747 (train.py:553) train_epoch: 35
Iteration:   2040, Loss function: 4.743, Average Loss: 4.443, avg. samples / sec: 24551.63
Iteration:   2040, Loss function: 4.261, Average Loss: 4.459, avg. samples / sec: 24605.14
Iteration:   2040, Loss function: 4.162, Average Loss: 4.462, avg. samples / sec: 24605.51
Iteration:   2040, Loss function: 4.088, Average Loss: 4.453, avg. samples / sec: 24545.91
Iteration:   2040, Loss function: 4.055, Average Loss: 4.463, avg. samples / sec: 24531.94
Iteration:   2040, Loss function: 4.849, Average Loss: 4.451, avg. samples / sec: 24552.44
Iteration:   2040, Loss function: 4.085, Average Loss: 4.450, avg. samples / sec: 24535.91
Iteration:   2040, Loss function: 4.033, Average Loss: 4.469, avg. samples / sec: 24552.55
Iteration:   2060, Loss function: 4.167, Average Loss: 4.456, avg. samples / sec: 24612.46
Iteration:   2060, Loss function: 4.291, Average Loss: 4.453, avg. samples / sec: 24610.71
Iteration:   2060, Loss function: 3.919, Average Loss: 4.457, avg. samples / sec: 24604.58
Iteration:   2060, Loss function: 4.127, Average Loss: 4.445, avg. samples / sec: 24581.71
Iteration:   2060, Loss function: 4.637, Average Loss: 4.446, avg. samples / sec: 24587.52
Iteration:   2060, Loss function: 3.883, Average Loss: 4.446, avg. samples / sec: 24561.70
Iteration:   2060, Loss function: 3.820, Average Loss: 4.461, avg. samples / sec: 24598.59
Iteration:   2060, Loss function: 3.908, Average Loss: 4.438, avg. samples / sec: 24547.06

:::MLPv0.5.0 ssd 1541756945.512587547 (train.py:553) train_epoch: 36
Iteration:   2080, Loss function: 4.491, Average Loss: 4.449, avg. samples / sec: 24582.56
Iteration:   2080, Loss function: 3.903, Average Loss: 4.439, avg. samples / sec: 24621.12
Iteration:   2080, Loss function: 3.994, Average Loss: 4.448, avg. samples / sec: 24567.81
Iteration:   2080, Loss function: 3.895, Average Loss: 4.450, avg. samples / sec: 24576.16
Iteration:   2080, Loss function: 3.814, Average Loss: 4.437, avg. samples / sec: 24598.82
Iteration:   2080, Loss function: 4.369, Average Loss: 4.457, avg. samples / sec: 24597.46
Iteration:   2080, Loss function: 3.849, Average Loss: 4.439, avg. samples / sec: 24583.20
Iteration:   2080, Loss function: 4.210, Average Loss: 4.433, avg. samples / sec: 24579.90
Iteration:   2100, Loss function: 4.129, Average Loss: 4.443, avg. samples / sec: 24605.64
Iteration:   2100, Loss function: 4.823, Average Loss: 4.453, avg. samples / sec: 24643.96
Iteration:   2100, Loss function: 4.293, Average Loss: 4.432, avg. samples / sec: 24616.16
Iteration:   2100, Loss function: 4.132, Average Loss: 4.446, avg. samples / sec: 24608.86
Iteration:   2100, Loss function: 4.710, Average Loss: 4.445, avg. samples / sec: 24589.88
Iteration:   2100, Loss function: 4.697, Average Loss: 4.436, avg. samples / sec: 24587.55
Iteration:   2100, Loss function: 4.123, Average Loss: 4.429, avg. samples / sec: 24630.42
Iteration:   2100, Loss function: 4.133, Average Loss: 4.432, avg. samples / sec: 24603.66
Iteration:   2120, Loss function: 4.313, Average Loss: 4.447, avg. samples / sec: 24558.47
Iteration:   2120, Loss function: 3.789, Average Loss: 4.442, avg. samples / sec: 24584.89
Iteration:   2120, Loss function: 3.726, Average Loss: 4.424, avg. samples / sec: 24582.08
Iteration:   2120, Loss function: 3.713, Average Loss: 4.439, avg. samples / sec: 24566.17
Iteration:   2120, Loss function: 4.073, Average Loss: 4.432, avg. samples / sec: 24560.09
Iteration:   2120, Loss function: 4.697, Average Loss: 4.428, avg. samples / sec: 24551.11
Iteration:   2120, Loss function: 3.515, Average Loss: 4.439, avg. samples / sec: 24507.48
Iteration:   2120, Loss function: 4.278, Average Loss: 4.427, avg. samples / sec: 24559.82

:::MLPv0.5.0 ssd 1541756950.346326828 (train.py:553) train_epoch: 37
Iteration:   2140, Loss function: 3.942, Average Loss: 4.438, avg. samples / sec: 24576.49
Iteration:   2140, Loss function: 4.151, Average Loss: 4.419, avg. samples / sec: 24582.42
Iteration:   2140, Loss function: 4.270, Average Loss: 4.433, avg. samples / sec: 24583.26
Iteration:   2140, Loss function: 3.712, Average Loss: 4.429, avg. samples / sec: 24582.97
Iteration:   2140, Loss function: 4.141, Average Loss: 4.434, avg. samples / sec: 24552.52
Iteration:   2140, Loss function: 4.360, Average Loss: 4.421, avg. samples / sec: 24560.70
Iteration:   2140, Loss function: 4.302, Average Loss: 4.420, avg. samples / sec: 24566.81
Iteration:   2140, Loss function: 4.214, Average Loss: 4.435, avg. samples / sec: 24562.30
Iteration:   2160, Loss function: 4.409, Average Loss: 4.431, avg. samples / sec: 24556.57
Iteration:   2160, Loss function: 4.072, Average Loss: 4.423, avg. samples / sec: 24577.64
Iteration:   2160, Loss function: 4.711, Average Loss: 4.432, avg. samples / sec: 24615.48
Iteration:   2160, Loss function: 3.740, Average Loss: 4.427, avg. samples / sec: 24565.50
Iteration:   2160, Loss function: 4.435, Average Loss: 4.430, avg. samples / sec: 24558.45
Iteration:   2160, Loss function: 4.150, Average Loss: 4.414, avg. samples / sec: 24574.70
Iteration:   2160, Loss function: 3.887, Average Loss: 4.414, avg. samples / sec: 24559.46
Iteration:   2160, Loss function: 4.042, Average Loss: 4.413, avg. samples / sec: 24516.94
Iteration:   2180, Loss function: 4.496, Average Loss: 4.420, avg. samples / sec: 24545.42
Iteration:   2180, Loss function: 4.119, Average Loss: 4.425, avg. samples / sec: 24544.30
Iteration:   2180, Loss function: 4.432, Average Loss: 4.423, avg. samples / sec: 24545.65
Iteration:   2180, Loss function: 3.996, Average Loss: 4.426, avg. samples / sec: 24542.21
Iteration:   2180, Loss function: 4.158, Average Loss: 4.409, avg. samples / sec: 24574.67
Iteration:   2180, Loss function: 4.353, Average Loss: 4.408, avg. samples / sec: 24554.59
Iteration:   2180, Loss function: 4.315, Average Loss: 4.427, avg. samples / sec: 24535.15
Iteration:   2180, Loss function: 4.062, Average Loss: 4.409, avg. samples / sec: 24546.19

:::MLPv0.5.0 ssd 1541756955.100965261 (train.py:553) train_epoch: 38
Iteration:   2200, Loss function: 4.369, Average Loss: 4.421, avg. samples / sec: 24535.47
Iteration:   2200, Loss function: 3.780, Average Loss: 4.415, avg. samples / sec: 24531.48
Iteration:   2200, Loss function: 3.870, Average Loss: 4.420, avg. samples / sec: 24538.34
Iteration:   2200, Loss function: 4.434, Average Loss: 4.403, avg. samples / sec: 24544.55
Iteration:   2200, Loss function: 4.827, Average Loss: 4.421, avg. samples / sec: 24540.43
Iteration:   2200, Loss function: 4.911, Average Loss: 4.405, avg. samples / sec: 24522.57
Iteration:   2200, Loss function: 3.961, Average Loss: 4.418, avg. samples / sec: 24485.60
Iteration:   2200, Loss function: 3.771, Average Loss: 4.401, avg. samples / sec: 24521.13
Iteration:   2220, Loss function: 4.527, Average Loss: 4.414, avg. samples / sec: 24583.21
Iteration:   2220, Loss function: 3.957, Average Loss: 4.416, avg. samples / sec: 24609.87
Iteration:   2220, Loss function: 4.484, Average Loss: 4.412, avg. samples / sec: 24582.56
Iteration:   2220, Loss function: 4.876, Average Loss: 4.412, avg. samples / sec: 24635.04
Iteration:   2220, Loss function: 4.252, Average Loss: 4.396, avg. samples / sec: 24631.12
Iteration:   2220, Loss function: 4.148, Average Loss: 4.400, avg. samples / sec: 24565.73
Iteration:   2220, Loss function: 4.111, Average Loss: 4.414, avg. samples / sec: 24546.50
Iteration:   2220, Loss function: 4.043, Average Loss: 4.399, avg. samples / sec: 24565.22
Iteration:   2240, Loss function: 4.084, Average Loss: 4.407, avg. samples / sec: 24595.39
Iteration:   2240, Loss function: 4.335, Average Loss: 4.409, avg. samples / sec: 24591.08
Iteration:   2240, Loss function: 3.898, Average Loss: 4.406, avg. samples / sec: 24584.73
Iteration:   2240, Loss function: 4.239, Average Loss: 4.397, avg. samples / sec: 24593.12
Iteration:   2240, Loss function: 4.762, Average Loss: 4.409, avg. samples / sec: 24601.84
Iteration:   2240, Loss function: 4.294, Average Loss: 4.394, avg. samples / sec: 24562.83
Iteration:   2240, Loss function: 4.101, Average Loss: 4.395, avg. samples / sec: 24618.92
Iteration:   2240, Loss function: 3.958, Average Loss: 4.411, avg. samples / sec: 24526.68

:::MLPv0.5.0 ssd 1541756959.934257269 (train.py:553) train_epoch: 39
Iteration:   2260, Loss function: 4.102, Average Loss: 4.406, avg. samples / sec: 24546.29
Iteration:   2260, Loss function: 3.872, Average Loss: 4.403, avg. samples / sec: 24542.32
Iteration:   2260, Loss function: 4.029, Average Loss: 4.401, avg. samples / sec: 24520.70
Iteration:   2260, Loss function: 3.692, Average Loss: 4.388, avg. samples / sec: 24533.90
Iteration:   2260, Loss function: 4.610, Average Loss: 4.406, avg. samples / sec: 24522.80
Iteration:   2260, Loss function: 4.196, Average Loss: 4.408, avg. samples / sec: 24554.41
Iteration:   2260, Loss function: 4.247, Average Loss: 4.392, avg. samples / sec: 24519.64
Iteration:   2260, Loss function: 4.375, Average Loss: 4.392, avg. samples / sec: 24501.49
Iteration:   2280, Loss function: 4.569, Average Loss: 4.388, avg. samples / sec: 24635.68
Iteration:   2280, Loss function: 4.226, Average Loss: 4.399, avg. samples / sec: 24564.50
Iteration:   2280, Loss function: 4.131, Average Loss: 4.382, avg. samples / sec: 24601.62
Iteration:   2280, Loss function: 4.731, Average Loss: 4.390, avg. samples / sec: 24619.03
Iteration:   2280, Loss function: 4.211, Average Loss: 4.401, avg. samples / sec: 24601.96
Iteration:   2280, Loss function: 4.193, Average Loss: 4.394, avg. samples / sec: 24528.22
Iteration:   2280, Loss function: 4.449, Average Loss: 4.401, avg. samples / sec: 24569.13
Iteration:   2280, Loss function: 4.658, Average Loss: 4.395, avg. samples / sec: 24541.47
Iteration:   2300, Loss function: 4.185, Average Loss: 4.396, avg. samples / sec: 24588.16
Iteration:   2300, Loss function: 4.368, Average Loss: 4.388, avg. samples / sec: 24620.19
Iteration:   2300, Loss function: 4.250, Average Loss: 4.396, avg. samples / sec: 24583.63
Iteration:   2300, Loss function: 4.195, Average Loss: 4.387, avg. samples / sec: 24566.97
Iteration:   2300, Loss function: 3.819, Average Loss: 4.378, avg. samples / sec: 24563.08
Iteration:   2300, Loss function: 3.780, Average Loss: 4.393, avg. samples / sec: 24581.83
Iteration:   2300, Loss function: 3.617, Average Loss: 4.390, avg. samples / sec: 24582.20
Iteration:   2300, Loss function: 3.744, Average Loss: 4.384, avg. samples / sec: 24517.82

:::MLPv0.5.0 ssd 1541756964.772576094 (train.py:553) train_epoch: 40
Iteration:   2320, Loss function: 4.482, Average Loss: 4.391, avg. samples / sec: 24521.59
Iteration:   2320, Loss function: 4.142, Average Loss: 4.393, avg. samples / sec: 24544.08
Iteration:   2320, Loss function: 4.295, Average Loss: 4.379, avg. samples / sec: 24580.04
Iteration:   2320, Loss function: 4.761, Average Loss: 4.375, avg. samples / sec: 24539.53
Iteration:   2320, Loss function: 4.388, Average Loss: 4.386, avg. samples / sec: 24534.79
Iteration:   2320, Loss function: 4.391, Average Loss: 4.387, avg. samples / sec: 24564.93
Iteration:   2320, Loss function: 4.264, Average Loss: 4.387, avg. samples / sec: 24543.98
Iteration:   2320, Loss function: 4.246, Average Loss: 4.384, avg. samples / sec: 24480.89
Iteration:   2340, Loss function: 3.987, Average Loss: 4.385, avg. samples / sec: 24558.22
Iteration:   2340, Loss function: 4.470, Average Loss: 4.375, avg. samples / sec: 24557.09
Iteration:   2340, Loss function: 4.281, Average Loss: 4.379, avg. samples / sec: 24590.54
Iteration:   2340, Loss function: 3.961, Average Loss: 4.381, avg. samples / sec: 24562.86
Iteration:   2340, Loss function: 4.556, Average Loss: 4.372, avg. samples / sec: 24535.48
Iteration:   2340, Loss function: 4.764, Average Loss: 4.382, avg. samples / sec: 24533.59
Iteration:   2340, Loss function: 4.240, Average Loss: 4.389, avg. samples / sec: 24501.87
Iteration:   2340, Loss function: 4.202, Average Loss: 4.385, avg. samples / sec: 24531.00
Iteration:   2360, Loss function: 4.261, Average Loss: 4.367, avg. samples / sec: 24613.87
Iteration:   2360, Loss function: 4.313, Average Loss: 4.378, avg. samples / sec: 24573.85
Iteration:   2360, Loss function: 3.992, Average Loss: 4.383, avg. samples / sec: 24631.24
Iteration:   2360, Loss function: 4.515, Average Loss: 4.377, avg. samples / sec: 24592.98
Iteration:   2360, Loss function: 4.093, Average Loss: 4.375, avg. samples / sec: 24545.64
Iteration:   2360, Loss function: 4.568, Average Loss: 4.377, avg. samples / sec: 24538.13
Iteration:   2360, Loss function: 4.170, Average Loss: 4.370, avg. samples / sec: 24526.33
Iteration:   2360, Loss function: 4.356, Average Loss: 4.378, avg. samples / sec: 24576.01

:::MLPv0.5.0 ssd 1541756969.609330416 (train.py:553) train_epoch: 41
Iteration:   2380, Loss function: 4.027, Average Loss: 4.373, avg. samples / sec: 24527.20
Iteration:   2380, Loss function: 4.500, Average Loss: 4.364, avg. samples / sec: 24572.09
Iteration:   2380, Loss function: 3.384, Average Loss: 4.371, avg. samples / sec: 24556.76
Iteration:   2380, Loss function: 3.928, Average Loss: 4.371, avg. samples / sec: 24509.82
Iteration:   2380, Loss function: 4.044, Average Loss: 4.362, avg. samples / sec: 24487.01
Iteration:   2380, Loss function: 3.990, Average Loss: 4.370, avg. samples / sec: 24531.89
Iteration:   2380, Loss function: 3.786, Average Loss: 4.378, avg. samples / sec: 24479.49
Iteration:   2380, Loss function: 4.277, Average Loss: 4.371, avg. samples / sec: 24524.38
Iteration:   2400, Loss function: 4.853, Average Loss: 4.371, avg. samples / sec: 24574.03
Iteration:   2400, Loss function: 4.365, Average Loss: 4.359, avg. samples / sec: 24583.51
Iteration:   2400, Loss function: 4.469, Average Loss: 4.372, avg. samples / sec: 24606.79
Iteration:   2400, Loss function: 4.271, Average Loss: 4.365, avg. samples / sec: 24573.60
Iteration:   2400, Loss function: 4.732, Average Loss: 4.355, avg. samples / sec: 24580.57
Iteration:   2400, Loss function: 4.209, Average Loss: 4.362, avg. samples / sec: 24581.33
Iteration:   2400, Loss function: 4.301, Average Loss: 4.366, avg. samples / sec: 24579.70
Iteration:   2400, Loss function: 4.370, Average Loss: 4.365, avg. samples / sec: 24581.58
Iteration:   2420, Loss function: 3.313, Average Loss: 4.368, avg. samples / sec: 24628.91
Iteration:   2420, Loss function: 4.113, Average Loss: 4.368, avg. samples / sec: 24636.92
Iteration:   2420, Loss function: 4.398, Average Loss: 4.364, avg. samples / sec: 24673.51
Iteration:   2420, Loss function: 4.126, Average Loss: 4.360, avg. samples / sec: 24648.44
Iteration:   2420, Loss function: 4.260, Average Loss: 4.353, avg. samples / sec: 24625.04
Iteration:   2420, Loss function: 4.646, Average Loss: 4.367, avg. samples / sec: 24606.11
Iteration:   2420, Loss function: 4.082, Average Loss: 4.362, avg. samples / sec: 24615.47
Iteration:   2420, Loss function: 3.842, Average Loss: 4.356, avg. samples / sec: 24582.14

:::MLPv0.5.0 ssd 1541756974.357979059 (train.py:553) train_epoch: 42
Iteration:   2440, Loss function: 3.719, Average Loss: 4.361, avg. samples / sec: 24603.56
Iteration:   2440, Loss function: 4.375, Average Loss: 4.363, avg. samples / sec: 24645.35
Iteration:   2440, Loss function: 3.656, Average Loss: 4.357, avg. samples / sec: 24605.60
Iteration:   2440, Loss function: 3.831, Average Loss: 4.363, avg. samples / sec: 24603.43
Iteration:   2440, Loss function: 4.379, Average Loss: 4.357, avg. samples / sec: 24619.90
Iteration:   2440, Loss function: 4.151, Average Loss: 4.347, avg. samples / sec: 24598.07
Iteration:   2440, Loss function: 4.473, Average Loss: 4.356, avg. samples / sec: 24566.43
Iteration:   2440, Loss function: 4.047, Average Loss: 4.350, avg. samples / sec: 24586.59
Iteration:   2460, Loss function: 4.285, Average Loss: 4.357, avg. samples / sec: 24581.48
Iteration:   2460, Loss function: 4.295, Average Loss: 4.358, avg. samples / sec: 24580.10
Iteration:   2460, Loss function: 4.072, Average Loss: 4.344, avg. samples / sec: 24640.97
Iteration:   2460, Loss function: 3.896, Average Loss: 4.341, avg. samples / sec: 24594.32
Iteration:   2460, Loss function: 3.877, Average Loss: 4.345, avg. samples / sec: 24584.67
Iteration:   2460, Loss function: 4.093, Average Loss: 4.348, avg. samples / sec: 24569.62
Iteration:   2460, Loss function: 4.010, Average Loss: 4.357, avg. samples / sec: 24529.73
Iteration:   2460, Loss function: 4.054, Average Loss: 4.352, avg. samples / sec: 24525.80
Iteration:   2480, Loss function: 4.414, Average Loss: 4.352, avg. samples / sec: 24583.40
Iteration:   2480, Loss function: 3.867, Average Loss: 4.350, avg. samples / sec: 24585.14
Iteration:   2480, Loss function: 4.316, Average Loss: 4.337, avg. samples / sec: 24584.52
Iteration:   2480, Loss function: 3.789, Average Loss: 4.341, avg. samples / sec: 24637.50
Iteration:   2480, Loss function: 3.279, Average Loss: 4.343, avg. samples / sec: 24616.10
Iteration:   2480, Loss function: 3.998, Average Loss: 4.335, avg. samples / sec: 24568.07
Iteration:   2480, Loss function: 3.568, Average Loss: 4.350, avg. samples / sec: 24586.72
Iteration:   2480, Loss function: 3.709, Average Loss: 4.340, avg. samples / sec: 24545.31

:::MLPv0.5.0 ssd 1541756979.189655781 (train.py:553) train_epoch: 43
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1

:::MLPv0.5.0 ssd 1541756980.527758837 (train.py:578) opt_learning_rate: 0.016
Iteration:   2500, Loss function: 4.345, Average Loss: 4.337, avg. samples / sec: 24555.32
Iteration:   2500, Loss function: 4.107, Average Loss: 4.345, avg. samples / sec: 24541.86
Iteration:   2500, Loss function: 3.794, Average Loss: 4.345, avg. samples / sec: 24535.25
Iteration:   2500, Loss function: 3.904, Average Loss: 4.338, avg. samples / sec: 24571.16
Iteration:   2500, Loss function: 4.510, Average Loss: 4.333, avg. samples / sec: 24539.92
Iteration:   2500, Loss function: 4.163, Average Loss: 4.328, avg. samples / sec: 24574.21
Iteration:   2500, Loss function: 3.905, Average Loss: 4.333, avg. samples / sec: 24563.89
Iteration:   2500, Loss function: 3.992, Average Loss: 4.343, avg. samples / sec: 24550.96

































































:::MLPv0.5.0 ssd 1541756980.611411095 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541756980.611954212 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541756980.612440586 (train.py:220) eval_start: 43
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 46
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.89 s
Predicting Ended, total time: 3.89 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.89 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.89 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.89 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.89 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.89 s
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(267958, 7)
0/267958
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.89 s
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(267958, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/267958
Converting ndarray to lists...
Loading and preparing results...
(267958, 7)
(267958, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(267958, 7)
0/267958
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(267958, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/267958
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(267958, 7)
Converting ndarray to lists...
(267958, 7)
Converting ndarray to lists...
Loading and preparing results...
(267958, 7)
0/267958
(267958, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(267958, 7)
(267958, 7)
0/267958
(267958, 7)
0/267958
(267958, 7)
0/267958
0/267958
Converting ndarray to lists...
0/267958
0/267958
Loading and preparing results...
Loading and preparing results...
0/267958
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(267958, 7)
Loading and preparing results...
Loading and preparing results...
(267958, 7)
Loading and preparing results...
0/267958
Loading and preparing results...
Loading and preparing results...
(267958, 7)
Converting ndarray to lists...
(267958, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
0/267958
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/267958
Loading and preparing results...
Loading and preparing results...
(267958, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(267958, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(267958, 7)
(267958, 7)
Loading and preparing results...
Loading and preparing results...
(267958, 7)
Loading and preparing results...
0/267958
Converting ndarray to lists...
(267958, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(267958, 7)
(267958, 7)
(267958, 7)
(267958, 7)
Loading and preparing results...
0/267958
Converting ndarray to lists...
0/267958
(267958, 7)
(267958, 7)
(267958, 7)
Loading and preparing results...
0/267958
(267958, 7)
Converting ndarray to lists...
0/267958
(267958, 7)
0/267958
0/267958
Loading and preparing results...
0/267958
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(267958, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(267958, 7)
0/267958
0/267958
0/267958
(267958, 7)
(267958, 7)
0/267958
0/267958
0/267958
(267958, 7)
0/267958
Converting ndarray to lists...
(267958, 7)
0/267958
Converting ndarray to lists...
0/267958
Converting ndarray to lists...
0/267958
Loading and preparing results...
0/267958
Converting ndarray to lists...
Converting ndarray to lists...
(267958, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(267958, 7)
(267958, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(267958, 7)
(267958, 7)
Loading and preparing results...
0/267958
(267958, 7)
(267958, 7)
(267958, 7)
0/267958
0/267958
(267958, 7)
(267958, 7)
Converting ndarray to lists...
0/267958
0/267958
(267958, 7)
0/267958
0/267958
Converting ndarray to lists...
Converting ndarray to lists...
0/267958
Loading and preparing results...
(267958, 7)
0/267958
0/267958
Loading and preparing results...
0/267958
Loading and preparing results...
0/267958
(267958, 7)
0/267958
0/267958
0/267958
(267958, 7)
0/267958
0/267958
(267958, 7)
Converting ndarray to lists...
(267958, 7)
0/267958
Loading and preparing results...
0/267958
(267958, 7)
0/267958
Converting ndarray to lists...
0/267958
Converting ndarray to lists...
(267958, 7)
0/267958
Converting ndarray to lists...
(267958, 7)
Loading and preparing results...
(267958, 7)
Loading and preparing results...
(267958, 7)
0/267958
Loading and preparing results...
Converting ndarray to lists...
0/267958
0/267958
0/267958
Converting ndarray to lists...
(267958, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/267958
(267958, 7)
(267958, 7)
(267958, 7)
0/267958
0/267958
0/267958
DONE (t=1.52s)
creating index...
DONE (t=1.52s)
creating index...
DONE (t=1.52s)
creating index...
DONE (t=1.53s)
creating index...
DONE (t=1.53s)
creating index...
DONE (t=1.54s)
creating index...
DONE (t=1.54s)
creating index...
DONE (t=1.54s)
creating index...
DONE (t=1.54s)
creating index...
DONE (t=1.54s)
creating index...
DONE (t=1.54s)
creating index...
DONE (t=1.55s)
creating index...
DONE (t=1.55s)
creating index...
DONE (t=1.55s)
creating index...
DONE (t=1.55s)
creating index...
DONE (t=1.55s)
DONE (t=1.55s)
creating index...
creating index...
DONE (t=1.55s)
creating index...
DONE (t=1.55s)
creating index...
DONE (t=1.55s)
creating index...
DONE (t=1.55s)
creating index...
DONE (t=1.55s)
creating index...
DONE (t=1.55s)
creating index...
DONE (t=1.56s)
creating index...
DONE (t=1.56s)
creating index...
DONE (t=1.56s)
creating index...
DONE (t=1.56s)
creating index...
DONE (t=1.56s)
creating index...
DONE (t=1.56s)
creating index...
DONE (t=1.56s)
creating index...
DONE (t=1.56s)
creating index...
DONE (t=1.56s)
creating index...
DONE (t=1.56s)
creating index...
DONE (t=1.57s)
creating index...
DONE (t=1.57s)
creating index...
DONE (t=1.57s)
creating index...
DONE (t=1.57s)
creating index...
DONE (t=1.57s)
creating index...
DONE (t=1.57s)
creating index...
DONE (t=1.57s)
creating index...
DONE (t=1.57s)
creating index...
DONE (t=1.57s)
creating index...
DONE (t=1.57s)
creating index...
DONE (t=1.57s)
creating index...
DONE (t=1.58s)
creating index...
DONE (t=1.58s)
creating index...
DONE (t=1.58s)
creating index...
DONE (t=1.58s)
creating index...
DONE (t=1.58s)
creating index...
DONE (t=1.58s)
creating index...
DONE (t=1.58s)
creating index...
DONE (t=1.58s)
creating index...
DONE (t=1.58s)
creating index...
DONE (t=1.58s)
creating index...
DONE (t=1.58s)
creating index...
DONE (t=1.58s)
creating index...
DONE (t=1.59s)
creating index...
DONE (t=1.59s)
creating index...
DONE (t=1.59s)
creating index...
DONE (t=1.59s)
creating index...
DONE (t=1.59s)
creating index...
DONE (t=1.60s)
creating index...
DONE (t=1.60s)
creating index...
DONE (t=1.61s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=3.11s).
Accumulating evaluation results...
DONE (t=3.12s).
Accumulating evaluation results...
DONE (t=3.11s).
Accumulating evaluation results...
DONE (t=3.13s).
Accumulating evaluation results...
DONE (t=3.11s).
Accumulating evaluation results...
DONE (t=3.14s).
Accumulating evaluation results...
DONE (t=3.19s).
Accumulating evaluation results...
DONE (t=3.15s).
Accumulating evaluation results...
DONE (t=1.02s).
DONE (t=1.01s).
DONE (t=1.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.151
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.151
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.287
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.151
DONE (t=1.02s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.287
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.287
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.151
DONE (t=1.02s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.287
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.151
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.244
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
DONE (t=1.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.287
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.248
DONE (t=1.03s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.068
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.276
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402
Current AP: 0.15070 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.151
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.068
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.276
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402
Current AP: 0.15070 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.151
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.068
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.276
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402
Current AP: 0.15070 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.287
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.145
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.287
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.248
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.068
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.276
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402
Current AP: 0.15070 AP goal: 0.21200
DONE (t=1.03s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.244
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.145
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.248
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.068
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.276
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402
Current AP: 0.15070 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.151
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.248
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.287
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.068
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.276
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402
Current AP: 0.15070 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.145
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.068
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.276
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402
Current AP: 0.15070 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.168
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.248
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.068
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.276
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.402
Current AP: 0.15070 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541756990.649937153 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541756990.650556087 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.1506999748181578}

:::MLPv0.5.0 ssd 1541756990.651022673 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.1506999748181578}

:::MLPv0.5.0 ssd 1541756990.651469707 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541756990.651913643 (train.py:338) eval_stop: 43
Iteration:   2520, Loss function: 4.022, Average Loss: 4.326, avg. samples / sec: 3382.75
Iteration:   2520, Loss function: 4.263, Average Loss: 4.318, avg. samples / sec: 3382.80
Iteration:   2520, Loss function: 3.953, Average Loss: 4.322, avg. samples / sec: 3382.61
Iteration:   2520, Loss function: 3.253, Average Loss: 4.334, avg. samples / sec: 3382.51
Iteration:   2520, Loss function: 4.028, Average Loss: 4.333, avg. samples / sec: 3382.51
Iteration:   2520, Loss function: 3.488, Average Loss: 4.321, avg. samples / sec: 3383.19
Iteration:   2520, Loss function: 3.350, Average Loss: 4.326, avg. samples / sec: 3381.85
Iteration:   2520, Loss function: 4.011, Average Loss: 4.332, avg. samples / sec: 3382.20
Iteration:   2540, Loss function: 3.931, Average Loss: 4.318, avg. samples / sec: 24584.99
Iteration:   2540, Loss function: 3.504, Average Loss: 4.312, avg. samples / sec: 24563.43
Iteration:   2540, Loss function: 3.241, Average Loss: 4.311, avg. samples / sec: 24599.30
Iteration:   2540, Loss function: 3.241, Average Loss: 4.307, avg. samples / sec: 24565.35
Iteration:   2540, Loss function: 3.144, Average Loss: 4.307, avg. samples / sec: 24557.73
Iteration:   2540, Loss function: 3.342, Average Loss: 4.302, avg. samples / sec: 24544.11
Iteration:   2540, Loss function: 3.184, Average Loss: 4.319, avg. samples / sec: 24518.88
Iteration:   2540, Loss function: 3.652, Average Loss: 4.316, avg. samples / sec: 24580.22

:::MLPv0.5.0 ssd 1541756994.468758345 (train.py:553) train_epoch: 44
Iteration:   2560, Loss function: 3.417, Average Loss: 4.298, avg. samples / sec: 24511.24
Iteration:   2560, Loss function: 3.782, Average Loss: 4.290, avg. samples / sec: 24525.32
Iteration:   2560, Loss function: 3.461, Average Loss: 4.301, avg. samples / sec: 24499.76
Iteration:   2560, Loss function: 3.837, Average Loss: 4.298, avg. samples / sec: 24561.82
Iteration:   2560, Loss function: 3.845, Average Loss: 4.290, avg. samples / sec: 24514.88
Iteration:   2560, Loss function: 4.177, Average Loss: 4.286, avg. samples / sec: 24519.52
Iteration:   2560, Loss function: 2.869, Average Loss: 4.304, avg. samples / sec: 24517.05
Iteration:   2560, Loss function: 3.408, Average Loss: 4.294, avg. samples / sec: 24459.94
Iteration:   2580, Loss function: 3.593, Average Loss: 4.282, avg. samples / sec: 24584.22
Iteration:   2580, Loss function: 3.909, Average Loss: 4.287, avg. samples / sec: 24588.23
Iteration:   2580, Loss function: 3.140, Average Loss: 4.275, avg. samples / sec: 24572.69
Iteration:   2580, Loss function: 3.414, Average Loss: 4.277, avg. samples / sec: 24606.19
Iteration:   2580, Loss function: 3.583, Average Loss: 4.286, avg. samples / sec: 24598.23
Iteration:   2580, Loss function: 3.508, Average Loss: 4.269, avg. samples / sec: 24558.22
Iteration:   2580, Loss function: 3.696, Average Loss: 4.272, avg. samples / sec: 24544.90
Iteration:   2580, Loss function: 3.278, Average Loss: 4.284, avg. samples / sec: 24542.31

:::MLPv0.5.0 ssd 1541756999.307695389 (train.py:553) train_epoch: 45
Iteration:   2600, Loss function: 3.583, Average Loss: 4.252, avg. samples / sec: 24568.08
Iteration:   2600, Loss function: 3.044, Average Loss: 4.270, avg. samples / sec: 24526.49
Iteration:   2600, Loss function: 3.201, Average Loss: 4.256, avg. samples / sec: 24547.22
Iteration:   2600, Loss function: 3.395, Average Loss: 4.259, avg. samples / sec: 24530.19
Iteration:   2600, Loss function: 3.338, Average Loss: 4.268, avg. samples / sec: 24550.70
Iteration:   2600, Loss function: 3.232, Average Loss: 4.259, avg. samples / sec: 24527.73
Iteration:   2600, Loss function: 2.928, Average Loss: 4.265, avg. samples / sec: 24481.96
Iteration:   2600, Loss function: 3.195, Average Loss: 4.269, avg. samples / sec: 24512.15
Iteration:   2620, Loss function: 3.509, Average Loss: 4.241, avg. samples / sec: 24602.99
Iteration:   2620, Loss function: 3.091, Average Loss: 4.252, avg. samples / sec: 24569.71
Iteration:   2620, Loss function: 3.350, Average Loss: 4.243, avg. samples / sec: 24587.95
Iteration:   2620, Loss function: 2.938, Average Loss: 4.252, avg. samples / sec: 24563.50
Iteration:   2620, Loss function: 3.323, Average Loss: 4.250, avg. samples / sec: 24583.77
Iteration:   2620, Loss function: 3.772, Average Loss: 4.239, avg. samples / sec: 24544.79
Iteration:   2620, Loss function: 3.722, Average Loss: 4.237, avg. samples / sec: 24510.18
Iteration:   2620, Loss function: 3.034, Average Loss: 4.250, avg. samples / sec: 24562.33
Iteration:   2640, Loss function: 3.193, Average Loss: 4.220, avg. samples / sec: 24638.61
Iteration:   2640, Loss function: 3.319, Average Loss: 4.237, avg. samples / sec: 24579.48
Iteration:   2640, Loss function: 3.240, Average Loss: 4.233, avg. samples / sec: 24607.04
Iteration:   2640, Loss function: 3.629, Average Loss: 4.227, avg. samples / sec: 24566.09
Iteration:   2640, Loss function: 3.195, Average Loss: 4.222, avg. samples / sec: 24545.45
Iteration:   2640, Loss function: 3.059, Average Loss: 4.234, avg. samples / sec: 24576.10
Iteration:   2640, Loss function: 3.461, Average Loss: 4.235, avg. samples / sec: 24595.77
Iteration:   2640, Loss function: 3.152, Average Loss: 4.222, avg. samples / sec: 24577.71

:::MLPv0.5.0 ssd 1541757004.061092615 (train.py:553) train_epoch: 46
Iteration:   2660, Loss function: 2.825, Average Loss: 4.204, avg. samples / sec: 24540.01
Iteration:   2660, Loss function: 3.033, Average Loss: 4.220, avg. samples / sec: 24541.02
Iteration:   2660, Loss function: 3.195, Average Loss: 4.212, avg. samples / sec: 24548.52
Iteration:   2660, Loss function: 3.222, Average Loss: 4.207, avg. samples / sec: 24535.28
Iteration:   2660, Loss function: 3.257, Average Loss: 4.218, avg. samples / sec: 24532.97
Iteration:   2660, Loss function: 3.533, Average Loss: 4.217, avg. samples / sec: 24517.70
Iteration:   2660, Loss function: 3.688, Average Loss: 4.214, avg. samples / sec: 24485.63
Iteration:   2660, Loss function: 3.975, Average Loss: 4.207, avg. samples / sec: 24526.02
Iteration:   2680, Loss function: 2.986, Average Loss: 4.188, avg. samples / sec: 24541.84
Iteration:   2680, Loss function: 2.930, Average Loss: 4.203, avg. samples / sec: 24537.35
Iteration:   2680, Loss function: 2.993, Average Loss: 4.197, avg. samples / sec: 24542.51
Iteration:   2680, Loss function: 3.347, Average Loss: 4.198, avg. samples / sec: 24594.84
Iteration:   2680, Loss function: 3.213, Average Loss: 4.189, avg. samples / sec: 24550.19
Iteration:   2680, Loss function: 3.337, Average Loss: 4.191, avg. samples / sec: 24568.20
Iteration:   2680, Loss function: 3.478, Average Loss: 4.199, avg. samples / sec: 24544.95
Iteration:   2680, Loss function: 3.616, Average Loss: 4.200, avg. samples / sec: 24542.50
Iteration:   2700, Loss function: 2.847, Average Loss: 4.173, avg. samples / sec: 24572.13
Iteration:   2700, Loss function: 3.231, Average Loss: 4.184, avg. samples / sec: 24578.81
Iteration:   2700, Loss function: 3.887, Average Loss: 4.173, avg. samples / sec: 24593.82
Iteration:   2700, Loss function: 3.373, Average Loss: 4.182, avg. samples / sec: 24567.79
Iteration:   2700, Loss function: 2.834, Average Loss: 4.179, avg. samples / sec: 24565.74
Iteration:   2700, Loss function: 3.533, Average Loss: 4.184, avg. samples / sec: 24568.52
Iteration:   2700, Loss function: 3.703, Average Loss: 4.176, avg. samples / sec: 24551.28
Iteration:   2700, Loss function: 3.515, Average Loss: 4.185, avg. samples / sec: 24566.44

:::MLPv0.5.0 ssd 1541757008.900911570 (train.py:553) train_epoch: 47
Iteration:   2720, Loss function: 3.448, Average Loss: 4.169, avg. samples / sec: 24520.56
Iteration:   2720, Loss function: 3.783, Average Loss: 4.160, avg. samples / sec: 24513.82
Iteration:   2720, Loss function: 3.045, Average Loss: 4.167, avg. samples / sec: 24525.05
Iteration:   2720, Loss function: 3.099, Average Loss: 4.167, avg. samples / sec: 24540.28
Iteration:   2720, Loss function: 3.346, Average Loss: 4.162, avg. samples / sec: 24546.75
Iteration:   2720, Loss function: 4.079, Average Loss: 4.156, avg. samples / sec: 24475.13
Iteration:   2720, Loss function: 3.170, Average Loss: 4.162, avg. samples / sec: 24467.99
Iteration:   2720, Loss function: 3.500, Average Loss: 4.170, avg. samples / sec: 24512.80
Iteration:   2740, Loss function: 3.620, Average Loss: 4.145, avg. samples / sec: 24604.42
Iteration:   2740, Loss function: 3.427, Average Loss: 4.154, avg. samples / sec: 24600.71
Iteration:   2740, Loss function: 3.444, Average Loss: 4.151, avg. samples / sec: 24597.81
Iteration:   2740, Loss function: 3.631, Average Loss: 4.152, avg. samples / sec: 24596.55
Iteration:   2740, Loss function: 3.485, Average Loss: 4.140, avg. samples / sec: 24601.16
Iteration:   2740, Loss function: 3.036, Average Loss: 4.146, avg. samples / sec: 24579.29
Iteration:   2740, Loss function: 3.215, Average Loss: 4.145, avg. samples / sec: 24612.32
Iteration:   2740, Loss function: 3.052, Average Loss: 4.153, avg. samples / sec: 24605.09
Iteration:   2760, Loss function: 3.039, Average Loss: 4.129, avg. samples / sec: 24566.44
Iteration:   2760, Loss function: 3.391, Average Loss: 4.139, avg. samples / sec: 24563.62
Iteration:   2760, Loss function: 3.321, Average Loss: 4.136, avg. samples / sec: 24628.07
Iteration:   2760, Loss function: 3.333, Average Loss: 4.134, avg. samples / sec: 24569.04
Iteration:   2760, Loss function: 3.402, Average Loss: 4.129, avg. samples / sec: 24605.16
Iteration:   2760, Loss function: 3.459, Average Loss: 4.134, avg. samples / sec: 24577.76
Iteration:   2760, Loss function: 3.239, Average Loss: 4.123, avg. samples / sec: 24586.02
Iteration:   2760, Loss function: 3.354, Average Loss: 4.133, avg. samples / sec: 24561.55

:::MLPv0.5.0 ssd 1541757013.738384962 (train.py:553) train_epoch: 48
Iteration:   2780, Loss function: 3.158, Average Loss: 4.116, avg. samples / sec: 24518.51
Iteration:   2780, Loss function: 4.135, Average Loss: 4.117, avg. samples / sec: 24511.74
Iteration:   2780, Loss function: 3.081, Average Loss: 4.113, avg. samples / sec: 24525.63
Iteration:   2780, Loss function: 3.797, Average Loss: 4.124, avg. samples / sec: 24515.63
Iteration:   2780, Loss function: 2.848, Average Loss: 4.119, avg. samples / sec: 24514.71
Iteration:   2780, Loss function: 3.308, Average Loss: 4.107, avg. samples / sec: 24517.13
Iteration:   2780, Loss function: 3.608, Average Loss: 4.116, avg. samples / sec: 24525.19
Iteration:   2780, Loss function: 3.190, Average Loss: 4.121, avg. samples / sec: 24464.86
Iteration:   2800, Loss function: 3.251, Average Loss: 4.108, avg. samples / sec: 24595.88
Iteration:   2800, Loss function: 3.418, Average Loss: 4.102, avg. samples / sec: 24589.67
Iteration:   2800, Loss function: 2.967, Average Loss: 4.101, avg. samples / sec: 24583.02
Iteration:   2800, Loss function: 3.580, Average Loss: 4.098, avg. samples / sec: 24621.31
Iteration:   2800, Loss function: 3.203, Average Loss: 4.101, avg. samples / sec: 24581.64
Iteration:   2800, Loss function: 3.114, Average Loss: 4.093, avg. samples / sec: 24582.10
Iteration:   2800, Loss function: 3.074, Average Loss: 4.096, avg. samples / sec: 24543.00
Iteration:   2800, Loss function: 2.690, Average Loss: 4.104, avg. samples / sec: 24575.93

































































:::MLPv0.5.0 ssd 1541757017.071466446 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757017.072007656 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757017.072471619 (train.py:220) eval_start: 48
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 46
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.09 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.09 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.09 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.09 s
Predicting Ended, total time: 4.09 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.09 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.09 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.09 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(322349, 7)
(322349, 7)
Converting ndarray to lists...
(322349, 7)
Loading and preparing results...
Converting ndarray to lists...
(322349, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(322349, 7)
0/322349
(322349, 7)
Loading and preparing results...
(322349, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(322349, 7)
(322349, 7)
0/322349
Loading and preparing results...
Converting ndarray to lists...
(322349, 7)
0/322349
Converting ndarray to lists...
Converting ndarray to lists...
0/322349
0/322349
0/322349
Converting ndarray to lists...
Loading and preparing results...
0/322349
0/322349
Loading and preparing results...
Loading and preparing results...
0/322349
(322349, 7)
Loading and preparing results...
(322349, 7)
(322349, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/322349
(322349, 7)
Loading and preparing results...
(322349, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(322349, 7)
(322349, 7)
Loading and preparing results...
0/322349
Converting ndarray to lists...
(322349, 7)
0/322349
0/322349
0/322349
Loading and preparing results...
(322349, 7)
(322349, 7)
Loading and preparing results...
0/322349
Converting ndarray to lists...
0/322349
Loading and preparing results...
Loading and preparing results...
(322349, 7)
Loading and preparing results...
0/322349
Converting ndarray to lists...
Loading and preparing results...
0/322349
Converting ndarray to lists...
0/322349
0/322349
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(322349, 7)
0/322349
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(322349, 7)
0/322349
(322349, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(322349, 7)
(322349, 7)
(322349, 7)
(322349, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/322349
Converting ndarray to lists...
Loading and preparing results...
(322349, 7)
Loading and preparing results...
Converting ndarray to lists...
0/322349
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(322349, 7)
(322349, 7)
0/322349
0/322349
0/322349
(322349, 7)
(322349, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(322349, 7)
(322349, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(322349, 7)
Loading and preparing results...
0/322349
(322349, 7)
Loading and preparing results...
Converting ndarray to lists...
0/322349
0/322349
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/322349
Converting ndarray to lists...
0/322349
Converting ndarray to lists...
Converting ndarray to lists...
0/322349
Converting ndarray to lists...
0/322349
0/322349
(322349, 7)
0/322349
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(322349, 7)
(322349, 7)
(322349, 7)
0/322349
Loading and preparing results...
(322349, 7)
Loading and preparing results...
(322349, 7)
Converting ndarray to lists...
(322349, 7)
(322349, 7)
0/322349
Converting ndarray to lists...
Loading and preparing results...
0/322349
Loading and preparing results...
Converting ndarray to lists...
(322349, 7)
0/322349
(322349, 7)
(322349, 7)
Converting ndarray to lists...
0/322349
(322349, 7)
(322349, 7)
0/322349
0/322349
(322349, 7)
0/322349
Loading and preparing results...
0/322349
Converting ndarray to lists...
0/322349
0/322349
Loading and preparing results...
0/322349
0/322349
(322349, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/322349
Converting ndarray to lists...
(322349, 7)
0/322349
(322349, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(322349, 7)
0/322349
Converting ndarray to lists...
0/322349
(322349, 7)
(322349, 7)
(322349, 7)
Converting ndarray to lists...
0/322349
(322349, 7)
Converting ndarray to lists...
Loading and preparing results...
0/322349
0/322349
Converting ndarray to lists...
(322349, 7)
0/322349
(322349, 7)
0/322349
0/322349
0/322349
0/322349
(322349, 7)
Converting ndarray to lists...
(322349, 7)
0/322349
(322349, 7)
0/322349
0/322349
DONE (t=1.84s)
creating index...
DONE (t=1.86s)
creating index...
DONE (t=1.87s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.88s)
creating index...
DONE (t=1.90s)
creating index...
DONE (t=1.90s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=2.11s)
creating index...
DONE (t=2.11s)
creating index...
DONE (t=2.12s)
creating index...
DONE (t=2.12s)
creating index...
DONE (t=2.12s)
creating index...
DONE (t=2.12s)
creating index...
DONE (t=2.13s)
creating index...
DONE (t=2.13s)
creating index...
DONE (t=2.13s)
creating index...
DONE (t=2.13s)
creating index...
DONE (t=2.13s)
creating index...
DONE (t=2.13s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.15s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.16s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.17s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.18s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.22s)
creating index...
index created!
DONE (t=2.22s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.25s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
DONE (t=3.32s).
Accumulating evaluation results...
DONE (t=3.40s).
Accumulating evaluation results...
DONE (t=3.37s).
Accumulating evaluation results...
DONE (t=3.39s).
Accumulating evaluation results...
DONE (t=3.38s).
Accumulating evaluation results...
DONE (t=3.37s).
Accumulating evaluation results...
DONE (t=3.42s).
Accumulating evaluation results...
DONE (t=3.50s).
Accumulating evaluation results...
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497
Current AP: 0.20780 AP goal: 0.21200
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
DONE (t=1.09s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
DONE (t=1.09s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
DONE (t=1.10s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497
Current AP: 0.20780 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
DONE (t=1.09s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497
Current AP: 0.20780 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497
Current AP: 0.20780 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497
Current AP: 0.20780 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
DONE (t=1.11s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497
Current AP: 0.20780 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.210
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497
Current AP: 0.20780 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.330
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.208
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.318
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.340
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.497
Current AP: 0.20780 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757028.116792917 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757028.117429495 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.20779588674613236}

:::MLPv0.5.0 ssd 1541757028.117916584 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.20779588674613236}

:::MLPv0.5.0 ssd 1541757028.118381262 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757028.118827820 (train.py:338) eval_stop: 48
Iteration:   2820, Loss function: 3.458, Average Loss: 4.093, avg. samples / sec: 3121.95
Iteration:   2820, Loss function: 3.116, Average Loss: 4.077, avg. samples / sec: 3122.58
Iteration:   2820, Loss function: 3.313, Average Loss: 4.086, avg. samples / sec: 3121.93
Iteration:   2820, Loss function: 3.548, Average Loss: 4.086, avg. samples / sec: 3122.01
Iteration:   2820, Loss function: 3.656, Average Loss: 4.083, avg. samples / sec: 3121.81
Iteration:   2820, Loss function: 3.217, Average Loss: 4.086, avg. samples / sec: 3121.93
Iteration:   2820, Loss function: 3.267, Average Loss: 4.079, avg. samples / sec: 3121.83
Iteration:   2820, Loss function: 3.489, Average Loss: 4.089, avg. samples / sec: 3122.08

:::MLPv0.5.0 ssd 1541757030.024242401 (train.py:553) train_epoch: 49
Iteration:   2840, Loss function: 3.397, Average Loss: 4.061, avg. samples / sec: 24581.74
Iteration:   2840, Loss function: 2.809, Average Loss: 4.064, avg. samples / sec: 24637.38
Iteration:   2840, Loss function: 3.094, Average Loss: 4.077, avg. samples / sec: 24577.77
Iteration:   2840, Loss function: 3.166, Average Loss: 4.072, avg. samples / sec: 24614.96
Iteration:   2840, Loss function: 2.991, Average Loss: 4.072, avg. samples / sec: 24585.52
Iteration:   2840, Loss function: 3.300, Average Loss: 4.073, avg. samples / sec: 24536.92
Iteration:   2840, Loss function: 2.943, Average Loss: 4.065, avg. samples / sec: 24546.26
Iteration:   2840, Loss function: 2.640, Average Loss: 4.070, avg. samples / sec: 24527.06
Iteration:   2860, Loss function: 3.302, Average Loss: 4.051, avg. samples / sec: 24627.95
Iteration:   2860, Loss function: 3.864, Average Loss: 4.046, avg. samples / sec: 24573.73
Iteration:   2860, Loss function: 3.419, Average Loss: 4.061, avg. samples / sec: 24568.80
Iteration:   2860, Loss function: 3.553, Average Loss: 4.059, avg. samples / sec: 24616.07
Iteration:   2860, Loss function: 3.784, Average Loss: 4.058, avg. samples / sec: 24586.24
Iteration:   2860, Loss function: 3.258, Average Loss: 4.048, avg. samples / sec: 24531.03
Iteration:   2860, Loss function: 3.072, Average Loss: 4.054, avg. samples / sec: 24587.52
Iteration:   2860, Loss function: 2.818, Average Loss: 4.055, avg. samples / sec: 24543.15
Iteration:   2880, Loss function: 3.276, Average Loss: 4.035, avg. samples / sec: 24625.62
Iteration:   2880, Loss function: 3.824, Average Loss: 4.039, avg. samples / sec: 24666.78
Iteration:   2880, Loss function: 3.007, Average Loss: 4.043, avg. samples / sec: 24624.02
Iteration:   2880, Loss function: 3.631, Average Loss: 4.040, avg. samples / sec: 24665.79
Iteration:   2880, Loss function: 4.440, Average Loss: 4.033, avg. samples / sec: 24608.22
Iteration:   2880, Loss function: 3.648, Average Loss: 4.033, avg. samples / sec: 24630.33
Iteration:   2880, Loss function: 3.701, Average Loss: 4.045, avg. samples / sec: 24594.07
Iteration:   2880, Loss function: 3.263, Average Loss: 4.042, avg. samples / sec: 24567.43

:::MLPv0.5.0 ssd 1541757034.771711349 (train.py:553) train_epoch: 50
Iteration:   2900, Loss function: 2.887, Average Loss: 4.021, avg. samples / sec: 24576.54
Iteration:   2900, Loss function: 3.185, Average Loss: 4.020, avg. samples / sec: 24568.59
Iteration:   2900, Loss function: 3.382, Average Loss: 4.028, avg. samples / sec: 24568.92
Iteration:   2900, Loss function: 3.315, Average Loss: 4.031, avg. samples / sec: 24598.02
Iteration:   2900, Loss function: 3.443, Average Loss: 4.019, avg. samples / sec: 24584.98
Iteration:   2900, Loss function: 3.100, Average Loss: 4.019, avg. samples / sec: 24544.03
Iteration:   2900, Loss function: 2.823, Average Loss: 4.027, avg. samples / sec: 24582.15
Iteration:   2900, Loss function: 3.178, Average Loss: 4.027, avg. samples / sec: 24522.40
Iteration:   2920, Loss function: 3.472, Average Loss: 4.004, avg. samples / sec: 24597.90
Iteration:   2920, Loss function: 2.910, Average Loss: 4.004, avg. samples / sec: 24606.65
Iteration:   2920, Loss function: 3.743, Average Loss: 4.015, avg. samples / sec: 24606.20
Iteration:   2920, Loss function: 3.263, Average Loss: 4.015, avg. samples / sec: 24604.40
Iteration:   2920, Loss function: 3.762, Average Loss: 4.011, avg. samples / sec: 24658.50
Iteration:   2920, Loss function: 3.263, Average Loss: 4.004, avg. samples / sec: 24584.83
Iteration:   2920, Loss function: 3.044, Average Loss: 4.016, avg. samples / sec: 24602.46
Iteration:   2920, Loss function: 2.963, Average Loss: 4.003, avg. samples / sec: 24586.93
Iteration:   2940, Loss function: 3.306, Average Loss: 3.987, avg. samples / sec: 24551.90
Iteration:   2940, Loss function: 3.389, Average Loss: 3.990, avg. samples / sec: 24609.05
Iteration:   2940, Loss function: 3.664, Average Loss: 3.990, avg. samples / sec: 24546.74
Iteration:   2940, Loss function: 3.070, Average Loss: 3.998, avg. samples / sec: 24548.35
Iteration:   2940, Loss function: 3.177, Average Loss: 4.000, avg. samples / sec: 24546.75
Iteration:   2940, Loss function: 3.306, Average Loss: 3.990, avg. samples / sec: 24553.46
Iteration:   2940, Loss function: 3.653, Average Loss: 3.997, avg. samples / sec: 24511.52
Iteration:   2940, Loss function: 3.512, Average Loss: 4.002, avg. samples / sec: 24552.55

:::MLPv0.5.0 ssd 1541757039.604897976 (train.py:553) train_epoch: 51
Iteration:   2960, Loss function: 3.096, Average Loss: 3.975, avg. samples / sec: 24567.87
Iteration:   2960, Loss function: 2.957, Average Loss: 3.973, avg. samples / sec: 24592.30
Iteration:   2960, Loss function: 3.026, Average Loss: 3.971, avg. samples / sec: 24553.76
Iteration:   2960, Loss function: 3.365, Average Loss: 3.984, avg. samples / sec: 24540.16
Iteration:   2960, Loss function: 3.569, Average Loss: 3.984, avg. samples / sec: 24536.85
Iteration:   2960, Loss function: 3.121, Average Loss: 3.975, avg. samples / sec: 24526.99
Iteration:   2960, Loss function: 3.446, Average Loss: 3.984, avg. samples / sec: 24548.08
Iteration:   2960, Loss function: 3.311, Average Loss: 3.988, avg. samples / sec: 24553.82
Iteration:   2980, Loss function: 3.411, Average Loss: 3.958, avg. samples / sec: 24582.76
Iteration:   2980, Loss function: 3.509, Average Loss: 3.962, avg. samples / sec: 24574.23
Iteration:   2980, Loss function: 3.481, Average Loss: 3.971, avg. samples / sec: 24597.93
Iteration:   2980, Loss function: 3.283, Average Loss: 3.969, avg. samples / sec: 24587.34
Iteration:   2980, Loss function: 3.491, Average Loss: 3.968, avg. samples / sec: 24598.17
Iteration:   2980, Loss function: 3.417, Average Loss: 3.974, avg. samples / sec: 24596.16
Iteration:   2980, Loss function: 3.540, Average Loss: 3.959, avg. samples / sec: 24528.22
Iteration:   2980, Loss function: 3.036, Average Loss: 3.963, avg. samples / sec: 24549.79
Iteration:   3000, Loss function: 3.111, Average Loss: 3.943, avg. samples / sec: 24572.23
Iteration:   3000, Loss function: 3.470, Average Loss: 3.948, avg. samples / sec: 24566.43
Iteration:   3000, Loss function: 3.176, Average Loss: 3.953, avg. samples / sec: 24584.96
Iteration:   3000, Loss function: 3.433, Average Loss: 3.958, avg. samples / sec: 24586.98
Iteration:   3000, Loss function: 2.912, Average Loss: 3.956, avg. samples / sec: 24557.19
Iteration:   3000, Loss function: 3.349, Average Loss: 3.944, avg. samples / sec: 24585.97
Iteration:   3000, Loss function: 3.103, Average Loss: 3.955, avg. samples / sec: 24550.30
Iteration:   3000, Loss function: 2.843, Average Loss: 3.949, avg. samples / sec: 24593.26

:::MLPv0.5.0 ssd 1541757044.439480305 (train.py:553) train_epoch: 52
Iteration:   3020, Loss function: 3.164, Average Loss: 3.929, avg. samples / sec: 24600.10
Iteration:   3020, Loss function: 3.440, Average Loss: 3.931, avg. samples / sec: 24561.96
Iteration:   3020, Loss function: 3.433, Average Loss: 3.934, avg. samples / sec: 24567.86
Iteration:   3020, Loss function: 3.247, Average Loss: 3.940, avg. samples / sec: 24591.39
Iteration:   3020, Loss function: 3.193, Average Loss: 3.943, avg. samples / sec: 24586.70
Iteration:   3020, Loss function: 2.897, Average Loss: 3.938, avg. samples / sec: 24528.31
Iteration:   3020, Loss function: 3.127, Average Loss: 3.945, avg. samples / sec: 24529.61
Iteration:   3020, Loss function: 3.279, Average Loss: 3.936, avg. samples / sec: 24536.83
Iteration:   3040, Loss function: 2.844, Average Loss: 3.928, avg. samples / sec: 24624.38
Iteration:   3040, Loss function: 2.724, Average Loss: 3.914, avg. samples / sec: 24561.41
Iteration:   3040, Loss function: 3.740, Average Loss: 3.919, avg. samples / sec: 24559.98
Iteration:   3040, Loss function: 2.911, Average Loss: 3.927, avg. samples / sec: 24559.30
Iteration:   3040, Loss function: 3.288, Average Loss: 3.927, avg. samples / sec: 24542.58
Iteration:   3040, Loss function: 3.216, Average Loss: 3.922, avg. samples / sec: 24571.74
Iteration:   3040, Loss function: 2.689, Average Loss: 3.914, avg. samples / sec: 24503.41
Iteration:   3040, Loss function: 2.918, Average Loss: 3.921, avg. samples / sec: 24555.52
Iteration:   3060, Loss function: 3.495, Average Loss: 3.900, avg. samples / sec: 24572.02
Iteration:   3060, Loss function: 3.310, Average Loss: 3.901, avg. samples / sec: 24624.37
Iteration:   3060, Loss function: 3.162, Average Loss: 3.912, avg. samples / sec: 24574.27
Iteration:   3060, Loss function: 4.029, Average Loss: 3.907, avg. samples / sec: 24564.20
Iteration:   3060, Loss function: 3.140, Average Loss: 3.914, avg. samples / sec: 24570.71
Iteration:   3060, Loss function: 3.161, Average Loss: 3.907, avg. samples / sec: 24581.08
Iteration:   3060, Loss function: 3.031, Average Loss: 3.912, avg. samples / sec: 24505.88
Iteration:   3060, Loss function: 2.972, Average Loss: 3.907, avg. samples / sec: 24558.75

:::MLPv0.5.0 ssd 1541757049.276975393 (train.py:553) train_epoch: 53
Iteration:   3080, Loss function: 3.024, Average Loss: 3.893, avg. samples / sec: 24574.13
Iteration:   3080, Loss function: 2.994, Average Loss: 3.897, avg. samples / sec: 24592.34
Iteration:   3080, Loss function: 3.610, Average Loss: 3.887, avg. samples / sec: 24561.42
Iteration:   3080, Loss function: 3.574, Average Loss: 3.900, avg. samples / sec: 24559.37
Iteration:   3080, Loss function: 3.114, Average Loss: 3.898, avg. samples / sec: 24612.02
Iteration:   3080, Loss function: 2.864, Average Loss: 3.894, avg. samples / sec: 24582.13
Iteration:   3080, Loss function: 2.986, Average Loss: 3.892, avg. samples / sec: 24568.65
Iteration:   3080, Loss function: 2.968, Average Loss: 3.889, avg. samples / sec: 24513.05
Iteration:   3100, Loss function: 3.109, Average Loss: 3.874, avg. samples / sec: 24624.61
Iteration:   3100, Loss function: 3.069, Average Loss: 3.880, avg. samples / sec: 24620.23
Iteration:   3100, Loss function: 3.124, Average Loss: 3.885, avg. samples / sec: 24624.76
Iteration:   3100, Loss function: 3.236, Average Loss: 3.885, avg. samples / sec: 24608.64
Iteration:   3100, Loss function: 3.352, Average Loss: 3.887, avg. samples / sec: 24602.81
Iteration:   3100, Loss function: 3.450, Average Loss: 3.877, avg. samples / sec: 24632.98
Iteration:   3100, Loss function: 3.129, Average Loss: 3.874, avg. samples / sec: 24622.19
Iteration:   3100, Loss function: 2.944, Average Loss: 3.880, avg. samples / sec: 24602.03

:::MLPv0.5.0 ssd 1541757054.020481348 (train.py:553) train_epoch: 54
Iteration:   3120, Loss function: 2.672, Average Loss: 3.870, avg. samples / sec: 24630.77
Iteration:   3120, Loss function: 3.309, Average Loss: 3.863, avg. samples / sec: 24612.85
Iteration:   3120, Loss function: 3.241, Average Loss: 3.868, avg. samples / sec: 24611.24
Iteration:   3120, Loss function: 2.962, Average Loss: 3.873, avg. samples / sec: 24632.43
Iteration:   3120, Loss function: 3.331, Average Loss: 3.860, avg. samples / sec: 24651.12
Iteration:   3120, Loss function: 3.320, Average Loss: 3.871, avg. samples / sec: 24570.62
Iteration:   3120, Loss function: 3.159, Average Loss: 3.868, avg. samples / sec: 24625.85
Iteration:   3120, Loss function: 3.084, Average Loss: 3.863, avg. samples / sec: 24594.57
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2

:::MLPv0.5.0 ssd 1541757054.524270773 (train.py:586) opt_learning_rate: 0.0016

































































:::MLPv0.5.0 ssd 1541757054.607331991 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757054.607862711 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757054.608309746 (train.py:220) eval_start: 54
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.43 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.43 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.43 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.43 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.43 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.43 s
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(317961, 7)
Converting ndarray to lists...
Loading and preparing results...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.43 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.43 s
0/317961
(317961, 7)
Loading and preparing results...
Converting ndarray to lists...
0/317961
Loading and preparing results...
(317961, 7)
Converting ndarray to lists...
Loading and preparing results...
0/317961
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(317961, 7)
Converting ndarray to lists...
(317961, 7)
0/317961
(317961, 7)
0/317961
0/317961
Loading and preparing results...
Loading and preparing results...
(317961, 7)
Converting ndarray to lists...
(317961, 7)
Converting ndarray to lists...
Loading and preparing results...
0/317961
Converting ndarray to lists...
Converting ndarray to lists...
0/317961
(317961, 7)
Loading and preparing results...
(317961, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(317961, 7)
(317961, 7)
Loading and preparing results...
(317961, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/317961
0/317961
0/317961
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(317961, 7)
(317961, 7)
(317961, 7)
(317961, 7)
0/317961
Converting ndarray to lists...
0/317961
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(317961, 7)
(317961, 7)
0/317961
Converting ndarray to lists...
Loading and preparing results...
0/317961
0/317961
0/317961
Converting ndarray to lists...
(317961, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(317961, 7)
Converting ndarray to lists...
0/317961
(317961, 7)
Converting ndarray to lists...
(317961, 7)
Loading and preparing results...
0/317961
(317961, 7)
(317961, 7)
0/317961
(317961, 7)
(317961, 7)
(317961, 7)
0/317961
Loading and preparing results...
(317961, 7)
(317961, 7)
Converting ndarray to lists...
Loading and preparing results...
0/317961
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
0/317961
Converting ndarray to lists...
Converting ndarray to lists...
0/317961
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(317961, 7)
Loading and preparing results...
(317961, 7)
Loading and preparing results...
0/317961
Loading and preparing results...
0/317961
Loading and preparing results...
Converting ndarray to lists...
(317961, 7)
(317961, 7)
0/317961
Loading and preparing results...
Converting ndarray to lists...
0/317961
Loading and preparing results...
(317961, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/317961
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/317961
0/317961
(317961, 7)
0/317961
0/317961
(317961, 7)
Loading and preparing results...
Converting ndarray to lists...
0/317961
(317961, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/317961
Converting ndarray to lists...
Converting ndarray to lists...
(317961, 7)
Converting ndarray to lists...
(317961, 7)
0/317961
(317961, 7)
Loading and preparing results...
(317961, 7)
Loading and preparing results...
Loading and preparing results...
(317961, 7)
Converting ndarray to lists...
0/317961
(317961, 7)
Converting ndarray to lists...
0/317961
Converting ndarray to lists...
(317961, 7)
Loading and preparing results...
0/317961
(317961, 7)
Loading and preparing results...
(317961, 7)
Loading and preparing results...
(317961, 7)
(317961, 7)
(317961, 7)
0/317961
Converting ndarray to lists...
(317961, 7)
0/317961
(317961, 7)
(317961, 7)
(317961, 7)
0/317961
Converting ndarray to lists...
0/317961
0/317961
0/317961
0/317961
Converting ndarray to lists...
0/317961
Converting ndarray to lists...
0/317961
Converting ndarray to lists...
(317961, 7)
0/317961
(317961, 7)
(317961, 7)
0/317961
(317961, 7)
Converting ndarray to lists...
0/317961
0/317961
Converting ndarray to lists...
0/317961
0/317961
0/317961
(317961, 7)
(317961, 7)
0/317961
Loading and preparing results...
0/317961
0/317961
(317961, 7)
0/317961
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(317961, 7)
(317961, 7)
0/317961
0/317961
0/317961
Converting ndarray to lists...
0/317961
(317961, 7)
0/317961
DONE (t=1.52s)
creating index...
DONE (t=1.53s)
creating index...
DONE (t=1.54s)
creating index...
DONE (t=1.55s)
creating index...
index created!
index created!
index created!
index created!
DONE (t=1.72s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.82s)
creating index...
index created!
DONE (t=1.84s)
creating index...
DONE (t=1.85s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=1.92s)
creating index...
DONE (t=1.94s)
creating index...
index created!
DONE (t=1.94s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
index created!
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
index created!
DONE (t=1.96s)
creating index...
DONE (t=1.97s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.98s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=1.99s)
creating index...
DONE (t=2.00s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.01s)
creating index...
DONE (t=2.02s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.03s)
creating index...
DONE (t=2.04s)
creating index...
index created!
DONE (t=2.04s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=2.05s)
creating index...
index created!
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
index created!
DONE (t=2.07s)
creating index...
index created!
DONE (t=2.07s)
creating index...
DONE (t=2.07s)
creating index...
index created!
index created!
index created!
index created!
index created!
DONE (t=2.09s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.33s).
Accumulating evaluation results...
DONE (t=3.46s).
Accumulating evaluation results...
DONE (t=3.33s).
Accumulating evaluation results...
DONE (t=3.38s).
Accumulating evaluation results...
DONE (t=3.35s).
Accumulating evaluation results...
DONE (t=3.37s).
Accumulating evaluation results...
DONE (t=3.36s).
Accumulating evaluation results...
DONE (t=3.36s).
Accumulating evaluation results...
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.095
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.492
Current AP: 0.21062 AP goal: 0.21200
DONE (t=1.07s).
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.095
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.492
Current AP: 0.21062 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.095
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.492
Current AP: 0.21062 AP goal: 0.21200
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
DONE (t=1.06s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.095
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.492
Current AP: 0.21062 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
DONE (t=1.08s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.211
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.095
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.492
Current AP: 0.21062 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.363
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.095
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.492
Current AP: 0.21062 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.221
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.336
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.095
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.492
Current AP: 0.21062 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.304
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.095
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.492
Current AP: 0.21062 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757065.743968487 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757065.744557858 (train.py:333) eval_accuracy: {"epoch": 54, "value": 0.21062319329516752}

:::MLPv0.5.0 ssd 1541757065.745009422 (train.py:336) eval_iteration_accuracy: {"epoch": 54, "value": 0.21062319329516752}

:::MLPv0.5.0 ssd 1541757065.745449781 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757065.745891094 (train.py:338) eval_stop: 54
Iteration:   3140, Loss function: 3.473, Average Loss: 3.856, avg. samples / sec: 3095.35
Iteration:   3140, Loss function: 3.187, Average Loss: 3.845, avg. samples / sec: 3095.47
Iteration:   3140, Loss function: 3.338, Average Loss: 3.853, avg. samples / sec: 3095.88
Iteration:   3140, Loss function: 3.140, Average Loss: 3.848, avg. samples / sec: 3095.76
Iteration:   3140, Loss function: 3.182, Average Loss: 3.851, avg. samples / sec: 3094.72
Iteration:   3140, Loss function: 3.032, Average Loss: 3.855, avg. samples / sec: 3094.63
Iteration:   3140, Loss function: 2.818, Average Loss: 3.858, avg. samples / sec: 3095.42
Iteration:   3140, Loss function: 2.875, Average Loss: 3.862, avg. samples / sec: 3094.58
Iteration:   3160, Loss function: 2.592, Average Loss: 3.844, avg. samples / sec: 24581.07
Iteration:   3160, Loss function: 3.581, Average Loss: 3.849, avg. samples / sec: 24637.07
Iteration:   3160, Loss function: 3.206, Average Loss: 3.833, avg. samples / sec: 24602.62
Iteration:   3160, Loss function: 2.592, Average Loss: 3.840, avg. samples / sec: 24588.94
Iteration:   3160, Loss function: 3.790, Average Loss: 3.840, avg. samples / sec: 24592.44
Iteration:   3160, Loss function: 3.372, Average Loss: 3.832, avg. samples / sec: 24539.44
Iteration:   3160, Loss function: 3.108, Average Loss: 3.841, avg. samples / sec: 24569.18
Iteration:   3160, Loss function: 3.257, Average Loss: 3.845, avg. samples / sec: 24567.45

:::MLPv0.5.0 ssd 1541757070.425223589 (train.py:553) train_epoch: 55
Iteration:   3180, Loss function: 2.753, Average Loss: 3.834, avg. samples / sec: 24497.31
Iteration:   3180, Loss function: 2.958, Average Loss: 3.821, avg. samples / sec: 24503.61
Iteration:   3180, Loss function: 3.545, Average Loss: 3.826, avg. samples / sec: 24502.54
Iteration:   3180, Loss function: 3.320, Average Loss: 3.824, avg. samples / sec: 24537.40
Iteration:   3180, Loss function: 3.424, Average Loss: 3.826, avg. samples / sec: 24470.28
Iteration:   3180, Loss function: 3.139, Average Loss: 3.832, avg. samples / sec: 24505.06
Iteration:   3180, Loss function: 3.073, Average Loss: 3.828, avg. samples / sec: 24493.38
Iteration:   3180, Loss function: 2.615, Average Loss: 3.828, avg. samples / sec: 24436.78
Iteration:   3200, Loss function: 2.791, Average Loss: 3.820, avg. samples / sec: 24487.18
Iteration:   3200, Loss function: 3.172, Average Loss: 3.809, avg. samples / sec: 24492.61
Iteration:   3200, Loss function: 2.973, Average Loss: 3.814, avg. samples / sec: 24532.12
Iteration:   3200, Loss function: 2.744, Average Loss: 3.813, avg. samples / sec: 24486.68
Iteration:   3200, Loss function: 3.714, Average Loss: 3.813, avg. samples / sec: 24488.04
Iteration:   3200, Loss function: 3.200, Average Loss: 3.813, avg. samples / sec: 24514.61
Iteration:   3200, Loss function: 3.592, Average Loss: 3.814, avg. samples / sec: 24505.66
Iteration:   3200, Loss function: 3.319, Average Loss: 3.819, avg. samples / sec: 24481.84
Iteration:   3220, Loss function: 2.999, Average Loss: 3.808, avg. samples / sec: 24528.99
Iteration:   3220, Loss function: 2.821, Average Loss: 3.796, avg. samples / sec: 24522.05
Iteration:   3220, Loss function: 2.908, Average Loss: 3.801, avg. samples / sec: 24529.27
Iteration:   3220, Loss function: 2.977, Average Loss: 3.799, avg. samples / sec: 24529.72
Iteration:   3220, Loss function: 3.245, Average Loss: 3.802, avg. samples / sec: 24492.20
Iteration:   3220, Loss function: 3.500, Average Loss: 3.808, avg. samples / sec: 24533.83
Iteration:   3220, Loss function: 2.979, Average Loss: 3.800, avg. samples / sec: 24481.70
Iteration:   3220, Loss function: 3.060, Average Loss: 3.800, avg. samples / sec: 24491.87

:::MLPv0.5.0 ssd 1541757075.272798300 (train.py:553) train_epoch: 56
Iteration:   3240, Loss function: 2.453, Average Loss: 3.795, avg. samples / sec: 24503.66
Iteration:   3240, Loss function: 3.321, Average Loss: 3.791, avg. samples / sec: 24505.14
Iteration:   3240, Loss function: 3.517, Average Loss: 3.786, avg. samples / sec: 24542.79
Iteration:   3240, Loss function: 2.728, Average Loss: 3.794, avg. samples / sec: 24526.12
Iteration:   3240, Loss function: 3.266, Average Loss: 3.788, avg. samples / sec: 24531.30
Iteration:   3240, Loss function: 3.386, Average Loss: 3.788, avg. samples / sec: 24482.79
Iteration:   3240, Loss function: 3.013, Average Loss: 3.785, avg. samples / sec: 24440.64
Iteration:   3240, Loss function: 3.324, Average Loss: 3.785, avg. samples / sec: 24465.05
Iteration:   3260, Loss function: 3.401, Average Loss: 3.782, avg. samples / sec: 24597.08
Iteration:   3260, Loss function: 3.040, Average Loss: 3.780, avg. samples / sec: 24595.50
Iteration:   3260, Loss function: 3.354, Average Loss: 3.781, avg. samples / sec: 24612.52
Iteration:   3260, Loss function: 2.973, Average Loss: 3.770, avg. samples / sec: 24577.55
Iteration:   3260, Loss function: 3.091, Average Loss: 3.771, avg. samples / sec: 24621.93
Iteration:   3260, Loss function: 3.705, Average Loss: 3.775, avg. samples / sec: 24610.43
Iteration:   3260, Loss function: 2.979, Average Loss: 3.772, avg. samples / sec: 24611.79
Iteration:   3260, Loss function: 3.266, Average Loss: 3.774, avg. samples / sec: 24586.94
Iteration:   3280, Loss function: 3.275, Average Loss: 3.769, avg. samples / sec: 24556.81
Iteration:   3280, Loss function: 3.307, Average Loss: 3.759, avg. samples / sec: 24605.04
Iteration:   3280, Loss function: 2.947, Average Loss: 3.759, avg. samples / sec: 24579.03
Iteration:   3280, Loss function: 3.102, Average Loss: 3.760, avg. samples / sec: 24592.99
Iteration:   3280, Loss function: 2.853, Average Loss: 3.768, avg. samples / sec: 24548.53
Iteration:   3280, Loss function: 3.226, Average Loss: 3.757, avg. samples / sec: 24578.38
Iteration:   3280, Loss function: 3.704, Average Loss: 3.761, avg. samples / sec: 24552.53
Iteration:   3280, Loss function: 3.222, Average Loss: 3.769, avg. samples / sec: 24504.09

:::MLPv0.5.0 ssd 1541757080.108816147 (train.py:553) train_epoch: 57
Iteration:   3300, Loss function: 2.672, Average Loss: 3.750, avg. samples / sec: 24559.38
Iteration:   3300, Loss function: 2.981, Average Loss: 3.757, avg. samples / sec: 24545.15
Iteration:   3300, Loss function: 3.249, Average Loss: 3.754, avg. samples / sec: 24608.67
Iteration:   3300, Loss function: 3.424, Average Loss: 3.748, avg. samples / sec: 24588.40
Iteration:   3300, Loss function: 2.886, Average Loss: 3.757, avg. samples / sec: 24545.23
Iteration:   3300, Loss function: 2.917, Average Loss: 3.748, avg. samples / sec: 24522.75
Iteration:   3300, Loss function: 2.851, Average Loss: 3.746, avg. samples / sec: 24498.07
Iteration:   3300, Loss function: 3.469, Average Loss: 3.745, avg. samples / sec: 24508.22
Iteration:   3320, Loss function: 3.366, Average Loss: 3.737, avg. samples / sec: 24634.20
Iteration:   3320, Loss function: 2.954, Average Loss: 3.744, avg. samples / sec: 24581.60
Iteration:   3320, Loss function: 2.741, Average Loss: 3.736, avg. samples / sec: 24596.53
Iteration:   3320, Loss function: 3.125, Average Loss: 3.738, avg. samples / sec: 24576.00
Iteration:   3320, Loss function: 2.653, Average Loss: 3.741, avg. samples / sec: 24563.59
Iteration:   3320, Loss function: 2.832, Average Loss: 3.747, avg. samples / sec: 24575.78
Iteration:   3320, Loss function: 3.524, Average Loss: 3.736, avg. samples / sec: 24565.87
Iteration:   3320, Loss function: 3.341, Average Loss: 3.734, avg. samples / sec: 24580.65
Iteration:   3340, Loss function: 3.147, Average Loss: 3.724, avg. samples / sec: 24583.52
Iteration:   3340, Loss function: 3.717, Average Loss: 3.733, avg. samples / sec: 24579.55
Iteration:   3340, Loss function: 3.188, Average Loss: 3.726, avg. samples / sec: 24627.31
Iteration:   3340, Loss function: 3.214, Average Loss: 3.732, avg. samples / sec: 24601.55
Iteration:   3340, Loss function: 3.176, Average Loss: 3.725, avg. samples / sec: 24540.77
Iteration:   3340, Loss function: 2.972, Average Loss: 3.724, avg. samples / sec: 24540.23
Iteration:   3340, Loss function: 3.573, Average Loss: 3.723, avg. samples / sec: 24587.58
Iteration:   3340, Loss function: 3.276, Average Loss: 3.730, avg. samples / sec: 24537.38

:::MLPv0.5.0 ssd 1541757084.858957052 (train.py:553) train_epoch: 58
Iteration:   3360, Loss function: 3.526, Average Loss: 3.722, avg. samples / sec: 24564.24
Iteration:   3360, Loss function: 3.258, Average Loss: 3.714, avg. samples / sec: 24552.90
Iteration:   3360, Loss function: 2.779, Average Loss: 3.712, avg. samples / sec: 24599.47
Iteration:   3360, Loss function: 3.595, Average Loss: 3.717, avg. samples / sec: 24554.28
Iteration:   3360, Loss function: 2.939, Average Loss: 3.712, avg. samples / sec: 24603.22
Iteration:   3360, Loss function: 2.911, Average Loss: 3.721, avg. samples / sec: 24549.85
Iteration:   3360, Loss function: 2.790, Average Loss: 3.715, avg. samples / sec: 24598.15
Iteration:   3360, Loss function: 2.914, Average Loss: 3.712, avg. samples / sec: 24556.51
Iteration:   3380, Loss function: 3.023, Average Loss: 3.703, avg. samples / sec: 24602.17
Iteration:   3380, Loss function: 3.519, Average Loss: 3.701, avg. samples / sec: 24615.25
Iteration:   3380, Loss function: 3.241, Average Loss: 3.708, avg. samples / sec: 24588.02
Iteration:   3380, Loss function: 3.217, Average Loss: 3.700, avg. samples / sec: 24597.72
Iteration:   3380, Loss function: 3.496, Average Loss: 3.710, avg. samples / sec: 24594.39
Iteration:   3380, Loss function: 2.977, Average Loss: 3.704, avg. samples / sec: 24601.65
Iteration:   3380, Loss function: 2.785, Average Loss: 3.709, avg. samples / sec: 24554.93
Iteration:   3380, Loss function: 3.246, Average Loss: 3.701, avg. samples / sec: 24550.56
Iteration:   3400, Loss function: 3.109, Average Loss: 3.690, avg. samples / sec: 24578.58
Iteration:   3400, Loss function: 3.549, Average Loss: 3.697, avg. samples / sec: 24583.29
Iteration:   3400, Loss function: 3.057, Average Loss: 3.691, avg. samples / sec: 24626.37
Iteration:   3400, Loss function: 2.942, Average Loss: 3.690, avg. samples / sec: 24569.91
Iteration:   3400, Loss function: 2.993, Average Loss: 3.687, avg. samples / sec: 24560.31
Iteration:   3400, Loss function: 3.465, Average Loss: 3.699, avg. samples / sec: 24571.94
Iteration:   3400, Loss function: 3.163, Average Loss: 3.697, avg. samples / sec: 24580.85
Iteration:   3400, Loss function: 3.182, Average Loss: 3.693, avg. samples / sec: 24560.90

:::MLPv0.5.0 ssd 1541757089.691606283 (train.py:553) train_epoch: 59
Iteration:   3420, Loss function: 3.216, Average Loss: 3.685, avg. samples / sec: 24566.67
Iteration:   3420, Loss function: 3.194, Average Loss: 3.680, avg. samples / sec: 24560.75
Iteration:   3420, Loss function: 3.634, Average Loss: 3.679, avg. samples / sec: 24570.59
Iteration:   3420, Loss function: 2.840, Average Loss: 3.687, avg. samples / sec: 24557.58
Iteration:   3420, Loss function: 3.201, Average Loss: 3.682, avg. samples / sec: 24584.28
Iteration:   3420, Loss function: 3.407, Average Loss: 3.680, avg. samples / sec: 24534.28
Iteration:   3420, Loss function: 2.968, Average Loss: 3.685, avg. samples / sec: 24550.32
Iteration:   3420, Loss function: 3.315, Average Loss: 3.677, avg. samples / sec: 24516.91

































































:::MLPv0.5.0 ssd 1541757092.193633318 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757092.194166183 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757092.194607973 (train.py:220) eval_start: 59
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.82 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.82 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.82 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.82 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.82 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.82 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.82 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.82 s
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(299940, 7)
Converting ndarray to lists...
Loading and preparing results...
0/299940
Loading and preparing results...
(299940, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(299940, 7)
(299940, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(299940, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(299940, 7)
0/299940
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(299940, 7)
Loading and preparing results...
(299940, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/299940
Loading and preparing results...
(299940, 7)
Loading and preparing results...
0/299940
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(299940, 7)
Loading and preparing results...
0/299940
Converting ndarray to lists...
(299940, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(299940, 7)
0/299940
Converting ndarray to lists...
0/299940
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(299940, 7)
(299940, 7)
(299940, 7)
Loading and preparing results...
0/299940
Converting ndarray to lists...
(299940, 7)
(299940, 7)
Converting ndarray to lists...
0/299940
Loading and preparing results...
(299940, 7)
0/299940
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
0/299940
(299940, 7)
(299940, 7)
Converting ndarray to lists...
(299940, 7)
0/299940
Converting ndarray to lists...
0/299940
(299940, 7)
(299940, 7)
0/299940
0/299940
0/299940
Loading and preparing results...
Converting ndarray to lists...
0/299940
0/299940
0/299940
Converting ndarray to lists...
0/299940
(299940, 7)
0/299940
0/299940
(299940, 7)
0/299940
(299940, 7)
0/299940
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(299940, 7)
(299940, 7)
Loading and preparing results...
0/299940
(299940, 7)
Loading and preparing results...
(299940, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(299940, 7)
0/299940
Converting ndarray to lists...
(299940, 7)
Loading and preparing results...
0/299940
(299940, 7)
0/299940
0/299940
0/299940
Loading and preparing results...
(299940, 7)
(299940, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/299940
0/299940
Converting ndarray to lists...
0/299940
0/299940
0/299940
(299940, 7)
Loading and preparing results...
Converting ndarray to lists...
(299940, 7)
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(299940, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(299940, 7)
Converting ndarray to lists...
Loading and preparing results...
(299940, 7)
0/299940
0/299940
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/299940
Loading and preparing results...
(299940, 7)
(299940, 7)
(299940, 7)
(299940, 7)
(299940, 7)
(299940, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/299940
(299940, 7)
0/299940
Loading and preparing results...
0/299940
0/299940
(299940, 7)
0/299940
Loading and preparing results...
0/299940
Loading and preparing results...
(299940, 7)
0/299940
Converting ndarray to lists...
0/299940
Converting ndarray to lists...
Loading and preparing results...
0/299940
(299940, 7)
Converting ndarray to lists...
0/299940
(299940, 7)
Converting ndarray to lists...
(299940, 7)
(299940, 7)
Loading and preparing results...
0/299940
0/299940
Loading and preparing results...
0/299940
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/299940
(299940, 7)
0/299940
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(299940, 7)
(299940, 7)
Converting ndarray to lists...
Loading and preparing results...
(299940, 7)
0/299940
0/299940
Converting ndarray to lists...
(299940, 7)
Loading and preparing results...
0/299940
Converting ndarray to lists...
(299940, 7)
0/299940
0/299940
Converting ndarray to lists...
(299940, 7)
0/299940
Converting ndarray to lists...
(299940, 7)
(299940, 7)
Converting ndarray to lists...
0/299940
(299940, 7)
0/299940
(299940, 7)
0/299940
0/299940
0/299940
DONE (t=1.45s)
creating index...
DONE (t=1.46s)
creating index...
DONE (t=1.48s)
creating index...
DONE (t=1.49s)
creating index...
index created!
index created!
index created!
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.82s)
creating index...
DONE (t=1.83s)
creating index...
DONE (t=1.85s)
creating index...
DONE (t=1.85s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=1.90s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
DONE (t=1.92s)
creating index...
index created!
index created!
index created!
DONE (t=1.92s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
DONE (t=1.93s)
creating index...
index created!
DONE (t=1.93s)
creating index...
DONE (t=1.94s)
creating index...
index created!
DONE (t=1.94s)
creating index...
DONE (t=1.95s)
creating index...
index created!
DONE (t=1.95s)
creating index...
DONE (t=1.95s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.95s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
index created!
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.96s)
creating index...
DONE (t=1.96s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.99s)
creating index...
index created!
DONE (t=2.02s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=2.06s)
creating index...
DONE (t=2.06s)
creating index...
index created!
DONE (t=2.06s)
creating index...
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
DONE (t=2.08s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=2.09s)
creating index...
DONE (t=2.09s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
DONE (t=2.10s)
creating index...
index created!
DONE (t=2.11s)
creating index...
DONE (t=2.11s)
creating index...
DONE (t=2.12s)
creating index...
index created!
DONE (t=2.13s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.14s)
creating index...
DONE (t=2.16s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=3.27s).
Accumulating evaluation results...
DONE (t=3.26s).
Accumulating evaluation results...
DONE (t=3.24s).
Accumulating evaluation results...
DONE (t=3.23s).
Accumulating evaluation results...
DONE (t=3.28s).
Accumulating evaluation results...
DONE (t=3.24s).
Accumulating evaluation results...
DONE (t=3.37s).
Accumulating evaluation results...
DONE (t=3.28s).
Accumulating evaluation results...
DONE (t=1.03s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
DONE (t=1.04s).
DONE (t=1.03s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.213
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.309
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.096
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21451 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.213
DONE (t=1.05s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.309
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.213
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.309
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.096
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21451 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.309
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.096
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21451 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.096
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21451 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
DONE (t=1.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.309
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.096
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21451 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.213
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.309
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.096
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21451 AP goal: 0.21200
DONE (t=1.05s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.215
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
DONE (t=1.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.213
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.309
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.368
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.096
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21451 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.217
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.343
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.213
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.309
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.324
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.096
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.502
Current AP: 0.21451 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757102.721295357 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757102.721899033 (train.py:333) eval_accuracy: {"epoch": 59, "value": 0.21451433153601884}

:::MLPv0.5.0 ssd 1541757102.722381592 (train.py:336) eval_iteration_accuracy: {"epoch": 59, "value": 0.21451433153601884}

:::MLPv0.5.0 ssd 1541757102.722825289 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757102.723264933 (train.py:338) eval_stop: 59

:::MLPv0.5.0 ssd 1541757103.805485010 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1541757103.806040525 (train.py:707) run_final
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2018-11-09 09:51:51 AM
RESULT,OBJECT_DETECTION,,400,nvidia,2018-11-09 09:45:11 AM
ENDING TIMING RUN AT 2018-11-09 09:51:45 AM
RESULT,OBJECT_DETECTION,,400,nvidia,2018-11-09 09:45:05 AM
ENDING TIMING RUN AT 2018-11-09 09:51:50 AM
RESULT,OBJECT_DETECTION,,400,nvidia,2018-11-09 09:45:10 AM
ENDING TIMING RUN AT 2018-11-09 09:51:49 AM
RESULT,OBJECT_DETECTION,,400,nvidia,2018-11-09 09:45:09 AM
ENDING TIMING RUN AT 2018-11-09 09:51:50 AM
RESULT,OBJECT_DETECTION,,401,nvidia,2018-11-09 09:45:09 AM
ENDING TIMING RUN AT 2018-11-09 09:51:48 AM
RESULT,OBJECT_DETECTION,,400,nvidia,2018-11-09 09:45:08 AM
ENDING TIMING RUN AT 2018-11-09 09:51:46 AM
RESULT,OBJECT_DETECTION,,401,nvidia,2018-11-09 09:45:05 AM
ENDING TIMING RUN AT 2018-11-09 09:51:51 AM
RESULT,OBJECT_DETECTION,,400,nvidia,2018-11-09 09:45:11 AM
