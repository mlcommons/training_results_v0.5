Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3

:::MLPv0.5.0 ssd 1541756966.636393785 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541756981.853058100 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541756975.196264267 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541756972.000104666 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541756972.326051474 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541756965.791190863 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541756972.803612709 (<string>:1) run_clear_caches

:::MLPv0.5.0 ssd 1541756977.720582724 (<string>:1) run_clear_caches
Launching on node circe-n001
+ pids+=($!)
+ set +x
Launching on node circe-n002
+ pids+=($!)
+ set +x
Launching on node circe-n003
++ eval echo srun -N 1 -n 1 -w '$hostn'
+ pids+=($!)
+++ echo srun -N 1 -n 1 -w circe-n001
+ set +x
Launching on node circe-n004
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n002
+ pids+=($!)
+ srun -N 1 -n 1 -w circe-n001 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=0 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35120 -e SLURM_NTASKS_PER_NODE=8 cont_35120 ./run_and_time.sh
+ set +x
Launching on node circe-n007
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n003
+ srun -N 1 -n 1 -w circe-n002 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=1 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35120 -e SLURM_NTASKS_PER_NODE=8 cont_35120 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n010
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n004
+ srun -N 1 -n 1 -w circe-n003 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=2 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35120 -e SLURM_NTASKS_PER_NODE=8 cont_35120 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n035
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n007
+ srun -N 1 -n 1 -w circe-n004 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=3 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35120 -e SLURM_NTASKS_PER_NODE=8 cont_35120 ./run_and_time.sh
+ pids+=($!)
+ set +x
Launching on node circe-n036
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n010
+ srun -N 1 -n 1 -w circe-n007 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=4 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35120 -e SLURM_NTASKS_PER_NODE=8 cont_35120 ./run_and_time.sh
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n035
+ srun -N 1 -n 1 -w circe-n010 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=5 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35120 -e SLURM_NTASKS_PER_NODE=8 cont_35120 ./run_and_time.sh
+ srun -N 1 -n 1 -w circe-n035 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=6 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35120 -e SLURM_NTASKS_PER_NODE=8 cont_35120 ./run_and_time.sh
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n036
+ srun -N 1 -n 1 -w circe-n036 docker exec -e DGXSYSTEM=DGX2_even_multi -e 'MULTI_NODE= --nnodes=8 --node_rank=7 --master_addr=10.0.1.1 --master_port=4242' -e SLURM_JOB_ID=35120 -e SLURM_NTASKS_PER_NODE=8 cont_35120 ./run_and_time.sh
Run vars: id 35120 gpus 8 mparams  --nnodes=8 --node_rank=0 --master_addr=10.0.1.1 --master_port=4242
Run vars: id 35120 gpus 8 mparams  --nnodes=8 --node_rank=1 --master_addr=10.0.1.1 --master_port=4242
Run vars: id 35120 gpus 8 mparams  --nnodes=8 --node_rank=2 --master_addr=10.0.1.1 --master_port=4242
Run vars: id 35120 gpus 8 mparams  --nnodes=8 --node_rank=4 --master_addr=10.0.1.1 --master_port=4242
Run vars: id 35120 gpus 8 mparams  --nnodes=8 --node_rank=5 --master_addr=10.0.1.1 --master_port=4242
Run vars: id 35120 gpus 8 mparams  --nnodes=8 --node_rank=7 --master_addr=10.0.1.1 --master_port=4242
Run vars: id 35120 gpus 8 mparams  --nnodes=8 --node_rank=6 --master_addr=10.0.1.1 --master_port=4242
STARTING TIMING RUN AT 2018-11-09 09:49:26 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=0 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:49:35 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=1 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
Run vars: id 35120 gpus 8 mparams  --nnodes=8 --node_rank=3 --master_addr=10.0.1.1 --master_port=4242
STARTING TIMING RUN AT 2018-11-09 09:49:27 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=2 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:49:32 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=4 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:49:42 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=5 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:49:37 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=7 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:49:33 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=6 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
STARTING TIMING RUN AT 2018-11-09 09:49:32 AM
running benchmark
+ echo 'running benchmark'
+ export DATASET_DIR=/data/coco2017
+ DATASET_DIR=/data/coco2017
+ export TORCH_MODEL_ZOO=/data/torchvision
+ TORCH_MODEL_ZOO=/data/torchvision
+ python -m bind_launch --nsockets_per_node 2 --ncores_per_socket 24 --nproc_per_node 8 --nnodes=8 --node_rank=3 --master_addr=10.0.1.1 --master_port=4242 train.py --use-fp16 --jit --delay-allreduce --epochs 70 --warmup-factor 0 --lr 2.5e-3 --eval-batch-size 216 --no-save --threshold=0.212 --data /data/coco2017 --batch-size 32 --warmup 900
2 Using seed = 2244442185
1 Using seed = 2244442184
3 Using seed = 2244442186
0 Using seed = 2244442183
4 Using seed = 2244442187
8 Using seed = 2244442191
13 Using seed = 2244442196
11 Using seed = 2244442194
10 Using seed = 2244442193
9 Using seed = 2244442192
14 Using seed = 2244442197
15 Using seed = 2244442198
12 Using seed = 2244442195
23 Using seed = 2244442206
20 Using seed = 2244442203
22 Using seed = 2244442205
19 Using seed = 2244442202
16 Using seed = 2244442199
18 Using seed = 2244442201
17 Using seed = 2244442200
21 Using seed = 2244442204
29 Using seed = 2244442212
31 Using seed = 2244442214
30 Using seed = 2244442213
27 Using seed = 2244442210
28 Using seed = 2244442211
24 Using seed = 2244442207
25 Using seed = 2244442208
26 Using seed = 2244442209
38 Using seed = 2244442221
39 Using seed = 2244442222
36 Using seed = 2244442219
37 Using seed = 2244442220
34 Using seed = 2244442217
35 Using seed = 2244442218
33 Using seed = 2244442216
32 Using seed = 2244442215
45 Using seed = 2244442228
44 Using seed = 2244442227
46 Using seed = 2244442229
40 Using seed = 2244442223
47 Using seed = 2244442230
42 Using seed = 2244442225
41 Using seed = 2244442224
43 Using seed = 2244442226
54 Using seed = 2244442237
55 Using seed = 2244442238
53 Using seed = 2244442236
52 Using seed = 2244442235
50 Using seed = 2244442233
48 Using seed = 2244442231
49 Using seed = 2244442232
51 Using seed = 2244442234
56 Using seed = 2244442239
63 Using seed = 2244442246
58 Using seed = 2244442241
57 Using seed = 2244442240
59 Using seed = 2244442242
62 Using seed = 2244442245
60 Using seed = 2244442243
61 Using seed = 2244442244
5 Using seed = 2244442188
7 Using seed = 2244442190
6 Using seed = 2244442189

:::MLPv0.5.0 ssd 1541756980.592926264 (train.py:371) run_start

:::MLPv0.5.0 ssd 1541756980.597416162 (train.py:178) feature_sizes: [38, 19, 10, 5, 3, 1]

:::MLPv0.5.0 ssd 1541756980.597910643 (train.py:180) steps: [8, 16, 32, 64, 100, 300]

:::MLPv0.5.0 ssd 1541756980.611618996 (train.py:183) scales: [21, 45, 99, 153, 207, 261, 315]

:::MLPv0.5.0 ssd 1541756980.612098217 (train.py:185) aspect_ratios: [[2], [2, 3], [2, 3], [2, 3], [2], [2]]

:::MLPv0.5.0 ssd 1541756980.661663294 (train.py:188) num_default_boxes: 8732

:::MLPv0.5.0 ssd 1541756980.680491447 (/workspace/single_stage_detector/utils.py:391) num_cropping_iterations: 1

:::MLPv0.5.0 ssd 1541756980.687499285 (/workspace/single_stage_detector/utils.py:510) random_flip_probability: 0.5

:::MLPv0.5.0 ssd 1541756980.688135386 (/workspace/single_stage_detector/utils.py:553) data_normalization_mean: [0.485, 0.456, 0.406]

:::MLPv0.5.0 ssd 1541756980.700247288 (/workspace/single_stage_detector/utils.py:554) data_normalization_std: [0.229, 0.224, 0.225]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...

:::MLPv0.5.0 ssd 1541756980.700855970 (train.py:382) input_size: 300
loading annotations into memory...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.44s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.45s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
Done (t=0.46s)
creating index...
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
Done (t=0.46s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
time_check a: 1541756987.870800972
time_check a: 1541756988.173457861
time_check a: 1541756983.146297693
time_check a: 1541756988.627492905
time_check a: 1541756981.620928764
time_check a: 1541756993.516576529
time_check a: 1541756991.090998411
time_check a: 1541756997.778985977
time_check b: 1541757015.579665422
time_check b: 1541757010.702997446
time_check b: 1541757013.166487694
time_check b: 1541757005.291253805
time_check b: 1541757010.061610460
time_check b: 1541757003.834733725
time_check b: 1541757010.423021555
time_check b: 1541757020.092111826

:::MLPv0.5.0 ssd 1541757004.821154594 (train.py:413) input_order

:::MLPv0.5.0 ssd 1541757004.835110903 (train.py:414) input_batch_size: 32

:::MLPv0.5.0 ssd 1541757006.000697851 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541757006.001272678 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541757006.026660919 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:54: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()
Delaying allreduces to the end of backward()

:::MLPv0.5.0 ssd 1541757007.678744078 (train.py:476) opt_name: "SGD"

:::MLPv0.5.0 ssd 1541757007.679271460 (train.py:477) opt_learning_rate: 0.16

:::MLPv0.5.0 ssd 1541757007.679705620 (train.py:478) opt_momentum: 0.9

:::MLPv0.5.0 ssd 1541757007.680119991 (train.py:480) opt_weight_decay: 0.0005

:::MLPv0.5.0 ssd 1541757007.680528879 (train.py:483) opt_learning_rate_warmup_steps: 900

:::MLPv0.5.0 ssd 1541757009.140233755 (/workspace/single_stage_detector/ssd300.py:47) backbone: "resnet34"

:::MLPv0.5.0 ssd 1541757009.140812635 (/workspace/single_stage_detector/ssd300.py:52) loc_conf_out_channels: [256, 512, 512, 256, 256, 256]

:::MLPv0.5.0 ssd 1541757009.166066885 (/workspace/single_stage_detector/ssd300.py:69) num_defaults_per_cell: [4, 6, 6, 6, 4, 4]
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss
epoch nbatch loss

:::MLPv0.5.0 ssd 1541757012.464972734 (train.py:551) train_loop

:::MLPv0.5.0 ssd 1541757012.465539694 (train.py:553) train_epoch: 0

:::MLPv0.5.0 ssd 1541757012.468484163 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 0, "value": 0.0}
Iteration:      0, Loss function: 22.858, Average Loss: 0.023, avg. samples / sec: 16356.67
Iteration:      0, Loss function: 22.614, Average Loss: 0.023, avg. samples / sec: 11434.86
Iteration:      0, Loss function: 22.837, Average Loss: 0.023, avg. samples / sec: 43033.37
Iteration:      0, Loss function: 22.220, Average Loss: 0.022, avg. samples / sec: 10905.69
Iteration:      0, Loss function: 22.741, Average Loss: 0.023, avg. samples / sec: 14421.76
Iteration:      0, Loss function: 22.623, Average Loss: 0.023, avg. samples / sec: 8815.78
Iteration:      0, Loss function: 22.984, Average Loss: 0.023, avg. samples / sec: 13689.97
Iteration:      0, Loss function: 22.886, Average Loss: 0.023, avg. samples / sec: 12518.30

:::MLPv0.5.0 ssd 1541757015.001024008 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 1, "value": 0.0001777777777777767}

:::MLPv0.5.0 ssd 1541757015.376185179 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 2, "value": 0.0003555555555555534}

:::MLPv0.5.0 ssd 1541757015.477414131 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 3, "value": 0.0005333333333333301}

:::MLPv0.5.0 ssd 1541757015.576534986 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 4, "value": 0.0007111111111111068}

:::MLPv0.5.0 ssd 1541757015.677983522 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 5, "value": 0.0008888888888888835}

:::MLPv0.5.0 ssd 1541757015.783047915 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 6, "value": 0.0010666666666666602}

:::MLPv0.5.0 ssd 1541757015.886013508 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 7, "value": 0.001244444444444437}

:::MLPv0.5.0 ssd 1541757015.984419823 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 8, "value": 0.0014222222222222136}

:::MLPv0.5.0 ssd 1541757016.080004692 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 9, "value": 0.0015999999999999903}

:::MLPv0.5.0 ssd 1541757016.174428701 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 10, "value": 0.001777777777777767}

:::MLPv0.5.0 ssd 1541757016.262180567 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 11, "value": 0.0019555555555555437}

:::MLPv0.5.0 ssd 1541757016.359507799 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 12, "value": 0.0021333333333333204}

:::MLPv0.5.0 ssd 1541757016.449329853 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 13, "value": 0.002311111111111097}

:::MLPv0.5.0 ssd 1541757016.541314602 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 14, "value": 0.002488888888888874}

:::MLPv0.5.0 ssd 1541757016.643079519 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 15, "value": 0.0026666666666666505}

:::MLPv0.5.0 ssd 1541757016.733737230 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 16, "value": 0.0028444444444444272}

:::MLPv0.5.0 ssd 1541757016.825574636 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 17, "value": 0.0030222222222222317}

:::MLPv0.5.0 ssd 1541757016.918121576 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 18, "value": 0.0032000000000000084}

:::MLPv0.5.0 ssd 1541757017.011457443 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 19, "value": 0.003377777777777785}

:::MLPv0.5.0 ssd 1541757017.100298166 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 20, "value": 0.003555555555555562}
Iteration:     20, Loss function: 20.959, Average Loss: 0.439, avg. samples / sec: 8858.51
Iteration:     20, Loss function: 20.100, Average Loss: 0.439, avg. samples / sec: 8858.05
Iteration:     20, Loss function: 20.387, Average Loss: 0.439, avg. samples / sec: 8857.51
Iteration:     20, Loss function: 20.497, Average Loss: 0.439, avg. samples / sec: 8857.76
Iteration:     20, Loss function: 19.610, Average Loss: 0.439, avg. samples / sec: 8856.79
Iteration:     20, Loss function: 20.789, Average Loss: 0.439, avg. samples / sec: 8856.65
Iteration:     20, Loss function: 20.785, Average Loss: 0.437, avg. samples / sec: 8853.10
Iteration:     20, Loss function: 20.426, Average Loss: 0.440, avg. samples / sec: 8849.70

:::MLPv0.5.0 ssd 1541757017.190910578 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 21, "value": 0.0037333333333333385}

:::MLPv0.5.0 ssd 1541757017.279565811 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 22, "value": 0.003911111111111115}

:::MLPv0.5.0 ssd 1541757017.373992205 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 23, "value": 0.004088888888888892}

:::MLPv0.5.0 ssd 1541757017.463562250 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 24, "value": 0.004266666666666669}

:::MLPv0.5.0 ssd 1541757017.551959991 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 25, "value": 0.004444444444444445}

:::MLPv0.5.0 ssd 1541757017.642183065 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 26, "value": 0.004622222222222222}

:::MLPv0.5.0 ssd 1541757017.739451885 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 27, "value": 0.004799999999999999}

:::MLPv0.5.0 ssd 1541757017.826229095 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 28, "value": 0.004977777777777775}

:::MLPv0.5.0 ssd 1541757017.912837029 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 29, "value": 0.005155555555555552}

:::MLPv0.5.0 ssd 1541757018.000724792 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 30, "value": 0.005333333333333329}

:::MLPv0.5.0 ssd 1541757018.091718197 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 31, "value": 0.0055111111111111055}

:::MLPv0.5.0 ssd 1541757018.182376862 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 32, "value": 0.005688888888888882}

:::MLPv0.5.0 ssd 1541757018.272342205 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 33, "value": 0.005866666666666659}

:::MLPv0.5.0 ssd 1541757018.361061811 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 34, "value": 0.006044444444444436}

:::MLPv0.5.0 ssd 1541757018.452706575 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 35, "value": 0.006222222222222212}

:::MLPv0.5.0 ssd 1541757018.542895794 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 36, "value": 0.006399999999999989}

:::MLPv0.5.0 ssd 1541757018.635183334 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 37, "value": 0.006577777777777766}

:::MLPv0.5.0 ssd 1541757018.727373838 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 38, "value": 0.0067555555555555424}

:::MLPv0.5.0 ssd 1541757018.813435555 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 39, "value": 0.006933333333333319}

:::MLPv0.5.0 ssd 1541757018.900143862 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 40, "value": 0.007111111111111096}
Iteration:     40, Loss function: 13.679, Average Loss: 0.798, avg. samples / sec: 22764.18
Iteration:     40, Loss function: 14.642, Average Loss: 0.801, avg. samples / sec: 22757.71
Iteration:     40, Loss function: 13.930, Average Loss: 0.796, avg. samples / sec: 22763.55
Iteration:     40, Loss function: 13.412, Average Loss: 0.798, avg. samples / sec: 22756.64
Iteration:     40, Loss function: 14.247, Average Loss: 0.799, avg. samples / sec: 22788.52
Iteration:     40, Loss function: 14.770, Average Loss: 0.799, avg. samples / sec: 22730.43
Iteration:     40, Loss function: 13.586, Average Loss: 0.793, avg. samples / sec: 22734.25
Iteration:     40, Loss function: 14.297, Average Loss: 0.800, avg. samples / sec: 22753.32

:::MLPv0.5.0 ssd 1541757018.994692326 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 41, "value": 0.0072888888888888725}

:::MLPv0.5.0 ssd 1541757019.082956076 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 42, "value": 0.007466666666666649}

:::MLPv0.5.0 ssd 1541757019.168900013 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 43, "value": 0.007644444444444454}

:::MLPv0.5.0 ssd 1541757019.261943817 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 44, "value": 0.00782222222222223}

:::MLPv0.5.0 ssd 1541757019.350716114 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 45, "value": 0.008000000000000007}

:::MLPv0.5.0 ssd 1541757019.442462921 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 46, "value": 0.008177777777777784}

:::MLPv0.5.0 ssd 1541757019.529671907 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 47, "value": 0.00835555555555556}

:::MLPv0.5.0 ssd 1541757019.619037867 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 48, "value": 0.008533333333333337}

:::MLPv0.5.0 ssd 1541757019.706329346 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 49, "value": 0.008711111111111114}

:::MLPv0.5.0 ssd 1541757019.791202784 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 50, "value": 0.00888888888888889}

:::MLPv0.5.0 ssd 1541757019.879055977 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 51, "value": 0.009066666666666667}

:::MLPv0.5.0 ssd 1541757019.965171814 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 52, "value": 0.009244444444444444}

:::MLPv0.5.0 ssd 1541757020.065825939 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 53, "value": 0.00942222222222222}

:::MLPv0.5.0 ssd 1541757020.153947830 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 54, "value": 0.009599999999999997}

:::MLPv0.5.0 ssd 1541757020.239899874 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 55, "value": 0.009777777777777774}

:::MLPv0.5.0 ssd 1541757020.327680588 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 56, "value": 0.00995555555555555}

:::MLPv0.5.0 ssd 1541757020.412713289 (train.py:349) opt_learning_rate: {"epoch": 0, "iteration": 57, "value": 0.010133333333333328}

:::MLPv0.5.0 ssd 1541757020.496568680 (train.py:553) train_epoch: 1

:::MLPv0.5.0 ssd 1541757020.500782967 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 58, "value": 0.010311111111111104}

:::MLPv0.5.0 ssd 1541757020.588214636 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 59, "value": 0.010488888888888881}

:::MLPv0.5.0 ssd 1541757020.674651861 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 60, "value": 0.010666666666666658}
Iteration:     60, Loss function: 11.160, Average Loss: 1.026, avg. samples / sec: 23090.05
Iteration:     60, Loss function: 11.184, Average Loss: 1.033, avg. samples / sec: 23087.10
Iteration:     60, Loss function: 11.533, Average Loss: 1.034, avg. samples / sec: 23088.92
Iteration:     60, Loss function: 11.209, Average Loss: 1.023, avg. samples / sec: 23110.28
Iteration:     60, Loss function: 11.211, Average Loss: 1.028, avg. samples / sec: 23076.59
Iteration:     60, Loss function: 11.438, Average Loss: 1.026, avg. samples / sec: 23066.67
Iteration:     60, Loss function: 11.393, Average Loss: 1.029, avg. samples / sec: 23086.24
Iteration:     60, Loss function: 11.747, Average Loss: 1.031, avg. samples / sec: 23107.56

:::MLPv0.5.0 ssd 1541757020.760974169 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 61, "value": 0.010844444444444434}

:::MLPv0.5.0 ssd 1541757020.847225428 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 62, "value": 0.011022222222222211}

:::MLPv0.5.0 ssd 1541757020.938606501 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 63, "value": 0.011199999999999988}

:::MLPv0.5.0 ssd 1541757021.023068428 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 64, "value": 0.011377777777777764}

:::MLPv0.5.0 ssd 1541757021.109913588 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 65, "value": 0.011555555555555541}

:::MLPv0.5.0 ssd 1541757021.198540688 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 66, "value": 0.011733333333333318}

:::MLPv0.5.0 ssd 1541757021.287251949 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 67, "value": 0.011911111111111095}

:::MLPv0.5.0 ssd 1541757021.377123356 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 68, "value": 0.012088888888888899}

:::MLPv0.5.0 ssd 1541757021.465434790 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 69, "value": 0.012266666666666676}

:::MLPv0.5.0 ssd 1541757021.551535845 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 70, "value": 0.012444444444444452}

:::MLPv0.5.0 ssd 1541757021.637653351 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 71, "value": 0.012622222222222229}

:::MLPv0.5.0 ssd 1541757021.722970009 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 72, "value": 0.012800000000000006}

:::MLPv0.5.0 ssd 1541757021.809206247 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 73, "value": 0.012977777777777783}

:::MLPv0.5.0 ssd 1541757021.894644976 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 74, "value": 0.01315555555555556}

:::MLPv0.5.0 ssd 1541757021.980276823 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 75, "value": 0.013333333333333336}

:::MLPv0.5.0 ssd 1541757022.066843748 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 76, "value": 0.013511111111111113}

:::MLPv0.5.0 ssd 1541757022.151904821 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 77, "value": 0.01368888888888889}

:::MLPv0.5.0 ssd 1541757022.238750696 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 78, "value": 0.013866666666666666}

:::MLPv0.5.0 ssd 1541757022.325661421 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 79, "value": 0.014044444444444443}

:::MLPv0.5.0 ssd 1541757022.414380789 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 80, "value": 0.01422222222222222}
Iteration:     80, Loss function: 9.378, Average Loss: 1.216, avg. samples / sec: 23547.18
Iteration:     80, Loss function: 9.702, Average Loss: 1.213, avg. samples / sec: 23571.04
Iteration:     80, Loss function: 9.138, Average Loss: 1.214, avg. samples / sec: 23584.54
Iteration:     80, Loss function: 9.626, Average Loss: 1.213, avg. samples / sec: 23542.35
Iteration:     80, Loss function: 9.247, Average Loss: 1.217, avg. samples / sec: 23525.43
Iteration:     80, Loss function: 9.639, Average Loss: 1.210, avg. samples / sec: 23535.78
Iteration:     80, Loss function: 9.447, Average Loss: 1.207, avg. samples / sec: 23521.38
Iteration:     80, Loss function: 9.205, Average Loss: 1.210, avg. samples / sec: 23488.50

:::MLPv0.5.0 ssd 1541757022.499968052 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 81, "value": 0.014399999999999996}

:::MLPv0.5.0 ssd 1541757022.584644794 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 82, "value": 0.014577777777777773}

:::MLPv0.5.0 ssd 1541757022.670105696 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 83, "value": 0.01475555555555555}

:::MLPv0.5.0 ssd 1541757022.755586147 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 84, "value": 0.014933333333333326}

:::MLPv0.5.0 ssd 1541757022.841960192 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 85, "value": 0.015111111111111103}

:::MLPv0.5.0 ssd 1541757022.928344727 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 86, "value": 0.01528888888888888}

:::MLPv0.5.0 ssd 1541757023.013955593 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 87, "value": 0.015466666666666656}

:::MLPv0.5.0 ssd 1541757023.105990648 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 88, "value": 0.015644444444444433}

:::MLPv0.5.0 ssd 1541757023.191437483 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 89, "value": 0.01582222222222221}

:::MLPv0.5.0 ssd 1541757023.275908947 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 90, "value": 0.015999999999999986}

:::MLPv0.5.0 ssd 1541757023.362523079 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 91, "value": 0.016177777777777763}

:::MLPv0.5.0 ssd 1541757023.446030855 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 92, "value": 0.01635555555555554}

:::MLPv0.5.0 ssd 1541757023.530290842 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 93, "value": 0.016533333333333317}

:::MLPv0.5.0 ssd 1541757023.616914511 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 94, "value": 0.01671111111111112}

:::MLPv0.5.0 ssd 1541757023.700816631 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 95, "value": 0.016888888888888898}

:::MLPv0.5.0 ssd 1541757023.790188313 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 96, "value": 0.017066666666666674}

:::MLPv0.5.0 ssd 1541757023.875833511 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 97, "value": 0.01724444444444445}

:::MLPv0.5.0 ssd 1541757023.962860346 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 98, "value": 0.017422222222222228}

:::MLPv0.5.0 ssd 1541757024.046527147 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 99, "value": 0.017600000000000005}

:::MLPv0.5.0 ssd 1541757024.132756472 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 100, "value": 0.01777777777777778}
Iteration:    100, Loss function: 9.134, Average Loss: 1.366, avg. samples / sec: 23875.70
Iteration:    100, Loss function: 8.433, Average Loss: 1.376, avg. samples / sec: 23865.77
Iteration:    100, Loss function: 8.991, Average Loss: 1.371, avg. samples / sec: 23837.87
Iteration:    100, Loss function: 8.805, Average Loss: 1.375, avg. samples / sec: 23826.23
Iteration:    100, Loss function: 9.267, Average Loss: 1.374, avg. samples / sec: 23844.65
Iteration:    100, Loss function: 8.909, Average Loss: 1.369, avg. samples / sec: 23852.12
Iteration:    100, Loss function: 8.885, Average Loss: 1.368, avg. samples / sec: 23873.10
Iteration:    100, Loss function: 8.569, Average Loss: 1.374, avg. samples / sec: 23821.18

:::MLPv0.5.0 ssd 1541757024.215593576 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 101, "value": 0.017955555555555558}

:::MLPv0.5.0 ssd 1541757024.299685478 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 102, "value": 0.018133333333333335}

:::MLPv0.5.0 ssd 1541757024.386111975 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 103, "value": 0.01831111111111111}

:::MLPv0.5.0 ssd 1541757024.470262051 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 104, "value": 0.018488888888888888}

:::MLPv0.5.0 ssd 1541757024.555088282 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 105, "value": 0.018666666666666665}

:::MLPv0.5.0 ssd 1541757024.638798952 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 106, "value": 0.01884444444444444}

:::MLPv0.5.0 ssd 1541757024.723177433 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 107, "value": 0.019022222222222218}

:::MLPv0.5.0 ssd 1541757024.808107853 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 108, "value": 0.019199999999999995}

:::MLPv0.5.0 ssd 1541757024.892999172 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 109, "value": 0.01937777777777777}

:::MLPv0.5.0 ssd 1541757024.988179922 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 110, "value": 0.019555555555555548}

:::MLPv0.5.0 ssd 1541757025.072758436 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 111, "value": 0.019733333333333325}

:::MLPv0.5.0 ssd 1541757025.157355070 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 112, "value": 0.0199111111111111}

:::MLPv0.5.0 ssd 1541757025.242850304 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 113, "value": 0.02008888888888888}

:::MLPv0.5.0 ssd 1541757025.327942848 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 114, "value": 0.020266666666666655}

:::MLPv0.5.0 ssd 1541757025.411867142 (train.py:349) opt_learning_rate: {"epoch": 1, "iteration": 115, "value": 0.020444444444444432}

:::MLPv0.5.0 ssd 1541757025.496227264 (train.py:553) train_epoch: 2

:::MLPv0.5.0 ssd 1541757025.500405312 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 116, "value": 0.02062222222222221}

:::MLPv0.5.0 ssd 1541757025.585407257 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 117, "value": 0.020799999999999985}

:::MLPv0.5.0 ssd 1541757025.671743393 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 118, "value": 0.020977777777777762}

:::MLPv0.5.0 ssd 1541757025.759754658 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 119, "value": 0.02115555555555554}

:::MLPv0.5.0 ssd 1541757025.843243122 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 120, "value": 0.021333333333333343}
Iteration:    120, Loss function: 8.914, Average Loss: 1.518, avg. samples / sec: 23962.88
Iteration:    120, Loss function: 9.001, Average Loss: 1.517, avg. samples / sec: 23944.90
Iteration:    120, Loss function: 8.827, Average Loss: 1.524, avg. samples / sec: 23947.58
Iteration:    120, Loss function: 9.449, Average Loss: 1.524, avg. samples / sec: 23970.58
Iteration:    120, Loss function: 8.540, Average Loss: 1.520, avg. samples / sec: 23940.04
Iteration:    120, Loss function: 8.935, Average Loss: 1.523, avg. samples / sec: 23946.32
Iteration:    120, Loss function: 8.811, Average Loss: 1.518, avg. samples / sec: 23947.04
Iteration:    120, Loss function: 9.426, Average Loss: 1.524, avg. samples / sec: 23923.74

:::MLPv0.5.0 ssd 1541757025.929558516 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 121, "value": 0.02151111111111112}

:::MLPv0.5.0 ssd 1541757026.014029264 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 122, "value": 0.021688888888888896}

:::MLPv0.5.0 ssd 1541757026.099038601 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 123, "value": 0.021866666666666673}

:::MLPv0.5.0 ssd 1541757026.184221745 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 124, "value": 0.02204444444444445}

:::MLPv0.5.0 ssd 1541757026.268199921 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 125, "value": 0.022222222222222227}

:::MLPv0.5.0 ssd 1541757026.354286194 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 126, "value": 0.022400000000000003}

:::MLPv0.5.0 ssd 1541757026.438670874 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 127, "value": 0.02257777777777778}

:::MLPv0.5.0 ssd 1541757026.524090767 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 128, "value": 0.022755555555555557}

:::MLPv0.5.0 ssd 1541757026.608813286 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 129, "value": 0.022933333333333333}

:::MLPv0.5.0 ssd 1541757026.693762541 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 130, "value": 0.02311111111111111}

:::MLPv0.5.0 ssd 1541757026.777156353 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 131, "value": 0.023288888888888887}

:::MLPv0.5.0 ssd 1541757026.863010883 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 132, "value": 0.023466666666666663}

:::MLPv0.5.0 ssd 1541757026.946966410 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 133, "value": 0.02364444444444444}

:::MLPv0.5.0 ssd 1541757027.031100273 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 134, "value": 0.023822222222222217}

:::MLPv0.5.0 ssd 1541757027.114806175 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 135, "value": 0.023999999999999994}

:::MLPv0.5.0 ssd 1541757027.200808048 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 136, "value": 0.02417777777777777}

:::MLPv0.5.0 ssd 1541757027.284351349 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 137, "value": 0.024355555555555547}

:::MLPv0.5.0 ssd 1541757027.368856668 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 138, "value": 0.024533333333333324}

:::MLPv0.5.0 ssd 1541757027.453575373 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 139, "value": 0.0247111111111111}

:::MLPv0.5.0 ssd 1541757027.537845135 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 140, "value": 0.024888888888888877}
Iteration:    140, Loss function: 8.705, Average Loss: 1.666, avg. samples / sec: 24173.92
Iteration:    140, Loss function: 8.859, Average Loss: 1.667, avg. samples / sec: 24165.11
Iteration:    140, Loss function: 9.140, Average Loss: 1.667, avg. samples / sec: 24179.63
Iteration:    140, Loss function: 8.924, Average Loss: 1.674, avg. samples / sec: 24170.85
Iteration:    140, Loss function: 9.130, Average Loss: 1.675, avg. samples / sec: 24149.41
Iteration:    140, Loss function: 9.636, Average Loss: 1.670, avg. samples / sec: 24137.62
Iteration:    140, Loss function: 9.022, Average Loss: 1.674, avg. samples / sec: 24153.91
Iteration:    140, Loss function: 9.329, Average Loss: 1.670, avg. samples / sec: 24092.14

:::MLPv0.5.0 ssd 1541757027.623757362 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 141, "value": 0.025066666666666654}

:::MLPv0.5.0 ssd 1541757027.707244873 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 142, "value": 0.02524444444444443}

:::MLPv0.5.0 ssd 1541757027.792104959 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 143, "value": 0.025422222222222207}

:::MLPv0.5.0 ssd 1541757027.877246141 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 144, "value": 0.025599999999999984}

:::MLPv0.5.0 ssd 1541757027.964266539 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 145, "value": 0.02577777777777779}

:::MLPv0.5.0 ssd 1541757028.054152250 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 146, "value": 0.025955555555555565}

:::MLPv0.5.0 ssd 1541757028.140606642 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 147, "value": 0.026133333333333342}

:::MLPv0.5.0 ssd 1541757028.225077152 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 148, "value": 0.02631111111111112}

:::MLPv0.5.0 ssd 1541757028.310110807 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 149, "value": 0.026488888888888895}

:::MLPv0.5.0 ssd 1541757028.395923376 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 150, "value": 0.026666666666666672}

:::MLPv0.5.0 ssd 1541757028.481361151 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 151, "value": 0.02684444444444445}

:::MLPv0.5.0 ssd 1541757028.565447569 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 152, "value": 0.027022222222222225}

:::MLPv0.5.0 ssd 1541757028.651792288 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 153, "value": 0.027200000000000002}

:::MLPv0.5.0 ssd 1541757028.735022545 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 154, "value": 0.02737777777777778}

:::MLPv0.5.0 ssd 1541757028.821270227 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 155, "value": 0.027555555555555555}

:::MLPv0.5.0 ssd 1541757028.905884743 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 156, "value": 0.027733333333333332}

:::MLPv0.5.0 ssd 1541757028.989506960 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 157, "value": 0.02791111111111111}

:::MLPv0.5.0 ssd 1541757029.072532415 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 158, "value": 0.028088888888888885}

:::MLPv0.5.0 ssd 1541757029.156279087 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 159, "value": 0.028266666666666662}

:::MLPv0.5.0 ssd 1541757029.240553379 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 160, "value": 0.02844444444444444}
Iteration:    160, Loss function: 7.694, Average Loss: 1.808, avg. samples / sec: 24137.55
Iteration:    160, Loss function: 8.356, Average Loss: 1.813, avg. samples / sec: 24107.55
Iteration:    160, Loss function: 8.282, Average Loss: 1.806, avg. samples / sec: 24067.86
Iteration:    160, Loss function: 7.965, Average Loss: 1.810, avg. samples / sec: 24062.09
Iteration:    160, Loss function: 8.260, Average Loss: 1.804, avg. samples / sec: 24046.05
Iteration:    160, Loss function: 8.113, Average Loss: 1.810, avg. samples / sec: 24068.64
Iteration:    160, Loss function: 8.328, Average Loss: 1.807, avg. samples / sec: 24026.96
Iteration:    160, Loss function: 8.359, Average Loss: 1.810, avg. samples / sec: 24057.76

:::MLPv0.5.0 ssd 1541757029.325222731 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 161, "value": 0.028622222222222216}

:::MLPv0.5.0 ssd 1541757029.410518885 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 162, "value": 0.028799999999999992}

:::MLPv0.5.0 ssd 1541757029.494780779 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 163, "value": 0.02897777777777777}

:::MLPv0.5.0 ssd 1541757029.578736782 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 164, "value": 0.029155555555555546}

:::MLPv0.5.0 ssd 1541757029.665395498 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 165, "value": 0.029333333333333322}

:::MLPv0.5.0 ssd 1541757029.750679016 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 166, "value": 0.0295111111111111}

:::MLPv0.5.0 ssd 1541757029.836391687 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 167, "value": 0.029688888888888876}

:::MLPv0.5.0 ssd 1541757029.923406601 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 168, "value": 0.029866666666666652}

:::MLPv0.5.0 ssd 1541757030.007324934 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 169, "value": 0.03004444444444443}

:::MLPv0.5.0 ssd 1541757030.092557669 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 170, "value": 0.030222222222222206}

:::MLPv0.5.0 ssd 1541757030.176067829 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 171, "value": 0.03040000000000001}

:::MLPv0.5.0 ssd 1541757030.261489868 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 172, "value": 0.030577777777777787}

:::MLPv0.5.0 ssd 1541757030.345245600 (train.py:349) opt_learning_rate: {"epoch": 2, "iteration": 173, "value": 0.030755555555555564}

:::MLPv0.5.0 ssd 1541757030.427180052 (train.py:553) train_epoch: 3

:::MLPv0.5.0 ssd 1541757030.431236029 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 174, "value": 0.03093333333333334}

:::MLPv0.5.0 ssd 1541757030.515523911 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 175, "value": 0.031111111111111117}

:::MLPv0.5.0 ssd 1541757030.601682186 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 176, "value": 0.031288888888888894}

:::MLPv0.5.0 ssd 1541757030.685169935 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 177, "value": 0.03146666666666667}

:::MLPv0.5.0 ssd 1541757030.769285917 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 178, "value": 0.03164444444444445}

:::MLPv0.5.0 ssd 1541757030.855417252 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 179, "value": 0.031822222222222224}

:::MLPv0.5.0 ssd 1541757030.939393997 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 180, "value": 0.032}
Iteration:    180, Loss function: 7.795, Average Loss: 1.933, avg. samples / sec: 24110.14
Iteration:    180, Loss function: 7.869, Average Loss: 1.937, avg. samples / sec: 24110.10
Iteration:    180, Loss function: 7.781, Average Loss: 1.930, avg. samples / sec: 24102.31
Iteration:    180, Loss function: 7.915, Average Loss: 1.935, avg. samples / sec: 24102.04
Iteration:    180, Loss function: 7.757, Average Loss: 1.935, avg. samples / sec: 24102.77
Iteration:    180, Loss function: 7.752, Average Loss: 1.934, avg. samples / sec: 24120.79
Iteration:    180, Loss function: 7.935, Average Loss: 1.931, avg. samples / sec: 24108.42
Iteration:    180, Loss function: 7.763, Average Loss: 1.928, avg. samples / sec: 24062.54

:::MLPv0.5.0 ssd 1541757031.023100615 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 181, "value": 0.03217777777777778}

:::MLPv0.5.0 ssd 1541757031.107150078 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 182, "value": 0.032355555555555554}

:::MLPv0.5.0 ssd 1541757031.191763163 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 183, "value": 0.03253333333333333}

:::MLPv0.5.0 ssd 1541757031.275413275 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 184, "value": 0.03271111111111111}

:::MLPv0.5.0 ssd 1541757031.359012604 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 185, "value": 0.032888888888888884}

:::MLPv0.5.0 ssd 1541757031.443187475 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 186, "value": 0.03306666666666666}

:::MLPv0.5.0 ssd 1541757031.527486324 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 187, "value": 0.03324444444444444}

:::MLPv0.5.0 ssd 1541757031.611353397 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 188, "value": 0.033422222222222214}

:::MLPv0.5.0 ssd 1541757031.695162296 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 189, "value": 0.03359999999999999}

:::MLPv0.5.0 ssd 1541757031.778992414 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 190, "value": 0.03377777777777777}

:::MLPv0.5.0 ssd 1541757031.862449169 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 191, "value": 0.033955555555555544}

:::MLPv0.5.0 ssd 1541757031.946374893 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 192, "value": 0.03413333333333332}

:::MLPv0.5.0 ssd 1541757032.030571699 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 193, "value": 0.0343111111111111}

:::MLPv0.5.0 ssd 1541757032.114215374 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 194, "value": 0.034488888888888874}

:::MLPv0.5.0 ssd 1541757032.202112436 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 195, "value": 0.03466666666666665}

:::MLPv0.5.0 ssd 1541757032.286143780 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 196, "value": 0.03484444444444443}

:::MLPv0.5.0 ssd 1541757032.369429350 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 197, "value": 0.03502222222222222}

:::MLPv0.5.0 ssd 1541757032.452868700 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 198, "value": 0.035199999999999995}

:::MLPv0.5.0 ssd 1541757032.536756754 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 199, "value": 0.03537777777777777}

:::MLPv0.5.0 ssd 1541757032.621168137 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 200, "value": 0.03555555555555555}
Iteration:    200, Loss function: 7.667, Average Loss: 2.053, avg. samples / sec: 24371.39
Iteration:    200, Loss function: 7.862, Average Loss: 2.049, avg. samples / sec: 24397.52
Iteration:    200, Loss function: 6.999, Average Loss: 2.057, avg. samples / sec: 24357.46
Iteration:    200, Loss function: 8.015, Average Loss: 2.052, avg. samples / sec: 24354.23
Iteration:    200, Loss function: 8.058, Average Loss: 2.053, avg. samples / sec: 24364.38
Iteration:    200, Loss function: 7.604, Average Loss: 2.052, avg. samples / sec: 24352.25
Iteration:    200, Loss function: 7.805, Average Loss: 2.055, avg. samples / sec: 24330.42
Iteration:    200, Loss function: 7.981, Average Loss: 2.048, avg. samples / sec: 24363.83

:::MLPv0.5.0 ssd 1541757032.707526922 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 201, "value": 0.035733333333333325}

:::MLPv0.5.0 ssd 1541757032.791897535 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 202, "value": 0.0359111111111111}

:::MLPv0.5.0 ssd 1541757032.878067255 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 203, "value": 0.03608888888888889}

:::MLPv0.5.0 ssd 1541757032.963246584 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 204, "value": 0.03626666666666667}

:::MLPv0.5.0 ssd 1541757033.047624588 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 205, "value": 0.036444444444444446}

:::MLPv0.5.0 ssd 1541757033.131632090 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 206, "value": 0.03662222222222222}

:::MLPv0.5.0 ssd 1541757033.215292931 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 207, "value": 0.0368}

:::MLPv0.5.0 ssd 1541757033.298644781 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 208, "value": 0.036977777777777776}

:::MLPv0.5.0 ssd 1541757033.382382393 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 209, "value": 0.03715555555555555}

:::MLPv0.5.0 ssd 1541757033.466721058 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 210, "value": 0.03733333333333333}

:::MLPv0.5.0 ssd 1541757033.549851894 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 211, "value": 0.037511111111111106}

:::MLPv0.5.0 ssd 1541757033.637082100 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 212, "value": 0.03768888888888888}

:::MLPv0.5.0 ssd 1541757033.722990513 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 213, "value": 0.03786666666666666}

:::MLPv0.5.0 ssd 1541757033.806692123 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 214, "value": 0.038044444444444436}

:::MLPv0.5.0 ssd 1541757033.891829252 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 215, "value": 0.03822222222222221}

:::MLPv0.5.0 ssd 1541757033.976922274 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 216, "value": 0.038400000000000004}

:::MLPv0.5.0 ssd 1541757034.061776638 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 217, "value": 0.03857777777777778}

:::MLPv0.5.0 ssd 1541757034.145268917 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 218, "value": 0.03875555555555556}

:::MLPv0.5.0 ssd 1541757034.229600906 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 219, "value": 0.038933333333333334}

:::MLPv0.5.0 ssd 1541757034.313426971 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 220, "value": 0.03911111111111111}
Iteration:    220, Loss function: 8.184, Average Loss: 2.167, avg. samples / sec: 24209.73
Iteration:    220, Loss function: 7.950, Average Loss: 2.173, avg. samples / sec: 24206.41
Iteration:    220, Loss function: 6.777, Average Loss: 2.170, avg. samples / sec: 24248.77
Iteration:    220, Loss function: 7.822, Average Loss: 2.169, avg. samples / sec: 24212.89
Iteration:    220, Loss function: 7.186, Average Loss: 2.165, avg. samples / sec: 24213.21
Iteration:    220, Loss function: 8.170, Average Loss: 2.168, avg. samples / sec: 24159.98
Iteration:    220, Loss function: 7.631, Average Loss: 2.163, avg. samples / sec: 24160.23
Iteration:    220, Loss function: 7.289, Average Loss: 2.165, avg. samples / sec: 24201.89

:::MLPv0.5.0 ssd 1541757034.397274017 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 221, "value": 0.03928888888888889}

:::MLPv0.5.0 ssd 1541757034.481437206 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 222, "value": 0.039466666666666664}

:::MLPv0.5.0 ssd 1541757034.565665245 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 223, "value": 0.03964444444444444}

:::MLPv0.5.0 ssd 1541757034.649739265 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 224, "value": 0.03982222222222222}

:::MLPv0.5.0 ssd 1541757034.733474731 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 225, "value": 0.039999999999999994}

:::MLPv0.5.0 ssd 1541757034.817190886 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 226, "value": 0.04017777777777777}

:::MLPv0.5.0 ssd 1541757034.901109695 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 227, "value": 0.04035555555555555}

:::MLPv0.5.0 ssd 1541757034.984957695 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 228, "value": 0.04053333333333334}

:::MLPv0.5.0 ssd 1541757035.068967342 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 229, "value": 0.040711111111111115}

:::MLPv0.5.0 ssd 1541757035.153322220 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 230, "value": 0.04088888888888889}

:::MLPv0.5.0 ssd 1541757035.236788988 (train.py:349) opt_learning_rate: {"epoch": 3, "iteration": 231, "value": 0.04106666666666667}

:::MLPv0.5.0 ssd 1541757035.319553614 (train.py:553) train_epoch: 4

:::MLPv0.5.0 ssd 1541757035.323664188 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 232, "value": 0.041244444444444445}

:::MLPv0.5.0 ssd 1541757035.408203363 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 233, "value": 0.04142222222222222}

:::MLPv0.5.0 ssd 1541757035.492327452 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 234, "value": 0.0416}

:::MLPv0.5.0 ssd 1541757035.575767756 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 235, "value": 0.041777777777777775}

:::MLPv0.5.0 ssd 1541757035.659184217 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 236, "value": 0.04195555555555555}

:::MLPv0.5.0 ssd 1541757035.743489504 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 237, "value": 0.04213333333333333}

:::MLPv0.5.0 ssd 1541757035.827989578 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 238, "value": 0.042311111111111105}

:::MLPv0.5.0 ssd 1541757035.912292957 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 239, "value": 0.04248888888888888}

:::MLPv0.5.0 ssd 1541757035.995320320 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 240, "value": 0.04266666666666666}
Iteration:    240, Loss function: 7.605, Average Loss: 2.274, avg. samples / sec: 24354.84
Iteration:    240, Loss function: 7.181, Average Loss: 2.274, avg. samples / sec: 24393.07
Iteration:    240, Loss function: 7.533, Average Loss: 2.274, avg. samples / sec: 24349.95
Iteration:    240, Loss function: 7.140, Average Loss: 2.278, avg. samples / sec: 24343.12
Iteration:    240, Loss function: 7.561, Average Loss: 2.275, avg. samples / sec: 24344.97
Iteration:    240, Loss function: 7.304, Average Loss: 2.269, avg. samples / sec: 24363.59
Iteration:    240, Loss function: 7.048, Average Loss: 2.271, avg. samples / sec: 24368.58
Iteration:    240, Loss function: 7.164, Average Loss: 2.271, avg. samples / sec: 24334.03

:::MLPv0.5.0 ssd 1541757036.079294443 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 241, "value": 0.04284444444444445}

:::MLPv0.5.0 ssd 1541757036.162687540 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 242, "value": 0.043022222222222226}

:::MLPv0.5.0 ssd 1541757036.246644735 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 243, "value": 0.0432}

:::MLPv0.5.0 ssd 1541757036.330538988 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 244, "value": 0.04337777777777778}

:::MLPv0.5.0 ssd 1541757036.414988756 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 245, "value": 0.043555555555555556}

:::MLPv0.5.0 ssd 1541757036.499360800 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 246, "value": 0.04373333333333333}

:::MLPv0.5.0 ssd 1541757036.583743572 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 247, "value": 0.04391111111111111}

:::MLPv0.5.0 ssd 1541757036.667107821 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 248, "value": 0.044088888888888886}

:::MLPv0.5.0 ssd 1541757036.751958370 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 249, "value": 0.04426666666666666}

:::MLPv0.5.0 ssd 1541757036.836223841 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 250, "value": 0.04444444444444444}

:::MLPv0.5.0 ssd 1541757036.920984268 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 251, "value": 0.044622222222222216}

:::MLPv0.5.0 ssd 1541757037.004556179 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 252, "value": 0.04479999999999999}

:::MLPv0.5.0 ssd 1541757037.088319302 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 253, "value": 0.04497777777777777}

:::MLPv0.5.0 ssd 1541757037.172196150 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 254, "value": 0.04515555555555556}

:::MLPv0.5.0 ssd 1541757037.255814314 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 255, "value": 0.04533333333333334}

:::MLPv0.5.0 ssd 1541757037.339819670 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 256, "value": 0.04551111111111111}

:::MLPv0.5.0 ssd 1541757037.422948599 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 257, "value": 0.04568888888888889}

:::MLPv0.5.0 ssd 1541757037.506592512 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 258, "value": 0.04586666666666667}

:::MLPv0.5.0 ssd 1541757037.589569569 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 259, "value": 0.04604444444444444}

:::MLPv0.5.0 ssd 1541757037.673025846 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 260, "value": 0.04622222222222222}
Iteration:    260, Loss function: 7.258, Average Loss: 2.376, avg. samples / sec: 24432.87
Iteration:    260, Loss function: 7.471, Average Loss: 2.374, avg. samples / sec: 24412.02
Iteration:    260, Loss function: 7.554, Average Loss: 2.378, avg. samples / sec: 24425.21
Iteration:    260, Loss function: 7.565, Average Loss: 2.374, avg. samples / sec: 24420.35
Iteration:    260, Loss function: 7.804, Average Loss: 2.372, avg. samples / sec: 24447.58
Iteration:    260, Loss function: 7.181, Average Loss: 2.376, avg. samples / sec: 24373.76
Iteration:    260, Loss function: 8.251, Average Loss: 2.373, avg. samples / sec: 24403.97
Iteration:    260, Loss function: 7.818, Average Loss: 2.372, avg. samples / sec: 24396.23

:::MLPv0.5.0 ssd 1541757037.756481647 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 261, "value": 0.0464}

:::MLPv0.5.0 ssd 1541757037.839659214 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 262, "value": 0.046577777777777774}

:::MLPv0.5.0 ssd 1541757037.923090458 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 263, "value": 0.04675555555555555}

:::MLPv0.5.0 ssd 1541757038.006352186 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 264, "value": 0.04693333333333333}

:::MLPv0.5.0 ssd 1541757038.089531422 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 265, "value": 0.047111111111111104}

:::MLPv0.5.0 ssd 1541757038.173179626 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 266, "value": 0.04728888888888888}

:::MLPv0.5.0 ssd 1541757038.256471872 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 267, "value": 0.04746666666666667}

:::MLPv0.5.0 ssd 1541757038.340251446 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 268, "value": 0.04764444444444445}

:::MLPv0.5.0 ssd 1541757038.423504829 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 269, "value": 0.047822222222222224}

:::MLPv0.5.0 ssd 1541757038.506576538 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 270, "value": 0.048}

:::MLPv0.5.0 ssd 1541757038.590920687 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 271, "value": 0.04817777777777778}

:::MLPv0.5.0 ssd 1541757038.675019026 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 272, "value": 0.048355555555555554}

:::MLPv0.5.0 ssd 1541757038.759342670 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 273, "value": 0.04853333333333333}

:::MLPv0.5.0 ssd 1541757038.842913866 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 274, "value": 0.04871111111111111}

:::MLPv0.5.0 ssd 1541757038.930152416 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 275, "value": 0.048888888888888885}

:::MLPv0.5.0 ssd 1541757039.013574123 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 276, "value": 0.04906666666666666}

:::MLPv0.5.0 ssd 1541757039.097157001 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 277, "value": 0.04924444444444444}

:::MLPv0.5.0 ssd 1541757039.181238413 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 278, "value": 0.049422222222222215}

:::MLPv0.5.0 ssd 1541757039.267325401 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 279, "value": 0.04959999999999999}

:::MLPv0.5.0 ssd 1541757039.350723028 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 280, "value": 0.04977777777777778}
Iteration:    280, Loss function: 7.632, Average Loss: 2.476, avg. samples / sec: 24463.75
Iteration:    280, Loss function: 7.187, Average Loss: 2.477, avg. samples / sec: 24418.90
Iteration:    280, Loss function: 7.061, Average Loss: 2.475, avg. samples / sec: 24408.88
Iteration:    280, Loss function: 6.729, Average Loss: 2.471, avg. samples / sec: 24456.80
Iteration:    280, Loss function: 6.878, Average Loss: 2.471, avg. samples / sec: 24404.77
Iteration:    280, Loss function: 7.278, Average Loss: 2.473, avg. samples / sec: 24383.65
Iteration:    280, Loss function: 6.815, Average Loss: 2.472, avg. samples / sec: 24361.31
Iteration:    280, Loss function: 7.279, Average Loss: 2.471, avg. samples / sec: 24410.21

:::MLPv0.5.0 ssd 1541757039.434610605 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 281, "value": 0.04995555555555556}

:::MLPv0.5.0 ssd 1541757039.518545151 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 282, "value": 0.050133333333333335}

:::MLPv0.5.0 ssd 1541757039.602549791 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 283, "value": 0.05031111111111111}

:::MLPv0.5.0 ssd 1541757039.686169386 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 284, "value": 0.05048888888888889}

:::MLPv0.5.0 ssd 1541757039.769414186 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 285, "value": 0.050666666666666665}

:::MLPv0.5.0 ssd 1541757039.852957964 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 286, "value": 0.05084444444444444}

:::MLPv0.5.0 ssd 1541757039.936454296 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 287, "value": 0.05102222222222222}

:::MLPv0.5.0 ssd 1541757040.019839287 (train.py:349) opt_learning_rate: {"epoch": 4, "iteration": 288, "value": 0.051199999999999996}

:::MLPv0.5.0 ssd 1541757040.102659941 (train.py:553) train_epoch: 5

:::MLPv0.5.0 ssd 1541757040.106671810 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 289, "value": 0.05137777777777777}

:::MLPv0.5.0 ssd 1541757040.190453291 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 290, "value": 0.05155555555555555}

:::MLPv0.5.0 ssd 1541757040.274294853 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 291, "value": 0.051733333333333326}

:::MLPv0.5.0 ssd 1541757040.358361006 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 292, "value": 0.0519111111111111}

:::MLPv0.5.0 ssd 1541757040.442137957 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 293, "value": 0.05208888888888889}

:::MLPv0.5.0 ssd 1541757040.525813818 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 294, "value": 0.05226666666666667}

:::MLPv0.5.0 ssd 1541757040.608820438 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 295, "value": 0.052444444444444446}

:::MLPv0.5.0 ssd 1541757040.691963196 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 296, "value": 0.05262222222222222}

:::MLPv0.5.0 ssd 1541757040.775722265 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 297, "value": 0.0528}

:::MLPv0.5.0 ssd 1541757040.861156940 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 298, "value": 0.052977777777777776}

:::MLPv0.5.0 ssd 1541757040.945581198 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 299, "value": 0.05315555555555555}

:::MLPv0.5.0 ssd 1541757041.030426264 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 300, "value": 0.05333333333333333}
Iteration:    300, Loss function: 7.363, Average Loss: 2.570, avg. samples / sec: 24383.01
Iteration:    300, Loss function: 6.813, Average Loss: 2.567, avg. samples / sec: 24385.28
Iteration:    300, Loss function: 6.918, Average Loss: 2.565, avg. samples / sec: 24435.18
Iteration:    300, Loss function: 6.762, Average Loss: 2.564, avg. samples / sec: 24391.41
Iteration:    300, Loss function: 7.261, Average Loss: 2.563, avg. samples / sec: 24441.73
Iteration:    300, Loss function: 7.385, Average Loss: 2.565, avg. samples / sec: 24391.76
Iteration:    300, Loss function: 6.830, Average Loss: 2.564, avg. samples / sec: 24431.04
Iteration:    300, Loss function: 6.959, Average Loss: 2.568, avg. samples / sec: 24365.01

:::MLPv0.5.0 ssd 1541757041.114632607 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 301, "value": 0.053511111111111107}

:::MLPv0.5.0 ssd 1541757041.198258400 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 302, "value": 0.05368888888888888}

:::MLPv0.5.0 ssd 1541757041.282627106 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 303, "value": 0.05386666666666666}

:::MLPv0.5.0 ssd 1541757041.366125584 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 304, "value": 0.05404444444444444}

:::MLPv0.5.0 ssd 1541757041.449483633 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 305, "value": 0.05422222222222223}

:::MLPv0.5.0 ssd 1541757041.533840418 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 306, "value": 0.054400000000000004}

:::MLPv0.5.0 ssd 1541757041.621009588 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 307, "value": 0.05457777777777778}

:::MLPv0.5.0 ssd 1541757041.704613924 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 308, "value": 0.05475555555555556}

:::MLPv0.5.0 ssd 1541757041.788120985 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 309, "value": 0.054933333333333334}

:::MLPv0.5.0 ssd 1541757041.872155905 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 310, "value": 0.05511111111111111}

:::MLPv0.5.0 ssd 1541757041.956112862 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 311, "value": 0.05528888888888889}

:::MLPv0.5.0 ssd 1541757042.039928436 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 312, "value": 0.055466666666666664}

:::MLPv0.5.0 ssd 1541757042.124500513 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 313, "value": 0.05564444444444444}

:::MLPv0.5.0 ssd 1541757042.208808184 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 314, "value": 0.05582222222222222}

:::MLPv0.5.0 ssd 1541757042.292676926 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 315, "value": 0.055999999999999994}

:::MLPv0.5.0 ssd 1541757042.376069307 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 316, "value": 0.05617777777777777}

:::MLPv0.5.0 ssd 1541757042.459878683 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 317, "value": 0.05635555555555555}

:::MLPv0.5.0 ssd 1541757042.544240713 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 318, "value": 0.05653333333333334}

:::MLPv0.5.0 ssd 1541757042.628471136 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 319, "value": 0.056711111111111115}

:::MLPv0.5.0 ssd 1541757042.712590933 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 320, "value": 0.05688888888888889}
Iteration:    320, Loss function: 7.264, Average Loss: 2.656, avg. samples / sec: 24354.96
Iteration:    320, Loss function: 6.970, Average Loss: 2.652, avg. samples / sec: 24352.85
Iteration:    320, Loss function: 6.494, Average Loss: 2.653, avg. samples / sec: 24354.78
Iteration:    320, Loss function: 6.989, Average Loss: 2.650, avg. samples / sec: 24346.76
Iteration:    320, Loss function: 6.500, Average Loss: 2.653, avg. samples / sec: 24322.86
Iteration:    320, Loss function: 7.051, Average Loss: 2.655, avg. samples / sec: 24323.44
Iteration:    320, Loss function: 7.206, Average Loss: 2.654, avg. samples / sec: 24296.48
Iteration:    320, Loss function: 7.058, Average Loss: 2.654, avg. samples / sec: 24249.09

:::MLPv0.5.0 ssd 1541757042.799043655 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 321, "value": 0.05706666666666667}

:::MLPv0.5.0 ssd 1541757042.883736849 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 322, "value": 0.057244444444444445}

:::MLPv0.5.0 ssd 1541757042.967547178 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 323, "value": 0.05742222222222222}

:::MLPv0.5.0 ssd 1541757043.051123142 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 324, "value": 0.0576}

:::MLPv0.5.0 ssd 1541757043.135046959 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 325, "value": 0.057777777777777775}

:::MLPv0.5.0 ssd 1541757043.219395161 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 326, "value": 0.05795555555555555}

:::MLPv0.5.0 ssd 1541757043.303617239 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 327, "value": 0.05813333333333333}

:::MLPv0.5.0 ssd 1541757043.387361288 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 328, "value": 0.058311111111111105}

:::MLPv0.5.0 ssd 1541757043.471145630 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 329, "value": 0.05848888888888888}

:::MLPv0.5.0 ssd 1541757043.555156946 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 330, "value": 0.05866666666666666}

:::MLPv0.5.0 ssd 1541757043.638460398 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 331, "value": 0.05884444444444445}

:::MLPv0.5.0 ssd 1541757043.722526789 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 332, "value": 0.059022222222222226}

:::MLPv0.5.0 ssd 1541757043.808422804 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 333, "value": 0.0592}

:::MLPv0.5.0 ssd 1541757043.892191172 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 334, "value": 0.05937777777777778}

:::MLPv0.5.0 ssd 1541757043.976119041 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 335, "value": 0.059555555555555556}

:::MLPv0.5.0 ssd 1541757044.060744524 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 336, "value": 0.05973333333333333}

:::MLPv0.5.0 ssd 1541757044.144253492 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 337, "value": 0.05991111111111111}

:::MLPv0.5.0 ssd 1541757044.230250359 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 338, "value": 0.060088888888888886}

:::MLPv0.5.0 ssd 1541757044.313690662 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 339, "value": 0.06026666666666666}

:::MLPv0.5.0 ssd 1541757044.397395134 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 340, "value": 0.06044444444444444}
Iteration:    340, Loss function: 6.970, Average Loss: 2.741, avg. samples / sec: 24312.32
Iteration:    340, Loss function: 6.937, Average Loss: 2.736, avg. samples / sec: 24415.49
Iteration:    340, Loss function: 6.784, Average Loss: 2.737, avg. samples / sec: 24312.30
Iteration:    340, Loss function: 6.422, Average Loss: 2.733, avg. samples / sec: 24310.40
Iteration:    340, Loss function: 7.110, Average Loss: 2.738, avg. samples / sec: 24354.23
Iteration:    340, Loss function: 6.758, Average Loss: 2.741, avg. samples / sec: 24351.95
Iteration:    340, Loss function: 6.964, Average Loss: 2.735, avg. samples / sec: 24313.17
Iteration:    340, Loss function: 7.039, Average Loss: 2.735, avg. samples / sec: 24265.36

:::MLPv0.5.0 ssd 1541757044.480905771 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 341, "value": 0.060622222222222216}

:::MLPv0.5.0 ssd 1541757044.564545870 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 342, "value": 0.06079999999999999}

:::MLPv0.5.0 ssd 1541757044.648120880 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 343, "value": 0.06097777777777777}

:::MLPv0.5.0 ssd 1541757044.731811523 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 344, "value": 0.06115555555555556}

:::MLPv0.5.0 ssd 1541757044.815875530 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 345, "value": 0.06133333333333334}

:::MLPv0.5.0 ssd 1541757044.906436443 (train.py:349) opt_learning_rate: {"epoch": 5, "iteration": 346, "value": 0.061511111111111114}

:::MLPv0.5.0 ssd 1541757044.987825155 (train.py:553) train_epoch: 6

:::MLPv0.5.0 ssd 1541757044.991915941 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 347, "value": 0.06168888888888889}

:::MLPv0.5.0 ssd 1541757045.076038361 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 348, "value": 0.06186666666666667}

:::MLPv0.5.0 ssd 1541757045.159565210 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 349, "value": 0.062044444444444444}

:::MLPv0.5.0 ssd 1541757045.243204832 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 350, "value": 0.06222222222222222}

:::MLPv0.5.0 ssd 1541757045.326165438 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 351, "value": 0.0624}

:::MLPv0.5.0 ssd 1541757045.409395933 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 352, "value": 0.06257777777777777}

:::MLPv0.5.0 ssd 1541757045.493131161 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 353, "value": 0.06275555555555555}

:::MLPv0.5.0 ssd 1541757045.576639414 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 354, "value": 0.06293333333333333}

:::MLPv0.5.0 ssd 1541757045.660062790 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 355, "value": 0.0631111111111111}

:::MLPv0.5.0 ssd 1541757045.744198799 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 356, "value": 0.0632888888888889}

:::MLPv0.5.0 ssd 1541757045.827633142 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 357, "value": 0.06346666666666667}

:::MLPv0.5.0 ssd 1541757045.911976814 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 358, "value": 0.06364444444444445}

:::MLPv0.5.0 ssd 1541757045.995315552 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 359, "value": 0.06382222222222222}

:::MLPv0.5.0 ssd 1541757046.078128338 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 360, "value": 0.064}
Iteration:    360, Loss function: 7.004, Average Loss: 2.821, avg. samples / sec: 24375.55
Iteration:    360, Loss function: 6.409, Average Loss: 2.815, avg. samples / sec: 24364.22
Iteration:    360, Loss function: 6.291, Average Loss: 2.813, avg. samples / sec: 24365.88
Iteration:    360, Loss function: 7.009, Average Loss: 2.814, avg. samples / sec: 24372.94
Iteration:    360, Loss function: 6.788, Average Loss: 2.816, avg. samples / sec: 24352.12
Iteration:    360, Loss function: 6.430, Average Loss: 2.815, avg. samples / sec: 24315.12
Iteration:    360, Loss function: 6.626, Average Loss: 2.820, avg. samples / sec: 24340.49
Iteration:    360, Loss function: 6.476, Average Loss: 2.815, avg. samples / sec: 24365.62

:::MLPv0.5.0 ssd 1541757046.162875175 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 361, "value": 0.06417777777777778}

:::MLPv0.5.0 ssd 1541757046.247506142 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 362, "value": 0.06435555555555555}

:::MLPv0.5.0 ssd 1541757046.331560612 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 363, "value": 0.06453333333333333}

:::MLPv0.5.0 ssd 1541757046.415565968 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 364, "value": 0.06471111111111111}

:::MLPv0.5.0 ssd 1541757046.499336243 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 365, "value": 0.06488888888888888}

:::MLPv0.5.0 ssd 1541757046.583415508 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 366, "value": 0.06506666666666666}

:::MLPv0.5.0 ssd 1541757046.667455196 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 367, "value": 0.06524444444444444}

:::MLPv0.5.0 ssd 1541757046.751167536 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 368, "value": 0.06542222222222221}

:::MLPv0.5.0 ssd 1541757046.835982800 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 369, "value": 0.0656}

:::MLPv0.5.0 ssd 1541757046.919756889 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 370, "value": 0.06577777777777778}

:::MLPv0.5.0 ssd 1541757047.003205776 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 371, "value": 0.06595555555555556}

:::MLPv0.5.0 ssd 1541757047.087702274 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 372, "value": 0.06613333333333334}

:::MLPv0.5.0 ssd 1541757047.171522856 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 373, "value": 0.06631111111111111}

:::MLPv0.5.0 ssd 1541757047.255194426 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 374, "value": 0.06648888888888889}

:::MLPv0.5.0 ssd 1541757047.339008808 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 375, "value": 0.06666666666666667}

:::MLPv0.5.0 ssd 1541757047.422891617 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 376, "value": 0.06684444444444444}

:::MLPv0.5.0 ssd 1541757047.507510185 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 377, "value": 0.06702222222222222}

:::MLPv0.5.0 ssd 1541757047.591705322 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 378, "value": 0.0672}

:::MLPv0.5.0 ssd 1541757047.675835848 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 379, "value": 0.06737777777777777}

:::MLPv0.5.0 ssd 1541757047.759724140 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 380, "value": 0.06755555555555555}
Iteration:    380, Loss function: 6.983, Average Loss: 2.894, avg. samples / sec: 24353.50
Iteration:    380, Loss function: 6.379, Average Loss: 2.887, avg. samples / sec: 24365.42
Iteration:    380, Loss function: 6.334, Average Loss: 2.889, avg. samples / sec: 24416.50
Iteration:    380, Loss function: 7.174, Average Loss: 2.889, avg. samples / sec: 24362.47
Iteration:    380, Loss function: 6.239, Average Loss: 2.887, avg. samples / sec: 24344.94
Iteration:    380, Loss function: 6.039, Average Loss: 2.896, avg. samples / sec: 24375.06
Iteration:    380, Loss function: 6.500, Average Loss: 2.892, avg. samples / sec: 24352.39
Iteration:    380, Loss function: 6.743, Average Loss: 2.888, avg. samples / sec: 24353.21

:::MLPv0.5.0 ssd 1541757047.843837976 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 381, "value": 0.06773333333333333}

:::MLPv0.5.0 ssd 1541757047.927666426 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 382, "value": 0.06791111111111112}

:::MLPv0.5.0 ssd 1541757048.011424780 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 383, "value": 0.0680888888888889}

:::MLPv0.5.0 ssd 1541757048.096786976 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 384, "value": 0.06826666666666667}

:::MLPv0.5.0 ssd 1541757048.180302858 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 385, "value": 0.06844444444444445}

:::MLPv0.5.0 ssd 1541757048.263839245 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 386, "value": 0.06862222222222222}

:::MLPv0.5.0 ssd 1541757048.347116947 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 387, "value": 0.0688}

:::MLPv0.5.0 ssd 1541757048.431830883 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 388, "value": 0.06897777777777778}

:::MLPv0.5.0 ssd 1541757048.515933275 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 389, "value": 0.06915555555555555}

:::MLPv0.5.0 ssd 1541757048.599283695 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 390, "value": 0.06933333333333333}

:::MLPv0.5.0 ssd 1541757048.683469296 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 391, "value": 0.0695111111111111}

:::MLPv0.5.0 ssd 1541757048.767080784 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 392, "value": 0.06968888888888888}

:::MLPv0.5.0 ssd 1541757048.850707293 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 393, "value": 0.06986666666666666}

:::MLPv0.5.0 ssd 1541757048.934419394 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 394, "value": 0.07004444444444444}

:::MLPv0.5.0 ssd 1541757049.019223690 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 395, "value": 0.07022222222222223}

:::MLPv0.5.0 ssd 1541757049.103369474 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 396, "value": 0.0704}

:::MLPv0.5.0 ssd 1541757049.187977791 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 397, "value": 0.07057777777777778}

:::MLPv0.5.0 ssd 1541757049.271571875 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 398, "value": 0.07075555555555556}

:::MLPv0.5.0 ssd 1541757049.356718302 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 399, "value": 0.07093333333333333}

:::MLPv0.5.0 ssd 1541757049.441209078 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 400, "value": 0.07111111111111111}
Iteration:    400, Loss function: 6.141, Average Loss: 2.962, avg. samples / sec: 24360.56
Iteration:    400, Loss function: 6.342, Average Loss: 2.975, avg. samples / sec: 24398.18
Iteration:    400, Loss function: 6.454, Average Loss: 2.964, avg. samples / sec: 24371.29
Iteration:    400, Loss function: 6.926, Average Loss: 2.971, avg. samples / sec: 24386.63
Iteration:    400, Loss function: 6.728, Average Loss: 2.965, avg. samples / sec: 24384.55
Iteration:    400, Loss function: 6.396, Average Loss: 2.964, avg. samples / sec: 24323.92
Iteration:    400, Loss function: 6.219, Average Loss: 2.966, avg. samples / sec: 24317.88
Iteration:    400, Loss function: 6.659, Average Loss: 2.971, avg. samples / sec: 24302.29

:::MLPv0.5.0 ssd 1541757049.525294065 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 401, "value": 0.07128888888888889}

:::MLPv0.5.0 ssd 1541757049.609299660 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 402, "value": 0.07146666666666666}

:::MLPv0.5.0 ssd 1541757049.693628788 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 403, "value": 0.07164444444444444}

:::MLPv0.5.0 ssd 1541757049.777888536 (train.py:349) opt_learning_rate: {"epoch": 6, "iteration": 404, "value": 0.07182222222222222}

:::MLPv0.5.0 ssd 1541757049.860230207 (train.py:553) train_epoch: 7

:::MLPv0.5.0 ssd 1541757049.864485979 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 405, "value": 0.072}

:::MLPv0.5.0 ssd 1541757049.948122263 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 406, "value": 0.07217777777777777}

:::MLPv0.5.0 ssd 1541757050.032521248 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 407, "value": 0.07235555555555555}

:::MLPv0.5.0 ssd 1541757050.117397308 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 408, "value": 0.07253333333333334}

:::MLPv0.5.0 ssd 1541757050.200935602 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 409, "value": 0.07271111111111112}

:::MLPv0.5.0 ssd 1541757050.284779549 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 410, "value": 0.07288888888888889}

:::MLPv0.5.0 ssd 1541757050.368855238 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 411, "value": 0.07306666666666667}

:::MLPv0.5.0 ssd 1541757050.452054977 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 412, "value": 0.07324444444444445}

:::MLPv0.5.0 ssd 1541757050.535740137 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 413, "value": 0.07342222222222222}

:::MLPv0.5.0 ssd 1541757050.619609833 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 414, "value": 0.0736}

:::MLPv0.5.0 ssd 1541757050.703871727 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 415, "value": 0.07377777777777778}

:::MLPv0.5.0 ssd 1541757050.787539005 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 416, "value": 0.07395555555555555}

:::MLPv0.5.0 ssd 1541757050.871133089 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 417, "value": 0.07413333333333333}

:::MLPv0.5.0 ssd 1541757050.955257893 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 418, "value": 0.0743111111111111}

:::MLPv0.5.0 ssd 1541757051.038842440 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 419, "value": 0.07448888888888888}

:::MLPv0.5.0 ssd 1541757051.122849464 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 420, "value": 0.07466666666666666}
Iteration:    420, Loss function: 6.474, Average Loss: 3.035, avg. samples / sec: 24396.81
Iteration:    420, Loss function: 6.460, Average Loss: 3.040, avg. samples / sec: 24408.38
Iteration:    420, Loss function: 7.089, Average Loss: 3.035, avg. samples / sec: 24374.44
Iteration:    420, Loss function: 6.108, Average Loss: 3.038, avg. samples / sec: 24363.22
Iteration:    420, Loss function: 6.683, Average Loss: 3.034, avg. samples / sec: 24333.66
Iteration:    420, Loss function: 6.389, Average Loss: 3.035, avg. samples / sec: 24361.74
Iteration:    420, Loss function: 6.510, Average Loss: 3.042, avg. samples / sec: 24314.78
Iteration:    420, Loss function: 7.112, Average Loss: 3.031, avg. samples / sec: 24304.68

:::MLPv0.5.0 ssd 1541757051.206793070 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 421, "value": 0.07484444444444445}

:::MLPv0.5.0 ssd 1541757051.291050196 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 422, "value": 0.07502222222222223}

:::MLPv0.5.0 ssd 1541757051.374723434 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 423, "value": 0.0752}

:::MLPv0.5.0 ssd 1541757051.459328651 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 424, "value": 0.07537777777777778}

:::MLPv0.5.0 ssd 1541757051.542877197 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 425, "value": 0.07555555555555556}

:::MLPv0.5.0 ssd 1541757051.626862764 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 426, "value": 0.07573333333333333}

:::MLPv0.5.0 ssd 1541757051.710982561 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 427, "value": 0.07591111111111111}

:::MLPv0.5.0 ssd 1541757051.795570135 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 428, "value": 0.07608888888888889}

:::MLPv0.5.0 ssd 1541757051.879372597 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 429, "value": 0.07626666666666666}

:::MLPv0.5.0 ssd 1541757051.964640617 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 430, "value": 0.07644444444444444}

:::MLPv0.5.0 ssd 1541757052.048341274 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 431, "value": 0.07662222222222222}

:::MLPv0.5.0 ssd 1541757052.132080317 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 432, "value": 0.0768}

:::MLPv0.5.0 ssd 1541757052.216935396 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 433, "value": 0.07697777777777778}

:::MLPv0.5.0 ssd 1541757052.300650835 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 434, "value": 0.07715555555555556}

:::MLPv0.5.0 ssd 1541757052.384471178 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 435, "value": 0.07733333333333334}

:::MLPv0.5.0 ssd 1541757052.468583584 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 436, "value": 0.07751111111111111}

:::MLPv0.5.0 ssd 1541757052.552686930 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 437, "value": 0.07768888888888889}

:::MLPv0.5.0 ssd 1541757052.636072636 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 438, "value": 0.07786666666666667}

:::MLPv0.5.0 ssd 1541757052.720122814 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 439, "value": 0.07804444444444444}

:::MLPv0.5.0 ssd 1541757052.803717613 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 440, "value": 0.07822222222222222}
Iteration:    440, Loss function: 6.285, Average Loss: 3.104, avg. samples / sec: 24392.79
Iteration:    440, Loss function: 6.226, Average Loss: 3.096, avg. samples / sec: 24420.29
Iteration:    440, Loss function: 6.558, Average Loss: 3.100, avg. samples / sec: 24373.15
Iteration:    440, Loss function: 6.465, Average Loss: 3.100, avg. samples / sec: 24388.15
Iteration:    440, Loss function: 6.133, Average Loss: 3.106, avg. samples / sec: 24325.31
Iteration:    440, Loss function: 5.929, Average Loss: 3.108, avg. samples / sec: 24367.35
Iteration:    440, Loss function: 6.262, Average Loss: 3.101, avg. samples / sec: 24318.23
Iteration:    440, Loss function: 6.251, Average Loss: 3.100, avg. samples / sec: 24354.89

:::MLPv0.5.0 ssd 1541757052.887369156 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 441, "value": 0.0784}

:::MLPv0.5.0 ssd 1541757052.971086264 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 442, "value": 0.07857777777777777}

:::MLPv0.5.0 ssd 1541757053.055068016 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 443, "value": 0.07875555555555555}

:::MLPv0.5.0 ssd 1541757053.138909101 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 444, "value": 0.07893333333333333}

:::MLPv0.5.0 ssd 1541757053.223267794 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 445, "value": 0.0791111111111111}

:::MLPv0.5.0 ssd 1541757053.306953907 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 446, "value": 0.0792888888888889}

:::MLPv0.5.0 ssd 1541757053.391382456 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 447, "value": 0.07946666666666667}

:::MLPv0.5.0 ssd 1541757053.474845171 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 448, "value": 0.07964444444444445}

:::MLPv0.5.0 ssd 1541757053.558286905 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 449, "value": 0.07982222222222222}

:::MLPv0.5.0 ssd 1541757053.642421246 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 450, "value": 0.08}

:::MLPv0.5.0 ssd 1541757053.726444960 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 451, "value": 0.08017777777777778}

:::MLPv0.5.0 ssd 1541757053.810980082 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 452, "value": 0.08035555555555556}

:::MLPv0.5.0 ssd 1541757053.895430565 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 453, "value": 0.08053333333333333}

:::MLPv0.5.0 ssd 1541757053.979127884 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 454, "value": 0.08071111111111111}

:::MLPv0.5.0 ssd 1541757054.062962294 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 455, "value": 0.08088888888888889}

:::MLPv0.5.0 ssd 1541757054.147412300 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 456, "value": 0.08106666666666666}

:::MLPv0.5.0 ssd 1541757054.231372356 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 457, "value": 0.08124444444444444}

:::MLPv0.5.0 ssd 1541757054.315067291 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 458, "value": 0.08142222222222222}

:::MLPv0.5.0 ssd 1541757054.398779631 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 459, "value": 0.0816}

:::MLPv0.5.0 ssd 1541757054.482353687 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 460, "value": 0.08177777777777778}
Iteration:    460, Loss function: 6.102, Average Loss: 3.171, avg. samples / sec: 24458.30
Iteration:    460, Loss function: 6.241, Average Loss: 3.163, avg. samples / sec: 24425.92
Iteration:    460, Loss function: 6.209, Average Loss: 3.168, avg. samples / sec: 24450.92
Iteration:    460, Loss function: 6.379, Average Loss: 3.157, avg. samples / sec: 24402.43
Iteration:    460, Loss function: 6.562, Average Loss: 3.161, avg. samples / sec: 24442.37
Iteration:    460, Loss function: 5.956, Average Loss: 3.159, avg. samples / sec: 24397.67
Iteration:    460, Loss function: 6.004, Average Loss: 3.160, avg. samples / sec: 24420.11
Iteration:    460, Loss function: 6.102, Average Loss: 3.165, avg. samples / sec: 24360.98

:::MLPv0.5.0 ssd 1541757054.566489458 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 461, "value": 0.08195555555555556}

:::MLPv0.5.0 ssd 1541757054.650000572 (train.py:349) opt_learning_rate: {"epoch": 7, "iteration": 462, "value": 0.08213333333333334}

:::MLPv0.5.0 ssd 1541757054.731438875 (train.py:553) train_epoch: 8

:::MLPv0.5.0 ssd 1541757054.735696793 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 463, "value": 0.08231111111111111}

:::MLPv0.5.0 ssd 1541757054.819763184 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 464, "value": 0.08248888888888889}

:::MLPv0.5.0 ssd 1541757054.904694557 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 465, "value": 0.08266666666666667}

:::MLPv0.5.0 ssd 1541757054.988849640 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 466, "value": 0.08284444444444444}

:::MLPv0.5.0 ssd 1541757055.073122740 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 467, "value": 0.08302222222222222}

:::MLPv0.5.0 ssd 1541757055.156753302 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 468, "value": 0.0832}

:::MLPv0.5.0 ssd 1541757055.240638971 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 469, "value": 0.08337777777777777}

:::MLPv0.5.0 ssd 1541757055.324763536 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 470, "value": 0.08355555555555555}

:::MLPv0.5.0 ssd 1541757055.409235477 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 471, "value": 0.08373333333333333}

:::MLPv0.5.0 ssd 1541757055.492974520 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 472, "value": 0.08391111111111112}

:::MLPv0.5.0 ssd 1541757055.577085018 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 473, "value": 0.0840888888888889}

:::MLPv0.5.0 ssd 1541757055.661337614 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 474, "value": 0.08426666666666667}

:::MLPv0.5.0 ssd 1541757055.745147467 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 475, "value": 0.08444444444444445}

:::MLPv0.5.0 ssd 1541757055.828899384 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 476, "value": 0.08462222222222222}

:::MLPv0.5.0 ssd 1541757055.913181543 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 477, "value": 0.0848}

:::MLPv0.5.0 ssd 1541757055.997064352 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 478, "value": 0.08497777777777778}

:::MLPv0.5.0 ssd 1541757056.080930471 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 479, "value": 0.08515555555555555}

:::MLPv0.5.0 ssd 1541757056.165595531 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 480, "value": 0.08533333333333333}
Iteration:    480, Loss function: 5.447, Average Loss: 3.218, avg. samples / sec: 24366.60
Iteration:    480, Loss function: 5.782, Average Loss: 3.219, avg. samples / sec: 24331.67
Iteration:    480, Loss function: 6.142, Average Loss: 3.216, avg. samples / sec: 24333.63
Iteration:    480, Loss function: 6.124, Average Loss: 3.223, avg. samples / sec: 24331.44
Iteration:    480, Loss function: 6.019, Average Loss: 3.229, avg. samples / sec: 24298.91
Iteration:    480, Loss function: 5.882, Average Loss: 3.226, avg. samples / sec: 24326.12
Iteration:    480, Loss function: 6.378, Average Loss: 3.225, avg. samples / sec: 24281.79
Iteration:    480, Loss function: 5.878, Average Loss: 3.219, avg. samples / sec: 24312.31

:::MLPv0.5.0 ssd 1541757056.249334812 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 481, "value": 0.08551111111111111}

:::MLPv0.5.0 ssd 1541757056.332678318 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 482, "value": 0.08568888888888888}

:::MLPv0.5.0 ssd 1541757056.416534901 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 483, "value": 0.08586666666666666}

:::MLPv0.5.0 ssd 1541757056.500496626 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 484, "value": 0.08604444444444445}

:::MLPv0.5.0 ssd 1541757056.584665537 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 485, "value": 0.08622222222222223}

:::MLPv0.5.0 ssd 1541757056.668438435 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 486, "value": 0.0864}

:::MLPv0.5.0 ssd 1541757056.752330303 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 487, "value": 0.08657777777777778}

:::MLPv0.5.0 ssd 1541757056.836728334 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 488, "value": 0.08675555555555556}

:::MLPv0.5.0 ssd 1541757056.920143366 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 489, "value": 0.08693333333333333}

:::MLPv0.5.0 ssd 1541757057.004124641 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 490, "value": 0.08711111111111111}

:::MLPv0.5.0 ssd 1541757057.087532997 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 491, "value": 0.08728888888888889}

:::MLPv0.5.0 ssd 1541757057.171788454 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 492, "value": 0.08746666666666666}

:::MLPv0.5.0 ssd 1541757057.255355120 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 493, "value": 0.08764444444444444}

:::MLPv0.5.0 ssd 1541757057.338693380 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 494, "value": 0.08782222222222222}

:::MLPv0.5.0 ssd 1541757057.422754526 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 495, "value": 0.088}

:::MLPv0.5.0 ssd 1541757057.506602049 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 496, "value": 0.08817777777777777}

:::MLPv0.5.0 ssd 1541757057.590573549 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 497, "value": 0.08835555555555556}

:::MLPv0.5.0 ssd 1541757057.675229549 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 498, "value": 0.08853333333333334}

:::MLPv0.5.0 ssd 1541757057.759379625 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 499, "value": 0.08871111111111112}

:::MLPv0.5.0 ssd 1541757057.843900204 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 500, "value": 0.08888888888888889}
Iteration:    500, Loss function: 6.598, Average Loss: 3.274, avg. samples / sec: 24406.20
Iteration:    500, Loss function: 6.211, Average Loss: 3.279, avg. samples / sec: 24457.88
Iteration:    500, Loss function: 6.486, Average Loss: 3.272, avg. samples / sec: 24402.45
Iteration:    500, Loss function: 6.666, Average Loss: 3.284, avg. samples / sec: 24420.72
Iteration:    500, Loss function: 6.782, Average Loss: 3.276, avg. samples / sec: 24413.48
Iteration:    500, Loss function: 6.338, Average Loss: 3.281, avg. samples / sec: 24404.79
Iteration:    500, Loss function: 6.769, Average Loss: 3.276, avg. samples / sec: 24355.66
Iteration:    500, Loss function: 6.557, Average Loss: 3.273, avg. samples / sec: 24333.41

:::MLPv0.5.0 ssd 1541757057.928239822 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 501, "value": 0.08906666666666667}

:::MLPv0.5.0 ssd 1541757058.013272285 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 502, "value": 0.08924444444444445}

:::MLPv0.5.0 ssd 1541757058.097016573 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 503, "value": 0.08942222222222222}

:::MLPv0.5.0 ssd 1541757058.180813074 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 504, "value": 0.0896}

:::MLPv0.5.0 ssd 1541757058.264918327 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 505, "value": 0.08977777777777778}

:::MLPv0.5.0 ssd 1541757058.349045753 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 506, "value": 0.08995555555555555}

:::MLPv0.5.0 ssd 1541757058.433199644 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 507, "value": 0.09013333333333333}

:::MLPv0.5.0 ssd 1541757058.516613483 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 508, "value": 0.0903111111111111}

:::MLPv0.5.0 ssd 1541757058.599994421 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 509, "value": 0.09048888888888888}

:::MLPv0.5.0 ssd 1541757058.683768511 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 510, "value": 0.09066666666666667}

:::MLPv0.5.0 ssd 1541757058.767985106 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 511, "value": 0.09084444444444445}

:::MLPv0.5.0 ssd 1541757058.852021456 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 512, "value": 0.09102222222222223}

:::MLPv0.5.0 ssd 1541757058.935985088 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 513, "value": 0.0912}

:::MLPv0.5.0 ssd 1541757059.020197392 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 514, "value": 0.09137777777777778}

:::MLPv0.5.0 ssd 1541757059.103848219 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 515, "value": 0.09155555555555556}

:::MLPv0.5.0 ssd 1541757059.187964678 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 516, "value": 0.09173333333333333}

:::MLPv0.5.0 ssd 1541757059.271690845 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 517, "value": 0.09191111111111111}

:::MLPv0.5.0 ssd 1541757059.356108904 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 518, "value": 0.09208888888888889}

:::MLPv0.5.0 ssd 1541757059.440050364 (train.py:349) opt_learning_rate: {"epoch": 8, "iteration": 519, "value": 0.09226666666666666}

:::MLPv0.5.0 ssd 1541757059.521746397 (train.py:553) train_epoch: 9

:::MLPv0.5.0 ssd 1541757059.525960684 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 520, "value": 0.09244444444444444}
Iteration:    520, Loss function: 6.377, Average Loss: 3.347, avg. samples / sec: 24407.20
Iteration:    520, Loss function: 6.206, Average Loss: 3.340, avg. samples / sec: 24347.63
Iteration:    520, Loss function: 5.751, Average Loss: 3.340, avg. samples / sec: 24413.06
Iteration:    520, Loss function: 6.113, Average Loss: 3.338, avg. samples / sec: 24335.50
Iteration:    520, Loss function: 6.433, Average Loss: 3.341, avg. samples / sec: 24378.98
Iteration:    520, Loss function: 6.288, Average Loss: 3.347, avg. samples / sec: 24313.80
Iteration:    520, Loss function: 6.478, Average Loss: 3.344, avg. samples / sec: 24347.29
Iteration:    520, Loss function: 6.406, Average Loss: 3.343, avg. samples / sec: 24295.95

:::MLPv0.5.0 ssd 1541757059.610119820 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 521, "value": 0.09262222222222222}

:::MLPv0.5.0 ssd 1541757059.693652153 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 522, "value": 0.0928}

:::MLPv0.5.0 ssd 1541757059.779047728 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 523, "value": 0.09297777777777778}

:::MLPv0.5.0 ssd 1541757059.863189459 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 524, "value": 0.09315555555555556}

:::MLPv0.5.0 ssd 1541757059.947657824 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 525, "value": 0.09333333333333334}

:::MLPv0.5.0 ssd 1541757060.030997276 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 526, "value": 0.09351111111111111}

:::MLPv0.5.0 ssd 1541757060.115398884 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 527, "value": 0.09368888888888889}

:::MLPv0.5.0 ssd 1541757060.199276686 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 528, "value": 0.09386666666666667}

:::MLPv0.5.0 ssd 1541757060.282848835 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 529, "value": 0.09404444444444444}

:::MLPv0.5.0 ssd 1541757060.367819071 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 530, "value": 0.09422222222222222}

:::MLPv0.5.0 ssd 1541757060.452553749 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 531, "value": 0.0944}

:::MLPv0.5.0 ssd 1541757060.536006212 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 532, "value": 0.09457777777777777}

:::MLPv0.5.0 ssd 1541757060.619649172 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 533, "value": 0.09475555555555555}

:::MLPv0.5.0 ssd 1541757060.702961445 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 534, "value": 0.09493333333333333}

:::MLPv0.5.0 ssd 1541757060.787344694 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 535, "value": 0.0951111111111111}

:::MLPv0.5.0 ssd 1541757060.871341228 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 536, "value": 0.0952888888888889}

:::MLPv0.5.0 ssd 1541757060.954890728 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 537, "value": 0.09546666666666667}

:::MLPv0.5.0 ssd 1541757061.038611889 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 538, "value": 0.09564444444444445}

:::MLPv0.5.0 ssd 1541757061.123093843 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 539, "value": 0.09582222222222223}

:::MLPv0.5.0 ssd 1541757061.208035231 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 540, "value": 0.096}
Iteration:    540, Loss function: 5.589, Average Loss: 3.390, avg. samples / sec: 24357.25
Iteration:    540, Loss function: 5.416, Average Loss: 3.396, avg. samples / sec: 24323.21
Iteration:    540, Loss function: 5.514, Average Loss: 3.388, avg. samples / sec: 24344.16
Iteration:    540, Loss function: 5.690, Average Loss: 3.393, avg. samples / sec: 24375.50
Iteration:    540, Loss function: 5.997, Average Loss: 3.394, avg. samples / sec: 24371.42
Iteration:    540, Loss function: 5.661, Average Loss: 3.390, avg. samples / sec: 24345.17
Iteration:    540, Loss function: 5.622, Average Loss: 3.398, avg. samples / sec: 24337.91
Iteration:    540, Loss function: 5.489, Average Loss: 3.392, avg. samples / sec: 24287.28

:::MLPv0.5.0 ssd 1541757061.292732000 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 541, "value": 0.09617777777777778}

:::MLPv0.5.0 ssd 1541757061.376347303 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 542, "value": 0.09635555555555556}

:::MLPv0.5.0 ssd 1541757061.459594488 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 543, "value": 0.09653333333333333}

:::MLPv0.5.0 ssd 1541757061.543130398 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 544, "value": 0.09671111111111111}

:::MLPv0.5.0 ssd 1541757061.626856089 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 545, "value": 0.09688888888888889}

:::MLPv0.5.0 ssd 1541757061.711098671 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 546, "value": 0.09706666666666666}

:::MLPv0.5.0 ssd 1541757061.794581413 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 547, "value": 0.09724444444444444}

:::MLPv0.5.0 ssd 1541757061.878762007 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 548, "value": 0.09742222222222222}

:::MLPv0.5.0 ssd 1541757061.962378502 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 549, "value": 0.09759999999999999}

:::MLPv0.5.0 ssd 1541757062.045423269 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 550, "value": 0.09777777777777777}

:::MLPv0.5.0 ssd 1541757062.129062653 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 551, "value": 0.09795555555555555}

:::MLPv0.5.0 ssd 1541757062.212852240 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 552, "value": 0.09813333333333334}

:::MLPv0.5.0 ssd 1541757062.296528578 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 553, "value": 0.09831111111111111}

:::MLPv0.5.0 ssd 1541757062.380171776 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 554, "value": 0.09848888888888889}

:::MLPv0.5.0 ssd 1541757062.464538574 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 555, "value": 0.09866666666666667}

:::MLPv0.5.0 ssd 1541757062.548979998 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 556, "value": 0.09884444444444444}

:::MLPv0.5.0 ssd 1541757062.633219957 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 557, "value": 0.09902222222222222}

:::MLPv0.5.0 ssd 1541757062.717733383 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 558, "value": 0.09920000000000001}

:::MLPv0.5.0 ssd 1541757062.801162481 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 559, "value": 0.09937777777777779}

:::MLPv0.5.0 ssd 1541757062.885574341 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 560, "value": 0.09955555555555556}
Iteration:    560, Loss function: 6.000, Average Loss: 3.440, avg. samples / sec: 24423.47
Iteration:    560, Loss function: 6.201, Average Loss: 3.443, avg. samples / sec: 24451.71
Iteration:    560, Loss function: 5.950, Average Loss: 3.447, avg. samples / sec: 24480.10
Iteration:    560, Loss function: 5.692, Average Loss: 3.439, avg. samples / sec: 24437.29
Iteration:    560, Loss function: 5.423, Average Loss: 3.442, avg. samples / sec: 24422.36
Iteration:    560, Loss function: 6.302, Average Loss: 3.444, avg. samples / sec: 24419.94
Iteration:    560, Loss function: 6.491, Average Loss: 3.440, avg. samples / sec: 24419.67
Iteration:    560, Loss function: 5.346, Average Loss: 3.438, avg. samples / sec: 24435.34

:::MLPv0.5.0 ssd 1541757062.969433308 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 561, "value": 0.09973333333333334}

:::MLPv0.5.0 ssd 1541757063.053599834 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 562, "value": 0.09991111111111112}

:::MLPv0.5.0 ssd 1541757063.137925386 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 563, "value": 0.1000888888888889}

:::MLPv0.5.0 ssd 1541757063.221471786 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 564, "value": 0.10026666666666667}

:::MLPv0.5.0 ssd 1541757063.305600882 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 565, "value": 0.10044444444444445}

:::MLPv0.5.0 ssd 1541757063.389251947 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 566, "value": 0.10062222222222222}

:::MLPv0.5.0 ssd 1541757063.473104715 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 567, "value": 0.1008}

:::MLPv0.5.0 ssd 1541757063.557268620 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 568, "value": 0.10097777777777778}

:::MLPv0.5.0 ssd 1541757063.641621351 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 569, "value": 0.10115555555555555}

:::MLPv0.5.0 ssd 1541757063.725175142 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 570, "value": 0.10133333333333333}

:::MLPv0.5.0 ssd 1541757063.809504747 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 571, "value": 0.10151111111111111}

:::MLPv0.5.0 ssd 1541757063.892902851 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 572, "value": 0.10168888888888888}

:::MLPv0.5.0 ssd 1541757063.976258993 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 573, "value": 0.10186666666666666}

:::MLPv0.5.0 ssd 1541757064.059848309 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 574, "value": 0.10204444444444444}

:::MLPv0.5.0 ssd 1541757064.144446373 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 575, "value": 0.10222222222222221}

:::MLPv0.5.0 ssd 1541757064.227880001 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 576, "value": 0.10239999999999999}

:::MLPv0.5.0 ssd 1541757064.312330008 (train.py:349) opt_learning_rate: {"epoch": 9, "iteration": 577, "value": 0.10257777777777778}

:::MLPv0.5.0 ssd 1541757064.394008160 (train.py:553) train_epoch: 10

:::MLPv0.5.0 ssd 1541757064.398217678 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 578, "value": 0.10275555555555556}

:::MLPv0.5.0 ssd 1541757064.482582092 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 579, "value": 0.10293333333333334}

:::MLPv0.5.0 ssd 1541757064.566932917 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 580, "value": 0.10311111111111111}
Iteration:    580, Loss function: 5.836, Average Loss: 3.485, avg. samples / sec: 24364.83
Iteration:    580, Loss function: 5.630, Average Loss: 3.488, avg. samples / sec: 24356.99
Iteration:    580, Loss function: 5.416, Average Loss: 3.481, avg. samples / sec: 24408.10
Iteration:    580, Loss function: 5.637, Average Loss: 3.485, avg. samples / sec: 24371.54
Iteration:    580, Loss function: 5.476, Average Loss: 3.488, avg. samples / sec: 24374.64
Iteration:    580, Loss function: 5.630, Average Loss: 3.489, avg. samples / sec: 24316.85
Iteration:    580, Loss function: 5.582, Average Loss: 3.485, avg. samples / sec: 24331.04
Iteration:    580, Loss function: 5.880, Average Loss: 3.482, avg. samples / sec: 24352.56

:::MLPv0.5.0 ssd 1541757064.651065588 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 581, "value": 0.10328888888888889}

:::MLPv0.5.0 ssd 1541757064.734723091 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 582, "value": 0.10346666666666667}

:::MLPv0.5.0 ssd 1541757064.818572283 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 583, "value": 0.10364444444444444}

:::MLPv0.5.0 ssd 1541757064.902282715 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 584, "value": 0.10382222222222223}

:::MLPv0.5.0 ssd 1541757064.986453056 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 585, "value": 0.10400000000000001}

:::MLPv0.5.0 ssd 1541757065.070497274 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 586, "value": 0.10417777777777779}

:::MLPv0.5.0 ssd 1541757065.154767036 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 587, "value": 0.10435555555555556}

:::MLPv0.5.0 ssd 1541757065.238303423 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 588, "value": 0.10453333333333334}

:::MLPv0.5.0 ssd 1541757065.322819471 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 589, "value": 0.10471111111111112}

:::MLPv0.5.0 ssd 1541757065.406376123 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 590, "value": 0.10488888888888889}

:::MLPv0.5.0 ssd 1541757065.490178823 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 591, "value": 0.10506666666666667}

:::MLPv0.5.0 ssd 1541757065.573094130 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 592, "value": 0.10524444444444445}

:::MLPv0.5.0 ssd 1541757065.657024860 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 593, "value": 0.10542222222222222}

:::MLPv0.5.0 ssd 1541757065.740880013 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 594, "value": 0.1056}

:::MLPv0.5.0 ssd 1541757065.825507641 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 595, "value": 0.10577777777777778}

:::MLPv0.5.0 ssd 1541757065.909270048 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 596, "value": 0.10595555555555555}

:::MLPv0.5.0 ssd 1541757065.993092060 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 597, "value": 0.10613333333333333}

:::MLPv0.5.0 ssd 1541757066.076819181 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 598, "value": 0.1063111111111111}

:::MLPv0.5.0 ssd 1541757066.160811424 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 599, "value": 0.10648888888888888}

:::MLPv0.5.0 ssd 1541757066.244776964 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 600, "value": 0.10666666666666666}
Iteration:    600, Loss function: 5.302, Average Loss: 3.524, avg. samples / sec: 24474.43
Iteration:    600, Loss function: 5.596, Average Loss: 3.529, avg. samples / sec: 24436.45
Iteration:    600, Loss function: 5.524, Average Loss: 3.527, avg. samples / sec: 24408.93
Iteration:    600, Loss function: 5.441, Average Loss: 3.525, avg. samples / sec: 24417.60
Iteration:    600, Loss function: 5.419, Average Loss: 3.524, avg. samples / sec: 24408.03
Iteration:    600, Loss function: 5.034, Average Loss: 3.522, avg. samples / sec: 24469.53
Iteration:    600, Loss function: 5.579, Average Loss: 3.523, avg. samples / sec: 24377.25
Iteration:    600, Loss function: 5.232, Average Loss: 3.529, avg. samples / sec: 24419.33

:::MLPv0.5.0 ssd 1541757066.328773022 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 601, "value": 0.10684444444444444}

:::MLPv0.5.0 ssd 1541757066.412650347 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 602, "value": 0.10702222222222221}

:::MLPv0.5.0 ssd 1541757066.496568680 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 603, "value": 0.1072}

:::MLPv0.5.0 ssd 1541757066.579884529 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 604, "value": 0.10737777777777778}

:::MLPv0.5.0 ssd 1541757066.664412737 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 605, "value": 0.10755555555555556}

:::MLPv0.5.0 ssd 1541757066.748029947 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 606, "value": 0.10773333333333333}

:::MLPv0.5.0 ssd 1541757066.831910372 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 607, "value": 0.10791111111111111}

:::MLPv0.5.0 ssd 1541757066.915917873 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 608, "value": 0.10808888888888889}

:::MLPv0.5.0 ssd 1541757066.999649525 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 609, "value": 0.10826666666666668}

:::MLPv0.5.0 ssd 1541757067.083942652 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 610, "value": 0.10844444444444445}

:::MLPv0.5.0 ssd 1541757067.168051720 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 611, "value": 0.10862222222222223}

:::MLPv0.5.0 ssd 1541757067.251711130 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 612, "value": 0.10880000000000001}

:::MLPv0.5.0 ssd 1541757067.335477591 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 613, "value": 0.10897777777777778}

:::MLPv0.5.0 ssd 1541757067.419423342 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 614, "value": 0.10915555555555556}

:::MLPv0.5.0 ssd 1541757067.503442287 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 615, "value": 0.10933333333333334}

:::MLPv0.5.0 ssd 1541757067.587142467 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 616, "value": 0.10951111111111111}

:::MLPv0.5.0 ssd 1541757067.670386076 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 617, "value": 0.10968888888888889}

:::MLPv0.5.0 ssd 1541757067.754416227 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 618, "value": 0.10986666666666667}

:::MLPv0.5.0 ssd 1541757067.838095188 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 619, "value": 0.11004444444444444}

:::MLPv0.5.0 ssd 1541757067.922483921 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 620, "value": 0.11022222222222222}
Iteration:    620, Loss function: 6.561, Average Loss: 3.573, avg. samples / sec: 24429.44
Iteration:    620, Loss function: 6.394, Average Loss: 3.567, avg. samples / sec: 24412.19
Iteration:    620, Loss function: 6.328, Average Loss: 3.574, avg. samples / sec: 24457.76
Iteration:    620, Loss function: 6.138, Average Loss: 3.574, avg. samples / sec: 24407.51
Iteration:    620, Loss function: 6.394, Average Loss: 3.572, avg. samples / sec: 24390.04
Iteration:    620, Loss function: 5.994, Average Loss: 3.566, avg. samples / sec: 24416.83
Iteration:    620, Loss function: 6.079, Average Loss: 3.566, avg. samples / sec: 24380.92
Iteration:    620, Loss function: 6.401, Average Loss: 3.571, avg. samples / sec: 24373.54

:::MLPv0.5.0 ssd 1541757068.006108999 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 621, "value": 0.1104}

:::MLPv0.5.0 ssd 1541757068.090099096 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 622, "value": 0.11057777777777777}

:::MLPv0.5.0 ssd 1541757068.174277782 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 623, "value": 0.11075555555555555}

:::MLPv0.5.0 ssd 1541757068.258634329 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 624, "value": 0.11093333333333333}

:::MLPv0.5.0 ssd 1541757068.342264652 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 625, "value": 0.1111111111111111}

:::MLPv0.5.0 ssd 1541757068.426388741 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 626, "value": 0.11128888888888888}

:::MLPv0.5.0 ssd 1541757068.509902954 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 627, "value": 0.11146666666666666}

:::MLPv0.5.0 ssd 1541757068.594648123 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 628, "value": 0.11164444444444445}

:::MLPv0.5.0 ssd 1541757068.679981232 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 629, "value": 0.11182222222222223}

:::MLPv0.5.0 ssd 1541757068.763630390 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 630, "value": 0.112}

:::MLPv0.5.0 ssd 1541757068.847370386 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 631, "value": 0.11217777777777778}

:::MLPv0.5.0 ssd 1541757068.931050539 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 632, "value": 0.11235555555555556}

:::MLPv0.5.0 ssd 1541757069.015055895 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 633, "value": 0.11253333333333333}

:::MLPv0.5.0 ssd 1541757069.098793983 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 634, "value": 0.11271111111111111}

:::MLPv0.5.0 ssd 1541757069.182693005 (train.py:349) opt_learning_rate: {"epoch": 10, "iteration": 635, "value": 0.1128888888888889}

:::MLPv0.5.0 ssd 1541757069.263863087 (train.py:553) train_epoch: 11

:::MLPv0.5.0 ssd 1541757069.268029928 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 636, "value": 0.11306666666666668}

:::MLPv0.5.0 ssd 1541757069.352200508 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 637, "value": 0.11324444444444445}

:::MLPv0.5.0 ssd 1541757069.435951710 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 638, "value": 0.11342222222222223}

:::MLPv0.5.0 ssd 1541757069.520308733 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 639, "value": 0.1136}

:::MLPv0.5.0 ssd 1541757069.604977846 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 640, "value": 0.11377777777777778}
Iteration:    640, Loss function: 5.243, Average Loss: 3.615, avg. samples / sec: 24353.00
Iteration:    640, Loss function: 5.891, Average Loss: 3.609, avg. samples / sec: 24369.88
Iteration:    640, Loss function: 5.281, Average Loss: 3.607, avg. samples / sec: 24362.91
Iteration:    640, Loss function: 4.741, Average Loss: 3.608, avg. samples / sec: 24320.01
Iteration:    640, Loss function: 5.875, Average Loss: 3.615, avg. samples / sec: 24307.96
Iteration:    640, Loss function: 5.285, Average Loss: 3.613, avg. samples / sec: 24340.64
Iteration:    640, Loss function: 5.248, Average Loss: 3.611, avg. samples / sec: 24350.91
Iteration:    640, Loss function: 5.188, Average Loss: 3.614, avg. samples / sec: 24306.46

:::MLPv0.5.0 ssd 1541757069.688895226 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 641, "value": 0.11395555555555556}

:::MLPv0.5.0 ssd 1541757069.773838520 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 642, "value": 0.11413333333333334}

:::MLPv0.5.0 ssd 1541757069.857911825 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 643, "value": 0.11431111111111111}

:::MLPv0.5.0 ssd 1541757069.942194223 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 644, "value": 0.11448888888888889}

:::MLPv0.5.0 ssd 1541757070.026580334 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 645, "value": 0.11466666666666667}

:::MLPv0.5.0 ssd 1541757070.110650063 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 646, "value": 0.11484444444444444}

:::MLPv0.5.0 ssd 1541757070.194131136 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 647, "value": 0.11502222222222222}

:::MLPv0.5.0 ssd 1541757070.277269840 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 648, "value": 0.1152}

:::MLPv0.5.0 ssd 1541757070.361391783 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 649, "value": 0.11537777777777777}

:::MLPv0.5.0 ssd 1541757070.445709229 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 650, "value": 0.11555555555555555}

:::MLPv0.5.0 ssd 1541757070.530741215 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 651, "value": 0.11573333333333333}

:::MLPv0.5.0 ssd 1541757070.614637613 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 652, "value": 0.1159111111111111}

:::MLPv0.5.0 ssd 1541757070.698470831 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 653, "value": 0.11608888888888888}

:::MLPv0.5.0 ssd 1541757070.782478094 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 654, "value": 0.11626666666666667}

:::MLPv0.5.0 ssd 1541757070.866330862 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 655, "value": 0.11644444444444445}

:::MLPv0.5.0 ssd 1541757070.949988842 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 656, "value": 0.11662222222222222}

:::MLPv0.5.0 ssd 1541757071.033803463 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 657, "value": 0.1168}

:::MLPv0.5.0 ssd 1541757071.117742777 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 658, "value": 0.11697777777777778}

:::MLPv0.5.0 ssd 1541757071.201848507 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 659, "value": 0.11715555555555555}

:::MLPv0.5.0 ssd 1541757071.285473347 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 660, "value": 0.11733333333333333}
Iteration:    660, Loss function: 5.351, Average Loss: 3.649, avg. samples / sec: 24390.85
Iteration:    660, Loss function: 5.251, Average Loss: 3.642, avg. samples / sec: 24378.27
Iteration:    660, Loss function: 5.356, Average Loss: 3.648, avg. samples / sec: 24388.65
Iteration:    660, Loss function: 5.942, Average Loss: 3.648, avg. samples / sec: 24370.25
Iteration:    660, Loss function: 5.875, Average Loss: 3.645, avg. samples / sec: 24356.20
Iteration:    660, Loss function: 5.509, Average Loss: 3.651, avg. samples / sec: 24316.63
Iteration:    660, Loss function: 5.418, Average Loss: 3.642, avg. samples / sec: 24338.25
Iteration:    660, Loss function: 5.463, Average Loss: 3.645, avg. samples / sec: 24356.00

:::MLPv0.5.0 ssd 1541757071.369866610 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 661, "value": 0.11751111111111112}

:::MLPv0.5.0 ssd 1541757071.454054117 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 662, "value": 0.1176888888888889}

:::MLPv0.5.0 ssd 1541757071.537464619 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 663, "value": 0.11786666666666668}

:::MLPv0.5.0 ssd 1541757071.621989012 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 664, "value": 0.11804444444444445}

:::MLPv0.5.0 ssd 1541757071.705955744 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 665, "value": 0.11822222222222223}

:::MLPv0.5.0 ssd 1541757071.789740086 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 666, "value": 0.1184}

:::MLPv0.5.0 ssd 1541757071.873184204 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 667, "value": 0.11857777777777778}

:::MLPv0.5.0 ssd 1541757071.956780195 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 668, "value": 0.11875555555555556}

:::MLPv0.5.0 ssd 1541757072.040840864 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 669, "value": 0.11893333333333334}

:::MLPv0.5.0 ssd 1541757072.125012159 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 670, "value": 0.11911111111111111}

:::MLPv0.5.0 ssd 1541757072.208626270 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 671, "value": 0.11928888888888889}

:::MLPv0.5.0 ssd 1541757072.292645454 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 672, "value": 0.11946666666666667}

:::MLPv0.5.0 ssd 1541757072.376390696 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 673, "value": 0.11964444444444444}

:::MLPv0.5.0 ssd 1541757072.460505962 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 674, "value": 0.11982222222222222}

:::MLPv0.5.0 ssd 1541757072.543931961 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 675, "value": 0.12}

:::MLPv0.5.0 ssd 1541757072.627822638 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 676, "value": 0.12017777777777777}

:::MLPv0.5.0 ssd 1541757072.711561203 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 677, "value": 0.12035555555555555}

:::MLPv0.5.0 ssd 1541757072.795730114 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 678, "value": 0.12053333333333333}

:::MLPv0.5.0 ssd 1541757072.879611731 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 679, "value": 0.1207111111111111}

:::MLPv0.5.0 ssd 1541757072.963658810 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 680, "value": 0.12088888888888889}
Iteration:    680, Loss function: 4.967, Average Loss: 3.685, avg. samples / sec: 24441.69
Iteration:    680, Loss function: 5.169, Average Loss: 3.682, avg. samples / sec: 24468.08
Iteration:    680, Loss function: 5.831, Average Loss: 3.687, avg. samples / sec: 24440.16
Iteration:    680, Loss function: 5.103, Average Loss: 3.680, avg. samples / sec: 24398.15
Iteration:    680, Loss function: 5.741, Average Loss: 3.689, avg. samples / sec: 24374.88
Iteration:    680, Loss function: 5.635, Average Loss: 3.682, avg. samples / sec: 24415.26
Iteration:    680, Loss function: 5.285, Average Loss: 3.690, avg. samples / sec: 24408.97
Iteration:    680, Loss function: 5.568, Average Loss: 3.683, avg. samples / sec: 24379.89

:::MLPv0.5.0 ssd 1541757073.047886372 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 681, "value": 0.12106666666666667}

:::MLPv0.5.0 ssd 1541757073.132146597 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 682, "value": 0.12124444444444445}

:::MLPv0.5.0 ssd 1541757073.216023445 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 683, "value": 0.12142222222222222}

:::MLPv0.5.0 ssd 1541757073.300025463 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 684, "value": 0.1216}

:::MLPv0.5.0 ssd 1541757073.383922100 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 685, "value": 0.12177777777777778}

:::MLPv0.5.0 ssd 1541757073.467648983 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 686, "value": 0.12195555555555557}

:::MLPv0.5.0 ssd 1541757073.551279068 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 687, "value": 0.12213333333333334}

:::MLPv0.5.0 ssd 1541757073.635411501 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 688, "value": 0.12231111111111112}

:::MLPv0.5.0 ssd 1541757073.719886780 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 689, "value": 0.1224888888888889}

:::MLPv0.5.0 ssd 1541757073.803447962 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 690, "value": 0.12266666666666667}

:::MLPv0.5.0 ssd 1541757073.887726784 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 691, "value": 0.12284444444444445}

:::MLPv0.5.0 ssd 1541757073.971489429 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 692, "value": 0.12302222222222223}

:::MLPv0.5.0 ssd 1541757074.055290222 (train.py:349) opt_learning_rate: {"epoch": 11, "iteration": 693, "value": 0.1232}

:::MLPv0.5.0 ssd 1541757074.137137413 (train.py:553) train_epoch: 12

:::MLPv0.5.0 ssd 1541757074.141342640 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 694, "value": 0.12337777777777778}

:::MLPv0.5.0 ssd 1541757074.225234747 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 695, "value": 0.12355555555555556}

:::MLPv0.5.0 ssd 1541757074.309638262 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 696, "value": 0.12373333333333333}

:::MLPv0.5.0 ssd 1541757074.393601418 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 697, "value": 0.12391111111111111}

:::MLPv0.5.0 ssd 1541757074.477182150 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 698, "value": 0.12408888888888889}

:::MLPv0.5.0 ssd 1541757074.561228991 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 699, "value": 0.12426666666666666}

:::MLPv0.5.0 ssd 1541757074.644961596 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 700, "value": 0.12444444444444444}
Iteration:    700, Loss function: 4.768, Average Loss: 3.719, avg. samples / sec: 24369.21
Iteration:    700, Loss function: 4.480, Average Loss: 3.712, avg. samples / sec: 24392.41
Iteration:    700, Loss function: 5.686, Average Loss: 3.713, avg. samples / sec: 24359.19
Iteration:    700, Loss function: 5.406, Average Loss: 3.722, avg. samples / sec: 24401.82
Iteration:    700, Loss function: 5.222, Average Loss: 3.715, avg. samples / sec: 24335.68
Iteration:    700, Loss function: 4.963, Average Loss: 3.719, avg. samples / sec: 24376.74
Iteration:    700, Loss function: 5.403, Average Loss: 3.714, avg. samples / sec: 24377.85
Iteration:    700, Loss function: 5.499, Average Loss: 3.712, avg. samples / sec: 24358.62

:::MLPv0.5.0 ssd 1541757074.728700876 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 701, "value": 0.12462222222222222}

:::MLPv0.5.0 ssd 1541757074.812654734 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 702, "value": 0.1248}

:::MLPv0.5.0 ssd 1541757074.897017956 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 703, "value": 0.12497777777777777}

:::MLPv0.5.0 ssd 1541757074.981643915 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 704, "value": 0.12515555555555555}

:::MLPv0.5.0 ssd 1541757075.066152334 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 705, "value": 0.12533333333333335}

:::MLPv0.5.0 ssd 1541757075.150659800 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 706, "value": 0.12551111111111113}

:::MLPv0.5.0 ssd 1541757075.234418392 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 707, "value": 0.1256888888888889}

:::MLPv0.5.0 ssd 1541757075.318183422 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 708, "value": 0.12586666666666668}

:::MLPv0.5.0 ssd 1541757075.402102709 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 709, "value": 0.12604444444444446}

:::MLPv0.5.0 ssd 1541757075.485987186 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 710, "value": 0.12622222222222224}

:::MLPv0.5.0 ssd 1541757075.570931673 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 711, "value": 0.1264}

:::MLPv0.5.0 ssd 1541757075.655203104 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 712, "value": 0.1265777777777778}

:::MLPv0.5.0 ssd 1541757075.739552021 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 713, "value": 0.12675555555555557}

:::MLPv0.5.0 ssd 1541757075.823838472 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 714, "value": 0.12693333333333334}

:::MLPv0.5.0 ssd 1541757075.907310247 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 715, "value": 0.12711111111111112}

:::MLPv0.5.0 ssd 1541757075.991745234 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 716, "value": 0.1272888888888889}

:::MLPv0.5.0 ssd 1541757076.075955391 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 717, "value": 0.12746666666666667}

:::MLPv0.5.0 ssd 1541757076.160097599 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 718, "value": 0.12764444444444445}

:::MLPv0.5.0 ssd 1541757076.244657993 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 719, "value": 0.12782222222222223}

:::MLPv0.5.0 ssd 1541757076.328414917 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 720, "value": 0.128}
Iteration:    720, Loss function: 6.993, Average Loss: 3.763, avg. samples / sec: 24335.16
Iteration:    720, Loss function: 6.820, Average Loss: 3.760, avg. samples / sec: 24382.48
Iteration:    720, Loss function: 6.966, Average Loss: 3.770, avg. samples / sec: 24327.42
Iteration:    720, Loss function: 6.669, Average Loss: 3.768, avg. samples / sec: 24295.03
Iteration:    720, Loss function: 7.168, Average Loss: 3.764, avg. samples / sec: 24340.05
Iteration:    720, Loss function: 7.490, Average Loss: 3.773, avg. samples / sec: 24298.56
Iteration:    720, Loss function: 7.042, Average Loss: 3.763, avg. samples / sec: 24283.33
Iteration:    720, Loss function: 7.191, Average Loss: 3.764, avg. samples / sec: 24306.38

:::MLPv0.5.0 ssd 1541757076.412248135 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 721, "value": 0.12817777777777778}

:::MLPv0.5.0 ssd 1541757076.495831728 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 722, "value": 0.12835555555555556}

:::MLPv0.5.0 ssd 1541757076.580329180 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 723, "value": 0.12853333333333333}

:::MLPv0.5.0 ssd 1541757076.664210081 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 724, "value": 0.1287111111111111}

:::MLPv0.5.0 ssd 1541757076.747850657 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 725, "value": 0.1288888888888889}

:::MLPv0.5.0 ssd 1541757076.831857681 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 726, "value": 0.12906666666666666}

:::MLPv0.5.0 ssd 1541757076.916061878 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 727, "value": 0.12924444444444444}

:::MLPv0.5.0 ssd 1541757076.999529839 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 728, "value": 0.12942222222222222}

:::MLPv0.5.0 ssd 1541757077.082855701 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 729, "value": 0.1296}

:::MLPv0.5.0 ssd 1541757077.166574478 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 730, "value": 0.12977777777777777}

:::MLPv0.5.0 ssd 1541757077.250831127 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 731, "value": 0.12995555555555555}

:::MLPv0.5.0 ssd 1541757077.334823608 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 732, "value": 0.13013333333333332}

:::MLPv0.5.0 ssd 1541757077.418889761 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 733, "value": 0.1303111111111111}

:::MLPv0.5.0 ssd 1541757077.502822399 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 734, "value": 0.13048888888888888}

:::MLPv0.5.0 ssd 1541757077.586886883 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 735, "value": 0.13066666666666665}

:::MLPv0.5.0 ssd 1541757077.671073437 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 736, "value": 0.13084444444444446}

:::MLPv0.5.0 ssd 1541757077.754824400 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 737, "value": 0.13102222222222223}

:::MLPv0.5.0 ssd 1541757077.838585377 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 738, "value": 0.1312}

:::MLPv0.5.0 ssd 1541757077.922590971 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 739, "value": 0.1313777777777778}

:::MLPv0.5.0 ssd 1541757078.006569624 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 740, "value": 0.13155555555555556}
Iteration:    740, Loss function: 5.747, Average Loss: 3.815, avg. samples / sec: 24433.56
Iteration:    740, Loss function: 5.873, Average Loss: 3.812, avg. samples / sec: 24416.64
Iteration:    740, Loss function: 5.788, Average Loss: 3.811, avg. samples / sec: 24374.89
Iteration:    740, Loss function: 5.201, Average Loss: 3.809, avg. samples / sec: 24411.23
Iteration:    740, Loss function: 5.432, Average Loss: 3.820, avg. samples / sec: 24413.20
Iteration:    740, Loss function: 5.591, Average Loss: 3.806, avg. samples / sec: 24369.00
Iteration:    740, Loss function: 5.341, Average Loss: 3.811, avg. samples / sec: 24411.17
Iteration:    740, Loss function: 5.463, Average Loss: 3.809, avg. samples / sec: 24396.96

:::MLPv0.5.0 ssd 1541757078.091162920 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 741, "value": 0.13173333333333334}

:::MLPv0.5.0 ssd 1541757078.174986839 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 742, "value": 0.13191111111111112}

:::MLPv0.5.0 ssd 1541757078.259352207 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 743, "value": 0.1320888888888889}

:::MLPv0.5.0 ssd 1541757078.343211412 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 744, "value": 0.13226666666666667}

:::MLPv0.5.0 ssd 1541757078.426486731 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 745, "value": 0.13244444444444445}

:::MLPv0.5.0 ssd 1541757078.510605097 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 746, "value": 0.13262222222222222}

:::MLPv0.5.0 ssd 1541757078.594557762 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 747, "value": 0.1328}

:::MLPv0.5.0 ssd 1541757078.678994417 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 748, "value": 0.13297777777777778}

:::MLPv0.5.0 ssd 1541757078.762955427 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 749, "value": 0.13315555555555555}

:::MLPv0.5.0 ssd 1541757078.847123146 (train.py:349) opt_learning_rate: {"epoch": 12, "iteration": 750, "value": 0.13333333333333333}

:::MLPv0.5.0 ssd 1541757078.928465366 (train.py:553) train_epoch: 13

:::MLPv0.5.0 ssd 1541757078.932822466 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 751, "value": 0.1335111111111111}

:::MLPv0.5.0 ssd 1541757079.017301798 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 752, "value": 0.13368888888888888}

:::MLPv0.5.0 ssd 1541757079.102200985 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 753, "value": 0.13386666666666666}

:::MLPv0.5.0 ssd 1541757079.186580896 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 754, "value": 0.13404444444444444}

:::MLPv0.5.0 ssd 1541757079.271134138 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 755, "value": 0.13422222222222221}

:::MLPv0.5.0 ssd 1541757079.356228590 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 756, "value": 0.1344}

:::MLPv0.5.0 ssd 1541757079.442412138 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 757, "value": 0.13457777777777777}

:::MLPv0.5.0 ssd 1541757079.526588917 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 758, "value": 0.13475555555555557}

:::MLPv0.5.0 ssd 1541757079.610660315 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 759, "value": 0.13493333333333335}

:::MLPv0.5.0 ssd 1541757079.694530249 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 760, "value": 0.13511111111111113}
Iteration:    760, Loss function: 4.695, Average Loss: 3.840, avg. samples / sec: 24303.54
Iteration:    760, Loss function: 5.585, Average Loss: 3.844, avg. samples / sec: 24266.68
Iteration:    760, Loss function: 4.846, Average Loss: 3.850, avg. samples / sec: 24288.63
Iteration:    760, Loss function: 5.169, Average Loss: 3.833, avg. samples / sec: 24277.60
Iteration:    760, Loss function: 5.470, Average Loss: 3.840, avg. samples / sec: 24286.19
Iteration:    760, Loss function: 5.324, Average Loss: 3.843, avg. samples / sec: 24244.50
Iteration:    760, Loss function: 4.961, Average Loss: 3.840, avg. samples / sec: 24257.40
Iteration:    760, Loss function: 5.332, Average Loss: 3.841, avg. samples / sec: 24233.46

:::MLPv0.5.0 ssd 1541757079.779407024 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 761, "value": 0.1352888888888889}

:::MLPv0.5.0 ssd 1541757079.863954782 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 762, "value": 0.13546666666666668}

:::MLPv0.5.0 ssd 1541757079.948597431 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 763, "value": 0.13564444444444446}

:::MLPv0.5.0 ssd 1541757080.033006430 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 764, "value": 0.13582222222222223}

:::MLPv0.5.0 ssd 1541757080.117034912 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 765, "value": 0.136}

:::MLPv0.5.0 ssd 1541757080.201264620 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 766, "value": 0.1361777777777778}

:::MLPv0.5.0 ssd 1541757080.286330223 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 767, "value": 0.13635555555555556}

:::MLPv0.5.0 ssd 1541757080.369961023 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 768, "value": 0.13653333333333334}

:::MLPv0.5.0 ssd 1541757080.454386950 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 769, "value": 0.13671111111111112}

:::MLPv0.5.0 ssd 1541757080.538546801 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 770, "value": 0.1368888888888889}

:::MLPv0.5.0 ssd 1541757080.623136282 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 771, "value": 0.13706666666666667}

:::MLPv0.5.0 ssd 1541757080.707564116 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 772, "value": 0.13724444444444445}

:::MLPv0.5.0 ssd 1541757080.792371273 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 773, "value": 0.13742222222222222}

:::MLPv0.5.0 ssd 1541757080.876598120 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 774, "value": 0.1376}

:::MLPv0.5.0 ssd 1541757080.961344481 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 775, "value": 0.13777777777777778}

:::MLPv0.5.0 ssd 1541757081.045499802 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 776, "value": 0.13795555555555555}

:::MLPv0.5.0 ssd 1541757081.130672455 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 777, "value": 0.13813333333333333}

:::MLPv0.5.0 ssd 1541757081.214478254 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 778, "value": 0.1383111111111111}

:::MLPv0.5.0 ssd 1541757081.298487902 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 779, "value": 0.13848888888888888}

:::MLPv0.5.0 ssd 1541757081.382891893 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 780, "value": 0.13866666666666666}
Iteration:    780, Loss function: 5.180, Average Loss: 3.857, avg. samples / sec: 24290.23
Iteration:    780, Loss function: 5.128, Average Loss: 3.867, avg. samples / sec: 24258.06
Iteration:    780, Loss function: 5.187, Average Loss: 3.868, avg. samples / sec: 24266.86
Iteration:    780, Loss function: 5.203, Average Loss: 3.877, avg. samples / sec: 24248.18
Iteration:    780, Loss function: 5.085, Average Loss: 3.866, avg. samples / sec: 24283.12
Iteration:    780, Loss function: 5.058, Average Loss: 3.870, avg. samples / sec: 24236.10
Iteration:    780, Loss function: 4.782, Average Loss: 3.866, avg. samples / sec: 24288.92
Iteration:    780, Loss function: 5.921, Average Loss: 3.869, avg. samples / sec: 24249.40

:::MLPv0.5.0 ssd 1541757081.467303991 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 781, "value": 0.13884444444444444}

:::MLPv0.5.0 ssd 1541757081.551462173 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 782, "value": 0.1390222222222222}

:::MLPv0.5.0 ssd 1541757081.635518551 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 783, "value": 0.1392}

:::MLPv0.5.0 ssd 1541757081.720036030 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 784, "value": 0.13937777777777777}

:::MLPv0.5.0 ssd 1541757081.804269075 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 785, "value": 0.13955555555555554}

:::MLPv0.5.0 ssd 1541757081.888686419 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 786, "value": 0.13973333333333332}

:::MLPv0.5.0 ssd 1541757081.973516464 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 787, "value": 0.13991111111111112}

:::MLPv0.5.0 ssd 1541757082.057668924 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 788, "value": 0.1400888888888889}

:::MLPv0.5.0 ssd 1541757082.142637730 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 789, "value": 0.14026666666666668}

:::MLPv0.5.0 ssd 1541757082.226773500 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 790, "value": 0.14044444444444446}

:::MLPv0.5.0 ssd 1541757082.310989618 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 791, "value": 0.14062222222222223}

:::MLPv0.5.0 ssd 1541757082.394991875 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 792, "value": 0.1408}

:::MLPv0.5.0 ssd 1541757082.480657101 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 793, "value": 0.14097777777777779}

:::MLPv0.5.0 ssd 1541757082.564843893 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 794, "value": 0.14115555555555556}

:::MLPv0.5.0 ssd 1541757082.649516344 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 795, "value": 0.14133333333333334}

:::MLPv0.5.0 ssd 1541757082.733074903 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 796, "value": 0.14151111111111112}

:::MLPv0.5.0 ssd 1541757082.817235231 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 797, "value": 0.1416888888888889}

:::MLPv0.5.0 ssd 1541757082.901941061 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 798, "value": 0.14186666666666667}

:::MLPv0.5.0 ssd 1541757082.986682415 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 799, "value": 0.14204444444444445}

:::MLPv0.5.0 ssd 1541757083.071104765 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 800, "value": 0.14222222222222222}
Iteration:    800, Loss function: 5.043, Average Loss: 3.894, avg. samples / sec: 24274.14
Iteration:    800, Loss function: 5.027, Average Loss: 3.902, avg. samples / sec: 24279.61
Iteration:    800, Loss function: 5.384, Average Loss: 3.894, avg. samples / sec: 24275.14
Iteration:    800, Loss function: 5.069, Average Loss: 3.894, avg. samples / sec: 24298.12
Iteration:    800, Loss function: 5.338, Average Loss: 3.895, avg. samples / sec: 24266.10
Iteration:    800, Loss function: 5.105, Average Loss: 3.892, avg. samples / sec: 24261.79
Iteration:    800, Loss function: 5.430, Average Loss: 3.884, avg. samples / sec: 24210.05
Iteration:    800, Loss function: 5.263, Average Loss: 3.891, avg. samples / sec: 24243.58

:::MLPv0.5.0 ssd 1541757083.155402184 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 801, "value": 0.1424}

:::MLPv0.5.0 ssd 1541757083.240179300 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 802, "value": 0.14257777777777778}

:::MLPv0.5.0 ssd 1541757083.324279308 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 803, "value": 0.14275555555555555}

:::MLPv0.5.0 ssd 1541757083.408570528 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 804, "value": 0.14293333333333333}

:::MLPv0.5.0 ssd 1541757083.492556095 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 805, "value": 0.1431111111111111}

:::MLPv0.5.0 ssd 1541757083.577418327 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 806, "value": 0.14328888888888888}

:::MLPv0.5.0 ssd 1541757083.661301136 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 807, "value": 0.14346666666666666}

:::MLPv0.5.0 ssd 1541757083.745230198 (train.py:349) opt_learning_rate: {"epoch": 13, "iteration": 808, "value": 0.14364444444444444}

:::MLPv0.5.0 ssd 1541757083.826336145 (train.py:553) train_epoch: 14

:::MLPv0.5.0 ssd 1541757083.830665112 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 809, "value": 0.14382222222222224}

:::MLPv0.5.0 ssd 1541757083.914319754 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 810, "value": 0.14400000000000002}

:::MLPv0.5.0 ssd 1541757083.998906374 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 811, "value": 0.1441777777777778}

:::MLPv0.5.0 ssd 1541757084.082614183 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 812, "value": 0.14435555555555557}

:::MLPv0.5.0 ssd 1541757084.167101860 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 813, "value": 0.14453333333333335}

:::MLPv0.5.0 ssd 1541757084.252097845 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 814, "value": 0.14471111111111112}

:::MLPv0.5.0 ssd 1541757084.335995674 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 815, "value": 0.1448888888888889}

:::MLPv0.5.0 ssd 1541757084.420308113 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 816, "value": 0.14506666666666668}

:::MLPv0.5.0 ssd 1541757084.504692316 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 817, "value": 0.14524444444444445}

:::MLPv0.5.0 ssd 1541757084.588918447 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 818, "value": 0.14542222222222223}

:::MLPv0.5.0 ssd 1541757084.673615217 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 819, "value": 0.1456}

:::MLPv0.5.0 ssd 1541757084.757529974 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 820, "value": 0.14577777777777778}
Iteration:    820, Loss function: 5.315, Average Loss: 3.919, avg. samples / sec: 24318.85
Iteration:    820, Loss function: 4.933, Average Loss: 3.919, avg. samples / sec: 24281.01
Iteration:    820, Loss function: 4.868, Average Loss: 3.917, avg. samples / sec: 24297.01
Iteration:    820, Loss function: 5.316, Average Loss: 3.926, avg. samples / sec: 24264.87
Iteration:    820, Loss function: 5.191, Average Loss: 3.915, avg. samples / sec: 24297.00
Iteration:    820, Loss function: 5.034, Average Loss: 3.918, avg. samples / sec: 24257.29
Iteration:    820, Loss function: 5.005, Average Loss: 3.918, avg. samples / sec: 24267.54
Iteration:    820, Loss function: 5.686, Average Loss: 3.907, avg. samples / sec: 24289.52

:::MLPv0.5.0 ssd 1541757084.842947721 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 821, "value": 0.14595555555555556}

:::MLPv0.5.0 ssd 1541757084.926878929 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 822, "value": 0.14613333333333334}

:::MLPv0.5.0 ssd 1541757085.011942625 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 823, "value": 0.14631111111111111}

:::MLPv0.5.0 ssd 1541757085.095681429 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 824, "value": 0.1464888888888889}

:::MLPv0.5.0 ssd 1541757085.179934740 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 825, "value": 0.14666666666666667}

:::MLPv0.5.0 ssd 1541757085.263952494 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 826, "value": 0.14684444444444444}

:::MLPv0.5.0 ssd 1541757085.348198652 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 827, "value": 0.14702222222222222}

:::MLPv0.5.0 ssd 1541757085.433089018 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 828, "value": 0.1472}

:::MLPv0.5.0 ssd 1541757085.517321825 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 829, "value": 0.14737777777777777}

:::MLPv0.5.0 ssd 1541757085.601516485 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 830, "value": 0.14755555555555555}

:::MLPv0.5.0 ssd 1541757085.688253164 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 831, "value": 0.14773333333333333}

:::MLPv0.5.0 ssd 1541757085.772918463 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 832, "value": 0.1479111111111111}

:::MLPv0.5.0 ssd 1541757085.857017279 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 833, "value": 0.14808888888888888}

:::MLPv0.5.0 ssd 1541757085.940425873 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 834, "value": 0.14826666666666666}

:::MLPv0.5.0 ssd 1541757086.024756432 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 835, "value": 0.14844444444444443}

:::MLPv0.5.0 ssd 1541757086.108991861 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 836, "value": 0.1486222222222222}

:::MLPv0.5.0 ssd 1541757086.194240332 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 837, "value": 0.14880000000000002}

:::MLPv0.5.0 ssd 1541757086.278430700 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 838, "value": 0.1489777777777778}

:::MLPv0.5.0 ssd 1541757086.362920046 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 839, "value": 0.14915555555555557}

:::MLPv0.5.0 ssd 1541757086.447553396 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 840, "value": 0.14933333333333335}
Iteration:    840, Loss function: 5.154, Average Loss: 3.942, avg. samples / sec: 24240.00
Iteration:    840, Loss function: 5.867, Average Loss: 3.940, avg. samples / sec: 24288.06
Iteration:    840, Loss function: 4.992, Average Loss: 3.943, avg. samples / sec: 24228.62
Iteration:    840, Loss function: 5.204, Average Loss: 3.942, avg. samples / sec: 24266.75
Iteration:    840, Loss function: 4.910, Average Loss: 3.940, avg. samples / sec: 24247.51
Iteration:    840, Loss function: 5.385, Average Loss: 3.948, avg. samples / sec: 24240.45
Iteration:    840, Loss function: 4.721, Average Loss: 3.942, avg. samples / sec: 24246.34
Iteration:    840, Loss function: 5.406, Average Loss: 3.931, avg. samples / sec: 24233.62

:::MLPv0.5.0 ssd 1541757086.531769276 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 841, "value": 0.14951111111111112}

:::MLPv0.5.0 ssd 1541757086.616117001 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 842, "value": 0.1496888888888889}

:::MLPv0.5.0 ssd 1541757086.700022697 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 843, "value": 0.14986666666666668}

:::MLPv0.5.0 ssd 1541757086.784222603 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 844, "value": 0.15004444444444445}

:::MLPv0.5.0 ssd 1541757086.868865490 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 845, "value": 0.15022222222222223}

:::MLPv0.5.0 ssd 1541757086.952946901 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 846, "value": 0.1504}

:::MLPv0.5.0 ssd 1541757087.037232637 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 847, "value": 0.15057777777777778}

:::MLPv0.5.0 ssd 1541757087.121730089 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 848, "value": 0.15075555555555556}

:::MLPv0.5.0 ssd 1541757087.205852509 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 849, "value": 0.15093333333333334}

:::MLPv0.5.0 ssd 1541757087.289907455 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 850, "value": 0.1511111111111111}

:::MLPv0.5.0 ssd 1541757087.374015570 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 851, "value": 0.1512888888888889}

:::MLPv0.5.0 ssd 1541757087.458582640 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 852, "value": 0.15146666666666667}

:::MLPv0.5.0 ssd 1541757087.543307543 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 853, "value": 0.15164444444444444}

:::MLPv0.5.0 ssd 1541757087.627805471 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 854, "value": 0.15182222222222222}

:::MLPv0.5.0 ssd 1541757087.713152170 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 855, "value": 0.152}

:::MLPv0.5.0 ssd 1541757087.797229290 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 856, "value": 0.15217777777777777}

:::MLPv0.5.0 ssd 1541757087.881429672 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 857, "value": 0.15235555555555555}

:::MLPv0.5.0 ssd 1541757087.966380119 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 858, "value": 0.15253333333333333}

:::MLPv0.5.0 ssd 1541757088.050198317 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 859, "value": 0.1527111111111111}

:::MLPv0.5.0 ssd 1541757088.134042740 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 860, "value": 0.15288888888888888}
Iteration:    860, Loss function: 5.169, Average Loss: 3.962, avg. samples / sec: 24287.79
Iteration:    860, Loss function: 5.272, Average Loss: 3.964, avg. samples / sec: 24327.51
Iteration:    860, Loss function: 4.917, Average Loss: 3.963, avg. samples / sec: 24289.82
Iteration:    860, Loss function: 4.703, Average Loss: 3.963, avg. samples / sec: 24322.64
Iteration:    860, Loss function: 4.946, Average Loss: 3.967, avg. samples / sec: 24276.38
Iteration:    860, Loss function: 5.537, Average Loss: 3.952, avg. samples / sec: 24316.87
Iteration:    860, Loss function: 5.111, Average Loss: 3.968, avg. samples / sec: 24285.11
Iteration:    860, Loss function: 4.920, Average Loss: 3.962, avg. samples / sec: 24238.55

:::MLPv0.5.0 ssd 1541757088.219131947 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 861, "value": 0.15306666666666666}

:::MLPv0.5.0 ssd 1541757088.303428650 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 862, "value": 0.15324444444444446}

:::MLPv0.5.0 ssd 1541757088.387845516 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 863, "value": 0.15342222222222224}

:::MLPv0.5.0 ssd 1541757088.471929312 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 864, "value": 0.15360000000000001}

:::MLPv0.5.0 ssd 1541757088.556719065 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 865, "value": 0.1537777777777778}

:::MLPv0.5.0 ssd 1541757088.640587807 (train.py:349) opt_learning_rate: {"epoch": 14, "iteration": 866, "value": 0.15395555555555557}

:::MLPv0.5.0 ssd 1541757088.721979618 (train.py:553) train_epoch: 15

:::MLPv0.5.0 ssd 1541757088.726165295 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 867, "value": 0.15413333333333334}

:::MLPv0.5.0 ssd 1541757088.810019255 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 868, "value": 0.15431111111111112}

:::MLPv0.5.0 ssd 1541757088.894131184 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 869, "value": 0.1544888888888889}

:::MLPv0.5.0 ssd 1541757088.979119539 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 870, "value": 0.15466666666666667}

:::MLPv0.5.0 ssd 1541757089.063082695 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 871, "value": 0.15484444444444445}

:::MLPv0.5.0 ssd 1541757089.147755861 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 872, "value": 0.15502222222222223}

:::MLPv0.5.0 ssd 1541757089.232351303 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 873, "value": 0.1552}

:::MLPv0.5.0 ssd 1541757089.316725731 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 874, "value": 0.15537777777777778}

:::MLPv0.5.0 ssd 1541757089.400818110 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 875, "value": 0.15555555555555556}

:::MLPv0.5.0 ssd 1541757089.484350204 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 876, "value": 0.15573333333333333}

:::MLPv0.5.0 ssd 1541757089.569296360 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 877, "value": 0.1559111111111111}

:::MLPv0.5.0 ssd 1541757089.653342724 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 878, "value": 0.1560888888888889}

:::MLPv0.5.0 ssd 1541757089.737401247 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 879, "value": 0.15626666666666666}

:::MLPv0.5.0 ssd 1541757089.821810007 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 880, "value": 0.15644444444444444}
Iteration:    880, Loss function: 4.810, Average Loss: 3.983, avg. samples / sec: 24273.10
Iteration:    880, Loss function: 5.133, Average Loss: 3.984, avg. samples / sec: 24277.59
Iteration:    880, Loss function: 4.785, Average Loss: 3.972, avg. samples / sec: 24296.71
Iteration:    880, Loss function: 5.345, Average Loss: 3.986, avg. samples / sec: 24303.53
Iteration:    880, Loss function: 5.537, Average Loss: 3.982, avg. samples / sec: 24312.75
Iteration:    880, Loss function: 5.380, Average Loss: 3.987, avg. samples / sec: 24249.40
Iteration:    880, Loss function: 5.107, Average Loss: 3.983, avg. samples / sec: 24228.88
Iteration:    880, Loss function: 5.133, Average Loss: 3.984, avg. samples / sec: 24223.67

:::MLPv0.5.0 ssd 1541757089.905957937 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 881, "value": 0.15662222222222222}

:::MLPv0.5.0 ssd 1541757089.989623785 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 882, "value": 0.1568}

:::MLPv0.5.0 ssd 1541757090.073593855 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 883, "value": 0.15697777777777777}

:::MLPv0.5.0 ssd 1541757090.157716274 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 884, "value": 0.15715555555555555}

:::MLPv0.5.0 ssd 1541757090.241894245 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 885, "value": 0.15733333333333333}

:::MLPv0.5.0 ssd 1541757090.326323271 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 886, "value": 0.1575111111111111}

:::MLPv0.5.0 ssd 1541757090.410576344 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 887, "value": 0.15768888888888888}

:::MLPv0.5.0 ssd 1541757090.494618893 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 888, "value": 0.15786666666666668}

:::MLPv0.5.0 ssd 1541757090.578803301 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 889, "value": 0.15804444444444446}

:::MLPv0.5.0 ssd 1541757090.662920713 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 890, "value": 0.15822222222222224}

:::MLPv0.5.0 ssd 1541757090.747537851 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 891, "value": 0.1584}

:::MLPv0.5.0 ssd 1541757090.831355810 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 892, "value": 0.1585777777777778}

:::MLPv0.5.0 ssd 1541757090.916129589 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 893, "value": 0.15875555555555557}

:::MLPv0.5.0 ssd 1541757091.000704288 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 894, "value": 0.15893333333333334}

:::MLPv0.5.0 ssd 1541757091.084870100 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 895, "value": 0.15911111111111112}

:::MLPv0.5.0 ssd 1541757091.168963671 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 896, "value": 0.1592888888888889}

:::MLPv0.5.0 ssd 1541757091.252985239 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 897, "value": 0.15946666666666667}

:::MLPv0.5.0 ssd 1541757091.336826801 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 898, "value": 0.15964444444444445}

:::MLPv0.5.0 ssd 1541757091.421269417 (train.py:349) opt_learning_rate: {"epoch": 15, "iteration": 899, "value": 0.15982222222222223}
Iteration:    900, Loss function: 5.688, Average Loss: 3.996, avg. samples / sec: 24339.98
Iteration:    900, Loss function: 5.086, Average Loss: 4.004, avg. samples / sec: 24321.69
Iteration:    900, Loss function: 5.518, Average Loss: 4.010, avg. samples / sec: 24352.62
Iteration:    900, Loss function: 4.958, Average Loss: 4.006, avg. samples / sec: 24353.44
Iteration:    900, Loss function: 5.922, Average Loss: 4.007, avg. samples / sec: 24347.20
Iteration:    900, Loss function: 5.606, Average Loss: 4.004, avg. samples / sec: 24297.89
Iteration:    900, Loss function: 5.345, Average Loss: 4.005, avg. samples / sec: 24284.45
Iteration:    900, Loss function: 4.997, Average Loss: 4.008, avg. samples / sec: 24288.13
Iteration:    920, Loss function: 4.776, Average Loss: 4.027, avg. samples / sec: 24536.60
Iteration:    920, Loss function: 5.222, Average Loss: 4.030, avg. samples / sec: 24550.19
Iteration:    920, Loss function: 5.485, Average Loss: 4.019, avg. samples / sec: 24513.90
Iteration:    920, Loss function: 4.704, Average Loss: 4.027, avg. samples / sec: 24531.92
Iteration:    920, Loss function: 4.881, Average Loss: 4.025, avg. samples / sec: 24551.05
Iteration:    920, Loss function: 5.259, Average Loss: 4.031, avg. samples / sec: 24516.94
Iteration:    920, Loss function: 4.917, Average Loss: 4.029, avg. samples / sec: 24526.61
Iteration:    920, Loss function: 4.907, Average Loss: 4.029, avg. samples / sec: 24523.23

:::MLPv0.5.0 ssd 1541757093.590250492 (train.py:553) train_epoch: 16
Iteration:    940, Loss function: 4.619, Average Loss: 4.041, avg. samples / sec: 24496.99
Iteration:    940, Loss function: 4.721, Average Loss: 4.045, avg. samples / sec: 24549.55
Iteration:    940, Loss function: 4.539, Average Loss: 4.044, avg. samples / sec: 24494.77
Iteration:    940, Loss function: 4.539, Average Loss: 4.047, avg. samples / sec: 24528.35
Iteration:    940, Loss function: 4.384, Average Loss: 4.042, avg. samples / sec: 24495.22
Iteration:    940, Loss function: 4.703, Average Loss: 4.041, avg. samples / sec: 24489.18
Iteration:    940, Loss function: 4.859, Average Loss: 4.035, avg. samples / sec: 24459.45
Iteration:    940, Loss function: 4.854, Average Loss: 4.043, avg. samples / sec: 24507.51
Iteration:    960, Loss function: 4.643, Average Loss: 4.058, avg. samples / sec: 24519.37
Iteration:    960, Loss function: 5.111, Average Loss: 4.064, avg. samples / sec: 24525.34
Iteration:    960, Loss function: 4.373, Average Loss: 4.057, avg. samples / sec: 24517.10
Iteration:    960, Loss function: 4.988, Average Loss: 4.061, avg. samples / sec: 24489.62
Iteration:    960, Loss function: 5.040, Average Loss: 4.057, avg. samples / sec: 24517.36
Iteration:    960, Loss function: 5.070, Average Loss: 4.051, avg. samples / sec: 24531.70
Iteration:    960, Loss function: 5.212, Average Loss: 4.061, avg. samples / sec: 24459.59
Iteration:    960, Loss function: 5.126, Average Loss: 4.059, avg. samples / sec: 24501.49
Iteration:    980, Loss function: 5.104, Average Loss: 4.080, avg. samples / sec: 24518.65
Iteration:    980, Loss function: 4.329, Average Loss: 4.076, avg. samples / sec: 24477.48
Iteration:    980, Loss function: 4.400, Average Loss: 4.082, avg. samples / sec: 24502.11
Iteration:    980, Loss function: 5.088, Average Loss: 4.086, avg. samples / sec: 24451.38
Iteration:    980, Loss function: 4.862, Average Loss: 4.071, avg. samples / sec: 24477.66
Iteration:    980, Loss function: 4.474, Average Loss: 4.077, avg. samples / sec: 24438.45
Iteration:    980, Loss function: 5.073, Average Loss: 4.078, avg. samples / sec: 24450.75
Iteration:    980, Loss function: 5.090, Average Loss: 4.083, avg. samples / sec: 24434.36

:::MLPv0.5.0 ssd 1541757098.358222246 (train.py:553) train_epoch: 17
Iteration:   1000, Loss function: 4.964, Average Loss: 4.099, avg. samples / sec: 24530.90
Iteration:   1000, Loss function: 5.082, Average Loss: 4.091, avg. samples / sec: 24525.01
Iteration:   1000, Loss function: 5.143, Average Loss: 4.092, avg. samples / sec: 24525.24
Iteration:   1000, Loss function: 5.185, Average Loss: 4.096, avg. samples / sec: 24518.49
Iteration:   1000, Loss function: 5.483, Average Loss: 4.097, avg. samples / sec: 24549.53
Iteration:   1000, Loss function: 5.064, Average Loss: 4.097, avg. samples / sec: 24494.52
Iteration:   1000, Loss function: 4.713, Average Loss: 4.094, avg. samples / sec: 24524.99
Iteration:   1000, Loss function: 5.328, Average Loss: 4.086, avg. samples / sec: 24490.32
Iteration:   1020, Loss function: 4.908, Average Loss: 4.111, avg. samples / sec: 24542.58
Iteration:   1020, Loss function: 4.549, Average Loss: 4.113, avg. samples / sec: 24507.67
Iteration:   1020, Loss function: 4.903, Average Loss: 4.100, avg. samples / sec: 24549.00
Iteration:   1020, Loss function: 5.232, Average Loss: 4.106, avg. samples / sec: 24515.15
Iteration:   1020, Loss function: 4.217, Average Loss: 4.106, avg. samples / sec: 24536.90
Iteration:   1020, Loss function: 4.501, Average Loss: 4.111, avg. samples / sec: 24502.24
Iteration:   1020, Loss function: 4.541, Average Loss: 4.104, avg. samples / sec: 24477.14
Iteration:   1020, Loss function: 4.302, Average Loss: 4.109, avg. samples / sec: 24460.64

:::MLPv0.5.0 ssd 1541757103.204640627 (train.py:553) train_epoch: 18
Iteration:   1040, Loss function: 5.316, Average Loss: 4.125, avg. samples / sec: 24478.04
Iteration:   1040, Loss function: 4.659, Average Loss: 4.120, avg. samples / sec: 24477.20
Iteration:   1040, Loss function: 4.411, Average Loss: 4.113, avg. samples / sec: 24470.75
Iteration:   1040, Loss function: 4.849, Average Loss: 4.123, avg. samples / sec: 24470.33
Iteration:   1040, Loss function: 4.274, Average Loss: 4.119, avg. samples / sec: 24474.88
Iteration:   1040, Loss function: 5.002, Average Loss: 4.127, avg. samples / sec: 24492.86
Iteration:   1040, Loss function: 4.953, Average Loss: 4.121, avg. samples / sec: 24466.23
Iteration:   1040, Loss function: 4.710, Average Loss: 4.120, avg. samples / sec: 24435.52
Iteration:   1060, Loss function: 4.651, Average Loss: 4.138, avg. samples / sec: 24512.09
Iteration:   1060, Loss function: 4.441, Average Loss: 4.132, avg. samples / sec: 24548.83
Iteration:   1060, Loss function: 5.158, Average Loss: 4.126, avg. samples / sec: 24511.49
Iteration:   1060, Loss function: 4.888, Average Loss: 4.140, avg. samples / sec: 24541.00
Iteration:   1060, Loss function: 3.851, Average Loss: 4.132, avg. samples / sec: 24492.83
Iteration:   1060, Loss function: 4.460, Average Loss: 4.130, avg. samples / sec: 24486.76
Iteration:   1060, Loss function: 5.103, Average Loss: 4.129, avg. samples / sec: 24512.97
Iteration:   1060, Loss function: 4.304, Average Loss: 4.132, avg. samples / sec: 24510.03
Iteration:   1080, Loss function: 4.448, Average Loss: 4.145, avg. samples / sec: 24500.95
Iteration:   1080, Loss function: 4.900, Average Loss: 4.143, avg. samples / sec: 24533.79
Iteration:   1080, Loss function: 4.466, Average Loss: 4.145, avg. samples / sec: 24520.75
Iteration:   1080, Loss function: 5.207, Average Loss: 4.146, avg. samples / sec: 24528.41
Iteration:   1080, Loss function: 5.059, Average Loss: 4.150, avg. samples / sec: 24469.35
Iteration:   1080, Loss function: 4.649, Average Loss: 4.143, avg. samples / sec: 24494.19
Iteration:   1080, Loss function: 5.150, Average Loss: 4.153, avg. samples / sec: 24466.63
Iteration:   1080, Loss function: 4.350, Average Loss: 4.138, avg. samples / sec: 24456.13

:::MLPv0.5.0 ssd 1541757108.052091837 (train.py:553) train_epoch: 19
Iteration:   1100, Loss function: 4.881, Average Loss: 4.157, avg. samples / sec: 24508.89
Iteration:   1100, Loss function: 5.190, Average Loss: 4.158, avg. samples / sec: 24501.15
Iteration:   1100, Loss function: 4.077, Average Loss: 4.156, avg. samples / sec: 24499.20
Iteration:   1100, Loss function: 4.666, Average Loss: 4.164, avg. samples / sec: 24523.40
Iteration:   1100, Loss function: 4.545, Average Loss: 4.152, avg. samples / sec: 24536.77
Iteration:   1100, Loss function: 4.661, Average Loss: 4.158, avg. samples / sec: 24506.91
Iteration:   1100, Loss function: 4.775, Average Loss: 4.167, avg. samples / sec: 24493.71
Iteration:   1100, Loss function: 4.531, Average Loss: 4.161, avg. samples / sec: 24460.35
Iteration:   1120, Loss function: 5.155, Average Loss: 4.167, avg. samples / sec: 24494.77
Iteration:   1120, Loss function: 4.250, Average Loss: 4.166, avg. samples / sec: 24496.42
Iteration:   1120, Loss function: 4.671, Average Loss: 4.166, avg. samples / sec: 24516.31
Iteration:   1120, Loss function: 4.769, Average Loss: 4.172, avg. samples / sec: 24495.58
Iteration:   1120, Loss function: 4.765, Average Loss: 4.166, avg. samples / sec: 24486.22
Iteration:   1120, Loss function: 4.861, Average Loss: 4.172, avg. samples / sec: 24538.14
Iteration:   1120, Loss function: 4.482, Average Loss: 4.177, avg. samples / sec: 24499.39
Iteration:   1120, Loss function: 4.590, Average Loss: 4.160, avg. samples / sec: 24453.53
Iteration:   1140, Loss function: 4.658, Average Loss: 4.175, avg. samples / sec: 24557.86
Iteration:   1140, Loss function: 4.515, Average Loss: 4.177, avg. samples / sec: 24566.53
Iteration:   1140, Loss function: 4.817, Average Loss: 4.177, avg. samples / sec: 24552.92
Iteration:   1140, Loss function: 4.554, Average Loss: 4.180, avg. samples / sec: 24563.75
Iteration:   1140, Loss function: 4.815, Average Loss: 4.185, avg. samples / sec: 24586.01
Iteration:   1140, Loss function: 4.851, Average Loss: 4.176, avg. samples / sec: 24530.56
Iteration:   1140, Loss function: 4.395, Average Loss: 4.175, avg. samples / sec: 24536.55
Iteration:   1140, Loss function: 4.622, Average Loss: 4.169, avg. samples / sec: 24562.08

:::MLPv0.5.0 ssd 1541757112.896106482 (train.py:553) train_epoch: 20
Iteration:   1160, Loss function: 4.519, Average Loss: 4.186, avg. samples / sec: 24510.49
Iteration:   1160, Loss function: 5.071, Average Loss: 4.187, avg. samples / sec: 24511.15
Iteration:   1160, Loss function: 4.699, Average Loss: 4.186, avg. samples / sec: 24544.85
Iteration:   1160, Loss function: 5.193, Average Loss: 4.185, avg. samples / sec: 24536.87
Iteration:   1160, Loss function: 4.933, Average Loss: 4.195, avg. samples / sec: 24515.16
Iteration:   1160, Loss function: 4.728, Average Loss: 4.191, avg. samples / sec: 24507.08
Iteration:   1160, Loss function: 4.674, Average Loss: 4.187, avg. samples / sec: 24503.86
Iteration:   1160, Loss function: 4.880, Average Loss: 4.177, avg. samples / sec: 24525.01
Iteration:   1180, Loss function: 4.383, Average Loss: 4.192, avg. samples / sec: 24519.48
Iteration:   1180, Loss function: 4.309, Average Loss: 4.190, avg. samples / sec: 24526.79
Iteration:   1180, Loss function: 4.690, Average Loss: 4.196, avg. samples / sec: 24514.79
Iteration:   1180, Loss function: 4.966, Average Loss: 4.203, avg. samples / sec: 24521.89
Iteration:   1180, Loss function: 4.575, Average Loss: 4.194, avg. samples / sec: 24504.08
Iteration:   1180, Loss function: 4.204, Average Loss: 4.193, avg. samples / sec: 24485.50
Iteration:   1180, Loss function: 4.481, Average Loss: 4.198, avg. samples / sec: 24502.32
Iteration:   1180, Loss function: 5.068, Average Loss: 4.185, avg. samples / sec: 24488.42
Iteration:   1200, Loss function: 4.948, Average Loss: 4.193, avg. samples / sec: 24580.11
Iteration:   1200, Loss function: 4.661, Average Loss: 4.201, avg. samples / sec: 24499.38
Iteration:   1200, Loss function: 4.874, Average Loss: 4.205, avg. samples / sec: 24533.34
Iteration:   1200, Loss function: 4.681, Average Loss: 4.202, avg. samples / sec: 24532.76
Iteration:   1200, Loss function: 4.576, Average Loss: 4.203, avg. samples / sec: 24528.36
Iteration:   1200, Loss function: 4.656, Average Loss: 4.197, avg. samples / sec: 24470.40
Iteration:   1200, Loss function: 4.152, Average Loss: 4.203, avg. samples / sec: 24453.19
Iteration:   1200, Loss function: 5.109, Average Loss: 4.209, avg. samples / sec: 24458.04

:::MLPv0.5.0 ssd 1541757117.660962820 (train.py:553) train_epoch: 21
Iteration:   1220, Loss function: 4.898, Average Loss: 4.213, avg. samples / sec: 24507.42
Iteration:   1220, Loss function: 4.232, Average Loss: 4.209, avg. samples / sec: 24533.31
Iteration:   1220, Loss function: 4.451, Average Loss: 4.212, avg. samples / sec: 24500.62
Iteration:   1220, Loss function: 4.432, Average Loss: 4.214, avg. samples / sec: 24497.41
Iteration:   1220, Loss function: 4.584, Average Loss: 4.202, avg. samples / sec: 24445.55
Iteration:   1220, Loss function: 4.320, Average Loss: 4.214, avg. samples / sec: 24503.02
Iteration:   1220, Loss function: 4.805, Average Loss: 4.214, avg. samples / sec: 24450.98
Iteration:   1220, Loss function: 4.379, Average Loss: 4.219, avg. samples / sec: 24492.62
Iteration:   1240, Loss function: 4.968, Average Loss: 4.221, avg. samples / sec: 24568.73
Iteration:   1240, Loss function: 4.510, Average Loss: 4.221, avg. samples / sec: 24624.13
Iteration:   1240, Loss function: 4.397, Average Loss: 4.218, avg. samples / sec: 24572.71
Iteration:   1240, Loss function: 4.626, Average Loss: 4.221, avg. samples / sec: 24567.04
Iteration:   1240, Loss function: 3.948, Average Loss: 4.208, avg. samples / sec: 24595.47
Iteration:   1240, Loss function: 4.693, Average Loss: 4.218, avg. samples / sec: 24598.48
Iteration:   1240, Loss function: 4.910, Average Loss: 4.212, avg. samples / sec: 24529.10
Iteration:   1240, Loss function: 4.565, Average Loss: 4.224, avg. samples / sec: 24576.76
Iteration:   1260, Loss function: 4.340, Average Loss: 4.219, avg. samples / sec: 24595.26
Iteration:   1260, Loss function: 4.334, Average Loss: 4.225, avg. samples / sec: 24551.00
Iteration:   1260, Loss function: 4.587, Average Loss: 4.216, avg. samples / sec: 24575.05
Iteration:   1260, Loss function: 5.171, Average Loss: 4.229, avg. samples / sec: 24544.51
Iteration:   1260, Loss function: 4.421, Average Loss: 4.228, avg. samples / sec: 24556.78
Iteration:   1260, Loss function: 4.316, Average Loss: 4.225, avg. samples / sec: 24556.45
Iteration:   1260, Loss function: 4.164, Average Loss: 4.227, avg. samples / sec: 24515.31
Iteration:   1260, Loss function: 3.940, Average Loss: 4.231, avg. samples / sec: 24562.55

:::MLPv0.5.0 ssd 1541757122.500251055 (train.py:553) train_epoch: 22
Iteration:   1280, Loss function: 4.531, Average Loss: 4.235, avg. samples / sec: 24501.26
Iteration:   1280, Loss function: 4.765, Average Loss: 4.230, avg. samples / sec: 24494.96
Iteration:   1280, Loss function: 4.364, Average Loss: 4.229, avg. samples / sec: 24513.95
Iteration:   1280, Loss function: 4.920, Average Loss: 4.222, avg. samples / sec: 24492.29
Iteration:   1280, Loss function: 4.614, Average Loss: 4.232, avg. samples / sec: 24493.26
Iteration:   1280, Loss function: 3.618, Average Loss: 4.230, avg. samples / sec: 24500.68
Iteration:   1280, Loss function: 4.688, Average Loss: 4.223, avg. samples / sec: 24457.95
Iteration:   1280, Loss function: 3.957, Average Loss: 4.234, avg. samples / sec: 24493.78
Iteration:   1300, Loss function: 4.850, Average Loss: 4.234, avg. samples / sec: 24483.07
Iteration:   1300, Loss function: 4.916, Average Loss: 4.241, avg. samples / sec: 24471.45
Iteration:   1300, Loss function: 4.884, Average Loss: 4.226, avg. samples / sec: 24513.93
Iteration:   1300, Loss function: 4.168, Average Loss: 4.238, avg. samples / sec: 24477.27
Iteration:   1300, Loss function: 4.752, Average Loss: 4.237, avg. samples / sec: 24482.67
Iteration:   1300, Loss function: 4.864, Average Loss: 4.230, avg. samples / sec: 24482.31
Iteration:   1300, Loss function: 4.775, Average Loss: 4.237, avg. samples / sec: 24503.12
Iteration:   1300, Loss function: 4.440, Average Loss: 4.240, avg. samples / sec: 24468.08
Iteration:   1320, Loss function: 4.274, Average Loss: 4.233, avg. samples / sec: 24464.83
Iteration:   1320, Loss function: 5.083, Average Loss: 4.238, avg. samples / sec: 24466.34
Iteration:   1320, Loss function: 4.264, Average Loss: 4.250, avg. samples / sec: 24467.57
Iteration:   1320, Loss function: 4.577, Average Loss: 4.243, avg. samples / sec: 24463.60
Iteration:   1320, Loss function: 4.465, Average Loss: 4.245, avg. samples / sec: 24456.88
Iteration:   1320, Loss function: 4.799, Average Loss: 4.245, avg. samples / sec: 24449.16
Iteration:   1320, Loss function: 4.080, Average Loss: 4.242, avg. samples / sec: 24434.70
Iteration:   1320, Loss function: 4.466, Average Loss: 4.247, avg. samples / sec: 24458.84

:::MLPv0.5.0 ssd 1541757127.354139566 (train.py:553) train_epoch: 23
Iteration:   1340, Loss function: 4.341, Average Loss: 4.251, avg. samples / sec: 24508.39
Iteration:   1340, Loss function: 4.438, Average Loss: 4.239, avg. samples / sec: 24507.71
Iteration:   1340, Loss function: 3.971, Average Loss: 4.249, avg. samples / sec: 24527.83
Iteration:   1340, Loss function: 4.135, Average Loss: 4.247, avg. samples / sec: 24508.44
Iteration:   1340, Loss function: 4.577, Average Loss: 4.242, avg. samples / sec: 24503.85
Iteration:   1340, Loss function: 4.247, Average Loss: 4.250, avg. samples / sec: 24485.92
Iteration:   1340, Loss function: 4.087, Average Loss: 4.246, avg. samples / sec: 24502.16
Iteration:   1340, Loss function: 4.085, Average Loss: 4.249, avg. samples / sec: 24523.56
Iteration:   1360, Loss function: 4.241, Average Loss: 4.252, avg. samples / sec: 24526.46
Iteration:   1360, Loss function: 4.500, Average Loss: 4.252, avg. samples / sec: 24559.38
Iteration:   1360, Loss function: 4.346, Average Loss: 4.254, avg. samples / sec: 24528.13
Iteration:   1360, Loss function: 4.211, Average Loss: 4.240, avg. samples / sec: 24525.79
Iteration:   1360, Loss function: 4.488, Average Loss: 4.248, avg. samples / sec: 24528.54
Iteration:   1360, Loss function: 4.185, Average Loss: 4.249, avg. samples / sec: 24529.90
Iteration:   1360, Loss function: 4.172, Average Loss: 4.247, avg. samples / sec: 24487.49
Iteration:   1360, Loss function: 4.609, Average Loss: 4.250, avg. samples / sec: 24532.45
Iteration:   1380, Loss function: 4.771, Average Loss: 4.258, avg. samples / sec: 24550.51
Iteration:   1380, Loss function: 4.649, Average Loss: 4.254, avg. samples / sec: 24593.65
Iteration:   1380, Loss function: 4.840, Average Loss: 4.257, avg. samples / sec: 24594.85
Iteration:   1380, Loss function: 4.525, Average Loss: 4.254, avg. samples / sec: 24554.19
Iteration:   1380, Loss function: 4.082, Average Loss: 4.248, avg. samples / sec: 24547.40
Iteration:   1380, Loss function: 4.473, Average Loss: 4.263, avg. samples / sec: 24544.80
Iteration:   1380, Loss function: 4.562, Average Loss: 4.253, avg. samples / sec: 24564.12
Iteration:   1380, Loss function: 3.958, Average Loss: 4.260, avg. samples / sec: 24512.42

:::MLPv0.5.0 ssd 1541757132.198345184 (train.py:553) train_epoch: 24
Iteration:   1400, Loss function: 4.209, Average Loss: 4.260, avg. samples / sec: 24439.24
Iteration:   1400, Loss function: 4.204, Average Loss: 4.256, avg. samples / sec: 24434.20
Iteration:   1400, Loss function: 3.739, Average Loss: 4.265, avg. samples / sec: 24441.58
Iteration:   1400, Loss function: 4.631, Average Loss: 4.250, avg. samples / sec: 24432.03
Iteration:   1400, Loss function: 4.620, Average Loss: 4.265, avg. samples / sec: 24467.03
Iteration:   1400, Loss function: 4.487, Average Loss: 4.256, avg. samples / sec: 24434.59
Iteration:   1400, Loss function: 4.368, Average Loss: 4.259, avg. samples / sec: 24405.51
Iteration:   1400, Loss function: 4.240, Average Loss: 4.256, avg. samples / sec: 24373.39
Iteration:   1420, Loss function: 4.585, Average Loss: 4.263, avg. samples / sec: 24493.06
Iteration:   1420, Loss function: 3.873, Average Loss: 4.269, avg. samples / sec: 24489.77
Iteration:   1420, Loss function: 4.921, Average Loss: 4.262, avg. samples / sec: 24551.90
Iteration:   1420, Loss function: 4.402, Average Loss: 4.252, avg. samples / sec: 24494.58
Iteration:   1420, Loss function: 4.916, Average Loss: 4.262, avg. samples / sec: 24483.50
Iteration:   1420, Loss function: 4.995, Average Loss: 4.261, avg. samples / sec: 24487.47
Iteration:   1420, Loss function: 4.592, Average Loss: 4.268, avg. samples / sec: 24465.47
Iteration:   1420, Loss function: 4.678, Average Loss: 4.264, avg. samples / sec: 24474.45
Iteration:   1440, Loss function: 3.890, Average Loss: 4.271, avg. samples / sec: 24493.84
Iteration:   1440, Loss function: 4.466, Average Loss: 4.266, avg. samples / sec: 24481.72
Iteration:   1440, Loss function: 4.437, Average Loss: 4.257, avg. samples / sec: 24492.29
Iteration:   1440, Loss function: 5.091, Average Loss: 4.266, avg. samples / sec: 24490.03
Iteration:   1440, Loss function: 4.961, Average Loss: 4.273, avg. samples / sec: 24515.51
Iteration:   1440, Loss function: 4.713, Average Loss: 4.265, avg. samples / sec: 24493.29
Iteration:   1440, Loss function: 4.572, Average Loss: 4.268, avg. samples / sec: 24467.28
Iteration:   1440, Loss function: 4.360, Average Loss: 4.268, avg. samples / sec: 24496.17

:::MLPv0.5.0 ssd 1541757136.968864679 (train.py:553) train_epoch: 25
Iteration:   1460, Loss function: 4.482, Average Loss: 4.269, avg. samples / sec: 24526.32
Iteration:   1460, Loss function: 4.090, Average Loss: 4.258, avg. samples / sec: 24525.27
Iteration:   1460, Loss function: 4.268, Average Loss: 4.270, avg. samples / sec: 24519.58
Iteration:   1460, Loss function: 4.253, Average Loss: 4.277, avg. samples / sec: 24495.43
Iteration:   1460, Loss function: 4.641, Average Loss: 4.271, avg. samples / sec: 24519.40
Iteration:   1460, Loss function: 4.202, Average Loss: 4.275, avg. samples / sec: 24478.05
Iteration:   1460, Loss function: 3.892, Average Loss: 4.270, avg. samples / sec: 24475.58
Iteration:   1460, Loss function: 4.486, Average Loss: 4.271, avg. samples / sec: 24508.73
Iteration:   1480, Loss function: 4.028, Average Loss: 4.259, avg. samples / sec: 24534.36
Iteration:   1480, Loss function: 4.636, Average Loss: 4.273, avg. samples / sec: 24578.42
Iteration:   1480, Loss function: 4.568, Average Loss: 4.271, avg. samples / sec: 24528.67
Iteration:   1480, Loss function: 4.864, Average Loss: 4.276, avg. samples / sec: 24534.86
Iteration:   1480, Loss function: 4.079, Average Loss: 4.270, avg. samples / sec: 24520.94
Iteration:   1480, Loss function: 4.427, Average Loss: 4.273, avg. samples / sec: 24532.53
Iteration:   1480, Loss function: 4.411, Average Loss: 4.272, avg. samples / sec: 24543.94
Iteration:   1480, Loss function: 4.128, Average Loss: 4.269, avg. samples / sec: 24518.52
Iteration:   1500, Loss function: 3.983, Average Loss: 4.261, avg. samples / sec: 24518.51
Iteration:   1500, Loss function: 3.929, Average Loss: 4.274, avg. samples / sec: 24521.71
Iteration:   1500, Loss function: 4.447, Average Loss: 4.274, avg. samples / sec: 24512.01
Iteration:   1500, Loss function: 4.820, Average Loss: 4.270, avg. samples / sec: 24574.09
Iteration:   1500, Loss function: 4.255, Average Loss: 4.273, avg. samples / sec: 24549.74
Iteration:   1500, Loss function: 4.221, Average Loss: 4.271, avg. samples / sec: 24526.97
Iteration:   1500, Loss function: 4.173, Average Loss: 4.279, avg. samples / sec: 24515.90
Iteration:   1500, Loss function: 4.248, Average Loss: 4.275, avg. samples / sec: 24503.74

:::MLPv0.5.0 ssd 1541757141.811542988 (train.py:553) train_epoch: 26
Iteration:   1520, Loss function: 4.090, Average Loss: 4.262, avg. samples / sec: 24462.60
Iteration:   1520, Loss function: 4.652, Average Loss: 4.276, avg. samples / sec: 24462.44
Iteration:   1520, Loss function: 4.072, Average Loss: 4.270, avg. samples / sec: 24461.90
Iteration:   1520, Loss function: 3.849, Average Loss: 4.274, avg. samples / sec: 24508.83
Iteration:   1520, Loss function: 4.256, Average Loss: 4.279, avg. samples / sec: 24465.48
Iteration:   1520, Loss function: 4.341, Average Loss: 4.276, avg. samples / sec: 24455.33
Iteration:   1520, Loss function: 3.913, Average Loss: 4.273, avg. samples / sec: 24431.44
Iteration:   1520, Loss function: 4.943, Average Loss: 4.276, avg. samples / sec: 24421.09
Iteration:   1540, Loss function: 4.422, Average Loss: 4.278, avg. samples / sec: 24478.37
Iteration:   1540, Loss function: 4.025, Average Loss: 4.262, avg. samples / sec: 24475.92
Iteration:   1540, Loss function: 4.576, Average Loss: 4.279, avg. samples / sec: 24503.68
Iteration:   1540, Loss function: 3.963, Average Loss: 4.269, avg. samples / sec: 24480.97
Iteration:   1540, Loss function: 4.041, Average Loss: 4.273, avg. samples / sec: 24508.65
Iteration:   1540, Loss function: 4.562, Average Loss: 4.277, avg. samples / sec: 24491.20
Iteration:   1540, Loss function: 4.431, Average Loss: 4.275, avg. samples / sec: 24442.92
Iteration:   1540, Loss function: 4.491, Average Loss: 4.278, avg. samples / sec: 24467.22

:::MLPv0.5.0 ssd 1541757146.664744854 (train.py:553) train_epoch: 27
Iteration:   1560, Loss function: 4.258, Average Loss: 4.281, avg. samples / sec: 24516.92
Iteration:   1560, Loss function: 4.602, Average Loss: 4.266, avg. samples / sec: 24468.10
Iteration:   1560, Loss function: 4.705, Average Loss: 4.277, avg. samples / sec: 24473.63
Iteration:   1560, Loss function: 4.028, Average Loss: 4.270, avg. samples / sec: 24470.64
Iteration:   1560, Loss function: 4.361, Average Loss: 4.280, avg. samples / sec: 24462.02
Iteration:   1560, Loss function: 4.938, Average Loss: 4.284, avg. samples / sec: 24469.15
Iteration:   1560, Loss function: 4.144, Average Loss: 4.279, avg. samples / sec: 24455.80
Iteration:   1560, Loss function: 4.483, Average Loss: 4.280, avg. samples / sec: 24487.51
Iteration:   1580, Loss function: 4.806, Average Loss: 4.268, avg. samples / sec: 24468.45
Iteration:   1580, Loss function: 4.132, Average Loss: 4.283, avg. samples / sec: 24463.07
Iteration:   1580, Loss function: 4.635, Average Loss: 4.277, avg. samples / sec: 24460.06
Iteration:   1580, Loss function: 4.005, Average Loss: 4.283, avg. samples / sec: 24457.72
Iteration:   1580, Loss function: 3.816, Average Loss: 4.271, avg. samples / sec: 24453.88
Iteration:   1580, Loss function: 4.212, Average Loss: 4.283, avg. samples / sec: 24471.78
Iteration:   1580, Loss function: 4.754, Average Loss: 4.281, avg. samples / sec: 24414.25
Iteration:   1580, Loss function: 4.500, Average Loss: 4.280, avg. samples / sec: 24445.80
Iteration:   1600, Loss function: 3.946, Average Loss: 4.273, avg. samples / sec: 24521.56
Iteration:   1600, Loss function: 4.016, Average Loss: 4.281, avg. samples / sec: 24525.29
Iteration:   1600, Loss function: 4.121, Average Loss: 4.285, avg. samples / sec: 24571.64
Iteration:   1600, Loss function: 5.020, Average Loss: 4.288, avg. samples / sec: 24534.44
Iteration:   1600, Loss function: 3.935, Average Loss: 4.287, avg. samples / sec: 24518.64
Iteration:   1600, Loss function: 4.126, Average Loss: 4.276, avg. samples / sec: 24491.46
Iteration:   1600, Loss function: 4.443, Average Loss: 4.285, avg. samples / sec: 24527.80
Iteration:   1600, Loss function: 4.441, Average Loss: 4.288, avg. samples / sec: 24479.52

:::MLPv0.5.0 ssd 1541757151.515625954 (train.py:553) train_epoch: 28
Iteration:   1620, Loss function: 3.942, Average Loss: 4.273, avg. samples / sec: 24476.82
Iteration:   1620, Loss function: 4.466, Average Loss: 4.289, avg. samples / sec: 24520.34
Iteration:   1620, Loss function: 4.534, Average Loss: 4.282, avg. samples / sec: 24473.20
Iteration:   1620, Loss function: 4.099, Average Loss: 4.278, avg. samples / sec: 24505.45
Iteration:   1620, Loss function: 4.491, Average Loss: 4.285, avg. samples / sec: 24454.30
Iteration:   1620, Loss function: 3.858, Average Loss: 4.290, avg. samples / sec: 24457.07
Iteration:   1620, Loss function: 4.712, Average Loss: 4.290, avg. samples / sec: 24468.44
Iteration:   1620, Loss function: 4.626, Average Loss: 4.288, avg. samples / sec: 24425.20
Iteration:   1640, Loss function: 4.394, Average Loss: 4.287, avg. samples / sec: 24560.33
Iteration:   1640, Loss function: 4.387, Average Loss: 4.270, avg. samples / sec: 24524.36
Iteration:   1640, Loss function: 4.816, Average Loss: 4.282, avg. samples / sec: 24527.82
Iteration:   1640, Loss function: 5.134, Average Loss: 4.278, avg. samples / sec: 24530.24
Iteration:   1640, Loss function: 3.969, Average Loss: 4.286, avg. samples / sec: 24544.70
Iteration:   1640, Loss function: 4.939, Average Loss: 4.290, avg. samples / sec: 24478.87
Iteration:   1640, Loss function: 4.481, Average Loss: 4.288, avg. samples / sec: 24528.18
Iteration:   1640, Loss function: 4.720, Average Loss: 4.286, avg. samples / sec: 24527.32
Iteration:   1660, Loss function: 4.182, Average Loss: 4.287, avg. samples / sec: 24511.83
Iteration:   1660, Loss function: 4.497, Average Loss: 4.269, avg. samples / sec: 24512.15
Iteration:   1660, Loss function: 4.305, Average Loss: 4.280, avg. samples / sec: 24525.49
Iteration:   1660, Loss function: 4.286, Average Loss: 4.286, avg. samples / sec: 24521.97
Iteration:   1660, Loss function: 4.623, Average Loss: 4.289, avg. samples / sec: 24557.05
Iteration:   1660, Loss function: 4.731, Average Loss: 4.292, avg. samples / sec: 24517.56
Iteration:   1660, Loss function: 4.319, Average Loss: 4.280, avg. samples / sec: 24472.73
Iteration:   1660, Loss function: 4.516, Average Loss: 4.286, avg. samples / sec: 24515.92

:::MLPv0.5.0 ssd 1541757156.278867722 (train.py:553) train_epoch: 29
Iteration:   1680, Loss function: 4.357, Average Loss: 4.269, avg. samples / sec: 24483.22
Iteration:   1680, Loss function: 4.291, Average Loss: 4.286, avg. samples / sec: 24490.51
Iteration:   1680, Loss function: 4.100, Average Loss: 4.285, avg. samples / sec: 24481.56
Iteration:   1680, Loss function: 4.111, Average Loss: 4.278, avg. samples / sec: 24475.52
Iteration:   1680, Loss function: 4.780, Average Loss: 4.286, avg. samples / sec: 24532.45
Iteration:   1680, Loss function: 3.984, Average Loss: 4.274, avg. samples / sec: 24517.58
Iteration:   1680, Loss function: 4.079, Average Loss: 4.284, avg. samples / sec: 24436.79
Iteration:   1680, Loss function: 3.832, Average Loss: 4.289, avg. samples / sec: 24471.29
Iteration:   1700, Loss function: 4.162, Average Loss: 4.287, avg. samples / sec: 24494.49
Iteration:   1700, Loss function: 4.062, Average Loss: 4.275, avg. samples / sec: 24504.57
Iteration:   1700, Loss function: 4.401, Average Loss: 4.269, avg. samples / sec: 24491.98
Iteration:   1700, Loss function: 4.250, Average Loss: 4.286, avg. samples / sec: 24531.42
Iteration:   1700, Loss function: 4.483, Average Loss: 4.289, avg. samples / sec: 24495.71
Iteration:   1700, Loss function: 4.313, Average Loss: 4.279, avg. samples / sec: 24489.01
Iteration:   1700, Loss function: 4.228, Average Loss: 4.284, avg. samples / sec: 24472.83
Iteration:   1700, Loss function: 3.783, Average Loss: 4.289, avg. samples / sec: 24521.57
Iteration:   1720, Loss function: 4.040, Average Loss: 4.284, avg. samples / sec: 24589.70
Iteration:   1720, Loss function: 4.269, Average Loss: 4.286, avg. samples / sec: 24563.90
Iteration:   1720, Loss function: 4.176, Average Loss: 4.279, avg. samples / sec: 24572.39
Iteration:   1720, Loss function: 4.480, Average Loss: 4.266, avg. samples / sec: 24560.37
Iteration:   1720, Loss function: 3.969, Average Loss: 4.273, avg. samples / sec: 24559.18
Iteration:   1720, Loss function: 3.828, Average Loss: 4.285, avg. samples / sec: 24560.36
Iteration:   1720, Loss function: 4.057, Average Loss: 4.288, avg. samples / sec: 24526.68
Iteration:   1720, Loss function: 3.623, Average Loss: 4.288, avg. samples / sec: 24534.84

:::MLPv0.5.0 ssd 1541757161.127411604 (train.py:553) train_epoch: 30
Iteration:   1740, Loss function: 3.549, Average Loss: 4.265, avg. samples / sec: 24482.34
Iteration:   1740, Loss function: 4.059, Average Loss: 4.279, avg. samples / sec: 24480.36
Iteration:   1740, Loss function: 4.062, Average Loss: 4.284, avg. samples / sec: 24477.24
Iteration:   1740, Loss function: 3.378, Average Loss: 4.283, avg. samples / sec: 24469.86
Iteration:   1740, Loss function: 4.394, Average Loss: 4.278, avg. samples / sec: 24441.31
Iteration:   1740, Loss function: 4.162, Average Loss: 4.289, avg. samples / sec: 24457.74
Iteration:   1740, Loss function: 4.387, Average Loss: 4.269, avg. samples / sec: 24422.02
Iteration:   1740, Loss function: 4.522, Average Loss: 4.287, avg. samples / sec: 24457.84
Iteration:   1760, Loss function: 4.444, Average Loss: 4.266, avg. samples / sec: 24469.38
Iteration:   1760, Loss function: 3.809, Average Loss: 4.281, avg. samples / sec: 24474.32
Iteration:   1760, Loss function: 4.295, Average Loss: 4.283, avg. samples / sec: 24469.78
Iteration:   1760, Loss function: 4.111, Average Loss: 4.283, avg. samples / sec: 24471.25
Iteration:   1760, Loss function: 4.035, Average Loss: 4.290, avg. samples / sec: 24520.93
Iteration:   1760, Loss function: 4.313, Average Loss: 4.282, avg. samples / sec: 24472.86
Iteration:   1760, Loss function: 4.121, Average Loss: 4.268, avg. samples / sec: 24479.22
Iteration:   1760, Loss function: 4.798, Average Loss: 4.286, avg. samples / sec: 24485.80
Iteration:   1780, Loss function: 4.052, Average Loss: 4.280, avg. samples / sec: 24553.59
Iteration:   1780, Loss function: 4.170, Average Loss: 4.286, avg. samples / sec: 24574.11
Iteration:   1780, Loss function: 4.341, Average Loss: 4.267, avg. samples / sec: 24510.42
Iteration:   1780, Loss function: 4.381, Average Loss: 4.283, avg. samples / sec: 24514.94
Iteration:   1780, Loss function: 3.597, Average Loss: 4.288, avg. samples / sec: 24511.03
Iteration:   1780, Loss function: 3.907, Average Loss: 4.281, avg. samples / sec: 24497.81
Iteration:   1780, Loss function: 3.758, Average Loss: 4.282, avg. samples / sec: 24479.03
Iteration:   1780, Loss function: 3.923, Average Loss: 4.267, avg. samples / sec: 24515.04

:::MLPv0.5.0 ssd 1541757165.974477768 (train.py:553) train_epoch: 31
Iteration:   1800, Loss function: 4.372, Average Loss: 4.267, avg. samples / sec: 24540.43
Iteration:   1800, Loss function: 4.390, Average Loss: 4.280, avg. samples / sec: 24574.18
Iteration:   1800, Loss function: 4.127, Average Loss: 4.276, avg. samples / sec: 24532.92
Iteration:   1800, Loss function: 4.130, Average Loss: 4.278, avg. samples / sec: 24547.05
Iteration:   1800, Loss function: 4.348, Average Loss: 4.282, avg. samples / sec: 24531.81
Iteration:   1800, Loss function: 4.432, Average Loss: 4.279, avg. samples / sec: 24484.62
Iteration:   1800, Loss function: 4.152, Average Loss: 4.266, avg. samples / sec: 24521.13
Iteration:   1800, Loss function: 4.244, Average Loss: 4.285, avg. samples / sec: 24479.82
Iteration:   1820, Loss function: 4.583, Average Loss: 4.268, avg. samples / sec: 24504.82
Iteration:   1820, Loss function: 4.031, Average Loss: 4.265, avg. samples / sec: 24567.00
Iteration:   1820, Loss function: 4.352, Average Loss: 4.278, avg. samples / sec: 24554.47
Iteration:   1820, Loss function: 5.063, Average Loss: 4.277, avg. samples / sec: 24498.87
Iteration:   1820, Loss function: 4.311, Average Loss: 4.284, avg. samples / sec: 24497.57
Iteration:   1820, Loss function: 4.181, Average Loss: 4.276, avg. samples / sec: 24482.08
Iteration:   1820, Loss function: 3.657, Average Loss: 4.279, avg. samples / sec: 24474.36
Iteration:   1820, Loss function: 4.636, Average Loss: 4.281, avg. samples / sec: 24521.82
Iteration:   1840, Loss function: 3.987, Average Loss: 4.269, avg. samples / sec: 24495.74
Iteration:   1840, Loss function: 3.895, Average Loss: 4.276, avg. samples / sec: 24524.48
Iteration:   1840, Loss function: 4.393, Average Loss: 4.266, avg. samples / sec: 24497.83
Iteration:   1840, Loss function: 4.393, Average Loss: 4.280, avg. samples / sec: 24525.33
Iteration:   1840, Loss function: 4.397, Average Loss: 4.278, avg. samples / sec: 24499.98
Iteration:   1840, Loss function: 3.521, Average Loss: 4.280, avg. samples / sec: 24483.98
Iteration:   1840, Loss function: 3.771, Average Loss: 4.276, avg. samples / sec: 24471.15
Iteration:   1840, Loss function: 3.911, Average Loss: 4.280, avg. samples / sec: 24485.28

:::MLPv0.5.0 ssd 1541757170.820600033 (train.py:553) train_epoch: 32
Iteration:   1860, Loss function: 3.880, Average Loss: 4.269, avg. samples / sec: 24525.36
Iteration:   1860, Loss function: 3.850, Average Loss: 4.273, avg. samples / sec: 24529.59
Iteration:   1860, Loss function: 4.516, Average Loss: 4.278, avg. samples / sec: 24522.67
Iteration:   1860, Loss function: 4.154, Average Loss: 4.265, avg. samples / sec: 24521.57
Iteration:   1860, Loss function: 4.122, Average Loss: 4.278, avg. samples / sec: 24579.16
Iteration:   1860, Loss function: 3.877, Average Loss: 4.276, avg. samples / sec: 24519.15
Iteration:   1860, Loss function: 3.908, Average Loss: 4.272, avg. samples / sec: 24490.69
Iteration:   1860, Loss function: 3.578, Average Loss: 4.273, avg. samples / sec: 24514.88

































































:::MLPv0.5.0 ssd 1541757173.079969168 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757173.080519915 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757173.080965042 (train.py:220) eval_start: 32
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 30
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1No object detected in idx: 46
No object detected in idx: 50
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.71 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.71 s
Predicting Ended, total time: 5.71 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.71 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.71 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.71 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 5.71 s
Predicting Ended, total time: 5.71 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(227261, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
0/227261
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(227261, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(227261, 7)
0/227261
(227261, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(227261, 7)
(227261, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(227261, 7)
Loading and preparing results...
(227261, 7)
0/227261
(227261, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/227261
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(227261, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(227261, 7)
Converting ndarray to lists...
0/227261
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(227261, 7)
0/227261
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(227261, 7)
Loading and preparing results...
(227261, 7)
(227261, 7)
Converting ndarray to lists...
(227261, 7)
0/227261
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(227261, 7)
Converting ndarray to lists...
0/227261
0/227261
Converting ndarray to lists...
Loading and preparing results...
0/227261
Converting ndarray to lists...
(227261, 7)
Converting ndarray to lists...
0/227261
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(227261, 7)
0/227261
(227261, 7)
Loading and preparing results...
Loading and preparing results...
(227261, 7)
Loading and preparing results...
Loading and preparing results...
(227261, 7)
(227261, 7)
(227261, 7)
(227261, 7)
0/227261
(227261, 7)
Loading and preparing results...
0/227261
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(227261, 7)
0/227261
Converting ndarray to lists...
(227261, 7)
Converting ndarray to lists...
Loading and preparing results...
0/227261
Converting ndarray to lists...
Loading and preparing results...
(227261, 7)
(227261, 7)
0/227261
Converting ndarray to lists...
0/227261
(227261, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(227261, 7)
0/227261
Loading and preparing results...
0/227261
Converting ndarray to lists...
(227261, 7)
0/227261
(227261, 7)
Loading and preparing results...
Loading and preparing results...
0/227261
(227261, 7)
Converting ndarray to lists...
(227261, 7)
(227261, 7)
Converting ndarray to lists...
0/227261
0/227261
0/227261
0/227261
(227261, 7)
Converting ndarray to lists...
0/227261
(227261, 7)
Converting ndarray to lists...
0/227261
0/227261
(227261, 7)
0/227261
Loading and preparing results...
(227261, 7)
Converting ndarray to lists...
0/227261
(227261, 7)
Loading and preparing results...
0/227261
0/227261
Converting ndarray to lists...
Loading and preparing results...
(227261, 7)
0/227261
Converting ndarray to lists...
0/227261
0/227261
(227261, 7)
Converting ndarray to lists...
0/227261
0/227261
0/227261
0/227261
Converting ndarray to lists...
Loading and preparing results...
(227261, 7)
0/227261
(227261, 7)
0/227261
(227261, 7)
(227261, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/227261
Loading and preparing results...
0/227261
Loading and preparing results...
0/227261
Converting ndarray to lists...
(227261, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(227261, 7)
(227261, 7)
0/227261
0/227261
Converting ndarray to lists...
0/227261
(227261, 7)
0/227261
(227261, 7)
(227261, 7)
Loading and preparing results...
0/227261
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(227261, 7)
(227261, 7)
0/227261
Converting ndarray to lists...
0/227261
0/227261
Loading and preparing results...
(227261, 7)
(227261, 7)
(227261, 7)
0/227261
Converting ndarray to lists...
Converting ndarray to lists...
(227261, 7)
(227261, 7)
(227261, 7)
0/227261
0/227261
0/227261
(227261, 7)
0/227261
0/227261
0/227261
(227261, 7)
0/227261
0/227261
0/227261
0/227261
DONE (t=1.32s)
creating index...
DONE (t=1.32s)
creating index...
DONE (t=1.32s)
creating index...
DONE (t=1.33s)
creating index...
DONE (t=1.33s)
creating index...
DONE (t=1.33s)
creating index...
DONE (t=1.34s)
creating index...
DONE (t=1.34s)
creating index...
DONE (t=1.34s)
creating index...
DONE (t=1.34s)
creating index...
DONE (t=1.34s)
creating index...
DONE (t=1.34s)
creating index...
DONE (t=1.34s)
creating index...
DONE (t=1.34s)
creating index...
DONE (t=1.35s)
creating index...
DONE (t=1.35s)
creating index...
DONE (t=1.35s)
creating index...
DONE (t=1.35s)
creating index...
DONE (t=1.35s)
creating index...
DONE (t=1.35s)
creating index...
DONE (t=1.35s)
creating index...
DONE (t=1.35s)
creating index...
DONE (t=1.35s)
creating index...
DONE (t=1.35s)
creating index...
DONE (t=1.35s)
creating index...
DONE (t=1.35s)
creating index...
DONE (t=1.35s)
creating index...
DONE (t=1.35s)
creating index...
DONE (t=1.35s)
creating index...
DONE (t=1.36s)
creating index...
DONE (t=1.36s)
creating index...
DONE (t=1.36s)
creating index...
DONE (t=1.36s)
creating index...
DONE (t=1.36s)
creating index...
DONE (t=1.36s)
creating index...
DONE (t=1.36s)
creating index...
DONE (t=1.36s)
creating index...
DONE (t=1.36s)
creating index...
DONE (t=1.36s)
creating index...
DONE (t=1.36s)
creating index...
DONE (t=1.36s)
creating index...
DONE (t=1.36s)
creating index...
DONE (t=1.36s)
creating index...
DONE (t=1.37s)
creating index...
DONE (t=1.37s)
creating index...
DONE (t=1.37s)
creating index...
DONE (t=1.37s)
creating index...
DONE (t=1.37s)
creating index...
DONE (t=1.37s)
creating index...
DONE (t=1.37s)
creating index...
DONE (t=1.37s)
creating index...
DONE (t=1.37s)
creating index...
DONE (t=1.38s)
creating index...
DONE (t=1.38s)
creating index...
DONE (t=1.38s)
creating index...
DONE (t=1.38s)
creating index...
DONE (t=1.38s)
creating index...
DONE (t=1.38s)
creating index...
DONE (t=1.38s)
creating index...
DONE (t=1.39s)
creating index...
DONE (t=1.39s)
creating index...
DONE (t=1.39s)
creating index...
DONE (t=1.39s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
DONE (t=1.62s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
DONE (t=2.85s).
Accumulating evaluation results...
DONE (t=2.87s).
Accumulating evaluation results...
DONE (t=3.16s).
Accumulating evaluation results...
DONE (t=2.88s).
Accumulating evaluation results...
DONE (t=3.17s).
Accumulating evaluation results...
DONE (t=2.86s).
Accumulating evaluation results...
DONE (t=3.19s).
Accumulating evaluation results...
DONE (t=2.88s).
Accumulating evaluation results...
DONE (t=0.88s).
DONE (t=0.89s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.127
DONE (t=0.89s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.249
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.127
DONE (t=0.89s).
DONE (t=0.89s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.127
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.118
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.249
DONE (t=0.90s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.127
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.127
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.249
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.118
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.118
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.249
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.249
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.127
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.203
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.148
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.118
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.118
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.348
Current AP: 0.12732 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.249
DONE (t=0.89s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.203
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.148
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.118
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.203
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
DONE (t=0.90s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.348
Current AP: 0.12732 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.148
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.127
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.348
Current AP: 0.12732 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.203
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.203
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.148
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.148
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.127
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.249
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.348
Current AP: 0.12732 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.348
Current AP: 0.12732 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.203
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.118
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.249
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.148
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.348
Current AP: 0.12732 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.118
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.203
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.148
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.348
Current AP: 0.12732 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.203
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.148
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.215
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.225
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.238
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.348
Current AP: 0.12732 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757184.517735481 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757184.518385410 (train.py:333) eval_accuracy: {"epoch": 32, "value": 0.12731700863042278}

:::MLPv0.5.0 ssd 1541757184.518856525 (train.py:336) eval_iteration_accuracy: {"epoch": 32, "value": 0.12731700863042278}

:::MLPv0.5.0 ssd 1541757184.519315004 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757184.519750595 (train.py:338) eval_stop: 32
Iteration:   1880, Loss function: 4.014, Average Loss: 4.266, avg. samples / sec: 2983.98
Iteration:   1880, Loss function: 4.389, Average Loss: 4.277, avg. samples / sec: 2984.10
Iteration:   1880, Loss function: 4.438, Average Loss: 4.277, avg. samples / sec: 2983.92
Iteration:   1880, Loss function: 3.883, Average Loss: 4.265, avg. samples / sec: 2983.82
Iteration:   1880, Loss function: 4.323, Average Loss: 4.276, avg. samples / sec: 2983.97
Iteration:   1880, Loss function: 4.334, Average Loss: 4.273, avg. samples / sec: 2984.00
Iteration:   1880, Loss function: 4.326, Average Loss: 4.273, avg. samples / sec: 2983.32
Iteration:   1880, Loss function: 4.425, Average Loss: 4.270, avg. samples / sec: 2983.85
Iteration:   1900, Loss function: 4.655, Average Loss: 4.264, avg. samples / sec: 24465.94
Iteration:   1900, Loss function: 4.384, Average Loss: 4.271, avg. samples / sec: 24510.00
Iteration:   1900, Loss function: 4.058, Average Loss: 4.273, avg. samples / sec: 24507.83
Iteration:   1900, Loss function: 4.922, Average Loss: 4.276, avg. samples / sec: 24472.70
Iteration:   1900, Loss function: 4.028, Average Loss: 4.268, avg. samples / sec: 24505.37
Iteration:   1900, Loss function: 4.081, Average Loss: 4.263, avg. samples / sec: 24478.65
Iteration:   1900, Loss function: 3.876, Average Loss: 4.273, avg. samples / sec: 24481.72
Iteration:   1900, Loss function: 3.924, Average Loss: 4.275, avg. samples / sec: 24450.13

:::MLPv0.5.0 ssd 1541757187.642121553 (train.py:553) train_epoch: 33
Iteration:   1920, Loss function: 4.271, Average Loss: 4.259, avg. samples / sec: 24508.24
Iteration:   1920, Loss function: 4.146, Average Loss: 4.266, avg. samples / sec: 24510.31
Iteration:   1920, Loss function: 4.736, Average Loss: 4.270, avg. samples / sec: 24507.81
Iteration:   1920, Loss function: 4.414, Average Loss: 4.275, avg. samples / sec: 24505.13
Iteration:   1920, Loss function: 4.019, Average Loss: 4.258, avg. samples / sec: 24494.25
Iteration:   1920, Loss function: 4.144, Average Loss: 4.271, avg. samples / sec: 24494.52
Iteration:   1920, Loss function: 4.217, Average Loss: 4.272, avg. samples / sec: 24484.64
Iteration:   1920, Loss function: 3.889, Average Loss: 4.271, avg. samples / sec: 24448.25
Iteration:   1940, Loss function: 3.506, Average Loss: 4.255, avg. samples / sec: 24519.94
Iteration:   1940, Loss function: 4.572, Average Loss: 4.264, avg. samples / sec: 24523.29
Iteration:   1940, Loss function: 4.277, Average Loss: 4.268, avg. samples / sec: 24522.52
Iteration:   1940, Loss function: 4.010, Average Loss: 4.271, avg. samples / sec: 24525.39
Iteration:   1940, Loss function: 3.975, Average Loss: 4.271, avg. samples / sec: 24541.84
Iteration:   1940, Loss function: 4.588, Average Loss: 4.272, avg. samples / sec: 24508.62
Iteration:   1940, Loss function: 4.284, Average Loss: 4.268, avg. samples / sec: 24537.93
Iteration:   1940, Loss function: 4.335, Average Loss: 4.257, avg. samples / sec: 24480.63
Iteration:   1960, Loss function: 4.089, Average Loss: 4.263, avg. samples / sec: 24533.20
Iteration:   1960, Loss function: 4.032, Average Loss: 4.267, avg. samples / sec: 24532.99
Iteration:   1960, Loss function: 4.098, Average Loss: 4.257, avg. samples / sec: 24554.80
Iteration:   1960, Loss function: 4.485, Average Loss: 4.271, avg. samples / sec: 24512.21
Iteration:   1960, Loss function: 3.967, Average Loss: 4.253, avg. samples / sec: 24477.75
Iteration:   1960, Loss function: 4.681, Average Loss: 4.274, avg. samples / sec: 24517.58
Iteration:   1960, Loss function: 3.987, Average Loss: 4.266, avg. samples / sec: 24475.97
Iteration:   1960, Loss function: 5.010, Average Loss: 4.266, avg. samples / sec: 24510.98

:::MLPv0.5.0 ssd 1541757192.486775637 (train.py:553) train_epoch: 34
Iteration:   1980, Loss function: 4.039, Average Loss: 4.262, avg. samples / sec: 24519.32
Iteration:   1980, Loss function: 3.868, Average Loss: 4.266, avg. samples / sec: 24515.90
Iteration:   1980, Loss function: 4.096, Average Loss: 4.265, avg. samples / sec: 24575.67
Iteration:   1980, Loss function: 4.420, Average Loss: 4.257, avg. samples / sec: 24542.74
Iteration:   1980, Loss function: 4.015, Average Loss: 4.271, avg. samples / sec: 24534.21
Iteration:   1980, Loss function: 4.052, Average Loss: 4.263, avg. samples / sec: 24533.42
Iteration:   1980, Loss function: 4.691, Average Loss: 4.251, avg. samples / sec: 24523.09
Iteration:   1980, Loss function: 3.844, Average Loss: 4.273, avg. samples / sec: 24522.73
Iteration:   2000, Loss function: 4.433, Average Loss: 4.247, avg. samples / sec: 24579.67
Iteration:   2000, Loss function: 3.963, Average Loss: 4.263, avg. samples / sec: 24531.45
Iteration:   2000, Loss function: 3.769, Average Loss: 4.260, avg. samples / sec: 24518.94
Iteration:   2000, Loss function: 4.079, Average Loss: 4.262, avg. samples / sec: 24508.10
Iteration:   2000, Loss function: 3.622, Average Loss: 4.267, avg. samples / sec: 24519.31
Iteration:   2000, Loss function: 3.824, Average Loss: 4.258, avg. samples / sec: 24527.89
Iteration:   2000, Loss function: 3.889, Average Loss: 4.254, avg. samples / sec: 24479.55
Iteration:   2000, Loss function: 4.138, Average Loss: 4.270, avg. samples / sec: 24511.32
Iteration:   2020, Loss function: 4.424, Average Loss: 4.261, avg. samples / sec: 24486.46
Iteration:   2020, Loss function: 4.155, Average Loss: 4.267, avg. samples / sec: 24507.39
Iteration:   2020, Loss function: 4.635, Average Loss: 4.244, avg. samples / sec: 24465.47
Iteration:   2020, Loss function: 4.754, Average Loss: 4.259, avg. samples / sec: 24455.13
Iteration:   2020, Loss function: 4.275, Average Loss: 4.267, avg. samples / sec: 24511.54
Iteration:   2020, Loss function: 4.404, Average Loss: 4.258, avg. samples / sec: 24480.21
Iteration:   2020, Loss function: 3.632, Average Loss: 4.253, avg. samples / sec: 24496.92
Iteration:   2020, Loss function: 3.998, Average Loss: 4.258, avg. samples / sec: 24454.99

:::MLPv0.5.0 ssd 1541757197.334043503 (train.py:553) train_epoch: 35
Iteration:   2040, Loss function: 4.045, Average Loss: 4.252, avg. samples / sec: 24536.86
Iteration:   2040, Loss function: 3.761, Average Loss: 4.265, avg. samples / sec: 24466.70
Iteration:   2040, Loss function: 4.050, Average Loss: 4.257, avg. samples / sec: 24491.84
Iteration:   2040, Loss function: 4.093, Average Loss: 4.255, avg. samples / sec: 24504.32
Iteration:   2040, Loss function: 4.920, Average Loss: 4.266, avg. samples / sec: 24476.18
Iteration:   2040, Loss function: 4.395, Average Loss: 4.242, avg. samples / sec: 24450.54
Iteration:   2040, Loss function: 5.031, Average Loss: 4.255, avg. samples / sec: 24470.01
Iteration:   2040, Loss function: 4.395, Average Loss: 4.260, avg. samples / sec: 24411.45
Iteration:   2060, Loss function: 4.304, Average Loss: 4.258, avg. samples / sec: 24583.65
Iteration:   2060, Loss function: 3.897, Average Loss: 4.257, avg. samples / sec: 24616.68
Iteration:   2060, Loss function: 4.361, Average Loss: 4.251, avg. samples / sec: 24526.23
Iteration:   2060, Loss function: 3.854, Average Loss: 4.262, avg. samples / sec: 24544.82
Iteration:   2060, Loss function: 3.543, Average Loss: 4.239, avg. samples / sec: 24559.68
Iteration:   2060, Loss function: 4.115, Average Loss: 4.252, avg. samples / sec: 24526.36
Iteration:   2060, Loss function: 4.280, Average Loss: 4.253, avg. samples / sec: 24546.43
Iteration:   2060, Loss function: 4.309, Average Loss: 4.263, avg. samples / sec: 24529.34

:::MLPv0.5.0 ssd 1541757202.177965403 (train.py:553) train_epoch: 36
Iteration:   2080, Loss function: 4.613, Average Loss: 4.258, avg. samples / sec: 24538.42
Iteration:   2080, Loss function: 3.818, Average Loss: 4.253, avg. samples / sec: 24509.44
Iteration:   2080, Loss function: 3.998, Average Loss: 4.245, avg. samples / sec: 24514.53
Iteration:   2080, Loss function: 3.778, Average Loss: 4.249, avg. samples / sec: 24551.27
Iteration:   2080, Loss function: 4.610, Average Loss: 4.237, avg. samples / sec: 24514.66
Iteration:   2080, Loss function: 4.086, Average Loss: 4.248, avg. samples / sec: 24518.50
Iteration:   2080, Loss function: 4.343, Average Loss: 4.254, avg. samples / sec: 24446.02
Iteration:   2080, Loss function: 3.802, Average Loss: 4.259, avg. samples / sec: 24514.87
Iteration:   2100, Loss function: 3.868, Average Loss: 4.252, avg. samples / sec: 24538.11
Iteration:   2100, Loss function: 4.088, Average Loss: 4.253, avg. samples / sec: 24581.35
Iteration:   2100, Loss function: 4.021, Average Loss: 4.262, avg. samples / sec: 24572.62
Iteration:   2100, Loss function: 3.657, Average Loss: 4.260, avg. samples / sec: 24509.15
Iteration:   2100, Loss function: 3.508, Average Loss: 4.248, avg. samples / sec: 24516.88
Iteration:   2100, Loss function: 4.205, Average Loss: 4.239, avg. samples / sec: 24507.61
Iteration:   2100, Loss function: 3.953, Average Loss: 4.244, avg. samples / sec: 24483.47
Iteration:   2100, Loss function: 4.166, Average Loss: 4.248, avg. samples / sec: 24522.74
Iteration:   2120, Loss function: 3.714, Average Loss: 4.248, avg. samples / sec: 24576.34
Iteration:   2120, Loss function: 4.327, Average Loss: 4.237, avg. samples / sec: 24627.25
Iteration:   2120, Loss function: 4.367, Average Loss: 4.249, avg. samples / sec: 24565.85
Iteration:   2120, Loss function: 3.821, Average Loss: 4.241, avg. samples / sec: 24568.86
Iteration:   2120, Loss function: 4.204, Average Loss: 4.244, avg. samples / sec: 24583.80
Iteration:   2120, Loss function: 4.326, Average Loss: 4.256, avg. samples / sec: 24540.17
Iteration:   2120, Loss function: 4.066, Average Loss: 4.234, avg. samples / sec: 24567.09
Iteration:   2120, Loss function: 4.205, Average Loss: 4.256, avg. samples / sec: 24527.49

:::MLPv0.5.0 ssd 1541757207.387977123 (train.py:553) train_epoch: 37
Iteration:   2140, Loss function: 3.978, Average Loss: 4.234, avg. samples / sec: 20077.67
Iteration:   2140, Loss function: 4.660, Average Loss: 4.243, avg. samples / sec: 20107.23
Iteration:   2140, Loss function: 4.511, Average Loss: 4.244, avg. samples / sec: 20071.57
Iteration:   2140, Loss function: 4.082, Average Loss: 4.253, avg. samples / sec: 20108.45
Iteration:   2140, Loss function: 3.377, Average Loss: 4.236, avg. samples / sec: 20087.07
Iteration:   2140, Loss function: 4.029, Average Loss: 4.229, avg. samples / sec: 20097.76
Iteration:   2140, Loss function: 4.274, Average Loss: 4.243, avg. samples / sec: 20051.50
Iteration:   2140, Loss function: 4.041, Average Loss: 4.250, avg. samples / sec: 20076.90
Iteration:   2160, Loss function: 4.418, Average Loss: 4.226, avg. samples / sec: 24561.30
Iteration:   2160, Loss function: 4.441, Average Loss: 4.251, avg. samples / sec: 24541.84
Iteration:   2160, Loss function: 4.194, Average Loss: 4.234, avg. samples / sec: 24534.29
Iteration:   2160, Loss function: 3.564, Average Loss: 4.240, avg. samples / sec: 24573.77
Iteration:   2160, Loss function: 4.219, Average Loss: 4.242, avg. samples / sec: 24531.31
Iteration:   2160, Loss function: 4.188, Average Loss: 4.240, avg. samples / sec: 24523.70
Iteration:   2160, Loss function: 4.501, Average Loss: 4.235, avg. samples / sec: 24529.84
Iteration:   2160, Loss function: 4.138, Average Loss: 4.247, avg. samples / sec: 24536.72
Iteration:   2180, Loss function: 4.318, Average Loss: 4.243, avg. samples / sec: 24518.29
Iteration:   2180, Loss function: 4.224, Average Loss: 4.232, avg. samples / sec: 24531.61
Iteration:   2180, Loss function: 3.618, Average Loss: 4.231, avg. samples / sec: 24477.67
Iteration:   2180, Loss function: 4.395, Average Loss: 4.224, avg. samples / sec: 24474.60
Iteration:   2180, Loss function: 3.814, Average Loss: 4.238, avg. samples / sec: 24493.22
Iteration:   2180, Loss function: 4.131, Average Loss: 4.245, avg. samples / sec: 24528.72
Iteration:   2180, Loss function: 4.479, Average Loss: 4.248, avg. samples / sec: 24464.11
Iteration:   2180, Loss function: 3.564, Average Loss: 4.239, avg. samples / sec: 24455.12

:::MLPv0.5.0 ssd 1541757212.150840759 (train.py:553) train_epoch: 38
Iteration:   2200, Loss function: 3.979, Average Loss: 4.243, avg. samples / sec: 24516.36
Iteration:   2200, Loss function: 4.262, Average Loss: 4.235, avg. samples / sec: 24498.03
Iteration:   2200, Loss function: 3.597, Average Loss: 4.227, avg. samples / sec: 24439.15
Iteration:   2200, Loss function: 4.039, Average Loss: 4.235, avg. samples / sec: 24474.94
Iteration:   2200, Loss function: 3.958, Average Loss: 4.240, avg. samples / sec: 24469.67
Iteration:   2200, Loss function: 3.994, Average Loss: 4.238, avg. samples / sec: 24421.63
Iteration:   2200, Loss function: 3.789, Average Loss: 4.226, avg. samples / sec: 24449.40
Iteration:   2200, Loss function: 4.104, Average Loss: 4.220, avg. samples / sec: 24448.62
Iteration:   2220, Loss function: 4.173, Average Loss: 4.215, avg. samples / sec: 24585.17
Iteration:   2220, Loss function: 3.965, Average Loss: 4.224, avg. samples / sec: 24555.73
Iteration:   2220, Loss function: 4.115, Average Loss: 4.239, avg. samples / sec: 24516.30
Iteration:   2220, Loss function: 4.029, Average Loss: 4.232, avg. samples / sec: 24541.74
Iteration:   2220, Loss function: 4.012, Average Loss: 4.228, avg. samples / sec: 24520.56
Iteration:   2220, Loss function: 4.060, Average Loss: 4.223, avg. samples / sec: 24540.63
Iteration:   2220, Loss function: 4.138, Average Loss: 4.235, avg. samples / sec: 24517.21
Iteration:   2220, Loss function: 3.864, Average Loss: 4.235, avg. samples / sec: 24504.64
Iteration:   2240, Loss function: 4.314, Average Loss: 4.240, avg. samples / sec: 24470.99
Iteration:   2240, Loss function: 3.963, Average Loss: 4.232, avg. samples / sec: 24455.96
Iteration:   2240, Loss function: 4.115, Average Loss: 4.225, avg. samples / sec: 24442.36
Iteration:   2240, Loss function: 4.198, Average Loss: 4.233, avg. samples / sec: 24497.51
Iteration:   2240, Loss function: 4.348, Average Loss: 4.234, avg. samples / sec: 24487.13
Iteration:   2240, Loss function: 4.723, Average Loss: 4.219, avg. samples / sec: 24424.95
Iteration:   2240, Loss function: 4.555, Average Loss: 4.224, avg. samples / sec: 24459.70
Iteration:   2240, Loss function: 4.135, Average Loss: 4.229, avg. samples / sec: 24430.96

:::MLPv0.5.0 ssd 1541757217.003687382 (train.py:553) train_epoch: 39
Iteration:   2260, Loss function: 4.690, Average Loss: 4.222, avg. samples / sec: 24507.53
Iteration:   2260, Loss function: 3.850, Average Loss: 4.236, avg. samples / sec: 24483.68
Iteration:   2260, Loss function: 4.155, Average Loss: 4.231, avg. samples / sec: 24493.85
Iteration:   2260, Loss function: 4.173, Average Loss: 4.232, avg. samples / sec: 24487.34
Iteration:   2260, Loss function: 4.500, Average Loss: 4.227, avg. samples / sec: 24516.67
Iteration:   2260, Loss function: 3.718, Average Loss: 4.216, avg. samples / sec: 24462.54
Iteration:   2260, Loss function: 3.864, Average Loss: 4.229, avg. samples / sec: 24451.27
Iteration:   2260, Loss function: 4.798, Average Loss: 4.222, avg. samples / sec: 24456.77
Iteration:   2280, Loss function: 3.920, Average Loss: 4.236, avg. samples / sec: 24502.34
Iteration:   2280, Loss function: 4.298, Average Loss: 4.231, avg. samples / sec: 24507.32
Iteration:   2280, Loss function: 3.516, Average Loss: 4.223, avg. samples / sec: 24489.12
Iteration:   2280, Loss function: 4.239, Average Loss: 4.233, avg. samples / sec: 24506.67
Iteration:   2280, Loss function: 4.088, Average Loss: 4.228, avg. samples / sec: 24519.23
Iteration:   2280, Loss function: 3.817, Average Loss: 4.223, avg. samples / sec: 24544.29
Iteration:   2280, Loss function: 4.107, Average Loss: 4.217, avg. samples / sec: 24529.67
Iteration:   2280, Loss function: 3.956, Average Loss: 4.228, avg. samples / sec: 24520.30
Iteration:   2300, Loss function: 3.580, Average Loss: 4.228, avg. samples / sec: 24501.20
Iteration:   2300, Loss function: 4.057, Average Loss: 4.215, avg. samples / sec: 24509.03
Iteration:   2300, Loss function: 3.695, Average Loss: 4.223, avg. samples / sec: 24489.59
Iteration:   2300, Loss function: 4.488, Average Loss: 4.218, avg. samples / sec: 24484.68
Iteration:   2300, Loss function: 3.445, Average Loss: 4.233, avg. samples / sec: 24463.43
Iteration:   2300, Loss function: 4.668, Average Loss: 4.221, avg. samples / sec: 24477.23
Iteration:   2300, Loss function: 4.082, Average Loss: 4.226, avg. samples / sec: 24438.54
Iteration:   2300, Loss function: 4.202, Average Loss: 4.224, avg. samples / sec: 24471.77

:::MLPv0.5.0 ssd 1541757221.853290319 (train.py:553) train_epoch: 40
Iteration:   2320, Loss function: 4.262, Average Loss: 4.223, avg. samples / sec: 24530.79
Iteration:   2320, Loss function: 4.429, Average Loss: 4.223, avg. samples / sec: 24473.88
Iteration:   2320, Loss function: 4.551, Average Loss: 4.219, avg. samples / sec: 24492.34
Iteration:   2320, Loss function: 4.653, Average Loss: 4.212, avg. samples / sec: 24461.48
Iteration:   2320, Loss function: 3.683, Average Loss: 4.213, avg. samples / sec: 24466.47
Iteration:   2320, Loss function: 4.274, Average Loss: 4.219, avg. samples / sec: 24449.55
Iteration:   2320, Loss function: 3.907, Average Loss: 4.219, avg. samples / sec: 24481.44
Iteration:   2320, Loss function: 3.763, Average Loss: 4.226, avg. samples / sec: 24448.23
Iteration:   2340, Loss function: 3.959, Average Loss: 4.221, avg. samples / sec: 24508.94
Iteration:   2340, Loss function: 4.415, Average Loss: 4.223, avg. samples / sec: 24508.58
Iteration:   2340, Loss function: 3.560, Average Loss: 4.210, avg. samples / sec: 24509.77
Iteration:   2340, Loss function: 3.781, Average Loss: 4.212, avg. samples / sec: 24512.27
Iteration:   2340, Loss function: 3.861, Average Loss: 4.225, avg. samples / sec: 24537.06
Iteration:   2340, Loss function: 3.984, Average Loss: 4.217, avg. samples / sec: 24500.49
Iteration:   2340, Loss function: 3.883, Average Loss: 4.218, avg. samples / sec: 24527.79
Iteration:   2340, Loss function: 4.235, Average Loss: 4.216, avg. samples / sec: 24499.34
Iteration:   2360, Loss function: 3.378, Average Loss: 4.214, avg. samples / sec: 24534.78
Iteration:   2360, Loss function: 4.088, Average Loss: 4.209, avg. samples / sec: 24506.12
Iteration:   2360, Loss function: 3.755, Average Loss: 4.220, avg. samples / sec: 24485.14
Iteration:   2360, Loss function: 4.069, Average Loss: 4.218, avg. samples / sec: 24458.11
Iteration:   2360, Loss function: 3.851, Average Loss: 4.213, avg. samples / sec: 24504.65
Iteration:   2360, Loss function: 3.710, Average Loss: 4.208, avg. samples / sec: 24473.39
Iteration:   2360, Loss function: 3.689, Average Loss: 4.212, avg. samples / sec: 24478.41
Iteration:   2360, Loss function: 3.853, Average Loss: 4.216, avg. samples / sec: 24435.64

:::MLPv0.5.0 ssd 1541757226.701053619 (train.py:553) train_epoch: 41
Iteration:   2380, Loss function: 3.684, Average Loss: 4.213, avg. samples / sec: 24574.33
Iteration:   2380, Loss function: 4.073, Average Loss: 4.210, avg. samples / sec: 24524.99
Iteration:   2380, Loss function: 4.034, Average Loss: 4.203, avg. samples / sec: 24536.89
Iteration:   2380, Loss function: 3.931, Average Loss: 4.202, avg. samples / sec: 24558.41
Iteration:   2380, Loss function: 3.945, Average Loss: 4.216, avg. samples / sec: 24528.60
Iteration:   2380, Loss function: 3.663, Average Loss: 4.205, avg. samples / sec: 24523.62
Iteration:   2380, Loss function: 4.253, Average Loss: 4.212, avg. samples / sec: 24540.74
Iteration:   2380, Loss function: 3.979, Average Loss: 4.209, avg. samples / sec: 24517.89
Iteration:   2400, Loss function: 3.727, Average Loss: 4.209, avg. samples / sec: 24483.22
Iteration:   2400, Loss function: 4.213, Average Loss: 4.209, avg. samples / sec: 24529.15
Iteration:   2400, Loss function: 4.164, Average Loss: 4.205, avg. samples / sec: 24526.32
Iteration:   2400, Loss function: 3.671, Average Loss: 4.205, avg. samples / sec: 24536.01
Iteration:   2400, Loss function: 4.023, Average Loss: 4.196, avg. samples / sec: 24477.29
Iteration:   2400, Loss function: 4.368, Average Loss: 4.200, avg. samples / sec: 24461.43
Iteration:   2400, Loss function: 4.733, Average Loss: 4.212, avg. samples / sec: 24470.02
Iteration:   2400, Loss function: 3.824, Average Loss: 4.207, avg. samples / sec: 24431.55
Iteration:   2420, Loss function: 4.096, Average Loss: 4.203, avg. samples / sec: 24510.48
Iteration:   2420, Loss function: 4.183, Average Loss: 4.205, avg. samples / sec: 24497.92
Iteration:   2420, Loss function: 4.774, Average Loss: 4.205, avg. samples / sec: 24497.95
Iteration:   2420, Loss function: 4.169, Average Loss: 4.203, avg. samples / sec: 24499.84
Iteration:   2420, Loss function: 4.110, Average Loss: 4.195, avg. samples / sec: 24508.52
Iteration:   2420, Loss function: 4.046, Average Loss: 4.192, avg. samples / sec: 24486.75
Iteration:   2420, Loss function: 3.741, Average Loss: 4.204, avg. samples / sec: 24514.55
Iteration:   2420, Loss function: 4.715, Average Loss: 4.210, avg. samples / sec: 24495.17

:::MLPv0.5.0 ssd 1541757231.466427565 (train.py:553) train_epoch: 42
Iteration:   2440, Loss function: 4.048, Average Loss: 4.199, avg. samples / sec: 24480.21
Iteration:   2440, Loss function: 4.495, Average Loss: 4.200, avg. samples / sec: 24517.64
Iteration:   2440, Loss function: 4.533, Average Loss: 4.192, avg. samples / sec: 24507.13
Iteration:   2440, Loss function: 4.340, Average Loss: 4.205, avg. samples / sec: 24471.70
Iteration:   2440, Loss function: 4.127, Average Loss: 4.210, avg. samples / sec: 24522.87
Iteration:   2440, Loss function: 4.179, Average Loss: 4.204, avg. samples / sec: 24466.98
Iteration:   2440, Loss function: 4.280, Average Loss: 4.195, avg. samples / sec: 24471.18
Iteration:   2440, Loss function: 4.533, Average Loss: 4.199, avg. samples / sec: 24443.38
Iteration:   2460, Loss function: 4.357, Average Loss: 4.205, avg. samples / sec: 24474.86
Iteration:   2460, Loss function: 4.077, Average Loss: 4.200, avg. samples / sec: 24470.19
Iteration:   2460, Loss function: 4.671, Average Loss: 4.194, avg. samples / sec: 24477.95
Iteration:   2460, Loss function: 4.114, Average Loss: 4.210, avg. samples / sec: 24479.47
Iteration:   2460, Loss function: 3.702, Average Loss: 4.203, avg. samples / sec: 24509.94
Iteration:   2460, Loss function: 4.294, Average Loss: 4.199, avg. samples / sec: 24473.20
Iteration:   2460, Loss function: 4.737, Average Loss: 4.215, avg. samples / sec: 24447.28
Iteration:   2460, Loss function: 4.406, Average Loss: 4.208, avg. samples / sec: 24423.15
Iteration:   2480, Loss function: 3.469, Average Loss: 4.195, avg. samples / sec: 24499.01
Iteration:   2480, Loss function: 4.000, Average Loss: 4.207, avg. samples / sec: 24500.58
Iteration:   2480, Loss function: 4.022, Average Loss: 4.198, avg. samples / sec: 24526.22
Iteration:   2480, Loss function: 3.559, Average Loss: 4.201, avg. samples / sec: 24501.91
Iteration:   2480, Loss function: 3.925, Average Loss: 4.191, avg. samples / sec: 24500.06
Iteration:   2480, Loss function: 4.053, Average Loss: 4.204, avg. samples / sec: 24495.08
Iteration:   2480, Loss function: 3.688, Average Loss: 4.212, avg. samples / sec: 24531.48
Iteration:   2480, Loss function: 4.918, Average Loss: 4.205, avg. samples / sec: 24515.03

:::MLPv0.5.0 ssd 1541757236.318010569 (train.py:553) train_epoch: 43
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1
lr decay step #1

:::MLPv0.5.0 ssd 1541757237.657950640 (train.py:578) opt_learning_rate: 0.016
Iteration:   2500, Loss function: 3.860, Average Loss: 4.188, avg. samples / sec: 24512.75
Iteration:   2500, Loss function: 4.482, Average Loss: 4.198, avg. samples / sec: 24508.03
Iteration:   2500, Loss function: 4.151, Average Loss: 4.183, avg. samples / sec: 24506.52
Iteration:   2500, Loss function: 3.826, Average Loss: 4.197, avg. samples / sec: 24555.71
Iteration:   2500, Loss function: 3.847, Average Loss: 4.191, avg. samples / sec: 24497.64
Iteration:   2500, Loss function: 4.297, Average Loss: 4.206, avg. samples / sec: 24475.15
Iteration:   2500, Loss function: 3.566, Average Loss: 4.201, avg. samples / sec: 24463.77
Iteration:   2500, Loss function: 3.678, Average Loss: 4.195, avg. samples / sec: 24464.45

































































:::MLPv0.5.0 ssd 1541757237.741299391 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757237.741831064 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757237.742286921 (train.py:220) eval_start: 43
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.18 s
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(374262, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(374262, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
0/374262
Loading and preparing results...
(374262, 7)
Loading and preparing results...
Converting ndarray to lists...
0/374262
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(374262, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(374262, 7)
(374262, 7)
Converting ndarray to lists...
(374262, 7)
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(374262, 7)
Loading and preparing results...
Converting ndarray to lists...
(374262, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/374262
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/374262
Converting ndarray to lists...
0/374262
Converting ndarray to lists...
0/374262
Loading and preparing results...
Loading and preparing results...
0/374262
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(374262, 7)
Converting ndarray to lists...
0/374262
0/374262
(374262, 7)
Loading and preparing results...
Loading and preparing results...
(374262, 7)
(374262, 7)
(374262, 7)
(374262, 7)
(374262, 7)
(374262, 7)
(374262, 7)
Converting ndarray to lists...
(374262, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/374262
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
0/374262
Converting ndarray to lists...
Converting ndarray to lists...
0/374262
(374262, 7)
(374262, 7)
Loading and preparing results...
(374262, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/374262
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(374262, 7)
Loading and preparing results...
0/374262
Converting ndarray to lists...
0/374262
Loading and preparing results...
(374262, 7)
0/374262
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
(374262, 7)
0/374262
Converting ndarray to lists...
(374262, 7)
Loading and preparing results...
(374262, 7)
Loading and preparing results...
Converting ndarray to lists...
0/374262
0/374262
(374262, 7)
(374262, 7)
Converting ndarray to lists...
(374262, 7)
0/374262
Converting ndarray to lists...
0/374262
Converting ndarray to lists...
(374262, 7)
(374262, 7)
Loading and preparing results...
(374262, 7)
Converting ndarray to lists...
(374262, 7)
0/374262
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(374262, 7)
0/374262
Loading and preparing results...
0/374262
(374262, 7)
(374262, 7)
(374262, 7)
0/374262
Loading and preparing results...
Converting ndarray to lists...
(374262, 7)
0/374262
0/374262
0/374262
Loading and preparing results...
0/374262
0/374262
0/374262
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/374262
0/374262
Loading and preparing results...
Loading and preparing results...
(374262, 7)
(374262, 7)
0/374262
Converting ndarray to lists...
(374262, 7)
(374262, 7)
0/374262
0/374262
Converting ndarray to lists...
0/374262
Loading and preparing results...
0/374262
Converting ndarray to lists...
(374262, 7)
(374262, 7)
0/374262
0/374262
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(374262, 7)
(374262, 7)
Converting ndarray to lists...
(374262, 7)
0/374262
0/374262
Loading and preparing results...
Converting ndarray to lists...
0/374262
0/374262
Converting ndarray to lists...
(374262, 7)
(374262, 7)
Converting ndarray to lists...
0/374262
(374262, 7)
(374262, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(374262, 7)
(374262, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(374262, 7)
0/374262
Converting ndarray to lists...
0/374262
0/374262
(374262, 7)
0/374262
0/374262
0/374262
0/374262
(374262, 7)
0/374262
(374262, 7)
0/374262
(374262, 7)
(374262, 7)
0/374262
Converting ndarray to lists...
0/374262
(374262, 7)
0/374262
0/374262
(374262, 7)
Converting ndarray to lists...
0/374262
0/374262
0/374262
(374262, 7)
0/374262
(374262, 7)
0/374262
0/374262
DONE (t=2.19s)
creating index...
DONE (t=2.19s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.20s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.21s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.22s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.23s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.24s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.25s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.26s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.27s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.28s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.29s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.30s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
DONE (t=2.31s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.55s).
Accumulating evaluation results...
DONE (t=3.56s).
Accumulating evaluation results...
DONE (t=3.60s).
Accumulating evaluation results...
DONE (t=3.57s).
Accumulating evaluation results...
DONE (t=3.56s).
Accumulating evaluation results...
DONE (t=3.56s).
Accumulating evaluation results...
DONE (t=3.59s).
Accumulating evaluation results...
DONE (t=3.59s).
Accumulating evaluation results...
DONE (t=1.29s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
DONE (t=1.29s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
DONE (t=1.31s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
DONE (t=1.31s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.232
DONE (t=1.31s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.144
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.144
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393
Current AP: 0.14365 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038
DONE (t=1.31s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
DONE (t=1.30s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.232
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.144
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393
Current AP: 0.14365 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.232
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.232
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.232
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393
Current AP: 0.14365 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
DONE (t=1.32s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393
Current AP: 0.14365 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393
Current AP: 0.14365 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.232
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.144
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.232
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393
Current AP: 0.14365 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.271
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393
Current AP: 0.14365 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.140
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.038
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.232
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.162
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.233
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.244
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.066
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.393
Current AP: 0.14365 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757249.377088070 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757249.377714634 (train.py:333) eval_accuracy: {"epoch": 43, "value": 0.14365229269197666}

:::MLPv0.5.0 ssd 1541757249.378191710 (train.py:336) eval_iteration_accuracy: {"epoch": 43, "value": 0.14365229269197666}

:::MLPv0.5.0 ssd 1541757249.378643513 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757249.379089117 (train.py:338) eval_stop: 43
Iteration:   2520, Loss function: 3.953, Average Loss: 4.177, avg. samples / sec: 2976.17
Iteration:   2520, Loss function: 3.211, Average Loss: 4.189, avg. samples / sec: 2976.35
Iteration:   2520, Loss function: 3.603, Average Loss: 4.188, avg. samples / sec: 2976.24
Iteration:   2520, Loss function: 3.501, Average Loss: 4.180, avg. samples / sec: 2976.32
Iteration:   2520, Loss function: 3.528, Average Loss: 4.196, avg. samples / sec: 2976.43
Iteration:   2520, Loss function: 3.559, Average Loss: 4.191, avg. samples / sec: 2976.08
Iteration:   2520, Loss function: 3.410, Average Loss: 4.175, avg. samples / sec: 2975.37
Iteration:   2520, Loss function: 3.656, Average Loss: 4.183, avg. samples / sec: 2976.00
Iteration:   2540, Loss function: 3.831, Average Loss: 4.164, avg. samples / sec: 24493.82
Iteration:   2540, Loss function: 3.610, Average Loss: 4.168, avg. samples / sec: 24500.97
Iteration:   2540, Loss function: 3.116, Average Loss: 4.177, avg. samples / sec: 24493.68
Iteration:   2540, Loss function: 3.750, Average Loss: 4.167, avg. samples / sec: 24549.54
Iteration:   2540, Loss function: 3.233, Average Loss: 4.175, avg. samples / sec: 24459.99
Iteration:   2540, Loss function: 3.264, Average Loss: 4.182, avg. samples / sec: 24477.92
Iteration:   2540, Loss function: 3.391, Average Loss: 4.179, avg. samples / sec: 24497.80
Iteration:   2540, Loss function: 3.759, Average Loss: 4.165, avg. samples / sec: 24502.76

:::MLPv0.5.0 ssd 1541757253.257428885 (train.py:553) train_epoch: 44
Iteration:   2560, Loss function: 3.261, Average Loss: 4.151, avg. samples / sec: 24519.37
Iteration:   2560, Loss function: 3.292, Average Loss: 4.160, avg. samples / sec: 24546.07
Iteration:   2560, Loss function: 3.252, Average Loss: 4.153, avg. samples / sec: 24520.44
Iteration:   2560, Loss function: 3.953, Average Loss: 4.151, avg. samples / sec: 24506.62
Iteration:   2560, Loss function: 3.706, Average Loss: 4.167, avg. samples / sec: 24549.20
Iteration:   2560, Loss function: 3.759, Average Loss: 4.151, avg. samples / sec: 24558.89
Iteration:   2560, Loss function: 4.071, Average Loss: 4.162, avg. samples / sec: 24488.10
Iteration:   2560, Loss function: 2.921, Average Loss: 4.162, avg. samples / sec: 24525.78
Iteration:   2580, Loss function: 3.207, Average Loss: 4.135, avg. samples / sec: 24488.04
Iteration:   2580, Loss function: 3.332, Average Loss: 4.136, avg. samples / sec: 24490.23
Iteration:   2580, Loss function: 3.791, Average Loss: 4.138, avg. samples / sec: 24493.90
Iteration:   2580, Loss function: 3.448, Average Loss: 4.147, avg. samples / sec: 24510.81
Iteration:   2580, Loss function: 4.106, Average Loss: 4.149, avg. samples / sec: 24483.48
Iteration:   2580, Loss function: 3.778, Average Loss: 4.153, avg. samples / sec: 24483.98
Iteration:   2580, Loss function: 3.547, Average Loss: 4.138, avg. samples / sec: 24470.73
Iteration:   2580, Loss function: 3.313, Average Loss: 4.147, avg. samples / sec: 24497.58

:::MLPv0.5.0 ssd 1541757258.108509302 (train.py:553) train_epoch: 45
Iteration:   2600, Loss function: 3.661, Average Loss: 4.120, avg. samples / sec: 24441.52
Iteration:   2600, Loss function: 3.671, Average Loss: 4.122, avg. samples / sec: 24447.53
Iteration:   2600, Loss function: 3.810, Average Loss: 4.132, avg. samples / sec: 24447.90
Iteration:   2600, Loss function: 3.737, Average Loss: 4.139, avg. samples / sec: 24453.20
Iteration:   2600, Loss function: 3.286, Average Loss: 4.122, avg. samples / sec: 24457.91
Iteration:   2600, Loss function: 3.083, Average Loss: 4.132, avg. samples / sec: 24454.23
Iteration:   2600, Loss function: 3.338, Average Loss: 4.134, avg. samples / sec: 24420.46
Iteration:   2600, Loss function: 3.264, Average Loss: 4.118, avg. samples / sec: 24412.34
Iteration:   2620, Loss function: 3.202, Average Loss: 4.106, avg. samples / sec: 24513.72
Iteration:   2620, Loss function: 3.143, Average Loss: 4.107, avg. samples / sec: 24511.32
Iteration:   2620, Loss function: 3.242, Average Loss: 4.118, avg. samples / sec: 24530.43
Iteration:   2620, Loss function: 3.099, Average Loss: 4.120, avg. samples / sec: 24536.65
Iteration:   2620, Loss function: 3.201, Average Loss: 4.123, avg. samples / sec: 24510.01
Iteration:   2620, Loss function: 2.800, Average Loss: 4.108, avg. samples / sec: 24508.56
Iteration:   2620, Loss function: 3.340, Average Loss: 4.102, avg. samples / sec: 24514.84
Iteration:   2620, Loss function: 3.193, Average Loss: 4.119, avg. samples / sec: 24479.70
Iteration:   2640, Loss function: 3.381, Average Loss: 4.092, avg. samples / sec: 24575.31
Iteration:   2640, Loss function: 3.470, Average Loss: 4.090, avg. samples / sec: 24568.01
Iteration:   2640, Loss function: 3.522, Average Loss: 4.104, avg. samples / sec: 24604.00
Iteration:   2640, Loss function: 2.917, Average Loss: 4.107, avg. samples / sec: 24568.85
Iteration:   2640, Loss function: 3.562, Average Loss: 4.088, avg. samples / sec: 24570.86
Iteration:   2640, Loss function: 3.011, Average Loss: 4.105, avg. samples / sec: 24540.64
Iteration:   2640, Loss function: 3.824, Average Loss: 4.095, avg. samples / sec: 24550.63
Iteration:   2640, Loss function: 3.150, Average Loss: 4.103, avg. samples / sec: 24522.80

:::MLPv0.5.0 ssd 1541757262.867908955 (train.py:553) train_epoch: 46
Iteration:   2660, Loss function: 3.344, Average Loss: 4.077, avg. samples / sec: 24496.98
Iteration:   2660, Loss function: 3.329, Average Loss: 4.075, avg. samples / sec: 24501.05
Iteration:   2660, Loss function: 3.191, Average Loss: 4.090, avg. samples / sec: 24507.06
Iteration:   2660, Loss function: 3.212, Average Loss: 4.072, avg. samples / sec: 24522.34
Iteration:   2660, Loss function: 2.914, Average Loss: 4.087, avg. samples / sec: 24489.01
Iteration:   2660, Loss function: 3.361, Average Loss: 4.090, avg. samples / sec: 24498.92
Iteration:   2660, Loss function: 3.184, Average Loss: 4.082, avg. samples / sec: 24489.96
Iteration:   2660, Loss function: 3.447, Average Loss: 4.088, avg. samples / sec: 24493.64
Iteration:   2680, Loss function: 2.924, Average Loss: 4.059, avg. samples / sec: 24539.15
Iteration:   2680, Loss function: 3.716, Average Loss: 4.064, avg. samples / sec: 24538.08
Iteration:   2680, Loss function: 2.878, Average Loss: 4.057, avg. samples / sec: 24540.10
Iteration:   2680, Loss function: 3.143, Average Loss: 4.075, avg. samples / sec: 24526.48
Iteration:   2680, Loss function: 2.944, Average Loss: 4.071, avg. samples / sec: 24527.67
Iteration:   2680, Loss function: 3.022, Average Loss: 4.066, avg. samples / sec: 24558.83
Iteration:   2680, Loss function: 3.413, Average Loss: 4.075, avg. samples / sec: 24543.32
Iteration:   2680, Loss function: 2.738, Average Loss: 4.074, avg. samples / sec: 24564.55
Iteration:   2700, Loss function: 3.466, Average Loss: 4.046, avg. samples / sec: 24548.65
Iteration:   2700, Loss function: 3.519, Average Loss: 4.050, avg. samples / sec: 24547.02
Iteration:   2700, Loss function: 3.820, Average Loss: 4.062, avg. samples / sec: 24559.93
Iteration:   2700, Loss function: 3.345, Average Loss: 4.057, avg. samples / sec: 24544.02
Iteration:   2700, Loss function: 3.406, Average Loss: 4.052, avg. samples / sec: 24538.12
Iteration:   2700, Loss function: 3.204, Average Loss: 4.043, avg. samples / sec: 24503.74
Iteration:   2700, Loss function: 3.511, Average Loss: 4.059, avg. samples / sec: 24525.61
Iteration:   2700, Loss function: 3.294, Average Loss: 4.057, avg. samples / sec: 24525.00

:::MLPv0.5.0 ssd 1541757267.711833954 (train.py:553) train_epoch: 47
Iteration:   2720, Loss function: 3.042, Average Loss: 4.034, avg. samples / sec: 24475.06
Iteration:   2720, Loss function: 3.390, Average Loss: 4.036, avg. samples / sec: 24470.86
Iteration:   2720, Loss function: 3.610, Average Loss: 4.047, avg. samples / sec: 24466.82
Iteration:   2720, Loss function: 3.373, Average Loss: 4.042, avg. samples / sec: 24506.42
Iteration:   2720, Loss function: 3.146, Average Loss: 4.026, avg. samples / sec: 24487.14
Iteration:   2720, Loss function: 3.530, Average Loss: 4.045, avg. samples / sec: 24486.80
Iteration:   2720, Loss function: 2.921, Average Loss: 4.036, avg. samples / sec: 24461.62
Iteration:   2720, Loss function: 2.484, Average Loss: 4.042, avg. samples / sec: 24454.62
Iteration:   2740, Loss function: 3.349, Average Loss: 4.023, avg. samples / sec: 24580.77
Iteration:   2740, Loss function: 3.265, Average Loss: 4.014, avg. samples / sec: 24620.06
Iteration:   2740, Loss function: 3.186, Average Loss: 4.023, avg. samples / sec: 24577.34
Iteration:   2740, Loss function: 3.523, Average Loss: 4.031, avg. samples / sec: 24581.77
Iteration:   2740, Loss function: 3.174, Average Loss: 4.019, avg. samples / sec: 24603.37
Iteration:   2740, Loss function: 3.818, Average Loss: 4.029, avg. samples / sec: 24604.56
Iteration:   2740, Loss function: 3.386, Average Loss: 4.029, avg. samples / sec: 24569.35
Iteration:   2740, Loss function: 2.936, Average Loss: 4.028, avg. samples / sec: 24546.14
Iteration:   2760, Loss function: 3.051, Average Loss: 4.010, avg. samples / sec: 24573.32
Iteration:   2760, Loss function: 2.895, Average Loss: 4.009, avg. samples / sec: 24560.32
Iteration:   2760, Loss function: 3.708, Average Loss: 4.017, avg. samples / sec: 24564.48
Iteration:   2760, Loss function: 2.987, Average Loss: 4.011, avg. samples / sec: 24569.48
Iteration:   2760, Loss function: 3.500, Average Loss: 4.001, avg. samples / sec: 24521.89
Iteration:   2760, Loss function: 3.236, Average Loss: 4.008, avg. samples / sec: 24545.08
Iteration:   2760, Loss function: 3.538, Average Loss: 4.013, avg. samples / sec: 24563.95
Iteration:   2760, Loss function: 3.621, Average Loss: 4.015, avg. samples / sec: 24547.05

:::MLPv0.5.0 ssd 1541757272.551481485 (train.py:553) train_epoch: 48
Iteration:   2780, Loss function: 3.470, Average Loss: 3.997, avg. samples / sec: 24496.02
Iteration:   2780, Loss function: 3.435, Average Loss: 3.995, avg. samples / sec: 24489.91
Iteration:   2780, Loss function: 3.484, Average Loss: 4.001, avg. samples / sec: 24552.06
Iteration:   2780, Loss function: 3.054, Average Loss: 3.998, avg. samples / sec: 24512.50
Iteration:   2780, Loss function: 3.306, Average Loss: 3.986, avg. samples / sec: 24488.27
Iteration:   2780, Loss function: 3.744, Average Loss: 4.002, avg. samples / sec: 24447.67
Iteration:   2780, Loss function: 3.257, Average Loss: 3.995, avg. samples / sec: 24453.81
Iteration:   2780, Loss function: 2.855, Average Loss: 3.990, avg. samples / sec: 24470.53
Iteration:   2800, Loss function: 3.489, Average Loss: 3.985, avg. samples / sec: 24520.37
Iteration:   2800, Loss function: 3.513, Average Loss: 3.981, avg. samples / sec: 24522.71
Iteration:   2800, Loss function: 3.558, Average Loss: 3.972, avg. samples / sec: 24561.91
Iteration:   2800, Loss function: 3.268, Average Loss: 3.985, avg. samples / sec: 24551.42
Iteration:   2800, Loss function: 2.959, Average Loss: 3.986, avg. samples / sec: 24569.43
Iteration:   2800, Loss function: 3.015, Average Loss: 3.985, avg. samples / sec: 24521.53
Iteration:   2800, Loss function: 3.397, Average Loss: 3.980, avg. samples / sec: 24543.69
Iteration:   2800, Loss function: 3.477, Average Loss: 3.977, avg. samples / sec: 24526.21

































































:::MLPv0.5.0 ssd 1541757275.893030882 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757275.893579006 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757275.894036293 (train.py:220) eval_start: 48
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.89 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.89 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.89 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.89 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.89 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.89 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.89 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 3.89 s
Converting ndarray to lists...
Loading and preparing results...
(298775, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(298775, 7)
0/298775
(298775, 7)
Loading and preparing results...
(298775, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/298775
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/298775
(298775, 7)
Loading and preparing results...
Loading and preparing results...
(298775, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(298775, 7)
Converting ndarray to lists...
0/298775
0/298775
(298775, 7)
Loading and preparing results...
(298775, 7)
(298775, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/298775
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
0/298775
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
0/298775
0/298775
Loading and preparing results...
Converting ndarray to lists...
(298775, 7)
0/298775
Loading and preparing results...
(298775, 7)
Loading and preparing results...
(298775, 7)
(298775, 7)
Converting ndarray to lists...
(298775, 7)
0/298775
Converting ndarray to lists...
Converting ndarray to lists...
(298775, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/298775
Converting ndarray to lists...
(298775, 7)
Loading and preparing results...
(298775, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(298775, 7)
0/298775
Loading and preparing results...
(298775, 7)
Converting ndarray to lists...
(298775, 7)
Converting ndarray to lists...
(298775, 7)
Converting ndarray to lists...
0/298775
Loading and preparing results...
Converting ndarray to lists...
0/298775
0/298775
Converting ndarray to lists...
(298775, 7)
Converting ndarray to lists...
(298775, 7)
(298775, 7)
(298775, 7)
0/298775
(298775, 7)
0/298775
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(298775, 7)
(298775, 7)
Loading and preparing results...
Converting ndarray to lists...
0/298775
Converting ndarray to lists...
0/298775
0/298775
(298775, 7)
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(298775, 7)
0/298775
(298775, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
0/298775
Converting ndarray to lists...
0/298775
0/298775
Loading and preparing results...
0/298775
Converting ndarray to lists...
(298775, 7)
Loading and preparing results...
0/298775
(298775, 7)
Converting ndarray to lists...
Loading and preparing results...
(298775, 7)
0/298775
(298775, 7)
(298775, 7)
0/298775
(298775, 7)
(298775, 7)
Converting ndarray to lists...
0/298775
0/298775
0/298775
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(298775, 7)
(298775, 7)
(298775, 7)
0/298775
(298775, 7)
Loading and preparing results...
(298775, 7)
Converting ndarray to lists...
0/298775
Converting ndarray to lists...
Converting ndarray to lists...
0/298775
0/298775
(298775, 7)
0/298775
Converting ndarray to lists...
0/298775
Loading and preparing results...
Converting ndarray to lists...
0/298775
Converting ndarray to lists...
Loading and preparing results...
(298775, 7)
0/298775
Converting ndarray to lists...
(298775, 7)
(298775, 7)
Loading and preparing results...
(298775, 7)
0/298775
Loading and preparing results...
0/298775
0/298775
0/298775
(298775, 7)
(298775, 7)
(298775, 7)
0/298775
0/298775
(298775, 7)
(298775, 7)
0/298775
0/298775
Converting ndarray to lists...
0/298775
(298775, 7)
Converting ndarray to lists...
0/298775
(298775, 7)
Converting ndarray to lists...
0/298775
0/298775
0/298775
Converting ndarray to lists...
(298775, 7)
Converting ndarray to lists...
0/298775
Loading and preparing results...
0/298775
0/298775
0/298775
(298775, 7)
(298775, 7)
Converting ndarray to lists...
(298775, 7)
0/298775
(298775, 7)
(298775, 7)
Converting ndarray to lists...
0/298775
Converting ndarray to lists...
(298775, 7)
0/298775
0/298775
0/298775
0/298775
(298775, 7)
0/298775
DONE (t=1.65s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.70s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.71s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
DONE (t=1.75s)
creating index...
index created!
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.77s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.78s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
DONE (t=1.79s)
creating index...
index created!
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
DONE (t=1.80s)
creating index...
index created!
DONE (t=1.80s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
index created!
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
DONE (t=1.81s)
creating index...
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=3.21s).
Accumulating evaluation results...
DONE (t=3.21s).
Accumulating evaluation results...
DONE (t=3.18s).
Accumulating evaluation results...
DONE (t=3.23s).
Accumulating evaluation results...
DONE (t=3.23s).
Accumulating evaluation results...
DONE (t=3.22s).
Accumulating evaluation results...
DONE (t=3.22s).
Accumulating evaluation results...
DONE (t=3.23s).
Accumulating evaluation results...
DONE (t=1.04s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
DONE (t=1.04s).
DONE (t=1.04s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.334
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.301
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.495
Current AP: 0.20825 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.334
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.334
DONE (t=1.04s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.301
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.301
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.495
Current AP: 0.20825 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.495
Current AP: 0.20825 AP goal: 0.21200
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
DONE (t=1.06s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
DONE (t=1.06s).
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.334
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.301
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.495
Current AP: 0.20825 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.334
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
DONE (t=1.07s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.334
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.301
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.495
Current AP: 0.20825 AP goal: 0.21200
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.301
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.495
Current AP: 0.20825 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.208
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.334
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.301
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.495
Current AP: 0.20825 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.361
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.334
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.301
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.316
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.337
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.495
Current AP: 0.20825 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757286.092431784 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757286.093045235 (train.py:333) eval_accuracy: {"epoch": 48, "value": 0.20824900707414826}

:::MLPv0.5.0 ssd 1541757286.093505621 (train.py:336) eval_iteration_accuracy: {"epoch": 48, "value": 0.20824900707414826}

:::MLPv0.5.0 ssd 1541757286.093954086 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757286.094398260 (train.py:338) eval_stop: 48
Iteration:   2820, Loss function: 2.991, Average Loss: 3.966, avg. samples / sec: 3334.10
Iteration:   2820, Loss function: 3.156, Average Loss: 3.971, avg. samples / sec: 3334.11
Iteration:   2820, Loss function: 3.174, Average Loss: 3.973, avg. samples / sec: 3334.16
Iteration:   2820, Loss function: 3.271, Average Loss: 3.956, avg. samples / sec: 3334.15
Iteration:   2820, Loss function: 3.733, Average Loss: 3.971, avg. samples / sec: 3334.16
Iteration:   2820, Loss function: 2.829, Average Loss: 3.969, avg. samples / sec: 3334.06
Iteration:   2820, Loss function: 2.730, Average Loss: 3.967, avg. samples / sec: 3334.08
Iteration:   2820, Loss function: 3.225, Average Loss: 3.963, avg. samples / sec: 3334.10

:::MLPv0.5.0 ssd 1541757288.010246515 (train.py:553) train_epoch: 49
Iteration:   2840, Loss function: 2.916, Average Loss: 3.941, avg. samples / sec: 24552.95
Iteration:   2840, Loss function: 3.357, Average Loss: 3.951, avg. samples / sec: 24542.92
Iteration:   2840, Loss function: 3.452, Average Loss: 3.956, avg. samples / sec: 24542.91
Iteration:   2840, Loss function: 3.290, Average Loss: 3.956, avg. samples / sec: 24546.44
Iteration:   2840, Loss function: 3.350, Average Loss: 3.956, avg. samples / sec: 24545.39
Iteration:   2840, Loss function: 3.263, Average Loss: 3.953, avg. samples / sec: 24552.77
Iteration:   2840, Loss function: 3.649, Average Loss: 3.950, avg. samples / sec: 24542.47
Iteration:   2840, Loss function: 3.514, Average Loss: 3.957, avg. samples / sec: 24478.51
Iteration:   2860, Loss function: 3.303, Average Loss: 3.940, avg. samples / sec: 24565.98
Iteration:   2860, Loss function: 3.486, Average Loss: 3.936, avg. samples / sec: 24565.48
Iteration:   2860, Loss function: 3.041, Average Loss: 3.944, avg. samples / sec: 24631.15
Iteration:   2860, Loss function: 3.418, Average Loss: 3.936, avg. samples / sec: 24624.23
Iteration:   2860, Loss function: 3.273, Average Loss: 3.942, avg. samples / sec: 24588.72
Iteration:   2860, Loss function: 2.777, Average Loss: 3.942, avg. samples / sec: 24570.97
Iteration:   2860, Loss function: 3.382, Average Loss: 3.928, avg. samples / sec: 24553.60
Iteration:   2860, Loss function: 3.298, Average Loss: 3.945, avg. samples / sec: 24544.97
Iteration:   2880, Loss function: 3.507, Average Loss: 3.923, avg. samples / sec: 24549.85
Iteration:   2880, Loss function: 3.452, Average Loss: 3.925, avg. samples / sec: 24541.52
Iteration:   2880, Loss function: 3.213, Average Loss: 3.921, avg. samples / sec: 24539.92
Iteration:   2880, Loss function: 3.573, Average Loss: 3.930, avg. samples / sec: 24541.46
Iteration:   2880, Loss function: 3.042, Average Loss: 3.913, avg. samples / sec: 24531.66
Iteration:   2880, Loss function: 3.184, Average Loss: 3.931, avg. samples / sec: 24529.05
Iteration:   2880, Loss function: 2.966, Average Loss: 3.930, avg. samples / sec: 24539.97
Iteration:   2880, Loss function: 3.566, Average Loss: 3.929, avg. samples / sec: 24510.97

:::MLPv0.5.0 ssd 1541757292.768375158 (train.py:553) train_epoch: 50
Iteration:   2900, Loss function: 3.128, Average Loss: 3.910, avg. samples / sec: 24498.13
Iteration:   2900, Loss function: 3.233, Average Loss: 3.908, avg. samples / sec: 24498.06
Iteration:   2900, Loss function: 2.799, Average Loss: 3.913, avg. samples / sec: 24511.21
Iteration:   2900, Loss function: 3.437, Average Loss: 3.916, avg. samples / sec: 24503.66
Iteration:   2900, Loss function: 3.642, Average Loss: 3.909, avg. samples / sec: 24469.60
Iteration:   2900, Loss function: 3.596, Average Loss: 3.900, avg. samples / sec: 24466.67
Iteration:   2900, Loss function: 3.390, Average Loss: 3.917, avg. samples / sec: 24481.57
Iteration:   2900, Loss function: 3.439, Average Loss: 3.918, avg. samples / sec: 24451.27
Iteration:   2920, Loss function: 3.237, Average Loss: 3.896, avg. samples / sec: 24584.48
Iteration:   2920, Loss function: 3.206, Average Loss: 3.900, avg. samples / sec: 24589.65
Iteration:   2920, Loss function: 3.172, Average Loss: 3.888, avg. samples / sec: 24631.05
Iteration:   2920, Loss function: 3.622, Average Loss: 3.893, avg. samples / sec: 24577.57
Iteration:   2920, Loss function: 2.853, Average Loss: 3.904, avg. samples / sec: 24630.33
Iteration:   2920, Loss function: 3.394, Average Loss: 3.905, avg. samples / sec: 24628.04
Iteration:   2920, Loss function: 2.960, Average Loss: 3.895, avg. samples / sec: 24592.68
Iteration:   2920, Loss function: 3.302, Average Loss: 3.903, avg. samples / sec: 24573.38
Iteration:   2940, Loss function: 2.945, Average Loss: 3.878, avg. samples / sec: 24577.34
Iteration:   2940, Loss function: 4.065, Average Loss: 3.882, avg. samples / sec: 24570.61
Iteration:   2940, Loss function: 3.264, Average Loss: 3.881, avg. samples / sec: 24586.33
Iteration:   2940, Loss function: 3.260, Average Loss: 3.891, avg. samples / sec: 24572.64
Iteration:   2940, Loss function: 3.159, Average Loss: 3.885, avg. samples / sec: 24565.67
Iteration:   2940, Loss function: 2.676, Average Loss: 3.873, avg. samples / sec: 24548.42
Iteration:   2940, Loss function: 2.872, Average Loss: 3.891, avg. samples / sec: 24537.51
Iteration:   2940, Loss function: 3.370, Average Loss: 3.890, avg. samples / sec: 24540.89

:::MLPv0.5.0 ssd 1541757297.604708433 (train.py:553) train_epoch: 51
Iteration:   2960, Loss function: 2.727, Average Loss: 3.866, avg. samples / sec: 24532.85
Iteration:   2960, Loss function: 3.004, Average Loss: 3.863, avg. samples / sec: 24534.04
Iteration:   2960, Loss function: 3.534, Average Loss: 3.868, avg. samples / sec: 24531.60
Iteration:   2960, Loss function: 3.010, Average Loss: 3.873, avg. samples / sec: 24538.65
Iteration:   2960, Loss function: 3.044, Average Loss: 3.876, avg. samples / sec: 24556.06
Iteration:   2960, Loss function: 3.301, Average Loss: 3.877, avg. samples / sec: 24538.32
Iteration:   2960, Loss function: 3.377, Average Loss: 3.862, avg. samples / sec: 24521.04
Iteration:   2960, Loss function: 3.490, Average Loss: 3.879, avg. samples / sec: 24463.07
Iteration:   2980, Loss function: 3.448, Average Loss: 3.854, avg. samples / sec: 24565.11
Iteration:   2980, Loss function: 3.127, Average Loss: 3.855, avg. samples / sec: 24563.27
Iteration:   2980, Loss function: 3.487, Average Loss: 3.848, avg. samples / sec: 24559.53
Iteration:   2980, Loss function: 2.860, Average Loss: 3.860, avg. samples / sec: 24561.21
Iteration:   2980, Loss function: 3.431, Average Loss: 3.848, avg. samples / sec: 24592.85
Iteration:   2980, Loss function: 3.125, Average Loss: 3.863, avg. samples / sec: 24592.46
Iteration:   2980, Loss function: 3.450, Average Loss: 3.863, avg. samples / sec: 24581.79
Iteration:   2980, Loss function: 3.030, Average Loss: 3.866, avg. samples / sec: 24574.36
Iteration:   3000, Loss function: 3.176, Average Loss: 3.843, avg. samples / sec: 24562.35
Iteration:   3000, Loss function: 3.211, Average Loss: 3.834, avg. samples / sec: 24563.42
Iteration:   3000, Loss function: 3.167, Average Loss: 3.841, avg. samples / sec: 24557.69
Iteration:   3000, Loss function: 3.604, Average Loss: 3.848, avg. samples / sec: 24562.60
Iteration:   3000, Loss function: 3.242, Average Loss: 3.852, avg. samples / sec: 24598.17
Iteration:   3000, Loss function: 2.821, Average Loss: 3.850, avg. samples / sec: 24535.33
Iteration:   3000, Loss function: 3.263, Average Loss: 3.846, avg. samples / sec: 24534.12
Iteration:   3000, Loss function: 3.100, Average Loss: 3.834, avg. samples / sec: 24497.54

:::MLPv0.5.0 ssd 1541757302.442831039 (train.py:553) train_epoch: 52
Iteration:   3020, Loss function: 3.248, Average Loss: 3.830, avg. samples / sec: 24497.15
Iteration:   3020, Loss function: 3.241, Average Loss: 3.828, avg. samples / sec: 24496.59
Iteration:   3020, Loss function: 2.978, Average Loss: 3.839, avg. samples / sec: 24519.86
Iteration:   3020, Loss function: 3.217, Average Loss: 3.819, avg. samples / sec: 24559.41
Iteration:   3020, Loss function: 3.257, Average Loss: 3.820, avg. samples / sec: 24486.55
Iteration:   3020, Loss function: 3.131, Average Loss: 3.836, avg. samples / sec: 24496.04
Iteration:   3020, Loss function: 2.927, Average Loss: 3.830, avg. samples / sec: 24482.53
Iteration:   3020, Loss function: 3.133, Average Loss: 3.835, avg. samples / sec: 24455.33
Iteration:   3040, Loss function: 3.153, Average Loss: 3.825, avg. samples / sec: 24564.76
Iteration:   3040, Loss function: 3.113, Average Loss: 3.816, avg. samples / sec: 24560.04
Iteration:   3040, Loss function: 3.565, Average Loss: 3.808, avg. samples / sec: 24569.61
Iteration:   3040, Loss function: 3.157, Average Loss: 3.818, avg. samples / sec: 24555.85
Iteration:   3040, Loss function: 3.331, Average Loss: 3.826, avg. samples / sec: 24588.16
Iteration:   3040, Loss function: 3.361, Average Loss: 3.817, avg. samples / sec: 24602.60
Iteration:   3040, Loss function: 3.584, Average Loss: 3.822, avg. samples / sec: 24593.56
Iteration:   3040, Loss function: 3.240, Average Loss: 3.806, avg. samples / sec: 24505.61
Iteration:   3060, Loss function: 2.755, Average Loss: 3.795, avg. samples / sec: 24577.60
Iteration:   3060, Loss function: 2.885, Average Loss: 3.809, avg. samples / sec: 24573.54
Iteration:   3060, Loss function: 2.777, Average Loss: 3.803, avg. samples / sec: 24578.01
Iteration:   3060, Loss function: 3.274, Average Loss: 3.805, avg. samples / sec: 24578.76
Iteration:   3060, Loss function: 3.281, Average Loss: 3.795, avg. samples / sec: 24633.54
Iteration:   3060, Loss function: 3.118, Average Loss: 3.811, avg. samples / sec: 24571.37
Iteration:   3060, Loss function: 3.093, Average Loss: 3.808, avg. samples / sec: 24572.75
Iteration:   3060, Loss function: 2.748, Average Loss: 3.802, avg. samples / sec: 24560.06

:::MLPv0.5.0 ssd 1541757307.283900023 (train.py:553) train_epoch: 53
Iteration:   3080, Loss function: 3.145, Average Loss: 3.793, avg. samples / sec: 24526.68
Iteration:   3080, Loss function: 2.735, Average Loss: 3.783, avg. samples / sec: 24524.82
Iteration:   3080, Loss function: 3.237, Average Loss: 3.792, avg. samples / sec: 24525.79
Iteration:   3080, Loss function: 3.644, Average Loss: 3.797, avg. samples / sec: 24525.99
Iteration:   3080, Loss function: 3.307, Average Loss: 3.784, avg. samples / sec: 24524.57
Iteration:   3080, Loss function: 3.309, Average Loss: 3.795, avg. samples / sec: 24535.96
Iteration:   3080, Loss function: 3.311, Average Loss: 3.788, avg. samples / sec: 24536.37
Iteration:   3080, Loss function: 2.862, Average Loss: 3.796, avg. samples / sec: 24491.67
Iteration:   3100, Loss function: 3.239, Average Loss: 3.779, avg. samples / sec: 24527.94
Iteration:   3100, Loss function: 2.762, Average Loss: 3.784, avg. samples / sec: 24529.20
Iteration:   3100, Loss function: 2.821, Average Loss: 3.771, avg. samples / sec: 24531.77
Iteration:   3100, Loss function: 3.232, Average Loss: 3.771, avg. samples / sec: 24524.82
Iteration:   3100, Loss function: 2.981, Average Loss: 3.781, avg. samples / sec: 24522.97
Iteration:   3100, Loss function: 3.401, Average Loss: 3.774, avg. samples / sec: 24530.77
Iteration:   3100, Loss function: 3.157, Average Loss: 3.786, avg. samples / sec: 24558.56
Iteration:   3100, Loss function: 3.063, Average Loss: 3.784, avg. samples / sec: 24494.46

:::MLPv0.5.0 ssd 1541757312.041335106 (train.py:553) train_epoch: 54
Iteration:   3120, Loss function: 2.986, Average Loss: 3.767, avg. samples / sec: 24533.87
Iteration:   3120, Loss function: 2.739, Average Loss: 3.771, avg. samples / sec: 24528.99
Iteration:   3120, Loss function: 3.374, Average Loss: 3.768, avg. samples / sec: 24531.40
Iteration:   3120, Loss function: 3.508, Average Loss: 3.759, avg. samples / sec: 24532.53
Iteration:   3120, Loss function: 3.148, Average Loss: 3.759, avg. samples / sec: 24524.34
Iteration:   3120, Loss function: 3.388, Average Loss: 3.773, avg. samples / sec: 24562.48
Iteration:   3120, Loss function: 3.135, Average Loss: 3.760, avg. samples / sec: 24481.28
Iteration:   3120, Loss function: 3.467, Average Loss: 3.771, avg. samples / sec: 24468.92
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2
lr decay step #2

:::MLPv0.5.0 ssd 1541757312.546723127 (train.py:586) opt_learning_rate: 0.0016

































































:::MLPv0.5.0 ssd 1541757312.629387379 (train.py:217) nms_threshold: 0.5

:::MLPv0.5.0 ssd 1541757312.629934549 (train.py:219) nms_max_detections: 200

:::MLPv0.5.0 ssd 1541757312.630395651 (train.py:220) eval_start: 54
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.09 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.09 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.08 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.09 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.09 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.09 s
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.09 s
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Parsing batch: 0/1Predicting Ended, total time: 4.09 s
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
(290497, 7)
(290497, 7)
0/290497
0/290497
(290497, 7)
Loading and preparing results...
Loading and preparing results...
(290497, 7)
(290497, 7)
Loading and preparing results...
Loading and preparing results...
0/290497
Loading and preparing results...
0/290497
Loading and preparing results...
0/290497
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
(290497, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(290497, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
0/290497
Converting ndarray to lists...
(290497, 7)
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
(290497, 7)
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/290497
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
0/290497
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
(290497, 7)
Converting ndarray to lists...
(290497, 7)
(290497, 7)
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
0/290497
Converting ndarray to lists...
(290497, 7)
Loading and preparing results...
(290497, 7)
Converting ndarray to lists...
Loading and preparing results...
(290497, 7)
Loading and preparing results...
(290497, 7)
0/290497
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(290497, 7)
Converting ndarray to lists...
0/290497
Converting ndarray to lists...
Loading and preparing results...
Converting ndarray to lists...
(290497, 7)
(290497, 7)
Loading and preparing results...
Converting ndarray to lists...
0/290497
(290497, 7)
0/290497
0/290497
Loading and preparing results...
0/290497
Converting ndarray to lists...
Loading and preparing results...
Loading and preparing results...
(290497, 7)
Converting ndarray to lists...
0/290497
Converting ndarray to lists...
0/290497
(290497, 7)
Converting ndarray to lists...
(290497, 7)
Converting ndarray to lists...
0/290497
Converting ndarray to lists...
Converting ndarray to lists...
Converting ndarray to lists...
(290497, 7)
0/290497
0/290497
Converting ndarray to lists...
(290497, 7)
(290497, 7)
0/290497
0/290497
0/290497
(290497, 7)
(290497, 7)
(290497, 7)
Converting ndarray to lists...
(290497, 7)
0/290497
Loading and preparing results...
Converting ndarray to lists...
(290497, 7)
Converting ndarray to lists...
(290497, 7)
(290497, 7)
(290497, 7)
Converting ndarray to lists...
Converting ndarray to lists...
(290497, 7)
Loading and preparing results...
(290497, 7)
0/290497
Converting ndarray to lists...
(290497, 7)
Loading and preparing results...
0/290497
Converting ndarray to lists...
Converting ndarray to lists...
(290497, 7)
(290497, 7)
Loading and preparing results...
Converting ndarray to lists...
Loading and preparing results...
0/290497
(290497, 7)
Converting ndarray to lists...
Converting ndarray to lists...
0/290497
Converting ndarray to lists...
0/290497
Loading and preparing results...
(290497, 7)
0/290497
Converting ndarray to lists...
(290497, 7)
Converting ndarray to lists...
0/290497
(290497, 7)
(290497, 7)
(290497, 7)
0/290497
0/290497
(290497, 7)
(290497, 7)
0/290497
(290497, 7)
0/290497
(290497, 7)
0/290497
(290497, 7)
(290497, 7)
0/290497
(290497, 7)
0/290497
0/290497
0/290497
(290497, 7)
Converting ndarray to lists...
(290497, 7)
Converting ndarray to lists...
0/290497
Converting ndarray to lists...
Converting ndarray to lists...
0/290497
0/290497
0/290497
(290497, 7)
0/290497
(290497, 7)
0/290497
(290497, 7)
Loading and preparing results...
0/290497
0/290497
(290497, 7)
0/290497
0/290497
0/290497
Loading and preparing results...
0/290497
0/290497
(290497, 7)
0/290497
0/290497
0/290497
(290497, 7)
0/290497
Converting ndarray to lists...
0/290497
0/290497
Loading and preparing results...
0/290497
(290497, 7)
Converting ndarray to lists...
0/290497
(290497, 7)
Converting ndarray to lists...
Loading and preparing results...
(290497, 7)
0/290497
0/290497
Converting ndarray to lists...
(290497, 7)
0/290497
DONE (t=1.56s)
creating index...
DONE (t=1.56s)
creating index...
DONE (t=1.56s)
creating index...
DONE (t=1.58s)
creating index...
DONE (t=1.58s)
creating index...
DONE (t=1.59s)
creating index...
DONE (t=1.59s)
creating index...
DONE (t=1.59s)
creating index...
DONE (t=1.60s)
creating index...
DONE (t=1.60s)
creating index...
DONE (t=1.60s)
creating index...
DONE (t=1.60s)
creating index...
DONE (t=1.60s)
creating index...
DONE (t=1.60s)
creating index...
DONE (t=1.60s)
creating index...
DONE (t=1.60s)
creating index...
DONE (t=1.60s)
creating index...
DONE (t=1.60s)
creating index...
DONE (t=1.60s)
creating index...
DONE (t=1.60s)
creating index...
DONE (t=1.60s)
creating index...
DONE (t=1.61s)
creating index...
DONE (t=1.61s)
creating index...
DONE (t=1.61s)
creating index...
DONE (t=1.61s)
creating index...
DONE (t=1.61s)
creating index...
DONE (t=1.61s)
creating index...
DONE (t=1.62s)
creating index...
DONE (t=1.62s)
creating index...
DONE (t=1.63s)
creating index...
DONE (t=1.63s)
creating index...
DONE (t=1.63s)
creating index...
DONE (t=1.64s)
creating index...
DONE (t=1.64s)
creating index...
DONE (t=1.65s)
creating index...
index created!
index created!
DONE (t=1.68s)
creating index...
DONE (t=1.68s)
creating index...
DONE (t=1.69s)
creating index...
DONE (t=1.69s)
creating index...
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
DONE (t=1.70s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=1.71s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
index created!
index created!
index created!
index created!
index created!
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.72s)
creating index...
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
index created!
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
index created!
index created!
DONE (t=1.73s)
creating index...
DONE (t=1.73s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
DONE (t=1.74s)
creating index...
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
DONE (t=1.74s)
creating index...
index created!
index created!
DONE (t=1.75s)
creating index...
index created!
DONE (t=1.76s)
creating index...
DONE (t=1.76s)
creating index...
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
index created!
index created!
DONE (t=1.88s)
creating index...
DONE (t=1.94s)
creating index...
DONE (t=1.97s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
DONE (t=3.16s).
Accumulating evaluation results...
DONE (t=3.20s).
Accumulating evaluation results...
DONE (t=3.19s).
Accumulating evaluation results...
DONE (t=3.20s).
Accumulating evaluation results...
DONE (t=3.21s).
Accumulating evaluation results...
DONE (t=3.51s).
Accumulating evaluation results...
DONE (t=3.58s).
Accumulating evaluation results...
DONE (t=3.59s).
Accumulating evaluation results...
DONE (t=1.02s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.345
DONE (t=1.03s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504
Current AP: 0.21330 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
DONE (t=1.01s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
DONE (t=1.02s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.345
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504
Current AP: 0.21330 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.345
DONE (t=1.04s).
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504
Current AP: 0.21330 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.345
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504
Current AP: 0.21330 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504
Current AP: 0.21330 AP goal: 0.21200
DONE (t=1.02s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
DONE (t=1.01s).
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504
Current AP: 0.21330 AP goal: 0.21200
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504
Current AP: 0.21330 AP goal: 0.21200
DONE (t=1.03s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.213
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.371
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.216
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.345
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.305
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.319
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.341
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.504
Current AP: 0.21330 AP goal: 0.21200

:::MLPv0.5.0 ssd 1541757323.173375845 (train.py:330) eval_size: 4952

:::MLPv0.5.0 ssd 1541757323.174105167 (train.py:333) eval_accuracy: {"epoch": 54, "value": 0.2132954238790146}

:::MLPv0.5.0 ssd 1541757323.174605370 (train.py:336) eval_iteration_accuracy: {"epoch": 54, "value": 0.2132954238790146}

:::MLPv0.5.0 ssd 1541757323.175075769 (train.py:337) eval_target: 0.212

:::MLPv0.5.0 ssd 1541757323.175543785 (train.py:338) eval_stop: 54

:::MLPv0.5.0 ssd 1541757324.255327940 (train.py:706) run_stop: {"success": true}

:::MLPv0.5.0 ssd 1541757324.255913258 (train.py:707) run_final
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2018-11-09 09:55:46 AM
RESULT,OBJECT_DETECTION,,364,nvidia,2018-11-09 09:49:42 AM
ENDING TIMING RUN AT 2018-11-09 09:55:42 AM
RESULT,OBJECT_DETECTION,,365,nvidia,2018-11-09 09:49:37 AM
ENDING TIMING RUN AT 2018-11-09 09:55:36 AM
RESULT,OBJECT_DETECTION,,364,nvidia,2018-11-09 09:49:32 AM
ENDING TIMING RUN AT 2018-11-09 09:55:37 AM
RESULT,OBJECT_DETECTION,,365,nvidia,2018-11-09 09:49:32 AM
ENDING TIMING RUN AT 2018-11-09 09:55:40 AM
RESULT,OBJECT_DETECTION,,365,nvidia,2018-11-09 09:49:35 AM
ENDING TIMING RUN AT 2018-11-09 09:55:32 AM
RESULT,OBJECT_DETECTION,,365,nvidia,2018-11-09 09:49:27 AM
ENDING TIMING RUN AT 2018-11-09 09:55:37 AM
RESULT,OBJECT_DETECTION,,364,nvidia,2018-11-09 09:49:33 AM
ENDING TIMING RUN AT 2018-11-09 09:55:30 AM
RESULT,OBJECT_DETECTION,,364,nvidia,2018-11-09 09:49:26 AM
