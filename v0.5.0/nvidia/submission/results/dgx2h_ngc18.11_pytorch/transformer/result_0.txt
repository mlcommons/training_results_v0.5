Beginning trial 1 of 1
Clearing caches
vm.drop_caches = 3
:::MLPv0.5.0 transformer 1541659942.365454912 (<string>:1) run_clear_caches
Launching on node circe-n019
+ pids+=($!)
+ set +x
++ eval echo srun -N 1 -n 1 -w '$hostn'
+++ echo srun -N 1 -n 1 -w circe-n019
+ srun -N 1 -n 1 -w circe-n019 docker exec -e DGXSYSTEM=DGX2 -e MULTI_NODE= -e SEED=7940 -e SLURM_JOB_ID=34487 -e SLURM_NTASKS_PER_NODE=16 -e MODE=TRAIN cont_34487 ./run_and_time.sh
Run vars: id 34487 gpus 16 mparams 
+ SEED=7940
+ MAX_TOKENS=10240
+ DATASET_DIR=/data
+ MODE=TRAIN
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START=1541659942
+++ date '+%Y-%m-%d %r'
++ START_FMT='2018-11-08 06:52:22 AM'
STARTING TIMING RUN AT 2018-11-08 06:52:22 AM
++ echo 'STARTING TIMING RUN AT 2018-11-08 06:52:22 AM'
++ python -m torch.distributed.launch --nproc_per_node 16 train.py /data --seed 7940 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 1200 --lr 1.35e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 10240 --max-epoch 12 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --distributed-init-method env://
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 7
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 8
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 2
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 13
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 5
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 1
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 9
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 10
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 0
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 14
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 3
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 15
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 11
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 6
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 4
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: 127.0.0.1, MASTER_PORT: 29500, WORLD_SIZE: 16, RANK: 12
| distributed init done!
| initialized host circe-n019 as rank 0 and device id 0
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLPv0.5.0 transformer 1541659957.599082470 (/workspace/translation/train.py:34) run_clear_caches
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLPv0.5.0 transformer 1541659977.799185038 (/workspace/translation/train.py:40) run_start
Namespace(adam_betas='(0.9, 0.997)', adam_eps=1e-09, adaptive_softmax_cutoff=None, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, beam=4, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', data='/data', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=0, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=0, distributed_world_size=16, dropout=0.1, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fp16=True, fuse_dropout_add=False, fuse_relu_dropout=False, gen_subset='test', ignore_case=True, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source='True', left_pad_target='False', lenpen=1, local_rank=0, log_format=None, log_interval=1000, log_translations=False, lr=[0.00135], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=12, max_len_a=0, max_len_b=200, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=10240, max_update=0, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_progress_bar=False, no_save=True, no_token_positional_embeddings=False, num_shards=1, online_eval=False, optimizer='adam', pad_sequence=1, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=None, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe=None, replace_unk=None, restore_file='checkpoint_last.pt', sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='checkpoints', save_interval=1, save_interval_updates=0, score_reference=False, seed=7940, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_bleu=25.0, target_lang=None, task='translation', train_subset='train', unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=1200, weight_decay=0.0)
:::MLPv0.5.0 transformer 1541659977.799881458 (/workspace/translation/train.py:44) opt_name: "adam"
:::MLPv0.5.0 transformer 1541659977.800163746 (/workspace/translation/train.py:45) opt_learning_rate: [0.00135]
:::MLPv0.5.0 transformer 1541659977.800445080 (/workspace/translation/train.py:46) opt_hp_Adam_beta1: 0.9
:::MLPv0.5.0 transformer 1541659977.800676823 (/workspace/translation/train.py:47) opt_hp_Adam_beta2: 0.997
:::MLPv0.5.0 transformer 1541659977.800896883 (/workspace/translation/train.py:48) opt_hp_Adam_epsilon: 1e-09
:::MLPv0.5.0 transformer 1541659977.809918165 (/workspace/translation/train.py:53) run_set_random_seed: 7940
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
:::MLPv0.5.0 transformer 1541659977.877410412 (/workspace/translation/train.py:61) model_hp_sequence_beam_search: {"alpha": 1, "beam_size": 4, "extra_decode_length": 200, "vocab_size": 33712}
| /data train 4575616 examples
| Sentences are being padded to multiples of: 1
| /data valid 3000 examples
| Sentences are being padded to multiples of: 1
:::MLPv0.5.0 transformer 1541659979.110760212 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.167954445 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.189683676 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.204740047 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.206601143 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.213466406 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.236222982 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.238529682 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.238972425 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.255405664 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.264898062 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.266546011 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.270678520 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.270771980 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.271104813 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.272691250 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.272524595 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.273760796 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.276344061 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.278372526 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.279828310 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.282262087 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.289648533 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.290550470 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.294092655 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.297940016 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.313685179 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.317780256 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.321640968 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.327276230 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.338189602 (/workspace/translation/fairseq/models/transformer.py:96) input_max_length: 1024
:::MLPv0.5.0 transformer 1541659979.350432158 (/workspace/translation/fairseq/models/transformer.py:119) model_hp_embedding_shared_weights: {"hidden_size": 1024, "vocab_size": 33712}
:::MLPv0.5.0 transformer 1541659979.917206526 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659979.921629190 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659979.930306673 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659979.931026220 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659979.935579300 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659979.936239004 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659979.912762165 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659979.940737247 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659979.944934845 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659979.945600271 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659979.946466923 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659979.946993113 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659979.947342157 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659979.947665930 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659979.956167459 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659979.956866980 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659979.960372925 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659979.960986376 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659979.965638161 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659979.967797279 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659979.974479675 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659979.975143194 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659979.983437300 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659979.986436129 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659979.989507914 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.000680208 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.006932020 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.011665106 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.030688286 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.031351089 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.087023020 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.097134352 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.110190630 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.110195875 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.110851526 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.111485243 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.135716677 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.146726131 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.158976793 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.159476042 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.164493561 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.167323589 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.168352604 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.172552347 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.172662735 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.180152416 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.198265314 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.198465586 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.201215029 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.204940081 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.207123518 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.214651108 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.215326309 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.220943451 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.231471539 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.234795094 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.235463142 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.252255678 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.257485867 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.271563292 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.282760859 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.286861420 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.289731264 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.306518078 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.310466290 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.318326235 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.318519354 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.318996906 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.321426868 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.322032452 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.325457096 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.326121807 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.327847004 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.328497887 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.329076529 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.329638004 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.330281258 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.330878496 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.343071699 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.348217010 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.356010437 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.356250048 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.357124329 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.357152224 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.357878923 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.358537912 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.359288216 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.359987497 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.372241259 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.376405001 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.377022982 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.377729893 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.378364086 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.379083872 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.379753828 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.387115955 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.389066219 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.391349316 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.391950607 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.392192841 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.392481804 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.399692535 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.400268316 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.400779486 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.401174545 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.401180983 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.401257753 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.401791811 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.401802301 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.401870489 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.402347088 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.402374744 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.402367115 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.402892828 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.403527498 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.403699875 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.404105425 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.409725904 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.412065268 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.414635420 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.416384459 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.420931578 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.424061537 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.425796509 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.430576563 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.433772802 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.442798138 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.442973852 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.443415403 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.443942308 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.444457531 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.445078135 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.445633173 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.451498270 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.452216148 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.453086615 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.455382824 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.456819773 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.457929611 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.458770990 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.465922117 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.469709635 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.471609354 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.476437569 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.477050781 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.479648352 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.482852697 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.483489752 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.484027624 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.484549522 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.485169888 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.485734701 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.490212440 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.498058319 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.501769066 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.502998590 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.505021811 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.506725788 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.509161234 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.512761831 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.513264656 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.516167879 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.516768217 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.516760826 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.517299652 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.517345190 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.517819166 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.517848253 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.518341541 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.518425941 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.518919706 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.518978834 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.519350767 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.521047354 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.524168968 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.527596474 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.529402256 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.530250072 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.530697584 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.531074524 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.531277657 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.531443357 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.531875372 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.532268286 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.537319183 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.540326118 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.540361643 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.541092873 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.541535139 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.541907549 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.542271376 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.542706013 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.543099642 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.544386625 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.545126438 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.545583963 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.545959949 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.546326876 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.546764851 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.547162771 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.548167944 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.548623800 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.549046040 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.549427271 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.549876690 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.550284624 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.551170826 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.555258512 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.556153536 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.557996273 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.558400869 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.560405016 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.561378241 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.561765671 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.562105894 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.562547207 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.562916040 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.563279629 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.563711405 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.564104557 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.566616535 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.566853285 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.568439484 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.568881989 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.569037676 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.569264174 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.569630861 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.570063591 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.570456743 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.572223425 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.575301170 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.578547716 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.579782963 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.583886147 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.584387541 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.584840298 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.585230112 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.585353374 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.585600138 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.585802317 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.586047411 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.586177349 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.586447716 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.586464882 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.586544037 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.586975336 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.587367535 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.587944269 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.588453293 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.590321779 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.590560436 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.590759277 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.591002703 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.591136217 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.591384411 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.591507435 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.591760635 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.591943502 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.592201948 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.592343330 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.592608452 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.594539165 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.595117092 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.595389843 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.600386620 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.600662470 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.600877762 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.601651192 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.604808807 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.607136488 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.610141039 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.614326715 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.615287781 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.616713047 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.617169857 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.617546320 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.617640495 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.617911816 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.618343115 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.618734360 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.619205236 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.619650602 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.620018959 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.620380402 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.620813370 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.621248245 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.623169422 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.623994827 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.626905918 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.628988981 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.629304171 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.630421162 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.631305695 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.632657290 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.633117914 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.633495569 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.633860826 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.634322882 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.634717464 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.637700081 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.639195681 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.639660358 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.639910698 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.640032291 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.640396833 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.640827894 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.641226530 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.642106771 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.642764807 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.649369955 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.653507233 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.654499769 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.655649424 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.656910419 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.659117460 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.659664392 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.659733057 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.663615227 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.663723469 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.664165258 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.664539337 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.664913654 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.665347576 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.665747881 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.665813446 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.666239977 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.666605473 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.666962385 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.667387724 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.667775631 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.669602156 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.670410395 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.671399832 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.673815727 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.675790787 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.678071737 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.680685282 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.681129456 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.681504011 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.681869030 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.682296753 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.682692528 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.685293198 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.685983419 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.687462330 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.687906027 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.688276768 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.688641310 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.689081430 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.689483881 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.689495564 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.690798759 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.691086054 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.693436861 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.693887949 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.694266081 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.694634438 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.695067883 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.695463181 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.695691586 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.696134090 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.696509600 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.696882010 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.697327852 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.697555304 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.697727919 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.701809645 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.702445030 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.703684092 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.704316854 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.705711365 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.708333492 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.708878994 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.709186077 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.709636450 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.710018873 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.710396528 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.710502863 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.710843325 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.711247921 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.713036060 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.713128805 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.714788675 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.715216637 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.715588093 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.715951443 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.716381550 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.716774225 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.719549417 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.719967127 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.724881649 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.726795197 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.732164383 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.732636213 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.733023882 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.733404160 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.733427525 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.733850718 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.734005928 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.734258890 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.734364748 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.734586000 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.734825373 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.734914303 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.735210419 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.735582829 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.736027956 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.736436844 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.736819029 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.737269402 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.737654686 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.737742424 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.738031149 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.738181829 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.738473892 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.738559961 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.738880873 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.738929749 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.739361048 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.739758730 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.740061045 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.742515802 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.744683743 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.745136023 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.746921778 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.747793198 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.748635292 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.753199577 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.753611088 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.755965233 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.757819176 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.763033867 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.763843775 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.763890505 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.764338732 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.764720678 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.765100002 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.765544415 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.765727043 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.765945911 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.769177675 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.769632339 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.770015001 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.770386219 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.770828724 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.771226406 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.771637201 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.773707867 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.774260044 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.775457621 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.776368141 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.779457808 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.779849529 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.780297756 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.780679941 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.780646801 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.781060934 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.781507492 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.781912088 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.784032345 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.786967039 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.787463188 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.787436008 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.787847281 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.788229465 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.788373470 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.788672209 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.789086103 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.790171623 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.797245741 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.802661657 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.803398848 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.805800676 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.805841208 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.806630373 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.808453083 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.809420824 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.811251879 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.811709404 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.812057972 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.812117338 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.812122583 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.812497139 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.812559605 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.812951565 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.812958002 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.813323736 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.813375711 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.813751698 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.814140320 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.818063021 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.819218159 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.819847345 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.821758747 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.822155237 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.826474190 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.833197832 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.833666086 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.834047794 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.834428310 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.834443331 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.834870100 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.835275173 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.835939407 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.836395502 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.836776018 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.837166071 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.837432384 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.837464809 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.837621212 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.838029385 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.839486122 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.841946363 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.842386246 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.842760563 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.843127728 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.843563795 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.843599319 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.843959808 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.845557213 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.846035957 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.846342802 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.846423388 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.846801758 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.847251892 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.847665310 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.850283146 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.850651503 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.850867748 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.851891756 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.855884552 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.856843948 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.857581854 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.859059095 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.859897852 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.861184597 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.861246109 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.861628056 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.861716270 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.862003803 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.862105370 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.862372637 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.862484455 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.862805843 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.862939358 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.863213778 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.863196850 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.863356590 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.871323347 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.871674299 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.872830629 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.875506401 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.880042791 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.880671263 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.881109715 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.881139755 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.881522179 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.881831408 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.881900787 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.882344723 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.882761240 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.882750988 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.883187294 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.883569717 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.883613110 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.883945942 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.884050131 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.884387493 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.884429932 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.884793520 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.884803534 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.885102987 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.885248423 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.885646582 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.887049198 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.887518883 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.887899399 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.888277292 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.888603210 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.888721228 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.889137983 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.891011477 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.892833471 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.893672228 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.895637035 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.897420883 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.899981737 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.902063131 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.904541731 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.904724121 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.906672239 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.911367178 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.911911249 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.912392139 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.912842751 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.913233995 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.913608789 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.914049625 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.914452553 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.916230202 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.919781923 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.920171499 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.920264959 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.920646191 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.921025038 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.921384811 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.921473980 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.921872377 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.922279835 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.922731876 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.928356409 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.928827286 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.929221392 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.929596663 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.930045605 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.930023670 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.930453539 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.930809259 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.932720900 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.933071852 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.934541464 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.935863733 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.935870171 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.936356783 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.936743021 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.937123537 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.937571764 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.937976837 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.938860893 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.946172714 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.951101542 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.951713800 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.951891184 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.952547789 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.958261490 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.958687305 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.959051371 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.959411144 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.959721804 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.959843636 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.960174799 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.960237741 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.960563898 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.960686445 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.960778713 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.960824013 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.962467670 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.962968588 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.963357687 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.963517904 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.964139223 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.967799902 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.968227386 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.970907927 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.971588373 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.975257158 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.982684135 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.984166861 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.984595537 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.984657049 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.985055208 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.985116959 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.985441208 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.985505819 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.985818148 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.985889435 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.986267090 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.986336231 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.986678839 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.986742735 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.988016367 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.988464355 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.988686562 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.988843441 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.989220858 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.989656210 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.990051270 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.991675615 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.994720459 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.994915009 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.995063782 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.995196104 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659980.995580912 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.995956421 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659980.996410847 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659980.996735334 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.996825457 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659980.997962713 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659980.998739243 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.000638962 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.004969120 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.005100012 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.005253077 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.005751133 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.006388187 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.007895947 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.008330822 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.008701324 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.009071112 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.009505272 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.009897470 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.012477398 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.012949705 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.013334990 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.013711214 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.017546415 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.017987013 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.024013281 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.024193764 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.027164936 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.027622223 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.028605938 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.029063940 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.029067755 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.029465675 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.029462099 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.029541492 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.029847145 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.029910564 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.029934645 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.030300140 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.030323982 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.030682325 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.030774355 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.031126261 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.031185865 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.031532288 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.031546116 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.033443213 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.033908844 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.034211874 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.036774158 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.039543390 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.039576530 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.041413546 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.041879177 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.042269707 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.042251587 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.042644501 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.043088675 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.043495178 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.046096563 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.046725750 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.051712513 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.052745819 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.053949594 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.055724382 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.056114674 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.056187868 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.057724953 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.060547352 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.060869694 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.061017275 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.061398745 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.061769724 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.062211990 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.062614202 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.064264774 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.066357851 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.068148375 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.068798304 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.070924044 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.071438313 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.072008371 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.072465420 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.072843313 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.073223591 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.076604128 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.077083826 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.077167034 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.077460527 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.077823162 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.078259945 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.078653812 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.080784798 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.080859184 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.084748983 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.085248470 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.085632801 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.086007357 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.086399317 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.086726665 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.092861891 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.093817234 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.094278574 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.098531723 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.099828482 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.100006580 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.102471352 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.103790998 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.104634523 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.105079651 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.105452299 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.105812311 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.106241703 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.106630802 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.108012438 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.108513355 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.109006405 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.110268831 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.110494614 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.110555649 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.110733032 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.111118555 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.111495733 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.112359285 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.114648342 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.115421295 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.117388248 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.120589018 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.123158932 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.130139589 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.130599976 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.130558729 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.131013632 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.131613970 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.131936312 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.132385731 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.132764101 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.133143902 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.133488655 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.135995865 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.136175632 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.136474133 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.136648655 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.136861324 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.137040615 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.137232780 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.137419939 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.137866974 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.138273001 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.138908863 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.143146038 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.144484043 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.144969702 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.145361423 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.145737410 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.146174669 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.146539688 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.146603346 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.152199030 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.153942823 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.154297829 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.154730797 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.155103207 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.155468225 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.155900478 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.156295538 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.157823324 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.159337521 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.159814119 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.163599014 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.163851023 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.164116383 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.166724443 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.166729212 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.168110132 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.168375254 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.172659159 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.176429987 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.176477194 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.177005291 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.177044392 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.177551985 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.177984715 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.178422928 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.178439617 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.178499937 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.178833485 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.178929806 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.179335356 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.179733038 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.180220366 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.180297613 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.180649757 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.182581186 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.183061838 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.183469057 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.183864355 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.184222937 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.184693098 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.186538935 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.186996222 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.189835787 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.190454960 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.191548347 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.203665972 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.203819513 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.204941034 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.208475828 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.208835840 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.222015381 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.222624302 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.222768068 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.223155499 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.223678112 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.224150181 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.224343300 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.224970818 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.225504160 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.226017475 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.226471901 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.234290123 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.235925198 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.236741543 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.237302065 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.243373871 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.244002819 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.244545698 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.245083332 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.245558739 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.248230934 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.248977184 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.249027491 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.249760866 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.250296116 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.261213303 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.262224913 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.275947571 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.276579142 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.278554440 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.280583143 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.288030624 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.288765907 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.289303064 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.289814472 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.290287733 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.290674686 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.292438984 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.301452637 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.302096844 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.302584410 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.303064585 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.303642988 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.304162025 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.325874329 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.326525211 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.330287218 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.330961466 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.331569195 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.332143545 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.332399845 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.332683802 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.349638700 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.355763435 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.366271496 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.379984140 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.387544155 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.390682697 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.395270109 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.396787643 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.399013281 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.415334463 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.416202545 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.418678284 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.420692921 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.421327591 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.424097300 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.425987482 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.426118135 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.426644087 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.426786900 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.427939415 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.428582907 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.420765162 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.482848883 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.483715296 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.484132051 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.484323263 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.486893654 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.488863468 (/workspace/translation/fairseq/models/transformer.py:419) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.490441322 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.490661383 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.491059303 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.491595507 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.492124319 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.492595673 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.499164581 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.499803543 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.500334024 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.500547647 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.500863075 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.501327515 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.508381367 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.510912180 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.520065308 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.534435987 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.535298347 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.535867214 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.568083048 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.570573330 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.571217299 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.578955173 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.579588413 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.580168486 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.580712080 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.584130526 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.584804296 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.599657536 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.601127625 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.601383686 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.601750374 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.607629061 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.608302832 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.609844685 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.610465288 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.610999823 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.611524343 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.611994505 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.623549223 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.624137640 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.624637365 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.625261545 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.626099110 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.626266479 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.627436399 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.627650261 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.628251553 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.628288269 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.628781557 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.628815651 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.629378557 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.631335497 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.632150412 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.632694006 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.640428066 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.645081997 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.645175219 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.646112442 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.646761894 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.650215149 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.659108400 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.659163713 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.659749269 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.667294502 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.667910099 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.668441057 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.668972969 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.669683456 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.679009676 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.679688454 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.679793358 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.680536032 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.681213617 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.681865931 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.682625771 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.683323145 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.692742586 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.696999788 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.698757887 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.700224161 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.703077555 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.717933893 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.718863726 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.719455957 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.726666212 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.740344524 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.741046667 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.741791248 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.742085695 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.742490053 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.742956877 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.743147373 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.744960785 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.745586395 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.746134281 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.746674061 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.747298241 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.747874260 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.753277302 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.759592772 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.760330439 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.763866901 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.764504671 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.782567263 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.783415794 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.784233809 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.784820557 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.786512136 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.787141800 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.791776419 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.792623043 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.793639660 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.797315836 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.800451279 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.803952456 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.804759026 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.805362701 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.805707932 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.810119390 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.811331511 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.817618132 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.832187176 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.839834929 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.841212749 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.841949940 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.843408585 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.844000816 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.844561338 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.844916582 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.845514059 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.845694065 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.846427917 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.851030827 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.852145195 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.852750540 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.857282877 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.860946178 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.868351221 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.872420073 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.881071568 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.884153128 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.884639263 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.885244131 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.888696909 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.889503002 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.890083313 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.893874645 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.894185066 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.895059824 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.895677805 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.900166988 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.902378082 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.905801058 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.906278610 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.906687737 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.907090187 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.907560587 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.907989264 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.908447266 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.909155130 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.909777880 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.910379171 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.911082506 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.911731243 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.916541100 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.922384977 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.922994137 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.923378468 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.923527479 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.924051762 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.924664974 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.925285101 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.926220894 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.930680275 (/workspace/translation/fairseq/models/transformer.py:428) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.931297302 (/workspace/translation/fairseq/models/transformer.py:430) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.931818962 (/workspace/translation/fairseq/models/transformer.py:431) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.932331324 (/workspace/translation/fairseq/models/transformer.py:432) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659981.932797194 (/workspace/translation/fairseq/models/transformer.py:214) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659981.937599421 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.946130276 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.946955681 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.947632790 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.947702885 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.948079824 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.956805468 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.960713625 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.964602709 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.965203047 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.966561794 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.970051289 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.970402956 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.970617056 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.978986263 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.979869604 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.980916739 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.981649399 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.981645107 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.982393026 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659981.982942581 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.991479397 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.992684603 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.996590376 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.998132229 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.998225689 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659981.998738766 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659981.999261618 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659981.999778032 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.000386953 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.000847578 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.000953674 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.003968477 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.004573107 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.005097866 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.005605221 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.006204844 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.006749630 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.007402658 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.008017778 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.008553743 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.009081841 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.009710550 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.010272980 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.013172626 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.015587568 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.018990993 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.021002531 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.022556067 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.023901463 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.024693727 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.025243044 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.030623674 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.037449121 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.037815332 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.040370464 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.040882349 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.041305065 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.041719198 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.042206049 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.042651176 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.046326876 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.048098564 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.051251173 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.053207397 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.054920197 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.055018425 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.057065248 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.057894468 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.058471441 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.062896490 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.063740253 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.064306974 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.066754103 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.067604542 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.067880869 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.068187237 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.068481207 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.069010735 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.069529533 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.070136309 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.070697308 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.070643663 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.072794437 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.073287725 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.073700428 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.074108124 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.074090719 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.074579716 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.075009108 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.076484680 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.079650640 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.080130577 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.080447674 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.080538034 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.080942869 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.081411839 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.081717730 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.081841469 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.082365274 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.082828999 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.082886934 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.083680868 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.090364218 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.090409994 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.091040373 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.091323614 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.091588020 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.092102766 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.092710257 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.093274117 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.100758791 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.103730202 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.105494022 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.106920481 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.111246347 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.113829374 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.114449501 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.114874363 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.120433331 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.121108532 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.121561766 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.122103453 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.122840166 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.122932911 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.123477221 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.124106407 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.124843836 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.125536203 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.126625538 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.127428055 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.127997637 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.129023552 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.129645586 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.130255938 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.135833025 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.136204481 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.136325359 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.136739731 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.137154102 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.137629747 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.138066292 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.140136957 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.140498877 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.142519951 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.143140793 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.143662691 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.144176245 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.144777775 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.145340681 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.146677971 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.148108721 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.148806810 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.149308681 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.149579048 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.150131226 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.150249004 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.150700569 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.150906086 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.151667118 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.152371407 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.153477669 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.157603741 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.162296772 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.162861586 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.165916443 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.166202307 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.166620731 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.166681767 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.167083979 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.167447567 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.167877197 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.168266535 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.176392555 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.176465988 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.177067757 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.177482128 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.181169033 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.181921721 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.182569027 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.183076382 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.183212042 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.183950901 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.184636354 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.185478687 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.189254761 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.189937592 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.190384865 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.190931082 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.192724705 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.193681240 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.194346666 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.197609186 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.198243856 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.198207855 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.202728748 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.206078053 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.206736088 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.206867218 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.207157135 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.207229614 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.207548141 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.208004713 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.215210915 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.215340376 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.216356277 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.217104912 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.219202757 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.219692707 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.221323252 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.221758366 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.222126245 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.222491026 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.222570419 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.222925425 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.223316193 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.226455450 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.226701975 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.226905823 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.227282524 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.227651119 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.228088617 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.228492260 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.230814219 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.231249094 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.231382847 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.231632233 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.232005119 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.232446671 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.232855558 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.234327316 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.234998703 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.235423565 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.235385895 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.235843658 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.236225843 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.236393929 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.236600637 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.236685038 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.237042427 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.237445354 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.240896463 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.242934465 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.243379354 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.243370295 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.243755341 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.243705034 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.244132280 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.244163275 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.244368315 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.244445086 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.244540215 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.244575500 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.244911909 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.244978666 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.245349646 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.245743990 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.246953011 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.253067970 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.253782272 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.257499933 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.260055542 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.260055780 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.260514975 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.260717869 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.260922432 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.261152744 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.261116505 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.261322737 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.261766672 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.262163877 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.265403748 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.266041040 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.266458988 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.268069267 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.268480539 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.268856525 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.269210577 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.269229174 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.269385815 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.269664764 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.269979954 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.270067453 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.270199776 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.270274401 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.270400763 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.273913383 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.274603367 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.275777340 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.276228905 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.276541233 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.276977301 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.278056145 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.278426647 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.281788111 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.282407999 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.282410860 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.282811165 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.283063650 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.283475637 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.284855604 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.285088062 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.285118103 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.285302162 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.285676479 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.286043167 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.286474943 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.286870718 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.290934086 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.291495323 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.294925213 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.297636509 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.298840284 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.299422026 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.299835443 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.299955130 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.300396204 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.300775051 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.300817966 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.301146269 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.301588535 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.301986456 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.306572199 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.307146311 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.307549477 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.307856083 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.310087442 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.311000586 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.315560818 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.321207285 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.321650267 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.322026968 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.322397947 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.322831869 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.323236942 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.323469877 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.323683500 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.324075460 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.324485064 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.327315092 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.329293251 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.331249475 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.332528114 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.333800554 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.335988760 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.336448193 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.336831331 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.336910963 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.337207556 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.337640762 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.338035822 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.338735342 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.339331388 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.339736462 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.340063334 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.343878031 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.346101999 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.347388744 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.347832680 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.347833633 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.348204613 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.348570347 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.349015951 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.349418640 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.350080013 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.351033688 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.351472139 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.351652861 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.351842642 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.352206707 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.352634907 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.353031635 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.353518009 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.357507467 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.359726667 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.360306501 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.360707045 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.361133575 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.363772154 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.364213705 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.364585876 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.364956617 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.365396023 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.365792036 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.366255045 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.368482351 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.373852015 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.374043941 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.374688864 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.375315428 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.375718355 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.377232075 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.377683640 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.378058434 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.378425598 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.378861666 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.379259586 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.382831097 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.383548737 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.386110544 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.386767387 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.387191057 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.387183905 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.387273788 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.389867783 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.390271187 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.390505075 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.390926600 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.390999556 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.395192623 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.397421837 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.398984194 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.402558804 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.403170347 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.403583527 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.403787851 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.406354427 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.407540798 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.408107519 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.408550739 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.408930302 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.409296274 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.409729004 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.410121918 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.411016226 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.411464691 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.411597252 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.411841869 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.412209034 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.412643909 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.413047314 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.413928986 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.414361954 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.414742470 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.415124416 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.415280581 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.415570736 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.415927410 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.415983677 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.416346312 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.418159246 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.419550896 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.421219349 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.423043489 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.423533678 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.423913002 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.424046993 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.424278975 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.424282074 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.424719095 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.425117254 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.426181793 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.427561045 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.427776337 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.428219795 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.428590536 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.428957701 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.429390192 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.429779053 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.431246042 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.431682348 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.432054758 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.432420015 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.432859182 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.433262825 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.433274508 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.437834740 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.441389799 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.443052530 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.443191290 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.443624735 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.444001198 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.444369078 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.444801807 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.445215464 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.445328712 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.446842670 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.447473526 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.447885513 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.449992657 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.450620651 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.451039553 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.451220036 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.451647043 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.452013969 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.452373743 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.452620506 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.452800989 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.453202724 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.453233004 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.453383207 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.453664780 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.453727007 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.455932140 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.457597256 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.459183216 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.459738731 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.461208582 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.461689949 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.461934566 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.462580919 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.462993622 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.466470718 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.467110872 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.467522860 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.469412088 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.469899893 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.470050573 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.470300913 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.470686197 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.470705986 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.471138954 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.471103430 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.471109152 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.471168518 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.471586704 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.475534916 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.479246616 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.479477167 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.479728460 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.481605053 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.482189417 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.482769251 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.483174562 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.483447313 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.483885527 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.484257221 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.484622478 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.485065937 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.485466003 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.489702702 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.490286112 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.490691423 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.491039753 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.493559599 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.496614695 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.498656511 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.503174543 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.503619909 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.503996372 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.504366875 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.504802227 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.505209446 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.506982803 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.508623123 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.509275913 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.509709358 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.510980606 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.513263941 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.515969515 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.517787218 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.517840862 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.520200014 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.520370960 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.520832062 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.521213770 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.521580696 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.522017002 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.522203922 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.522411823 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.522820950 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.523233175 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.524680853 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.529719591 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.530537367 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.530718327 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.531164408 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.531285286 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.531538486 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.531901836 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.532338858 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.532734156 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.534091711 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.534386635 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.534701586 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.535137653 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.535506248 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.535870552 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.536299229 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.536689043 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.540080786 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.540752649 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.541769743 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.542373419 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.542790413 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.544763565 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.548395634 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.549489737 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.548841476 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.550749302 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.550805807 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.551117659 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.551542759 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.551927328 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.557188034 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.558103085 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.558553457 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.558930397 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.559298754 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.559734344 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.560015917 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.560137033 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.560838223 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.561507225 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.561922073 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.568042278 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.569278479 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.569527388 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.569921732 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.569944382 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.570335627 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.572229624 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.573470116 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.573517561 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.574121475 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.574527502 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.577133417 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.578340292 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.582591057 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.583195448 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.587718248 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.588645220 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.589287281 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.589701653 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.589775562 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.593262911 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.593705416 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.594074965 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.594439030 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.594872236 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.595267296 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.595732450 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.595958233 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.596134901 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.596407652 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.596768618 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.596788645 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.597094774 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.597167015 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.597228765 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.597545385 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.597612381 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.597733021 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.597941160 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.598025799 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.598319292 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.598762274 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.599166632 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.602805138 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.603330851 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.605120659 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.606203794 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.607201815 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.608336449 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.610295057 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.610744476 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.610694885 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.611120462 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.611489296 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.611713648 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.611935854 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.612163067 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.612341642 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.612533569 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.612900019 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.613335133 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.613728046 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.619450569 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.619888306 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.620260000 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.620438099 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.620627642 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.621066570 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.621462584 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.621762514 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.626470804 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.626909494 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.627283573 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.627651215 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.628084660 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.628487110 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.628558397 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.629636288 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.631194353 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.632984638 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.633648157 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.634064913 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.634347200 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.634782791 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.634928703 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.635154009 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.635523796 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.635580778 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.635732174 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.635965109 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.636010647 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.636359930 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.636416674 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.636598110 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.636805058 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.636862278 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.641237259 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.642107725 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.643093824 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.644172192 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.644358635 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.644880295 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.649094582 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.649753332 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.650173903 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.650403738 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.651043653 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.651455641 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.655235052 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.655711651 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.656100750 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.656399012 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.656482458 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.656943321 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.657355070 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.658258200 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.658295631 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.658934116 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.659353256 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.659499168 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.661636829 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.662424803 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.665210724 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.665542841 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.665828466 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.666239500 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.666798592 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.667240381 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.667441845 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.667612076 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.667974949 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.668407440 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.668803692 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.672804594 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.673396111 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.673803806 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.674313545 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.676893711 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.681785345 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.684653044 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.685272217 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.685708761 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.686086893 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.686457396 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.686891794 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.687289953 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.690060377 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.694459677 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.694712400 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.695088863 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.695288181 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.695522785 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.700778723 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.702801943 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.703597069 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.704916954 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.705525875 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.706325769 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.706765175 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.708406687 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.708871365 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.709246635 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.709614277 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.710047007 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.710440636 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.711441994 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.713748932 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.714186668 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.714560270 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.714842796 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.714927197 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.715233803 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.715359688 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.715754747 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.716798544 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.717966080 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.718410969 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.718515158 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.718844652 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.719213486 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.719576359 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.720003843 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.720393419 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.723777533 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.724155188 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.724750996 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.725220203 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.728130102 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.728492498 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.733067513 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.733433247 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.735352516 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.735815763 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.736201286 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.736564875 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.737017632 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.737415075 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.739032745 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.739487648 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.739862442 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.740230560 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.740673065 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.740807056 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.741084337 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.745485306 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.747123241 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.747731447 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.748131037 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.749037743 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.752317429 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.752951384 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.753367901 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.754680395 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.756798506 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.756878138 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.757173777 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.757788420 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.758194208 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.758674860 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.761390924 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.762214899 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.766279459 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.770235062 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.771628618 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.773389339 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.774215937 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.774836779 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.775249243 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.777211428 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.777866602 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.778282166 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.778692961 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.779147625 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.779526711 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.779892206 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.780351400 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.780503273 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.780750036 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.780954838 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.781336069 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.781709194 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.782155752 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.782327652 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.782565594 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.782775402 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.783164740 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.783292055 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.783545971 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.783697367 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.783887863 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659982.786127806 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.787566185 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.788831711 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.790691137 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.791247845 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.794611454 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.794965982 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.795418739 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.795610428 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.795797348 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.796059847 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.796165228 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.796428919 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.796601057 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.796792030 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.797010422 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.797238111 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.797630548 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.805132866 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.805650711 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.807482719 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.807926178 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.808300257 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.808665752 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.809105158 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.809501171 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.811388016 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.811829567 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.812213898 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.812588692 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.813055277 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.813470125 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.815429926 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.816058636 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.817474365 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.817623138 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.818139553 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.818258286 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.818560839 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.818687439 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.819053173 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.819343328 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.819414854 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.819860458 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.819896936 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.820156574 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.820260525 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.820591927 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.821790695 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.824927568 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.826575041 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.826805353 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.828394175 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.828758478 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.833847284 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.834298134 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.834477901 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.834903240 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.835049868 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.835481882 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.839881659 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.840338945 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.840710878 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.841107368 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.841460466 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659982.841980219 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.842992783 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.843492508 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.843525887 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.844881296 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.846357822 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.847013235 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.847426891 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.850583553 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.850892544 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.851027966 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.851403713 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.851508379 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.851776600 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.851931334 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.852162600 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659982.855546951 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.857136965 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.857750416 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.858176470 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.860149145 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.866236210 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.868567228 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.869022131 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.869424105 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.869805098 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.870166302 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659982.870356560 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.873461485 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.878755093 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.888139963 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.888424635 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.894341230 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.894804239 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.895176411 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.895539761 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.895973682 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.896364689 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.897213221 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.897316694 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.897756577 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.898127079 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.898492336 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.898740768 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.898926020 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.899318933 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.902366877 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.902628422 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.902688026 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.903081656 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.903450727 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.903812170 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.904142857 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659982.904556513 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.907479286 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.917323351 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.919390202 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.921115637 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.921576262 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.921953201 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.922322750 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.922677517 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.922766447 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.923126221 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.923172951 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.923495531 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.923859358 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.924292803 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.924685240 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.925181150 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.931125164 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.932757139 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.933545113 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.934322357 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.934748650 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.936339855 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.937101603 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.937517881 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.942265749 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.942632437 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.943780184 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.945580006 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.956141233 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.956622839 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.959348202 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.960105896 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.960542202 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.961576700 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.962256908 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.962677956 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.966101646 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.966546059 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.966915607 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.967278957 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.967572689 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.967718840 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659982.968019485 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.968114376 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.968397379 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.968437433 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.968764067 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.969152927 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659982.970694304 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.973099232 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.973196507 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.976163387 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.979846239 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659982.980711460 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.981223583 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.981444120 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.981605291 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.981971502 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.981970787 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.982311487 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659982.982349157 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.982717037 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.983105421 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
| model transformer_wmt_en_de_big_t2t, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 210808832
:::MLPv0.5.0 transformer 1541659982.996899605 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.997044802 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.997351646 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.997510672 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659982.997744799 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.997891188 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659982.998131990 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.998263121 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659982.998489141 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659982.998601198 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659983.002967834 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659983.003457308 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659983.003880739 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659983.004256725 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659983.004317522 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659983.004617214 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659983.004953384 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659983.006852150 (/workspace/translation/fairseq/models/transformer.py:484) model_hp_relu_dropout: 0.1
:::MLPv0.5.0 transformer 1541659983.007497549 (/workspace/translation/fairseq/modules/multihead_attention.py:158) model_hp_attention_dense: {"num_heads": 16, "hidden_size": 1024, "use_bias": false}
:::MLPv0.5.0 transformer 1541659983.007914782 (/workspace/translation/fairseq/modules/multihead_attention.py:162) model_hp_attention_dropout: 0.1
:::MLPv0.5.0 transformer 1541659983.015988588 (/workspace/translation/fairseq/modules/multihead_attention.py:188) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659983.026084423 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659983.029768944 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659983.056745052 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659983.057995081 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659983.074827671 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659983.078908920 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659983.081711054 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659983.082031727 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659983.082155228 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659983.082493544 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659983.082528353 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659983.082866907 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659983.082895041 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659983.083228827 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659983.083236694 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659983.083622694 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659983.084498167 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659983.102613926 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659983.103063822 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659983.103446007 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659983.103812933 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659983.104151487 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659983.108178139 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659983.108618021 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659983.109001160 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659983.109367847 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659983.109756231 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
:::MLPv0.5.0 transformer 1541659983.128916025 (/workspace/translation/fairseq/models/transformer.py:593) model_hp_initializer_gain: 1
:::MLPv0.5.0 transformer 1541659983.152632713 (/workspace/translation/fairseq/models/transformer.py:508) model_hp_ffn_filter_dense: {"filter_size": 4096, "activation": "relu", "use_bias": true}
:::MLPv0.5.0 transformer 1541659983.153085709 (/workspace/translation/fairseq/models/transformer.py:510) model_hp_ffn_output_dense: {"hidden_size": 1024, "use_bias": true}
:::MLPv0.5.0 transformer 1541659983.153455973 (/workspace/translation/fairseq/models/transformer.py:511) model_hp_layer_postprocess_dropout: 0.1
:::MLPv0.5.0 transformer 1541659983.153818607 (/workspace/translation/fairseq/models/transformer.py:512) model_hp_norm: 1024
:::MLPv0.5.0 transformer 1541659983.154199839 (/workspace/translation/fairseq/models/transformer.py:302) model_hp_hidden_layers: 6
| training on 16 GPUs
| max tokens per GPU = 10240 and max sentences per GPU = None
:::MLPv0.5.0 transformer 1541659984.969385624 (/workspace/translation/train.py:88) input_batch_size: 10240
:::MLPv0.5.0 transformer 1541659984.969763994 (/workspace/translation/train.py:89) input_order
:::MLPv0.5.0 transformer 1541659991.907357693 (/workspace/translation/fairseq/optim/lr_scheduler/inverse_square_root_schedule.py:42) opt_learning_rate_warmup_steps: 1200
:::MLPv0.5.0 transformer 1541659991.907816887 (/workspace/translation/train.py:114) train_loop
:::MLPv0.5.0 transformer 1541659991.908069611 (/workspace/translation/train.py:116) train_epoch: 0
generated batches in  2.212585210800171 s
got epoch iterator 2.2130300998687744
| WARNING: overflow detected, setting loss scale to: 64.0
| WARNING: overflow detected, setting loss scale to: 32.0
| WARNING: overflow detected, setting loss scale to: 16.0
| WARNING: overflow detected, setting loss scale to: 8.0
| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 001 | loss 8.056 | nll_loss 6.925 | ppl 121.49 | wps 590107 | ups 3.9 | wpb 146376 | bsz 4704 | num_updates 967 | lr 0.00108788 | gnorm 43994.947 | clip 100% | oom 0 | loss_scale 4.000 | wall 249
epoch time  240.21547174453735
generated batches in  0.0006055831909179688 s
| epoch 001 | valid on 'valid' subset | valid_loss 5.18147 | valid_nll_loss 3.5412 | valid_ppl 11.64 | num_updates 967
:::MLPv0.5.0 transformer 1541660234.587062597 (/workspace/translation/train.py:149) eval_start: -1
| /data test 3003 examples
| Sentences are being padded to multiples of: 1
generated batches in  0.0005638599395751953 s
| Translated 192 sentences (4197 tokens) in 3.5s (55.49 sentences/s, 1212.98 tokens/s)
| Generate test with beam=4: BLEU4 = 18.53, 51.7/24.0/12.9/7.3 (BP=1.000, ratio=1.023, syslen=66018, reflen=64503)
| Eval completed in: 4.57s
:::MLPv0.5.0 transformer 1541660239.166606426 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": -1, "value": 18.527770088453423}
:::MLPv0.5.0 transformer 1541660239.167345762 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541660239.167613506 (/workspace/translation/train.py:154) eval_stop: -1
validation and scoring  4.83096170425415
:::MLPv0.5.0 transformer 1541660239.168045998 (/workspace/translation/train.py:116) train_epoch: 1
generated batches in  2.081923007965088 s
got epoch iterator 2.189505100250244
| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 002 | loss 7.164 | nll_loss 5.892 | ppl 59.37 | wps 592184 | ups 3.9 | wpb 146350 | bsz 4708 | num_updates 1938 | lr 0.0010623 | gnorm 26781.268 | clip 100% | oom 0 | loss_scale 2.000 | wall 497
epoch time  240.2235836982727
generated batches in  0.0006060600280761719 s
| epoch 002 | valid on 'valid' subset | valid_loss 7.85532 | valid_nll_loss 6.65505 | valid_ppl 100.78 | num_updates 1938
:::MLPv0.5.0 transformer 1541660481.826369286 (/workspace/translation/train.py:149) eval_start: 0
generated batches in  0.0005609989166259766 s
| Translated 192 sentences (3415 tokens) in 1.4s (135.07 sentences/s, 2402.36 tokens/s)
| Generate test with beam=4: BLEU4 = 0.37, 18.2/1.1/0.1/0.0 (BP=0.871, ratio=0.879, syslen=56671, reflen=64503)
| Eval completed in: 3.02s
:::MLPv0.5.0 transformer 1541660484.847033501 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": 0, "value": 0.3685269670255619}
:::MLPv0.5.0 transformer 1541660484.847401381 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541660484.847639322 (/workspace/translation/train.py:154) eval_stop: 0
validation and scoring  3.2665014266967773
:::MLPv0.5.0 transformer 1541660484.847934961 (/workspace/translation/train.py:116) train_epoch: 2
generated batches in  2.045311689376831 s
got epoch iterator 2.151047706604004
| epoch 003 | loss 6.263 | nll_loss 4.864 | ppl 29.13 | wps 592920 | ups 4.0 | wpb 146353 | bsz 4707 | num_updates 2910 | lr 0.000866918 | gnorm 19475.808 | clip 100% | oom 0 | loss_scale 2.000 | wall 742
epoch time  240.06105732917786
generated batches in  0.000614166259765625 s
| epoch 003 | valid on 'valid' subset | valid_loss 4.55893 | valid_nll_loss 2.88045 | valid_ppl 7.36 | num_updates 2910
:::MLPv0.5.0 transformer 1541660727.332975864 (/workspace/translation/train.py:149) eval_start: 1
generated batches in  0.0005841255187988281 s
| Translated 192 sentences (4120 tokens) in 1.3s (150.49 sentences/s, 3229.32 tokens/s)
| Generate test with beam=4: BLEU4 = 21.32, 53.4/26.9/15.4/9.3 (BP=1.000, ratio=1.057, syslen=68183, reflen=64503)
| Eval completed in: 4.16s
:::MLPv0.5.0 transformer 1541660731.495446444 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": 1, "value": 21.318917033674964}
:::MLPv0.5.0 transformer 1541660731.495933771 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541660731.496154547 (/workspace/translation/train.py:154) eval_stop: 1
validation and scoring  4.436183214187622
:::MLPv0.5.0 transformer 1541660731.496513128 (/workspace/translation/train.py:116) train_epoch: 3
generated batches in  2.2544362545013428 s
got epoch iterator 2.3296210765838623
| epoch 004 | loss 4.760 | nll_loss 3.173 | ppl 9.02 | wps 592885 | ups 3.9 | wpb 146353 | bsz 4707 | num_updates 3882 | lr 0.000750579 | gnorm 15613.341 | clip 100% | oom 0 | loss_scale 4.000 | wall 989
epoch time  240.16840291023254
generated batches in  0.0006136894226074219 s
| epoch 004 | valid on 'valid' subset | valid_loss 4.30008 | valid_nll_loss 2.60792 | valid_ppl 6.10 | num_updates 3882
:::MLPv0.5.0 transformer 1541660974.231796741 (/workspace/translation/train.py:149) eval_start: 2
generated batches in  0.0005733966827392578 s
| Translated 192 sentences (4260 tokens) in 2.8s (67.58 sentences/s, 1499.46 tokens/s)
| Generate test with beam=4: BLEU4 = 23.05, 55.1/28.7/17.0/10.5 (BP=1.000, ratio=1.049, syslen=67653, reflen=64503)
| Eval completed in: 3.82s
:::MLPv0.5.0 transformer 1541660978.052126646 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": 2, "value": 23.046180360472984}
:::MLPv0.5.0 transformer 1541660978.052485228 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541660978.052724123 (/workspace/translation/train.py:154) eval_stop: 2
validation and scoring  4.058184385299683
:::MLPv0.5.0 transformer 1541660978.053015947 (/workspace/translation/train.py:116) train_epoch: 4
generated batches in  1.9891304969787598 s
got epoch iterator 2.0983517169952393
| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 005 | loss 4.564 | nll_loss 2.960 | ppl 7.78 | wps 592175 | ups 3.9 | wpb 146353 | bsz 4704 | num_updates 4853 | lr 0.000671304 | gnorm 13366.158 | clip 100% | oom 0 | loss_scale 2.000 | wall 1236
epoch time  240.36639523506165
generated batches in  0.0006253719329833984 s
| epoch 005 | valid on 'valid' subset | valid_loss 4.17613 | valid_nll_loss 2.4693 | valid_ppl 5.54 | num_updates 4853
:::MLPv0.5.0 transformer 1541661220.775044918 (/workspace/translation/train.py:149) eval_start: 3
generated batches in  0.0005521774291992188 s
| Translated 192 sentences (4099 tokens) in 1.5s (126.02 sentences/s, 2690.43 tokens/s)
| Generate test with beam=4: BLEU4 = 24.65, 56.7/30.4/18.4/11.6 (BP=1.000, ratio=1.036, syslen=66836, reflen=64503)
| Eval completed in: 3.81s
:::MLPv0.5.0 transformer 1541661224.593245983 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": 3, "value": 24.64818076696263}
:::MLPv0.5.0 transformer 1541661224.593770027 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541661224.594002008 (/workspace/translation/train.py:154) eval_stop: 3
validation and scoring  4.076310873031616
:::MLPv0.5.0 transformer 1541661224.594385147 (/workspace/translation/train.py:116) train_epoch: 5
generated batches in  2.100888967514038 s
got epoch iterator 2.2079997062683105
| epoch 006 | loss 4.445 | nll_loss 2.830 | ppl 7.11 | wps 593752 | ups 3.9 | wpb 146353 | bsz 4707 | num_updates 5825 | lr 0.00061274 | gnorm 11503.316 | clip 100% | oom 0 | loss_scale 2.000 | wall 1482
epoch time  239.7975492477417
generated batches in  0.0006325244903564453 s
| epoch 006 | valid on 'valid' subset | valid_loss 4.08159 | valid_nll_loss 2.37136 | valid_ppl 5.17 | num_updates 5825
:::MLPv0.5.0 transformer 1541661466.841699600 (/workspace/translation/train.py:149) eval_start: 4
generated batches in  0.0005698204040527344 s
| Translated 192 sentences (4066 tokens) in 1.3s (147.25 sentences/s, 3118.24 tokens/s)
| Generate test with beam=4: BLEU4 = 25.37, 57.1/31.1/19.1/12.2 (BP=1.000, ratio=1.043, syslen=67281, reflen=64503)
| Eval completed in: 3.91s
:::MLPv0.5.0 transformer 1541661470.755713224 (/workspace/translation/train.py:152) eval_accuracy: {"epoch": 4, "value": 25.371600594525933}
:::MLPv0.5.0 transformer 1541661470.756079197 (/workspace/translation/train.py:153) eval_target: 25.0
:::MLPv0.5.0 transformer 1541661470.756316662 (/workspace/translation/train.py:154) eval_stop: 4
validation and scoring  4.156368970870972
:::MLPv0.5.0 transformer 1541661470.756598949 (/workspace/translation/train.py:167) run_stop
:::MLPv0.5.0 transformer 1541661470.756814957 (/workspace/translation/train.py:168) run_final
| done training in 1478.8 seconds
+++ date +%s
++ END=1541661479
+++ date '+%Y-%m-%d %r'
++ END_FMT='2018-11-08 07:17:59 AM'
++ echo 'ENDING TIMING RUN AT 2018-11-08 07:17:59 AM'
++ RESULT=1537
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,7940,1537,,2018-11-08 06:52:22 AM'
+ set +x
ENDING TIMING RUN AT 2018-11-08 07:17:59 AM
RESULT,transformer,7940,1537,,2018-11-08 06:52:22 AM
